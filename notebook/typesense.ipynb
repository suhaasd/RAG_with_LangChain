{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e422476b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deprecation warning: AnalyticsRulesV1 is deprecated on v30+. Use client.analytics instead.\n"
     ]
    }
   ],
   "source": [
    "import typesense\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "client = typesense.Client({\n",
    "  'nodes': [{\n",
    "    'host': os.getenv('TYPESENSE_HOST_NAME'),  \n",
    "    'port': '443',      \n",
    "    'protocol': 'https'\n",
    "  }],\n",
    "  'api_key': os.getenv('TYPESENSE_API_KEY'),\n",
    "  'connection_timeout_seconds': 2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "412078da",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_schema={\n",
    "    'name':'books',\n",
    "    'fields':[\n",
    "        {'name':'title', 'type':'string'},\n",
    "        {'name':'authors', 'type':'string[]', 'facet':True},\n",
    "        {'name':'publication_year', 'type':'int32', 'facet':True},\n",
    "        {'name':'ratings_count', 'type':'int32'},\n",
    "        {'name':'average_rating', 'type':'float'},\n",
    "    ],\n",
    "    'default_sorting_field':'ratings_count'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f824b46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': 1771085778, 'curation_sets': [], 'default_sorting_field': 'ratings_count', 'enable_nested_fields': False, 'fields': [{'facet': False, 'index': True, 'infix': False, 'locale': '', 'name': 'title', 'optional': False, 'sort': False, 'stem': False, 'stem_dictionary': '', 'store': True, 'truncate_len': 100, 'type': 'string'}, {'facet': True, 'index': True, 'infix': False, 'locale': '', 'name': 'authors', 'optional': False, 'sort': False, 'stem': False, 'stem_dictionary': '', 'store': True, 'truncate_len': 100, 'type': 'string[]'}, {'facet': True, 'index': True, 'infix': False, 'locale': '', 'name': 'publication_year', 'optional': False, 'sort': True, 'stem': False, 'stem_dictionary': '', 'store': True, 'truncate_len': 100, 'type': 'int32'}, {'facet': False, 'index': True, 'infix': False, 'locale': '', 'name': 'ratings_count', 'optional': False, 'sort': True, 'stem': False, 'stem_dictionary': '', 'store': True, 'truncate_len': 100, 'type': 'int32'}, {'facet': False, 'index': True, 'infix': False, 'locale': '', 'name': 'average_rating', 'optional': False, 'sort': True, 'stem': False, 'stem_dictionary': '', 'store': True, 'truncate_len': 100, 'type': 'float'}], 'name': 'books', 'num_documents': 0, 'symbols_to_index': [], 'synonym_sets': [], 'token_separators': []}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    client.collections['books'].delete()\n",
    "except:\n",
    "    pass\n",
    "print(client.collections.create(books_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d29f69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/json/books.jsonl', 'r', encoding='utf-8') as json1_file:\n",
    "    data=json1_file.read()\n",
    "    client.collections['books'].documents.import_(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb10811f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'facet_counts': [],\n",
       " 'found': 1,\n",
       " 'hits': [{'document': {'authors': ['J.K. Rowling', ' Mary GrandPr√©'],\n",
       "    'average_rating': 4.44,\n",
       "    'id': '2',\n",
       "    'image_url': 'https://images.gr-assets.com/books/1474154022m/3.jpg',\n",
       "    'publication_year': 1997,\n",
       "    'ratings_count': 4602479,\n",
       "    'title': \"Harry Potter and the Philosopher's Stone\"},\n",
       "   'highlight': {'title': {'matched_tokens': ['Harry', 'Potter'],\n",
       "     'snippet': \"<mark>Harry</mark> <mark>Potter</mark> and the Philosopher's Stone\"}},\n",
       "   'highlights': [{'field': 'title',\n",
       "     'matched_tokens': ['Harry', 'Potter'],\n",
       "     'snippet': \"<mark>Harry</mark> <mark>Potter</mark> and the Philosopher's Stone\"}],\n",
       "   'text_match': 1157451471441102969,\n",
       "   'text_match_info': {'best_field_score': '2211897868289',\n",
       "    'best_field_weight': 15,\n",
       "    'fields_matched': 1,\n",
       "    'num_tokens_dropped': 0,\n",
       "    'score': '1157451471441102969',\n",
       "    'tokens_matched': 2,\n",
       "    'typo_prefix_score': 0}}],\n",
       " 'out_of': 9979,\n",
       " 'page': 1,\n",
       " 'request_params': {'collection_name': 'books',\n",
       "  'first_q': 'harry potter',\n",
       "  'per_page': 10,\n",
       "  'q': 'harry potter'},\n",
       " 'search_cutoff': False,\n",
       " 'search_time_ms': 7}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_parameters={\n",
    "    'q':\"harry potter\",\n",
    "    'query_by':'title,authors',\n",
    "    'filter_by':'publication_year:<1998',\n",
    "    'sort_by':'publication_year:desc'\n",
    "}\n",
    "\n",
    "client.collections['books'].documents.search(search_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c5d9c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dmsuhaas\\Downloads\\RAG_with_LangChain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Typesense\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from pathlib import Path\n",
    "# from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08723829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatGroq(\n",
    "#     model=\"llama-3.3-70b-versatile\",\n",
    "#     api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "#     temperature=0,\n",
    "#     max_tokens=None,\n",
    "#     timeout=None,\n",
    "#     max_retries=2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d55785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 pdf files to process\n",
      "\n",
      "Processing: code generation using LLMs (compressed).pdf\n",
      "loaded 70 pages\n",
      "\n",
      "Processing: flashfill.pdf\n",
      "loaded 30 pages\n",
      "\n",
      "Processing: Systematic mapping study of template based code generation.pdf\n",
      "loaded 20 pages\n",
      "\n",
      "Total document pages loaded: 120\n"
     ]
    }
   ],
   "source": [
    "#read all pdfs inside directory\n",
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    all_documents=[]\n",
    "    pdf_dir=Path(pdf_directory)\n",
    "    pdf_files=list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(f\"found {len(pdf_files)} pdf files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader=PyMuPDFLoader(str(pdf_file))\n",
    "            documents=loader.load()\n",
    "\n",
    "            #adding more info to the metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file']=pdf_file.name\n",
    "                doc.metadata['file_type']='pdf'\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"loaded {len(documents)} pages\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal document pages loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents=process_all_pdfs(\"../data/pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bddc300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 120 document pages into 644 chunks\n"
     ]
    }
   ],
   "source": [
    "#split text into chunks\n",
    "#sliding window chunking\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\",\" \",\"\"]\n",
    "    )\n",
    "    split_docs=text_splitter.split_documents(documents)\n",
    "    print(f\"split {len(documents)} document pages into {len(split_docs)} chunks\")\n",
    "\n",
    "    return split_docs\n",
    "\n",
    "chunks=split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba867425",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6dfbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch=Typesense.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    typesense_client_params={\n",
    "        'host': os.getenv('TYPESENSE_HOST_NAME'),  \n",
    "        'port': '443',      \n",
    "        'protocol': 'https',\n",
    "        'typesense_api_key': os.getenv('TYPESENSE_API_KEY'),\n",
    "        'typesense_collection_name':'pdf',\n",
    "        'connection_timeout_seconds': 60\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ae61fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "researchers in establishing a comprehensive, up-to-date, and advanced understanding of LLMs for\n",
      "code generation. This includes discussing various aspects of this rapidly evolving domain, such as\n",
      "data curation, latest advancements, performance evaluation, ethical and environmental implications,\n",
      "and real-world applications. A historical overview of the evolution of LLMs for code generation is\n",
      "J. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.\n"
     ]
    }
   ],
   "source": [
    "query=\"how is LLM used for code generation\"\n",
    "found_docs=docsearch.similarity_search(query)\n",
    "print(found_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88377c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': '', 'creationDate': 'D:20241112015859Z', 'creationdate': '2024-11-12T01:58:59+00:00', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_type': 'pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241112015859Z', 'moddate': '2024-11-12T01:58:59+00:00', 'page': 8, 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'source_file': 'code generation using LLMs (compressed).pdf', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'title': 'A Survey on Large Language Models for Code Generation', 'total_pages': 70, 'trapped': ''}, page_content='researchers in establishing a comprehensive, up-to-date, and advanced understanding of LLMs for\\ncode generation. This includes discussing various aspects of this rapidly evolving domain, such as\\ndata curation, latest advancements, performance evaluation, ethical and environmental implications,\\nand real-world applications. A historical overview of the evolution of LLMs for code generation is\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20241112015859Z', 'creationdate': '2024-11-12T01:58:59+00:00', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_type': 'pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241112015859Z', 'moddate': '2024-11-12T01:58:59+00:00', 'page': 8, 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'source_file': 'code generation using LLMs (compressed).pdf', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'title': 'A Survey on Large Language Models for Code Generation', 'total_pages': 70, 'trapped': ''}, page_content='researchers in establishing a comprehensive, up-to-date, and advanced understanding of LLMs for\\ncode generation. This includes discussing various aspects of this rapidly evolving domain, such as\\ndata curation, latest advancements, performance evaluation, ethical and environmental implications,\\nand real-world applications. A historical overview of the evolution of LLMs for code generation is\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20241112015859Z', 'creationdate': '2024-11-12T01:58:59+00:00', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_type': 'pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241112015859Z', 'moddate': '2024-11-12T01:58:59+00:00', 'page': 8, 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'source_file': 'code generation using LLMs (compressed).pdf', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'title': 'A Survey on Large Language Models for Code Generation', 'total_pages': 70, 'trapped': ''}, page_content='researchers in establishing a comprehensive, up-to-date, and advanced understanding of LLMs for\\ncode generation. This includes discussing various aspects of this rapidly evolving domain, such as\\ndata curation, latest advancements, performance evaluation, ethical and environmental implications,\\nand real-world applications. A historical overview of the evolution of LLMs for code generation is\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20241112015859Z', 'creationdate': '2024-11-12T01:58:59+00:00', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_type': 'pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241112015859Z', 'moddate': '2024-11-12T01:58:59+00:00', 'page': 54, 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'source_file': 'code generation using LLMs (compressed).pdf', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'title': 'A Survey on Large Language Models for Code Generation', 'total_pages': 70, 'trapped': ''}, page_content='offer an empirical comparison using the widely recognized HumanEval, MBPP, and the more\\npractical and challenging BigCodeBench benchmarks to highlight the progressive enhancements\\nin LLM capabilities for code generation. Critical challenges and promising opportunities regarding\\nthe gap between academia and practical development are also identified for future investigation.\\nFurthermore, we have established a dedicated resource website to continuously document and\\ndisseminate the most recent advances in the field. We hope this survey can contribute to a compre-\\nhensive and systematic overview of LLM for code generation and promote its thriving evolution.\\nWe optimistically believe that LLM will ultimately change all aspects of coding and automatically\\nwrite safe, helpful, accurate, trustworthy, and controllable code, like professional programmers,\\nand even solve coding problems that currently cannot be solved by humans.\\nREFERENCES'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20241112015859Z', 'creationdate': '2024-11-12T01:58:59+00:00', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_type': 'pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241112015859Z', 'moddate': '2024-11-12T01:58:59+00:00', 'page': 54, 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'source_file': 'code generation using LLMs (compressed).pdf', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'title': 'A Survey on Large Language Models for Code Generation', 'total_pages': 70, 'trapped': ''}, page_content='offer an empirical comparison using the widely recognized HumanEval, MBPP, and the more\\npractical and challenging BigCodeBench benchmarks to highlight the progressive enhancements\\nin LLM capabilities for code generation. Critical challenges and promising opportunities regarding\\nthe gap between academia and practical development are also identified for future investigation.\\nFurthermore, we have established a dedicated resource website to continuously document and\\ndisseminate the most recent advances in the field. We hope this survey can contribute to a compre-\\nhensive and systematic overview of LLM for code generation and promote its thriving evolution.\\nWe optimistically believe that LLM will ultimately change all aspects of coding and automatically\\nwrite safe, helpful, accurate, trustworthy, and controllable code, like professional programmers,\\nand even solve coding problems that currently cannot be solved by humans.\\nREFERENCES'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20241112015859Z', 'creationdate': '2024-11-12T01:58:59+00:00', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_type': 'pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241112015859Z', 'moddate': '2024-11-12T01:58:59+00:00', 'page': 54, 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'source_file': 'code generation using LLMs (compressed).pdf', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'title': 'A Survey on Large Language Models for Code Generation', 'total_pages': 70, 'trapped': ''}, page_content='offer an empirical comparison using the widely recognized HumanEval, MBPP, and the more\\npractical and challenging BigCodeBench benchmarks to highlight the progressive enhancements\\nin LLM capabilities for code generation. Critical challenges and promising opportunities regarding\\nthe gap between academia and practical development are also identified for future investigation.\\nFurthermore, we have established a dedicated resource website to continuously document and\\ndisseminate the most recent advances in the field. We hope this survey can contribute to a compre-\\nhensive and systematic overview of LLM for code generation and promote its thriving evolution.\\nWe optimistically believe that LLM will ultimately change all aspects of coding and automatically\\nwrite safe, helpful, accurate, trustworthy, and controllable code, like professional programmers,\\nand even solve coding problems that currently cannot be solved by humans.\\nREFERENCES'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20241112015859Z', 'creationdate': '2024-11-12T01:58:59+00:00', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_type': 'pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241112015859Z', 'moddate': '2024-11-12T01:58:59+00:00', 'page': 13, 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'source_file': 'code generation using LLMs (compressed).pdf', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'title': 'A Survey on Large Language Models for Code Generation', 'total_pages': 70, 'trapped': ''}, page_content='capture the breadth of advancements in LLMs for code generation, we conducted a distribution\\nanalysis of the research topics covered in the included papers, as shown at the bottom of Figure 4.\\nWe observe that the development of LLMs for code generation closely aligns with broader trends\\nin general-purpose LLM research. Notably, the most prevalent research topics are Pre-training and\\nFoundation Models (21.5%), Prompting (11.8%), and Evaluation and Benchmarks (24.1%). These\\nareas hold significant promise for enhancing, refining, and evaluating LLM-driven code generation.\\n4\\nTAXONOMY\\nThe recent surge in the development of LLMs has led to a significant number of these models\\nbeing repurposed for code generation task through continual pre-training or fine-tuning. This\\ntrend is particularly observable in the realm of open-source models. For instance, Meta AI initially\\nmade the LLaMA [252] model publicly available, which was followed by the release of Code Llama'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20241112015859Z', 'creationdate': '2024-11-12T01:58:59+00:00', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_type': 'pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241112015859Z', 'moddate': '2024-11-12T01:58:59+00:00', 'page': 13, 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'source_file': 'code generation using LLMs (compressed).pdf', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'title': 'A Survey on Large Language Models for Code Generation', 'total_pages': 70, 'trapped': ''}, page_content='capture the breadth of advancements in LLMs for code generation, we conducted a distribution\\nanalysis of the research topics covered in the included papers, as shown at the bottom of Figure 4.\\nWe observe that the development of LLMs for code generation closely aligns with broader trends\\nin general-purpose LLM research. Notably, the most prevalent research topics are Pre-training and\\nFoundation Models (21.5%), Prompting (11.8%), and Evaluation and Benchmarks (24.1%). These\\nareas hold significant promise for enhancing, refining, and evaluating LLM-driven code generation.\\n4\\nTAXONOMY\\nThe recent surge in the development of LLMs has led to a significant number of these models\\nbeing repurposed for code generation task through continual pre-training or fine-tuning. This\\ntrend is particularly observable in the realm of open-source models. For instance, Meta AI initially\\nmade the LLaMA [252] model publicly available, which was followed by the release of Code Llama'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20241112015859Z', 'creationdate': '2024-11-12T01:58:59+00:00', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_type': 'pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241112015859Z', 'moddate': '2024-11-12T01:58:59+00:00', 'page': 13, 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'source_file': 'code generation using LLMs (compressed).pdf', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'title': 'A Survey on Large Language Models for Code Generation', 'total_pages': 70, 'trapped': ''}, page_content='capture the breadth of advancements in LLMs for code generation, we conducted a distribution\\nanalysis of the research topics covered in the included papers, as shown at the bottom of Figure 4.\\nWe observe that the development of LLMs for code generation closely aligns with broader trends\\nin general-purpose LLM research. Notably, the most prevalent research topics are Pre-training and\\nFoundation Models (21.5%), Prompting (11.8%), and Evaluation and Benchmarks (24.1%). These\\nareas hold significant promise for enhancing, refining, and evaluating LLM-driven code generation.\\n4\\nTAXONOMY\\nThe recent surge in the development of LLMs has led to a significant number of these models\\nbeing repurposed for code generation task through continual pre-training or fine-tuning. This\\ntrend is particularly observable in the realm of open-source models. For instance, Meta AI initially\\nmade the LLaMA [252] model publicly available, which was followed by the release of Code Llama'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20241112015859Z', 'creationdate': '2024-11-12T01:58:59+00:00', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_type': 'pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241112015859Z', 'moddate': '2024-11-12T01:58:59+00:00', 'page': 8, 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'source_file': 'code generation using LLMs (compressed).pdf', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'title': 'A Survey on Large Language Models for Code Generation', 'total_pages': 70, 'trapped': ''}, page_content='generation task. While the adaptation of LLMs for code generation essentially follows the evolution\\nof LLMs, this evolution encompasses a broad spectrum of research directions and advancements.\\nFor software engineering (SE) researchers, it can be challenging and time-consuming to fully grasp\\nthe comprehensive research landscape of LLMs and their adaptation to code generation. RQ1 aims\\nto propose a taxonomy that serves as a comprehensive reference for researchers, enabling them to\\nquickly familiarize themselves with the state-of-the-art in this dynamic field and identify specific\\nresearch problems and directions of interest.\\nRQ2: What are the key insights into LLMs for code generation? RQ2 seeks to assist\\nresearchers in establishing a comprehensive, up-to-date, and advanced understanding of LLMs for\\ncode generation. This includes discussing various aspects of this rapidly evolving domain, such as')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=docsearch.as_retriever()\n",
    "query=\"how is LLM used for code generation\"\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc8584c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "load_dotenv()\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,          \n",
    "    base_url=os.getenv('OLLAMA_URL')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03ba776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, retriever, llm, top_k=6):\n",
    "    #retrieve the context\n",
    "    results = retriever.invoke(query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question\"\n",
    "    \n",
    "    prompt_template = \"\"\"You are an expert Computer Science research assistant. \n",
    "Your task is to answer the question based STRICTLY on the provided context.\n",
    "\n",
    "Guidelines:\n",
    "1. **Be Precise**: Use technical terminology found in the context.\n",
    "2. **Structure**: If the answer has multiple parts, use bullet points.\n",
    "3. **No Hallucination**: If the context does not contain the answer, say \"I cannot find the answer in the provided documents.\" Do not make up information.\n",
    "4. **Synthesis**: If the answer is split across multiple chunks, combine them into a coherent explanation.\n",
    "\n",
    "----------------\n",
    "Context:\n",
    "{context}\n",
    "----------------\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    final_prompt = prompt_template.format(context=context, query=query)\n",
    "    response=llm.invoke(final_prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea88da8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the characteristics of template-based code generation (TBCG) can be identified as follows:\\n\\n* **Synthesis technique**: TBCG is considered a synthesis technique that uses templates to produce code from high-level specifications.\\n* **Use of templates**: Templates are used in TBCG to favor reuse following the principle of \"write once, produce many\".\\n* **Components**: The literature agrees on three components in TBCG: \\n\\t+ Data\\n\\t+ Template\\n\\t+ Output\\nHowever, another component is also present but not mentioned in some sources:\\n\\t+ Meta-information: This refers to the meta-information that the generation logic of the template relies on.\\n* **Code generation from high-level specifications**: TBCG produces code from high-level specifications, called templates.\\n* **Abstraction and automation**: TBCG emphasizes abstraction and automation, which is also a key aspect of Model-Driven Engineering (MDE).\\n* **Reusability**: Templates in TBCG favor reuse, allowing the same generator to be reused to produce many different artifacts according to varying inputs.\\n\\nThese characteristics are based on the provided context, specifically from Section 2.3: Template-based code generation and Figure 1 summarizing the main concepts of TBCG.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer=rag(\"What are the characteristics of template-based code generation? Explain in detail.\", retriever, llm)\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_with_LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
