{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4607f268",
   "metadata": {},
   "source": [
    "## RAG Pipelines - Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d4223df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dmsuhaas\\Downloads\\RAG_with_LangChain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043cb661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 pdf files to process\n",
      "\n",
      "Processing: code generation using LLMs (compressed).pdf\n",
      "loaded 70 pages\n",
      "\n",
      "Processing: flashfill.pdf\n",
      "loaded 30 pages\n",
      "\n",
      "Processing: Systematic mapping study of template based code generation.pdf\n",
      "loaded 20 pages\n",
      "\n",
      "Total document pages loaded: 120\n"
     ]
    }
   ],
   "source": [
    "#read all pdfs inside directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    all_documents=[]\n",
    "    pdf_dir=Path(pdf_directory)\n",
    "    pdf_files=list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(f\"found {len(pdf_files)} pdf files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader=PyMuPDFLoader(str(pdf_file))\n",
    "            documents=loader.load()\n",
    "\n",
    "            #adding more info to the metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file']=pdf_file.name\n",
    "                doc.metadata['file_type']='pdf'\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"loaded {len(documents)} pages\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal document pages loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents=process_all_pdfs(\"../data/pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b86e881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 0, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1\\nA Survey on Large Language Models for Code Generation\\nJUYONG JIANG‚àó, The Hong Kong University of Science and Technology (Guangzhou), China\\nFAN WANG‚àó, The Hong Kong University of Science and Technology (Guangzhou), China\\nJIASI SHEN‚Ä†, The Hong Kong University of Science and Technology, China\\nSUNGJU KIM‚Ä†, NAVER Cloud, South Korea\\nSUNGHUN KIM‚Ä†, The Hong Kong University of Science and Technology (Guangzhou), China\\nLarge Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks,\\nknown as Code LLMs, particularly in code generation that generates source code with LLM from natural\\nlanguage descriptions. This burgeoning field has captured significant interest from both academic researchers\\nand industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite\\nthe active exploration of LLMs for a variety of code tasks, either from the perspective of natural language\\nprocessing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive\\nand up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge\\nthis gap by providing a systematic literature review that serves as a valuable reference for researchers\\ninvestigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize\\nand discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest\\nadvances, performance evaluation, ethical implications, environmental impact, and real-world applications.\\nIn addition, we present a historical overview of the evolution of LLMs for code generation and offer an\\nempirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels\\nof difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities\\nfor code generation. We identify critical challenges and promising opportunities regarding the gap between\\nacademia and practical development. Furthermore, we have established a dedicated resource GitHub page\\n(https://github.com/juyongjiang/CodeLLMSurvey) to continuously document and disseminate the most recent\\nadvances in the field.\\nCCS Concepts: ‚Ä¢ General and reference ‚ÜíSurveys and overviews; ‚Ä¢ Software and its engineering ‚Üí\\nSoftware development techniques; ‚Ä¢ Computing methodologies ‚ÜíArtificial intelligence.\\nAdditional Key Words and Phrases: Large Language Models, Code Large Language Models, Code Generation\\nACM Reference Format:\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2018. A Survey on Large Language Models\\nfor Code Generation. J. ACM 37, 4, Article 1 (August 2018), 70 pages. https://doi.org/XXXXXXX.XXXXXXX\\n‚àóEqually major contributors.\\n‚Ä†Corresponding authors.\\nAuthors‚Äô addresses: Juyong Jiang, jjiang472@connect.hkust-gz.edu.cn, The Hong Kong University of Science and Technology\\n(Guangzhou), Guangzhou, China; Fan Wang, fwang380@connect.hkust-gz.edu.cn, The Hong Kong University of Science\\nand Technology (Guangzhou), Guangzhou, China; Jiasi Shen, sjs@cse.ust.hk, The Hong Kong University of Science and\\nTechnology, Hong Kong, China; Sungju Kim, sungju.kim@navercorp.com, NAVER Cloud, Seoul, South Korea; Sunghun\\nKim, hunkim@cse.ust.hk, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\\n¬© 2018 Association for Computing Machinery.\\n0004-5411/2018/8-ART1 $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.\\narXiv:2406.00515v2  [cs.CL]  10 Nov 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 1, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:2\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n1\\nINTRODUCTION\\nThe advent of Large Language Models (LLMs) such as ChatGPT1 [196] has profoundly transformed\\nthe landscape of automated code-related tasks [48], including code completion [87, 171, 270, 282],\\ncode translation [52, 135, 245], and code repair [75, 126, 195, 204, 291, 310]. A particularly intriguing\\napplication of LLMs is code generation, a task that involves producing source code from natural\\nlanguage descriptions. Despite varying definitions across studies [51, 221, 238, 269], for the main\\nscope of this survey, we focus on the code generation task and adopt a consistent definition of code\\ngeneration as the natural-language-to-code (NL2Code) task [16, 17, 307]. To enhance clarity, the\\ndifferentiation between code generation and other code-related tasks, along with a more nuanced\\ndefinition, is summarized in Table 1. This area has garnered substantial interest from both academia\\nand industry, as evidenced by the development of tools like GitHub Copilot2 [48], CodeGeeX3 [321],\\nand Amazon CodeWhisperer4, which leverage groundbreaking code LLMs to facilitate software\\ndevelopment.\\nInitial investigations into code generation primarily utilized heuristic rules or expert systems,\\nsuch as probabilistic grammar-based frameworks [10, 62, 119, 125, 288] and specialized language\\nmodels [64, 83, 117]. These early techniques were typically rigid and difficult to scale. However,\\nthe introduction of Transformer-based LLMs has shifted the paradigm, establishing them as the\\npreferred method due to their superior proficiency and versatility. One remarkable aspect of LLMs is\\ntheir capability to follow instructions [56, 187, 200, 275, 289], enabling even novice programmers to\\nwrite code by simply articulating their requirements. This emergent ability has democratized coding,\\nmaking it accessible to a broader audience [307]. The performance of LLMs on code generation\\ntasks has seen remarkable improvements, as illustrated by the HumanEval leaderboard5, which\\nshowcases the evolution from PaLM 8B [54] of 3.6% to LDB [325] of 95.1% on Pass@1 metrics.\\nAs can be seen, the HumanEval benchmark [48] has been established as a de facto standard for\\nevaluating the coding proficiency of LLMs [48].\\nTo offer a comprehensive chronological evolution, we present an overview of the development\\nof LLMs for code generation, as illustrated in Figure 1. The landscape of LLMs for code generation\\nis characterized by a spectrum of models, with certain models like ChatGPT [200], GPT4 [5],\\nLLaMA [252, 253], and Claude 3 [14] serving general-purpose applications, while others such\\nas StarCoder [147, 170], Code LLaMA [227], DeepSeek-Coder [88], and Code Gemma [59] are\\ntailored specifically for code-centric tasks. The convergence of code generation with the latest LLM\\nadvancements is pivotal, especially when programming languages can be considered as distinct\\ndialects of multilingual natural language [16, 321]. These models are not only tested against software\\nengineering (SE) requirements but also propel the advancement of LLMs into practical production\\n[317].\\nWhile recent surveys have shed light on code LLMs from the lenses of Natural Language Pro-\\ncessing (NLP), Software Engineering (SE), or a combination of both disciplines [74, 101, 174, 307,\\n317, 324], they have often encompassed a broad range of code-related tasks. There remains a dearth\\nof literature specifically reviewing advanced topics in code generation, such as meticulous data\\ncuration, instruction tuning, alignment with feedback, prompting techniques, the development of\\nautonomous coding agents, retrieval augmented code generation, LLM-as-a-Judge for code genera-\\ntion, among others. A notably pertinent study [16, 307] also concentrates on LLMs for text-to-code\\ngeneration (NL2Code), yet it primarily examines models released from 2020 to 2022. Consequently,\\n1https://chat.openai.com\\n2https://github.com/features/copilot\\n3https://codegeex.cn/en-US\\n4https://aws.amazon.com/codewhisperer\\n5https://paperswithcode.com/sota/code-generation-on-humaneval\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 2, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:3\\nTable 1. The applications of code LLMs in various code-related understanding and generation tasks. The I-O\\ncolumn indicates the type of input and output for each task, where C, NL, and K represent code, natural\\nlanguage, and label, respectively. Note that the detailed definitions of each task aligns with the descriptions\\nin [7, 16, 17, 194, 307]. The main scope of this survey focuses on code generation while it may involve code\\ncompletion in Section 5.7 and 5.8, aiming to illustrate the corresponding advancements.\\nType\\nI-O\\nTask\\nDefinition\\nUnderstanding\\nC-K\\nCode Classification\\nClassify code snippets based on functionality, purpose, or attributes\\nto aid in organization and analysis.\\nBug Detection\\nDetect and diagnose bugs or vulnerabilities in code to ensure\\nfunctionality and security.\\nClone Detection\\nIdentifying duplicate or similar code snippets in software to enhance\\nmaintainability, reduce redundancy, and check plagiarism.\\nException Type Prediction\\nPredict different exception types in code to manage and handle\\nexceptions effectively.\\nC-C\\nCode-to-Code Retrieval\\nRetrieve relevant code snippets based on a given\\ncode query for reuse or analysis.\\nNL-C\\nCode Search\\nFind relevant code snippets based on natural language\\nqueries to facilitate coding and development tasks.\\nGeneration\\nC-C\\nCode Completion\\nPredict and suggest the next portion of code, given contextual\\ninformation from the prefix (and suffix), while typing to enhance\\ndevelopment speed and accuracy.\\nCode Translation\\nTranslate the code from one programming language to another\\nwhile preserving functionality and logic.\\nCode Repair\\nIdentify and fix bugs in code by generating the correct version to\\nimprove functionality and reliability.\\nMutant Generation\\nGenerate modified versions of code to test and evaluate the\\neffectiveness of testing strategies.\\nTest Generation\\nGenerate test cases to validate code functionality, performance,\\nand robustness.\\nC-NL\\nCode Summarization\\nGenerate concise textual descriptions or explanations of code to\\nenhance understanding and documentation.\\nNL-C\\nCode Generation\\nGenerate source code from natural language descriptions to\\nstreamline development and reduce manual coding efforts.\\nthis noticeable temporal gap has resulted in an absence of up-to-date literature reviews that con-\\ntemplate the latest advancements, including models like CodeQwen [249], WizardCoder [173],\\nCodeFusion [241], and PPOCoder [238], as well as the comprehensive exploration of the advanced\\ntopics previously mentioned.\\nRecognizing the need for a dedicated and up-to-date literature review, this survey endeavors to fill\\nthat void. We provide a systematic review that will serve as a foundational reference for researchers\\nquickly exploring the latest progress in LLMs for code generation. A taxonomy is introduced to\\ncategorize and examine recent advancements, encompassing data curation [173, 268, 278], advanced\\ntopics [45, 51, 104, 139, 163, 171, 187, 190, 205, 239, 309], evaluation methods [48, 95, 123, 332], and\\npractical applications [48, 321]. This category aligns with the comprehensive lifecycle of an LLM for\\ncode generation. Furthermore, we pinpoint critical challenges and identify promising opportunities\\nto bridge the research-practicality divide. Therefore, this survey allows NLP and SE researchers\\nto seamlessly equip with a thorough understanding of LLM for code generation, highlighting\\ncutting-edge directions and current hurdles and prospects.\\nThe remainder of the survey is organized following the structure outlined in our taxonomy in\\nFigure 6. In Section 2, we introduce the preliminaries of LLM with Transformer architecture and\\nformulate the task of LLM for code generation. Section 3, we detail the systematic methodologies\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 3, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:4\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nemployed in conducting literature reviews. Then, in Section 4, we propose a taxonomy, categorizing\\nthe complete process of LLMs in code generation. Section 5 delves into the specifics of LLMs\\nfor code generation within this taxonomy framework. In Section 6, we underscore the critical\\nchallenges and promising opportunities for bridging the research-practicality gap and conclude\\nthis work in Section 7.\\n2\\nBACKGROUND\\n2.1\\nLarge Language Models\\nThe effectiveness of large language models (LLMs) is fundamentally attributed to their substantial\\nquantity of model parameters, large-scale and diversified datasets, and the immense computational\\npower utilized during training [97, 127]. Generally, scaling up language models consistently results\\nin enhanced performance and sample efficiency across a broad array of downstream tasks [275, 319].\\nHowever, with the expansion of the model size to a certain extent (e.g., GPT-3 [33] with 175B-\\nparameters and PaLM [54] with 540B), LLMs have exhibited an unpredictable phenomenon known\\nas emergent abilities6, including instruction following [200], in-context learning [70], and step-\\nby-step reasoning [105, 276], which are absent in smaller models but apparent in larger ones\\n[275].\\nAdhering to the same architectures of the Transformer [257] in LLMs, code LLMs are specifically\\npre-trained (or continually pre-trained on general LLMs) using large-scale unlabeled code corpora\\nwith a smaller portion of text (and math) data, whereas general-purpose LLMs are pre-trained\\nprimarily on large-scale text data, incorporating a smaller amount of code and math data to\\nenhance logical reasoning capabilities. Additionally, some code LLMs, such as Qwen2.5-Coder\\n[109], incorporate synthetic data in their training processes, a practice that is attracting increasing\\nattention from both industry and academia. Analogous to LLMs, Code LLMs can also be classified\\ninto three architectural categories: encoder-only models, decoder-only models, and encoder-decoder\\nmodels. For encoder-only models, such as CodeBERT [76], they are typically suitable for code\\ncomprehension tasks including type prediction, code retrieval, and clone detection. For decoder-\\nonly models, such as StarCoder [33], they predominantly excel in generation tasks, such as code\\ngeneration, code translation, and code summarization. Encoder-decoder models, such as CodeT5\\n[271], can accommodate both code understanding and generation tasks but do not necessarily\\noutperform encoder-only or decoder-only models. The overall architectures of the different Code\\nLLMs for code generation are depicted in Figure 2.\\nIn the following subsection, we will delineate the key modules of the Transformer layers in Code\\nLLMs.\\n2.1.1\\nMulti-Head Self-Attention Modules. Each Transformer layer incorporates a multi-head self-\\nattention (MHSA) mechanism to discern the inherent semantic relationships within a sequence\\nof tokens across ‚Ñédistinct latent representation spaces. Formally, the MHSA employed by the\\nTransformer can be formulated as follows:\\nh(ùëô) = MultiHeadSelfAttn(Q, K, V) = Concat {Headùëñ}‚Ñé\\nùëñ=1 WO,\\n(1)\\nHeadùëñ= Attention(H(ùëô‚àí1)WQ\\nùëñ\\n|      {z      }\\nQ\\n, H(ùëô‚àí1)WK\\nùëñ\\n|      {z      }\\nK\\n, H(ùëô‚àí1)WV\\nùëñ\\n|      {z      }\\nV\\n),\\n(2)\\n6It should be noted that an LLM is not necessarily superior to a smaller language model, and emergent abilities may not\\nmanifest in all LLMs [319].\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 4, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:5\\nphi-2\\n2021\\nMay\\nGPT-C\\nCodeGPT\\nFeb.\\nMar.\\nMay\\nGPT-Neo PLBART\\nGPT-J\\nJul.\\nCodex\\nSep.\\nCodeT5\\nNov.\\nCodeParrot\\nPolyCoder\\nAlphaCode\\nJuPyT5\\nCodeGen\\nGPT-NeoX\\nPaLM-Coder InCoder\\nCodeRL\\nPanGu-Coder\\nPyCodeGPT\\nCodeGeeX\\nBLOOM\\nERNIE-Code\\nSantaCoder\\nJan.\\nPPOCoder\\nLLaMA\\nFeb.\\nMay\\nCodeGen2\\nreplit-code\\nStarCoder\\nCodeT5+\\nCodeTF\\nJun.\\nWizardCoder\\nphi-1\\nJul.\\nChainCoder\\nCodeGeeX2\\nPanGu-Coder2\\nAug.\\nOctoPack\\nSep.\\nMFTCoder\\nOct.\\nCodeShell\\nphi-1.5\\nCodeFusion\\nNov.\\nDeepSeek-Coder\\nDec.\\nMagicoder\\nAlphaCode 2\\nWaveCoder\\nJan.\\nFeb.\\nAST-T5\\nToolGen\\nStableCode\\nAlphaCodium\\nStepCoder OpenCodeInterpreter StarCoder2\\nMar.\\nDevin\\nOpenDevin\\nCodeS\\nApr.\\nProCoder\\nCodeQwen1.5\\nCodeGemma\\nCode Llama\\nApr.\\nSelf-Debugging\\nJan.\\nFeb.\\nJun.\\nJul.\\nSep.\\nNov.\\nDec.\\nApr.\\nMar.\\n2020\\n2022\\n2023\\n2024\\nChatGPT\\nMar.\\nGPT4\\nLlama 2\\nLlama 3\\nClaude 3\\nCodeT\\nSelfEvolve\\nLEVER\\nRLTF\\nOct.\\nPyMT5\\nStarCoder2-Instruct\\nOpen Source Closed Source\\n9\\n3\\n6\\n6\\n5\\n3\\n1\\n4\\nMar.\\nCodestral\\nFig. 1. A chronological overview of large language models (LLMs) for code generation in recent years. The\\ntimeline was established mainly according to the release date. The models with publicly available model\\ncheckpoints are highlighted in green color.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 5, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:6\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nAttention(Q, K, V) = softmax\\n \\nQKùëá\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñé\\n!\\nV,\\n(3)\\nwhere H(ùëô‚àí1) ‚ààRùëõ√óùëëùëöùëúùëëùëíùëôdenotes the input to the ùëô-th Transformer layer, while h(ùëô) ‚ààRùëõ√óùëëùëöùëúùëëùëíùëô\\nrepresents the output of MHSA sub-layer. The quantity of distinct attention heads is represented\\nby ‚Ñé, and ùëëùëöùëúùëëùëíùëôrefers to the model dimension. The set of projections\\nn\\nWQ\\nùëñ, WK\\nùëñ, WV\\nùëñ, WO\\nùëñ\\no\\n‚àà\\nRùëëùëöùëúùëëùëíùëô√óùëëùëöùëúùëëùëíùëô/‚Ñéencompasses the affine transformation parameters for each attention head Headùëñ,\\ntransforming the Query Q, Key K, Value V, and the output of the attention sub-layer. The softmax\\nfunction is applied in a row-wise manner. The dot-products of queries and keys are divided by\\na scaling factor\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñéto counteract the potential risk of excessive large inner products and\\ncorrespondingly diminished gradients in the softmax function, thus encouraging a more balanced\\nattention landscape.\\nIn addition to multi-head self-attention, there are two other types of attention based on the\\nsource of queries and key-value pairs:\\n‚Ä¢ Masked Multi-Head Self-Attention. Within the decoder layers of the Transformer, the\\nself-attention mechanism is constrained by introducing an attention mask, ensuring that\\nqueries at each position can only attend to all key-value pairs up to and inclusive of that\\nposition. To facilitate parallel training, this is typically executed by assigning a value of 0\\nto the lower triangular part and setting the remaining elements to ‚àí‚àû. Consequently, each\\nitem attends only to its predecessors and itself. Formally, this modification in Equation 3 can\\nbe depicted as follows:\\nAttention(Q, K, V) = softmax\\n \\nQKùëá\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñé\\n+ Mùëöùëéùë†ùëò\\n!\\nV,\\n(4)\\nMùëöùëéùë†ùëò=\\n\\x10\\nùëöùëñùëó\\n\\x11\\nùëõ√óùëõ=\\n\\x10\\nI(ùëñ‚â•ùëó)\\n\\x11\\nùëõ√óùëõ=\\n(\\n0\\nfor ùëñ‚â•ùëó\\n‚àí‚àû\\notherwise ,\\n(5)\\nThis form of self-attention is commonly denoted as autoregressive or causal attention [157].\\n‚Ä¢ Cross-Layer Multi-Head Self-Attention. The queries are derived from the outputs of the\\npreceding (decoder) layer, while the keys and values are projected from the outputs of the\\nencoder.\\n2.1.2\\nPosition-wise Feed-Forward Networks. Within each Transformer layer, a Position-wise Feed-\\nForward Network (PFFN) is leveraged following the MHSA sub-layer to refine the sequence\\nembeddings at each position ùëñin a separate and identical manner, thereby encoding more intricate\\nfeature representations. The PFFN is composed of a pair of linear transformations, interspersed\\nwith a ReLU activation function. Formally,\\nPFFN(‚Ñé(ùëô)) =\\n\\x10\\nConcat\\nn\\nFFN(‚Ñé(ùëô)\\nùëñ)ùëáoùëõ\\nùëñ=1\\n\\x11ùëá\\n,\\n(6)\\nFFN(‚Ñé(ùëô)\\nùëñ) = ReLU(‚Ñé(ùëô)\\nùëñW(1) + ùëè(1))W(2) + ùëè(2),\\n(7)\\nwhere ‚Ñé(ùëô) ‚ààRùëõ√óùëëùëöùëúùëëùëíùëôis the outputs of MHSA sub-layer in ùëô-th Transformer layer, and ‚Ñé(ùëô)\\nùëñ\\n‚àà\\nRùëëùëöùëúùëëùëíùëôdenotes the latent representation at each sequence position. The projection matrices\\n\\x08\\nW(1), (W(2))ùëá\\t\\n‚ààRùëëùëöùëúùëëùëíùëô√ó4ùëëùëöùëúùëëùëíùëôand bias vectors {b(1), b(2)} ‚ààRùëëùëöùëúùëëùëíùëôare parameters learned\\nduring training. These parameters remain consistent across all positions while are individually\\ninitialized from layer to layer. In this context, ùëárepresents the transpose operation on a matrix.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 6, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:7\\nMasked\\nMulti-Head\\nSelf-Attention\\nMulti-Head\\nSelf-Attention\\n+\\n+\\n+\\n+\\n+\\nLayer Norm\\nPosition-wise\\nFeed Forward\\nLinear & Softmax\\nToken & Position\\nEmbedding\\nInputs\\nOutputs (Shifted Right)\\nMulti-Head\\nSelf-Attention\\nLayer Norm\\nLayer Norm\\nLayer Norm\\nLayer Norm\\nPosition-wise\\nFeed Forward\\nToken & Position\\nEmbedding\\nOutput Probabilities\\nùëÅ√ó\\nùëÅ√ó\\n(a) Encoder-Decoder Models\\nMasked\\nMulti-Head\\nSelf-Attention\\nPosition-wise\\nFeed Forward\\n+\\n+\\nLinear & Softmax\\nToken & Position\\nEmbedding\\nInputs\\nLayer Norm\\nLayer Norm\\nOutput Probabilities\\nùëÅ√ó\\n(b) Decoder-only Models\\nFig. 2. The overview of large language models (LLMs) with encoder-decoder and decoder-only Transformer\\narchitecture for code generation, adapted from [257].\\n2.1.3\\nResidual Connection and Normalization. To alleviate the issue of vanishing or exploding\\ngradients resulting from network deepening, the Transformer model incorporates a residual con-\\nnection [94] around each of the aforementioned modules, followed by Layer Normalization [18].\\nFor the placement of Layer Normalization operation, there are two widely used approaches: 1)\\nPost-Norm: Layer normalization is implemented subsequent to the element-wise residual addition,\\nin accordance with the vanilla Transformer [257]. 2) Pre-Norm: Layer normalization is applied to\\nthe input of each sub-layer, as seen in models like GPT-2 [214]. Formally, it can be formulated as:\\nPost-Norm : H(l) = LayerNorm(PFFN(h(l)) + h(l)),\\nh(l) = LayerNorm(MHSA(H(l‚àí1)) + H(l‚àí1))\\n(8)\\nPre-Norm : H(l) = PFFN(LayerNorm(h(l))) + h(l),\\nh(l) = MHSA(LayerNorm(H(l‚àí1))) + H(l‚àí1)\\n(9)\\n2.1.4\\nPositional Encoding. Given that self-attention alone cannot discern the positional information\\nof each input token, the vanilla Transformer introduces an absolute positional encoding method to\\nsupplement this positional information, known as sinusoidal position embeddings [257]. Specifically,\\nfor a token at position ùëùùëúùë†, the position embedding is defined as:\\npùëùùëúùë†,2ùëñ= sin(\\nùëùùëúùë†\\n100002ùëñ/ùëëùëöùëúùëëùëíùëô),\\n(10)\\npùëùùëúùë†,2ùëñ+1 = cos(\\nùëùùëúùë†\\n100002ùëñ/ùëëùëöùëúùëëùëíùëô),\\n(11)\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 7, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:8\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nwhere 2ùëñ, 2ùëñ+1 represent the dimensions of the position embedding, while ùëëùëöùëúùëëùëíùëôdenotes the model\\ndimension. Subsequently, each position embedding is added to the corresponding token embedding,\\nand the sum is fed into the Transformer. Since the inception of this method, a variety of innovative\\npositional encoding approaches have emerged, such as learnable embeddings [66], relative position\\nembeddings [232], RoPE [243], and ALiBi [211]. For more detailed descriptions of each method,\\nplease consult [157, 318].\\n2.1.5\\nArchitecture. There are two types of Transformer architecture for code generation task,\\nincluding encoder-decoder and decoder-only. For the encoder-decoder architecture, it consists of\\nboth an encoder and a decoder, in which the encoder processes the input data and generates a\\nset of representations, which are then used by the decoder to produce the output. However, for\\ndecoder-only architecture, it consists only of the decoder part of the transformer, where it uses\\na single stack of layers to both process input data and generate output. Therefore, the encoder-\\ndecoder architecture is suited for tasks requiring mapping between different input and output\\ndomains, while the decoder-only architecture is designed for tasks focused on sequence generation\\nand continuation. The overview of LLMs with these two architectures are illustrated in Figure 2.\\n2.2\\nCode Generation\\nLarge language models (LLMs) for code generation refer to the use of LLM to generate source\\ncode from natural language descriptions, a process also known as a natural-language-to-code\\ntask. Typically, these natural language descriptions encompass programming problem statements\\n(or docstrings) and may optionally include some programming context (e.g., function signatures,\\nassertions, etc.). Formally, these natural language (NL) descriptions can be represented as x. Given x,\\nthe use of an LLM with model parameters ùúÉto generate a code solution y can be denoted as ùëÉùúÉ(y | x).\\nThe advent of in-context learning abilities in LLM [275] has led to the appending of exemplars to\\nthe natural language description x as demonstrations to enhance code generation performance or\\nconstrain the generation format [145, 206]. A fixed set of ùëÄexemplars is denoted as {(xi, yi)}ùëÄ\\nùëñ=1.\\nConsequently, following [190], a more general formulation of LLMs for code generation with\\nfew-shot (or zero-shot) exemplars can be revised as:\\nùëÉùúÉ(y | x) ‚áíùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(12)\\nwhere prompt(x, {(xi, yi)}ùëò\\nùëñ=1)) is a string representation of the overall input, and {(xi, yi)}ùëò\\nùëñ=1\\ndenotes a set of ùëòexemplars randomly selected from {(xi, yi)}ùëÄ\\nùëñ=1. In particular, when ùëò= 0, this\\ndenotes zero-shot code generation, equivalent to vanilla ones without in-context learning. In the\\ndecoding process, a variety of decoding strategies can be performed for code generation, including\\ndeterministic-based strategies (e.g., greedy search and beam search) and sampling-based strategies\\n(e.g., temperature sampling, top-k sampling, and top-p (nucleus) sampling). For more detailed\\ndescriptions of each decoding strategy, please consult [99]. For example, the greedy search and\\nsampling-based decoding strategies can be formulated as follows:\\nGreedy Search : y‚àó= argmax\\ny\\nùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(13)\\nSampling : y ‚àºùëÉùúÉ(y | prompt(x, {(xi, yùëñ)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(14)\\nTo verify the functionality correctness of the generated code solution, y is subsequently executed\\nvia a compiler or interpreter, represented as Exe(¬∑), on a suit of unit tests T. The feedback from\\nthis execution can be denoted as Feedback(Exe(y, T)). If the generated code solution fails to pass\\nall test cases, the error feedback can be iteratively utilized to refine the code by leveraging the\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 8, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:9\\nResearch\\nQuestions\\nLarge Language\\nModel (LLM)\\nCode Generation\\nTop-tier LLMs\\nand SE Venues\\nTotal 235 Papers\\nSnowballing\\nSearch\\nQuality\\nAssessment\\nInclusion and\\nExclusion Criteria\\nAutomatic\\nFiltering\\nSearch Strings\\nAutomated Search\\nManual Search\\n235\\n247\\n351\\n294\\n73\\n36\\n261\\nFig. 3. Overview of the paper search and collection process.\\nprevious attempt (yùëùùëüùëí) and the associated feedback. Formally,\\ny ‚àºùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1, yùëùùëüùëí, Feedback(Exe(y, T)))),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(15)\\nFurther details and relevant studies on using feedback to improve code generation are comprehen-\\nsively discussed in Section 5.5 and 5.6.\\n3\\nMETHODOLOGY\\nIn this section, we detail the systematic methodologies employed in conducting literature reviews.\\nWe follow the systematic literature review methodology outlined by [131], which has been widely\\nadopted in numerous software engineering literature reviews [101, 146, 169, 219, 262]. The overall\\nprocess is illustrated in Figure 3, and the detailed steps in our methodology are documented below.\\n3.1\\nResearch Questions\\nTo deliver a comprehensive and up-to-date literature review on the latest advancements in large\\nlanguage models (LLMs) for code generation, this systematic literature review addresses the\\nfollowing research questions (RQs):\\nRQ1: How can we categorize and evaluate the latest advances in LLMs for code genera-\\ntion? The recent proliferation of LLMs has resulted in many of these models being adapted for code\\ngeneration task. While the adaptation of LLMs for code generation essentially follows the evolution\\nof LLMs, this evolution encompasses a broad spectrum of research directions and advancements.\\nFor software engineering (SE) researchers, it can be challenging and time-consuming to fully grasp\\nthe comprehensive research landscape of LLMs and their adaptation to code generation. RQ1 aims\\nto propose a taxonomy that serves as a comprehensive reference for researchers, enabling them to\\nquickly familiarize themselves with the state-of-the-art in this dynamic field and identify specific\\nresearch problems and directions of interest.\\nRQ2: What are the key insights into LLMs for code generation? RQ2 seeks to assist\\nresearchers in establishing a comprehensive, up-to-date, and advanced understanding of LLMs for\\ncode generation. This includes discussing various aspects of this rapidly evolving domain, such as\\ndata curation, latest advancements, performance evaluation, ethical and environmental implications,\\nand real-world applications. A historical overview of the evolution of LLMs for code generation is\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 9, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:10\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 2. Publication venues for conference proceedings and journals articles for manual search.\\nDomain\\nVenue\\nAcronym\\nLLMs\\nInternational Conference on Learning Representations\\nICLR\\nConference on Neural Information Processing Systems\\nNeurIPS\\nInternational Conference on Machine Learning\\nICML\\nAnnual Meeting of the Association for Computational Linguistics\\nACL\\nConference on Empirical Methods in Natural Language Processing\\nEMNLP\\nInternational Joint Conference on Artificial Intelligence\\nNAACL\\nAAAI Conference on Artificial Intelligence\\nAAAI\\nSE\\nInternational Conference on Software Engineering\\nICSE\\nJoint European Software Engineering Conference and Symposium on the Foundations of Software Engineering\\nESEC/FSE\\nInternational Conference on Automated Software Engineering\\nASE\\nTransactions on Software Engineering and Methodology\\nTOSEM\\nTransactions on Software Engineering\\nTSE\\nInternational Symposium on Software Testing and Analysis\\nISSTA\\nTable 3. Keywords related to LLMs and code generation task for automated search.\\nKeywords Related to LLMs\\nKeywords Related to Code Generation Task\\nCode Large Language Model‚àó, Code LLMs, Code Language Model,\\nCode LMs, Large Language Model‚àó, LLM, Language Model‚àó, LM,\\nPre-trained Language Model‚àó, PLM, Pre-trained model,\\nNatural Language Processing, NLP, GPT-3, ChatGPT, GPT-4, LLaMA,\\nCodeLlama, PaLM‚àó, CodeT5, Codex, CodeGen, InstructGPT\\nCode Generation, Program Synthesis, Code Intelligence,\\n‚àóCoder‚àó, natural-language-to-code, NL2Code, Programming\\nprovided, along with an empirical comparison using the widely recognized HumanEval and MBPP\\nbenchmarks, as well as the more practical and challenging BigCodeBench benchmark, to highlight\\nthe progressive enhancements in LLM capabilities for code generation. RQ2 offers an in-depth\\nanalysis of critical insights related to LLMs for code generation.\\nRQ3: What are the critical challenges and promising research opportunities in LLMs for\\ncode generation? Despite the revolutionary impact of LLMs on the paradigm of code generation\\nand their remarkable performance, numerous challenges remain unaddressed. These challenges\\nprimarily stem from the gap between academic research and practical development. For instance,\\nwhile the HumanEval benchmark is established as a de facto standard for evaluating the coding\\nproficiency of LLMs in academia, it has been shown that this evaluation does not adequately reflect\\npractical development scenarios [68, 72, 123, 162]. RQ3 aims to identify critical challenges and\\nhighlight promising opportunities to bridge the gap between research and practical application.\\n3.2\\nSearch Process\\n3.2.1\\nSearch Strings. To address the aforementioned three research questions (RQs), we initiate a\\nmanual review of conference proceedings and journal articles from top-tier venues in the fields of\\nLLMs and SE, as detailed in Table 2. This process allowed us to identify relevant studies and derive\\nsearch strings, which are subsequently utilized for an automated search across various scientific\\ndatabases. The complete set of search keywords is presented in Table 3.\\n3.2.2\\nSearch Databases. Following the development of search strings, we executed an automated\\nsearch using four popular scientific databases: the ACM Digital Library, IEEE Xplore Digital Library,\\narXiv, and DBLP. Our search focus on identifying papers whose titles contain keywords pertinent\\nto LLMs and code generation. This approach enhances the likelihood of retrieving relevant papers\\nsince both sets of keywords must be present in the title. Although this title-based search strategy\\neffectively retrieves a large volume of papers, it is important to note that in some instances [238],\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 10, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:11\\nthe scope of code generation can be broader, encompassing areas such as code completion, code\\ntranslation, and program synthesis. As outlined in Section 1, this survey adopts a prevalent definition\\nof code generation as the natural-language-to-code (NL2Code) task [16, 17, 307].\\nConsequently, we conduct further automatic filtering based on the content of the papers. Papers\\nfocusing on ‚Äúcode completion‚Äù and ‚Äúcode translation‚Äù are excluded unless they pertain to the\\nspecific topics discussed in Section 5.7 and Section 5.8, where code completion is a primary focus.\\nAfter completing the automated search, the results from each database are merged and deduplicated\\nusing scripts. This process yields 294 papers from arXiv, 73 papers from the ACM Digital Library,\\n36 papers from IEEE Xplore, and 261 papers from DBLP.\\n3.3\\nInclusion and Exclusion Criteria\\nThe search process conducted across various databases and venues is intentionally broad to gather\\na comprehensive pool of candidate papers. This approach maximizes the collection of potentially\\nrelevant studies. However, such inclusivity may lead to the inclusion of papers that do not align\\nwith the scope of this survey, as well as duplicate entries from multiple sources. To address this,\\nwe have established a clear set of inclusion and exclusion criteria, based on the guidelines from\\n[101, 260]. These criteria are applied to each paper to ensure alignment with our research scope\\nand questions, and to eliminate irrelevant studies.\\nInclusion Criteria. A paper will be included if it meets any of the following criteria:\\n‚Ä¢ It is available in full text.\\n‚Ä¢ It presents a dataset or benchmark specifically designed for code generation with LLMs.\\n‚Ä¢ It explores specific LLM techniques, such as pre-training or instruction tuning, for code\\ngeneration.\\n‚Ä¢ It provides an empirical study or evaluation related to the use of LLMs for code generation.\\n‚Ä¢ It discusses the ethical considerations and environmental impact of deploying LLMs for code\\ngeneration.\\n‚Ä¢ It proposes tools or applications powered by LLMs for code generation.\\nExclusion Criteria. Conversely, papers will be excluded if they meet any of the following\\nconditions:\\n‚Ä¢ They are not written in English.\\n‚Ä¢ They are found in books, theses, monographs, keynotes, panels, or venues (excluding arXiv)\\nthat do not undergo a full peer-review process.\\n‚Ä¢ They are duplicate papers or different versions of similar studies by the same authors.\\n‚Ä¢ They focus on text generation rather than source code generation, such as generating code\\ncomments, questions, test cases, or summarization.\\n‚Ä¢ They do not address the task of code generation, for instance, focusing on code translation\\ninstead.\\n‚Ä¢ They leverage software engineering methods to enhance code generation without emphasiz-\\ning LLMs.\\n‚Ä¢ They do not utilize LLMs, opting for other models like Long Short-Term Memory (LSTM)\\nnetworks.\\n‚Ä¢ They use encoder-only language models, such as BERT, which are not directly applicable to\\ncode generation task.\\n‚Ä¢ LLMs are mentioned only in future work or discussions without being central to the proposed\\napproach.\\nPapers identified through both manual and automated searches undergo a detailed manual review\\nto ensure they meet the inclusion criteria and do not fall under the exclusion criteria. Specifically,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 11, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:12\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nthe first two authors independently review each paper to determine its eligibility. In cases of\\ndisagreement, the third author makes the final inclusion decision.\\n3.4\\nQuality Assessment\\nTo ensure the inclusion of high-quality studies, we have developed a comprehensive set of ten\\nQuality Assessment Criteria (QAC) following [101]. These QAC are designed to evaluate the\\nrelevance, clarity, validity, and significance of the papers considered for our review.\\nIn accordance with [101], the first three QAC assess the study‚Äôs alignment with our objectives.\\nThese criteria are rated as ‚Äúirrelevant/unmet‚Äù, ‚Äúpartially relevant/met‚Äù, or ‚Äúrelevant/fully met‚Äù,\\ncorresponding to scores of -1, 0, and 1, respectively. If a study receive a score of -1 across these\\ninitial three criteria, it is deemed ineligible for further consideration and subsequently excluded\\nfrom our review process.\\nThe subsequent seven QAC focus on a more detailed content evaluation, employing a scoring\\nrange of -1 to 2, representing ‚Äúpoor‚Äù, ‚Äúfair‚Äù, ‚Äúgood‚Äù, and ‚Äúexcellent‚Äù. We compute a cumulative score\\nbased on the responses to QAC4 through QAC10 for each paper. For published works, the maximum\\nachievable score is 14 (2 points per question). We retain those with a score of 11.2 (80% of the total\\nscore) or higher. For unpublished papers available on arXiv, QAC4 defaults to a score of 0, making\\nthe maximum possible score for the remaining criteria 12. Accordingly, we retain papers scoring\\n9.6 (80% of the adjusted total score) or above .\\n‚Ä¢ QAC1: Is the research not classified as a secondary study, such as a systematic literature\\nreview or survey? (-1, 0, 1)\\n‚Ä¢ QAC2: Does the study incorporate the use of LLMs? (-1, 0, 1)\\n‚Ä¢ QAC3: Is the study relevant to the code generation task? (-1, 0, 1)\\n‚Ä¢ QAC4: Is the research published in a prestigious venue? (-1, 0, 1, 2)\\n‚Ä¢ QAC5: Does the study present a clear research motivation? (-1, 0, 1, 2)\\n‚Ä¢ QAC6: Are the key contributions and limitations of the study discussed? (-1, 0, 1, 2)\\n‚Ä¢ QAC7: Does the study contribute to the academic or industrial community? (-1, 0, 1, 2)\\n‚Ä¢ QAC8: Are the LLM techniques employed in the study clearly described? (-1, 0, 1, 2)\\n‚Ä¢ QAC9: Are the experimental setups, including experimental environments and dataset infor-\\nmation, thoroughly detailed? (-1, 0, 1, 2)\\n‚Ä¢ QAC10: Does the study clearly confirm its experimental findings? (-1, 0, 1, 2)\\n3.5\\nSnowballing Search\\nFollowing the quality assessment, we establish an initial set of papers for our study. To minimize\\nthe risk of excluding pertinent literature, we implement a snowballing search strategy. Snowballing\\nsearch involves utilizing a paper‚Äôs reference list or its citations to discover additional relevant\\nstudies, known as backward and forward snowballing, respectively. In this survey, we exclusively\\nemployed backward snowballing following [260]. Despite this effort, no additional studies are\\nidentified through this method. This could be attributed to the task-specific nature of the code\\ngeneration (natural-language-to-code), where reference studies are typically published earlier.\\nConsequently, our methodology, which encompassed an extensive manual and automated search,\\nlikely covered the relevant literature comprehensively, explaining the lack of additional studies\\nthrough snowballing search.\\n3.6\\nData Collection and Analysis\\nThe data collection process for our study, illustrated in Figure 3, began with a manual search\\nthrough conference proceedings and journal articles from leading venues in LLMs and SE. This\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 12, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:13\\n2018\\n2019\\n2020\\n2021\\n2022\\n2023\\n2024\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n# Number of Papers\\n1\\n1\\n1\\n6\\n11\\n75\\n140\\nVenue\\nTrend\\nAAAI\\nACL\\nCAV\\nCCS\\nCHI\\nCVPR\\nEMNLP\\nFSE\\nICLR\\nICML\\nICSE\\nISSTA\\nKDD\\nNAACL\\nNeurIPS\\nOthers\\nTACL\\nTOSEM\\nTSE\\nUSENIX\\narXiv\\nPre-Training & \\nFoundation Model (21.5%)\\nFine-tuning (7.5%)\\nReinforcement Learning (4.8%)\\nPrompting (11.8%)\\nEvaluation &\\n Benchmark (24.1%)\\nData\\n Synthesis (1.8%)\\nRepository\\n Level (5.7%)\\nRetrieval Augmented (3.1%)\\nOthers (8.3%)\\nCode LLMs\\n Alignment (7.0%)\\nAutonomous Coding \\nAgents (4.4%)\\nTotal Papers:\\n235\\nResearch Topics\\nPre-Training & Foundation Model\\nFine-tuning\\nReinforcement Learning\\nPrompting\\nEvaluation & Benchmark\\nData Synthesis\\nRepository Level\\nRetrieval Augmented\\nOthers\\nCode LLMs Alignment\\nAutonomous Coding Agents\\nFig. 4. Data qualitative analysis. Top: Annual distribution of selected papers across various publication\\nvenues. Bottom: Distribution analysis of research topics covered in the included papers.\\ninitial step yielded 42 papers, from which we extracted relevant search strings. Following this,\\nwe performed an automated search across four academic databases using keyword-based queries,\\nresulting in the retrieval of 664 papers. After performing automatic filtering (351 papers), applying\\ninclusion and exclusion criteria (247 papers), conducting quality assessments (235 papers), and\\nutilizing snowballing search (235 papers), we finalize a collection of 235 papers focusing on LLMs\\nfor code generation.\\nTo provide insights from the selected papers, we begin by presenting an overview of their distribu-\\ntion across publication venues each year, as illustrated at the top of Figure 4. Our analysis indicates\\nthat 14% of the papers are published in LLM-specific venues and 7% in SE venues. Remarkably, 49%\\nof the papers remain unpublished in peer-reviewed venues and are available on arXiv. This trend is\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 13, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:14\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nunderstandable given the emerging nature of this field, with many works being recent and pending\\nformal submission. Despite the absence of peer review on arXiv, our quality assessment process\\nensures that only high-quality papers are included, thereby maintaining the integrity of this survey.\\nFurthermore, the annual trend in the number of collected papers indicates nearly exponential\\ngrowth in the field. From a single paper in the period 2018 to 2020, the numbers increased to 6 in\\n2021, 11 in 2022, 75 in 2023, and 140 in 2024. This trend reflects growing interest and attention\\nin this research area, with expectations for continued expansion in the future. Additionally, to\\ncapture the breadth of advancements in LLMs for code generation, we conducted a distribution\\nanalysis of the research topics covered in the included papers, as shown at the bottom of Figure 4.\\nWe observe that the development of LLMs for code generation closely aligns with broader trends\\nin general-purpose LLM research. Notably, the most prevalent research topics are Pre-training and\\nFoundation Models (21.5%), Prompting (11.8%), and Evaluation and Benchmarks (24.1%). These\\nareas hold significant promise for enhancing, refining, and evaluating LLM-driven code generation.\\n4\\nTAXONOMY\\nThe recent surge in the development of LLMs has led to a significant number of these models\\nbeing repurposed for code generation task through continual pre-training or fine-tuning. This\\ntrend is particularly observable in the realm of open-source models. For instance, Meta AI initially\\nmade the LLaMA [252] model publicly available, which was followed by the release of Code Llama\\n[227], designed specifically for code generation. Similarly, DeepSeek LLM [26] developed and\\nreleased by DeepSeek has been extended to create DeepSeek Coder [88], a variant tailored for code\\ngeneration. The Qwen team has developed and released Code Qwen [249], building on their original\\nQwen [20] model. Microsoft, on the other hand, has unveiled WizardLM [289] and is exploring its\\ncoding-oriented counterpart, WizardCoder [173]. Google has joined the fray by releasing Gemma\\n[248], subsequently followed by Code Gemma [59]. Beyond simply adapting general-purpose LLMs\\nfor code-related tasks, there has been a proliferation of models specifically engineered for code\\ngeneration. Notable examples include StarCoder [147], OctoCoder [187], and CodeGen [193]. These\\nmodels underscore the trend of LLMs being developed with a focus on code generation.\\nRecognizing the importance of these developments, we conduct a thorough analysis of selected\\npapers on LLMs for code generation, sourced from widely used scientific databases as mentioned\\nin Section 3. Based on this analysis, we propose a taxonomy that categorizes and evaluates the\\nlatest advancements in LLMs for code generation. This taxonomy, depicted in Figure 6, serves as a\\ncomprehensive reference for researchers seeking to quickly familiarize themselves with the state-\\nof-the-art in this dynamic field. It is important to highlight that the category of recent advances\\nemphasizes the core techniques used in the current state-of-the-art code LLMs.\\nIn the subsequent sections, we will provide an in-depth analysis of each category related to code\\ngeneration. This will encompass a definition of the problem, the challenges to be addressed, and a\\ncomparison of the most prominent models and their performance evaluation.\\n5\\nLARGE LANGAUGE MODELS FOR CODE GENERATION\\nLLMs with Transformer architecture have revolutionized a multitude of fields, and their application\\nin code generation has been particularly impactful. These models follow a comprehensive process\\nthat starts with the curation and synthesis of code data, followed by a structured training approach\\nthat includes pre-training and fine-tuning (instruction tuning), reinforcement learning with various\\nfeedback, and the use of sophisticated prompt engineering techniques. Recent advancements have\\nseen the integration of repository-level and retrieval-augmented code generation, as well as the\\ndevelopment of autonomous coding agents. Furthermore, the evaluation of coding abilities of LLMs\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 14, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:15\\nPretrained\\n(Base) LLM\\nInstruct\\nCode LLM\\nSupervised\\nFine-tuning\\n(SFT)\\nHuman Preference\\nAlignment with RL\\n(e.g., RLHF)\\n(Optional)\\nPre-training Database\\nInstruction Database\\nPreference Database\\nStage ‚ë†\\nStage ‚ë¢\\nStage ‚ë£\\nPre-training Database\\nStage ‚ë°\\nContinual\\nPre-training\\n(Optional)\\nTask\\nDescription\\nGenerated\\nSource Code\\nInference\\nEvaluation\\nBenchmark\\nFig. 5. A diagram illustrating the general training, inference, and evaluation workflow for Code LLMs and\\ntheir associated databases. The training workflow is mainly divided into four distinct stages: Stage 1‚óãand 2‚óã\\nare the pre-training phase, whereas Stages 3‚óãand 4‚óãrepresent the post-training phases. It is important to\\nnote that Stage 2‚óãand 4‚óãare optional. For instance, StarCoder [147] incorporates only Stage 1‚óã. WizardCoder\\n[173], fine-tuned upon StarCoder, includes only Stage 3‚óã, while Code Llama [227], continually pre-trained on\\nLlama 2, encompasses Stages 2‚óãand 3‚óã. DeepSeek-Coder-V2 [331], continually pre-trained on DeepSeek-V2,\\ncovers Stages 2‚óã, 3‚óã, and 4‚óã. Note that pre-trained model can be directly used for inference through prompt\\nengineering.\\nhas become a critical component of this research area. Figure 5 illustrates the general training,\\ninference, and evaluation workflow for Code LLMs and their associated databases.\\nIn the forthcoming sections, we will explore these dimensions of LLMs in the context of code\\ngeneration in detail. Section 5.1 will address the data curation and processing strategies employed\\nthroughout the various stages of LLM development. Section 5.2 will discuss data synthesis methods\\ndesigned to mitigate the scarcity of high-quality data. Section 5.3 will outline the prevalent model\\narchitectures used in LLMs for code generation. Moving to Section 5.4, we will examine the\\ntechniques for full parameter fine-tuning and parameter-efficient fine-tuning, which are essential\\nfor tailoring LLMs to code generation task. Section 5.5 will shed light on enhancing code quality\\nthrough reinforcement learning, utilizing the power of feedback. Section 5.6 will delve into the\\nstrategic use of prompts to maximize the coding capabilities of LLMs. The innovative approaches\\nof repository-level and retrieval-augmented code generation will be elaborated in Sections 5.7 and\\n5.8, respectively. Additionally, Section 5.9 will discuss the exciting field of autonomous coding\\nagents. Section 5.10 discusses various evaluation strategies and offer an empirical comparison using\\nthe widely recognized HumanEval, MBPP, and the more practical and challenging BigCodeBench\\nbenchmarks to highlight the progressive enhancements in LLM capabilities for code generation.\\nFurthermore, the ethical implications and the environmental impact of using LLMs for code\\ngeneration are discussed in Section 5.11, aiming to establish a trustworthiness, responsibility, safety,\\nefficiency, and green of LLM for code generation. Lastly, Section 5.12 will provide insights into some\\nof the practical applications that leverage LLMs for code generation, demonstrating the real-world\\nimpact of these sophisticated models. Through this comprehensive exploration, we aim to highlight\\nthe significance and potential of LLMs within the domain of automated code generation.\\n5.1\\nData Curation & Processing\\nThe exceptional performance of LLMs can be attributed to their training on large-scale and diverse\\ndatasets [307]. Meanwhile, the extensive parameters of these models necessitate substantial data to\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 15, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:16\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nLLMs for Code Generation\\nData\\nCuration\\n(Sec. 5.1)\\nPre-training\\nCodeSearchNet[110], Google BigQuery[96], The Pile[78], CodeParrot[254], GitHub Code[254]\\nROOTS[137], The Stack[132], The Stack v2[170]\\nInstruction\\nTuning\\nCommitPackFT [187], Code Alpaca[43], OA-Leet[63], OSS-Instruct[278], Evol-instruction[225]\\nSelf-OSS-Instruct-SC2-Exec-Filter[304]\\nBenchmarks\\nGeneral\\nHumanEval[48], HumanEval+[162], HumanEvalPack[187], MBPP[17]\\nMBPP+[162], CoNaLa[297], Spider[300], CONCODE[113], ODEX[273]\\nCoderEval[299], ReCode[263], StudentEval[19]\\nCompetitions\\nAPPS[95], CodeContests[151]\\nData Science\\nDSP[41], DS-1000[136], ExeDS[107]\\nMultilingual\\nMBXP[16], Multilingual HumanEval[16], HumanEval-X[321], MultiPL-E[39]\\nxCodeEval[128]\\nReasoning\\nMathQA-X[16], MathQA-Python[17], GSM8K[58], GSM-HARD[79]\\nRepository\\nRepoEval[309], Stack-Repo[239], Repobench[167], EvoCodeBench[144]\\nSWE-bench[123], CrossCodeEval[68], SketchEval[308]\\nRecent\\nAdvances\\nData\\nSynthesis\\n(Sec. 5.2)\\nSelf-Instruct [268], Evol-Instruct [289], Phi-1[84], Code Alpaca[43], WizardCoder[173]\\nMagicoder[278], StarCoder2-instruct [304]\\nPre-training\\n(Sec. 5.3)\\nModel\\nArchitectures\\nEncoder-Decoder\\nPyMT5[57], PLBART[7], CodeT5[271], JuPyT5[41]\\nAlphaCode[151], CodeRL[139], ERNIE-Code[40]\\nPPOCoder[238], CodeT5+[269], CodeFusion[241]\\nAST-T5[81]\\nDecoder-Only\\nGPT-C[244], GPT-Neo[30], GPT-J[258], Codex[48]\\nCodeGPT[172], CodeParrot[254], PolyCoder[290]\\nCodeGen[193], GPT-NeoX[29], PaLM-Coder[54]\\nInCoder[77], PanGu-Coder[55], PyCodeGPT[306]\\nCodeGeeX[321], BLOOM[140], ChatGPT[196]\\nSantaCoder[9], LLaMA[252], GPT-4[5]\\nCodeGen2[192], replit-code[223], StarCoder[147]\\nWizardCoder[173], phi-1[84], ChainCoder[323]\\nCodeGeeX2[321], PanGu-Coder2[234], Llama 2[253]\\nOctoPack[187], Code Llama[227], MFTCoder[160]\\nphi-1.5[150], CodeShell[285], Magicoder[278]\\nAlphaCode 2[11], StableCode[210], WaveCoder[301]\\nphi-2[182], DeepSeek-Coder[88], StepCoder[71]\\nOpenCodeInterpreter[322], StarCoder 2[170]\\nClaude 3[14], ProCoder[27], CodeGemma[59]\\nCodeQwen[249], Llama3[180]\\nStarCoder2-Instruct[304], Codestral[181]\\nPre-training\\nTasks\\nCLM[88, 147, 173, 278], DAE[7, 269, 271], Auxiliary[40, 269, 271]\\nFine-tuning\\nInstruction\\nTuning\\n(Sec. 5.4)\\nFull Parameter\\nFine-tuning\\nCode Alpaca[43], CodeT5+[271], WizardCoder[173]\\nStarCoder[147], Pangu-Coder2[234], OctoPack[187]\\nCodeGeeX2[321], Magicoder[278], CodeGemma[59]\\nStarCoder2-instruct[304]\\nParameter\\nEfficient\\nFine-tuning\\nCodeUp[121], ASTRAIOS[334]\\nReinforcement\\nLearning\\nwith Feedback\\n(Sec. 5.5)\\nCodeRL[139], CompCoder[266], PPOCoder[238], RLTF[163]\\nPanGu-Coder2[234], StepCoder[71]\\nPrompting\\nEngineering\\n(Sec. 5.6)\\nReflexion[236], LATS[327], Self-Debugging[51], SelfEvolve[122]\\nTheo X. et al.[195], CodeT[45], LEVER[190], AlphaCodium[224]\\nRepository\\nLevel & Long\\nContext\\n(Sec. 5.7)\\nRepoCoder[309], CoCoMIC[69], RepoHyper[209], RLPG[240]\\nRepoformer[282], RepoFusion[239], ToolGen[259], CodePlan[22]\\nCodeS[308]\\nRetrieval\\nAugmented\\n(Sec. 5.8)\\nHGNN[166], REDCODER[205], ReACC[171], DocPrompting[330]\\nRepoCoder[309], Su et al.[242]\\nAutonomous\\nCoding Agents\\n(Sec. 5.9)\\nAgentCoder [104], MetaGPT[100], CodeAct [265], AutoCodeRover [316], Devin[61]\\nOpenDevin[199], SWE-agent[124], L2MAC[98], OpenDevin CodeAct 1.0[287]\\nEvaluation\\n(Sec. 5.10)\\nMetrics\\nExact Match, BLEU[203], ROUGE[156], METEOR[23], CodeBLEU[221], pass@k[48]\\nn@k[151], test case average[95], execution accuracy[218], pass@t[195], perplexity[116]\\nHuman\\nEvaluation\\nCodePlan[22], RepoFusion[239], CodeBLEU[221]\\nLLM-as-a-Judge\\nAlpacaEval[148], MT-bench[320], ICE-Score[332]\\nCode LLMs\\nAlignment\\n(Sec. 5.10.3)\\nGreen[235, 277], Responsibility[168, 292], Efficiency[293], Safety[8, 9, 77, 91, 231, 294, 302], Trustworthiness[120, 202]\\nApplication\\n(Sec. 5.12)\\nGitHub Copilot[48], CodeGeeX[321], CodeWhisperer[12], Codeium[60], CodeArts Snap[234], TabNine[246], Replit[222]\\nFig. 6. Taxonomy of LLMs for code generation.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 16, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:17\\nunlock their full potential, in alignment with established scaling law [97, 127]. For a general-purpose\\nLLM, amassing a large-scale corpus of natural language from a variety of sources is imperative.\\nSuch sources include webpages, conversation data, books and news, scientific data, and code\\n[20, 33, 54, 252, 253, 298], while these data are often crawled from the web and must undergo\\nmeticulous and aggressive pre-processing [217, 317]. Fortunately, multiple platforms and websites\\noffer large-scale, open-source, and permissively licensed code corpora, such as GitHub7 and Stack\\nOverflow8. Notably, the number of stars or forks of GitHub repositories has emerged as a valuable\\nmetric for filtering high-quality code datasets. In a similar vein, the quantity of votes on Stack\\nOverflow can serve to discern the most relevant and superior answers.\\nNonetheless, raw datasets are frequently laden with redundant, noisy data and personal infor-\\nmation, eliciting concerns regarding privacy leakage, which may include the names and email\\naddresses of repository contributors [8, 37, 137]. Consequently, it is essential to undertake rigorous\\ndata-cleaning procedures. Typically, this process encompasses exact match deduplication, code\\ndata filtering based on average line length and a defined threshold for the fraction of alphanumeric\\ncharacters, the removal of auto-generated files through keyword searches, and the expunction of\\npersonal user data [132, 254]. Specifically, the standard data preprocessing workflow is depicted in\\nFigure 7.\\nThe development of a proficient LLM for code generation necessitates the utilization of various\\ntypes of code data at different developmental stages. Therefore, we categorize code data into three\\ndistinct classes: pre-training datasets, instruction-tuning datasets, and benchmarks for performance\\nevaluation. The subsequent subsections will provide a detailed illustration of code data within each\\nclassification.\\n5.1.1\\nPre-training. The remarkable success of bidirectional pre-trained language models (PLMs)\\nsuch as BERT [66] and unidirectional PLMs like GPT [213] has firmly established the practice of\\npre-training on large-scale unlabeled datasets to endow models with a broad spectrum of general\\nknowledge. Extending this principle to the realm of code generation enables LLMs to assimilate\\nfundamental coding principles, including the understanding of code structure dependencies, the\\nsemantics of code identifiers, and the intrinsic logic of code sequences [48, 85, 269, 271]. In light of\\nthis advancement, there has been a proliferation of large-scale unlabeled code datasets proposed\\nto serve as the foundational training ground for LLMs to develop coding proficiency. A brief\\nintroduction of these datasets is as follows, with the statistics available in Table 4.\\n‚Ä¢ CodeSearchNet [110]: CodeSearchNet corpus is a comprehensive dataset, consisting of 2\\nmillion (comment, code) pairs from open-source repositories on GitHub. It includes code\\nand documentation in several programming languages including Go, Java, PHP, Python,\\nJavaScript, and Ruby. The dataset was primarily compiled to promote research into the\\nproblem of code retrieval using natural language.\\n‚Ä¢ Google BigQuery [96]: the Google BigQuery Public Datasets program offers a full snapshot\\nof the content of more than 2.8 million open source GitHub repositories in BigQuery.\\n‚Ä¢ The Pile [78]: the Pile is an 825 GiB diverse and open source language modeling dataset\\naggregating 22 smaller, high-quality datasets including GitHub, Books3, and Wikipedia (en).\\nIt aims to encompass text from as many modalities as possible, thereby facilitating the\\ndevelopment of models with broader generalization capabilities. For code generation, the\\nGitHub composite is specifically utilized.\\n‚Ä¢ CodeParrot [254]: the CodeParrot dataset contains Python files used to train the code genera-\\ntion model in Chapter 10: Training Transformers from Scratch in the ‚ÄúNLP with Transformers\\n7https://github.com\\n8https://stackoverflow.com\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 17, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:18\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nQuality Filtering\\n‚Ä¢\\nProgramming Language \\n‚Ä¢\\nStatistic Number\\n‚Ä¢\\nMetric Threshold\\n‚Ä¢\\nKeyword Search\\nDe-duplication\\n‚Ä¢\\nExact Match\\n‚Ä¢\\nSimilarity Metrics\\n‚Ä¢\\nFunction Level\\nPrivacy Reduction\\n‚Ä¢\\nDetect Personally Identifiable \\nInformation (PII)\\n‚Ä¢\\nDelete PII\\nRaw Corpus\\nTokenization\\n‚Ä¢\\nOpen Source Tokenizer\\n‚Ä¢\\nSentencePiece\\n‚Ä¢\\nByte-level BPE\\nPre-training Database\\n# Sum numbers from 1 to 10 \\nand print the result\\ntotal = sum(range(1, 11))\\nprint(total)\\ntotal = 0\\nfor i in range(1, 11):\\ntotal += i\\ntotal = sum(range(1, 11))\\n# Copyright 2024 @ John\\n# Email: csjohn@gmail.com\\n# Institution: HKUST\\ninputs = tokenizer.encode([\"def \\nprint_hello_world():\",...], \\nreturn_tensors=\"pt\").to(\"cuda\")\\n[\\n[755, 1194, 97824, \\n32892, 4658],\\n[755, 4062, 18942, \\n11179, 997, 262, 4304, \\n10442, 264, 1160, 315, \\n5219, 304, 36488, 2015, \\n1701, 279, 17697, 6354, \\n371, 12384,...],...\\n]\\n. . .\\nFig. 7. A diagram depicting the standard data preprocessing workflow utilized in the pre-training phase of\\nLLMs for code generation.\\nbook‚Äù [254]. Created with the GitHub dataset available via Google‚Äôs BigQuery, the CodeParrot\\ndataset includes approximately 22 million Python files and is 180 GB (50 GB compressed) big.\\n‚Ä¢ GitHub Code [254]: the GitHub Code dataset comprises 115M code files derived from GitHub,\\nspanning 32 programming languages and 60 extensions totaling 1TB of data. The dataset\\nwas created from the public GitHub dataset on Google BiqQuery.\\n‚Ä¢ ROOTS [137]: the BigScience ROOTS Corpus is a 1.6TB dataset spanning 59 languages that\\nwas used to train the 176B BigScience Large Open-science Open-access Multilingual (BLOOM)\\nlanguage model. For the code generation task, the code subset of the ROOTS Corpus will be\\nspecifically utilized.\\n‚Ä¢ The Stack [132]: the Stack contains over 6TB of permissively licensed source code files that\\ncover 358 programming languages. The dataset was compiled as part of the BigCode Project,\\nan open scientific collaboration working on the responsible development of Large Language\\nModels for Code (Code LLMs).\\n‚Ä¢ The Stack v2 [170]: The Stack v2, a dataset created as part of the BigCode Project, contains\\nover 3B files across more than 600 programming and markup languages. The dataset is\\nderived from the Software Heritage archive9, the largest public archive of software source\\ncode and accompanying development history.\\n5.1.2\\nInstruction Tuning. Instruction tuning refers to the process of supervised fine-tuning LLMs\\nusing a collection of datasets structured as various instructions, with the purpose of following a\\nwide range of task instructions [56, 200, 229, 274]. This method has demonstrated a considerable\\nimprovement in model performance and an enhanced ability to generalize to unseen tasks that the\\n9https://archive.softwareheritage.org\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 18, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:19\\nTable 4. The statistics of some commonly-used pre-training datasets for LLMs aimed at code generation. The\\ncolumn labeled ‚Äò#PL‚Äô indicates the number of programming languages included in each dataset. It should\\nbe noted that in the CodeSearchNet [110] dataset, each file represents a function, and for the Pile [78] and\\nROOTS [137] datasets, only the code components are considered.\\nDataset\\nSize (GB)\\nFiles (M)\\n#PL\\nDate\\nLink\\nCodeSearchNet [110]\\n20\\n6.5\\n6\\n2022-01\\nhttps://huggingface.co/datasets/code_search_net\\nGoogle BigQuery[96]\\n-\\n-\\n-\\n2016-06\\ngithub-on-bigquery-analyze-all-the-open-source-code\\nThe Pile [78]\\n95\\n19\\n-\\n2022-01\\nhttps://huggingface.co/datasets/EleutherAI/pile\\nCodeParrot [254]\\n180\\n22\\n1\\n2021-08\\nhttps://huggingface.co/datasets/transformersbook/codeparrot\\nGitHub Code[254]\\n1,024\\n115\\n32\\n2022-02\\nhttps://huggingface.co/datasets/codeparrot/github-code\\nROOTS [137]\\n163\\n15\\n13\\n2023-03\\nhttps://huggingface.co/bigscience-data\\nThe Stack [132]\\n3,136\\n317\\n30\\n2022-10\\nhttps://huggingface.co/datasets/bigcode/the-stack\\nThe Stack v2 [170]\\n32K\\n3K\\n619\\n2024-04\\nhttps://huggingface.co/datasets/bigcode/the-stack-v2\\nTable 5. The statistics of several representative datasets used in instruction-tuning LLMs for code generation.\\nThe column labeled ‚Äò#PL‚Äô indicates the number of programming languages encompassed by each dataset.\\nDataset\\nSize\\n#PL\\nDate\\nLink\\nCodeAlpaca-20K [43]\\n20k\\n-\\n2023-03\\nhttps://huggingface.co/datasets/sahil2801/CodeAlpaca-20k\\nCommitPackFT [187]\\n2GB\\n277\\n2023-08\\nhttps://huggingface.co/datasets/bigcode/commitpackft\\nEvol-Instruct-Code-80k [225]\\n80k\\n-\\n2023-07\\nhttps://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1\\nevol-codealpaca-v1 [251]\\n110K\\n-\\n2023-07\\nhttps://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1\\nMagicoder-OSS-Instruct-75k [278]\\n75k\\nPython, Shell,\\nTypeScript, C++,\\nRust, PHP, Java,\\nSwift, C#\\n2023-12\\nhttps://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K\\nSelf-OSS-Instruct-SC2-Exec-Filter-50k [304]\\n50k\\nPython\\n2024-04\\nhttps://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k\\nmodel has not previously encountered, as evidenced by recent studies [56, 200]. Leveraging the\\nbenefits of instruction tuning, instruction tuning has been expanded into coding domains, especially\\nfor code generation, which involves the automatic generation of the intended code from a natural\\nlanguage description. The promise of instruction tuning in this area has led numerous researchers\\nto develop large-scale instruction-tuning datasets tailored for code generation. Below, we provide an\\noverview of several notable datasets tailored for instruction tuning, with their respective statistics\\ndetailed in Table 5.\\n‚Ä¢ CodeAlpaca-20k [43]: CodeAlpaca-20k is a collection of 20K instruction-following data\\ngenerated using the data synthesis techniques termed Self-Instruct outlined in [268], with\\nmodifications for code generation, editing, and optimization tasks instead of general tasks.\\n‚Ä¢ CommitPackFT [187]: CommitPackFT is a 2GB refined version of CommitPack. It is filtered\\nto only include high-quality commit messages that resemble natural language instructions.\\n‚Ä¢ Evol-Instruct-Code-80k [225]: Evol-Instruct-Code-80k is an open-source implementation of\\nEvol-Instruct-Code described in the WizardCoder paper [173], which enhances the fine-tuning\\neffect of pre-trained code large models by adding complex code instructions.\\n‚Ä¢ Magicoder-OSS-Instruct-75k [278]: is a 75k synthetic data generated through OSS-Instruct\\nwith gpt-3.5-turbo-1106 and used to train both Magicoder and Magicoder-S series models.\\n‚Ä¢ Self-OSS-Instruct-SC2-Exec-Filter-50k [304]: Self-OSS-Instruct-SC2-Exec-Filter-50k is gen-\\nerated by StarCoder2-15B using the OSS-Instruct [278] data synthesis approach. It was\\nsubsequently used to fine-tune StarCoder-15B without any human annotations or distilled\\ndata from huge and proprietary LLMs.\\n5.1.3\\nBenchmarks. To rigorously assess the efficacy of LLMs for code generation, the research\\ncommunity has introduced a variety of high-quality benchmarks in recent years. Building on\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 19, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:20\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nthe foundational work by [48], numerous variations of the HumanEval dataset and additional\\nbenchmarks have emerged, aiming to evaluate a broader spectrum of code generation capabilities\\nin LLMs. We roughly divide these benchmarks into six distinct categories based on their application\\ncontexts, including general-purpose, competitive programming, data science, multilingual, logical\\nreasoning, and repository-level. It is important to highlight that logical reasoning encompasses math-\\nrelated benchmarks, as it aims to create ‚Äúcode-based solutions‚Äù for solving complex mathematical\\nproblems [50, 79, 326]. This strategy can therefore mitigate the limitations of LLMs in performing\\nintricate mathematical computations. The statistics for these benchmarks are presented in Table 6.\\nGeneral\\n‚Ä¢ HumanEval [48]: HumanEval comprises 164 manually scripted Python programming prob-\\nlems, each featuring a function signature, docstring, body, and multiple unit tests.\\n‚Ä¢ HumanEval+ [162]: HumanEval+ extends the original HumanEval [48] benchmark by in-\\ncreasing the scale of the test cases by 80 times. As the test cases increase, HumanEval+ can\\ncatch significant amounts of previously undetected incorrect code synthesized by LLMs.\\n‚Ä¢ HumanEvalPack [187]: expands HumanEval [48] by extending it to encompass three coding\\ntasks across six programming languages, namely code synthesis, code repair, and code\\nexplanation.\\n‚Ä¢ MBPP [17]: MBPP is a collection of approximately 974 Python programming problems, crowd-\\nsourced and designed for entry-level programmers. Each problem comes with an English\\ntask description, a code solution, and three automated test cases.\\n‚Ä¢ MBPP+ [162]: MBPP+ enhances MBPP [17] by eliminating ill-formed problems and rectifying\\nproblems with incorrect implementations. The test scale of MBPP+ is also expanded by 35\\ntimes for test augmentation.\\n‚Ä¢ CoNaLa [297]: CoNaLa contains almost 597K data samples for evaluating Python code\\ngeneration. The curated part of CoNaLa is crawled from Stack Overflow, automatically\\nfiltered, and then curated by annotators. The mined part of CoNaLais automatically mined,\\nwith almost 600k examples.\\n‚Ä¢ Spider [300]: Spider is large-scale complex text-to-SQL dataset covering 138 different domains.\\nIt has over 10K questions and 5.6K complex SQL queries on 200 databases. This dataset aims\\nto test a model‚Äôs ability to generalize to SQL queries, database schemas, and new domains.\\n‚Ä¢ CONCODE [113]: CONCODE is a dataset with over 100K samples consisting of Java classes\\nfrom public GitHub repositories. It provides near zero-shot conditions that can test the\\nmodel‚Äôs ability to generalize to unseen natural language tokens with unseen environments.\\n‚Ä¢ ODEX [273]: ODEX is an open-domain dataset focused on the execution-based generation\\nof Python code from natural language. It features 945 pairs of natural language queries and\\ntheir corresponding Python code, all extracted from StackOverflow forums.\\n‚Ä¢ CoderEval [299]: CoderEval is a pragmatic code generation benchmark that includes 230\\nPython and 230 Java code generation problems. It can be used to evaluate the model perfor-\\nmance in generating pragmatic code beyond just generating standalone functions.\\n‚Ä¢ ReCode [263]: Recode serves as a comprehensive robustness evaluation benchmark. ReCode\\napplies perturbations to docstrings, function and variable names, code syntax, and code\\nformat, thereby providing multifaceted assessments of a model‚Äôs robustness performance.\\n‚Ä¢ StudentEval [19]: StudentEval is a dataset of 1,749 prompts for 48 problems, authored by 80\\nstudents who have only completed a one-semester Python programming class. Unlike many\\nother benchmarks, it has multiple prompts per problem and multiple attempts by the same\\nparticipant, each problem is also accompanied by a set of instructor-written test cases.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 20, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:21\\n‚Ä¢ BigCodeBench [333]: BigCodeBench has 1,140 complex Python programming tasks, covering\\n723 function calls from 139 popular libraries across 7 domains. This benchmark is specifically\\ndesigned to assess LLMs‚Äô ability to call multiple functions from cross-domain libraries and\\nfollow complex instructions to solve programming tasks, helping to bridge the evaluation\\ngap between isolated coding exercises and the real-world programming scenario.\\n‚Ä¢ ClassEval [72]: ClassEval is a manually-crafted benchmark consisting of 100 classes and\\n412 methods for evaluating LLMs in the class-level code generation scenario. Particularly,\\nthe task samples of ClassEval present higher complexities, involving long code generation\\nand sophisticated docstring information, thereby benefiting the evaluation of the LLMs‚Äô\\ncapabilities in generating complicated code.\\n‚Ä¢ NaturalCodeBench [314]: NaturalCodeBench is a comprehensive code benchmark featuring\\n402 high-quality problems in Python and Java. These problems are selected from natural\\nuser queries from online coding services and span 6 distinct domains, shaping an evaluation\\nenvironment aligned with real-world applications.\\nCompetitions\\n‚Ä¢ APPS [95]: The APPS benchmark is composed of 10K Python problems, spanning three levels\\nof difficulty: introductory, interview, and competition. Each entry in the dataset includes a\\nprogramming problem described in English, corresponding ground truth Python solutions,\\ntest cases defined by their inputs and outputs or function names if provided.\\n‚Ä¢ CodeContests [151]: CodeContests is a competitive programming dataset consisting of sam-\\nples from various sources including Aizu, AtCoder, CodeChef, Codeforces, and HackerEarth.\\nThe dataset encompasses programming problems accompanied by test cases in the form of\\npaired inputs and outputs, along with both correct and incorrect human solutions in multiple\\nprogramming languages.\\n‚Ä¢ LiveCodeBench [188]: LiveCodeBench is a comprehensive and contamination-free benchmark\\nfor evaluating a wide array of code-related capabilities of LLMs, including code generation,\\nself-repair, code execution, and test output prediction. It continuously gathers new coding\\nproblems from contests across three reputable competition platforms: LeetCode, AtCoder,\\nand CodeForces. The latest release of the dataset includes 713 problems that were released\\nbetween May 2023 and September 2024.\\nData Science\\n‚Ä¢ DSP [41]: DSP allows for model evaluation based on real data science pedagogical notebooks.\\nIt includes well-structured problems, along with unit tests to verify the correctness of solutions\\nand a Docker environment for reproducible execution.\\n‚Ä¢ DS-1000 [136]: DS-1000 has 1K science questions from seven Python libraries, namely NumPy,\\nPandas, TensorFlow, PyTorch, SciPy, Scikit-learn, and Matplotlib. The DS-1000 benchmark\\nfeatures: (1) realistic problems with diverse contexts (2) implementation of multi-criteria\\nevaluation metrics, and (3) defense against memorization.\\n‚Ä¢ ExeDS [107]: ExeDS is a data science code generation dataset specifically designed for execu-\\ntion evaluation. It contains 534 problems with execution outputs from Jupyter Notebooks, as\\nwell as 123K examples for training and validation.\\nMultilingual\\n‚Ä¢ MBXP [16]: MBXP is a multilingual adaptation of the original MBPP [17] dataset. It is created\\nusing a framework that translates prompts and test cases from the original Python datasets\\ninto the corresponding data in the targeted programming language.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 21, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:22\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n‚Ä¢ Multilingual HumanEval [16]: Multilingual HumanEval is a dataset derived from HumanEval\\n[48]. It is designed to assess the performance of models in a multilingual context. It helps\\nuncover the generalization ability of the given model on languages that are out-of-domain.\\n‚Ä¢ HumanEval-X [321]: HumanEval-X is developed for evaluating the multilingual ability of\\ncode generation models with 820 hand-writing data samples in C++, Java, JavaScript, and Go.\\n‚Ä¢ MultiPL-E [39]: MultiPL-E is a dataset for evaluating LLMs for code generation across 18 pro-\\ngramming languages. It adopts the HumanEval [48] and the MBPP [17] Python benchmarks\\nand uses little compilers to translate them to other languages.\\n‚Ä¢ xCodeEval [128]: xCodeEval is an executable multilingual multitask benchmark consisting of\\n25M examples covering 17 programming languages. Its tasks include code understanding,\\ngeneration, translation, and retrieval.\\nReasoning\\n‚Ä¢ MathQA-X [16] MathQA-X is the multilingual version of MathQA [13]. It is generated by\\nutilizing a conversion framework that converts samples from Python datasets into the target\\nlanguage.\\n‚Ä¢ MathQA-Python [17] MathQA-Python is a Python version of the MathQA benchmark[13].\\nThe benchmark, containing more than 23K problems, is designed to assess the capability of\\nmodels to synthesize code from complex textual descriptions.\\n‚Ä¢ GSM8K [58]: GSM8K is a dataset of 8.5K linguistically diverse grade school math problems.\\nThe dataset is crafted to facilitate the task of question answering on basic mathematical\\nproblems that requires multi-step reasoning.\\n‚Ä¢ GSM-HARD [79]: GSM-HARD is a more challenging version of the GSM8K [58] dataset. It\\nreplaces the numbers in the GSM8K questions with larger, less common numbers, thereby\\nincreasing the complexity and difficulty level of the problems.\\n‚Ä¢ CRUXEval [82]: CRUXEval contains 800 Python functions, each paired with an input-output\\nexample. This benchmark supports two tasks: input prediction and output prediction, designed\\nto evaluate the code reasoning, understanding, and execution capabilities of code LLMs.\\nRepository\\n‚Ä¢ RepoEval [309]: RepoEval enables the evaluation of repository-level code completion. It can\\noffer different levels of granularity and improved evaluation accuracy through the use of unit\\ntests.\\n‚Ä¢ Stack-Repo [239]: Stack-Repo is a dataset of 200 Java repositories from GitHub with near-\\ndeduplicated files. These files are augmented with three types of repository contexts: prompt\\nproposal contexts, BM25 Contexts (based on BM25 similarity scores), and RandomNN Con-\\ntexts (obtained using the nearest neighbors in the representation space of an embedding\\nmodel).\\n‚Ä¢ Repobench [167]: Repobench is a benchmark specifically used for evaluating repository-\\nlevel code auto-completion systems. Supporting both Python and Java, it consists of three\\ninterconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion),\\nand RepoBench-P (Pipeline).\\n‚Ä¢ EvoCodeBench [144]: EvoCodeBench is an evolutionary code generation benchmark, con-\\nstructed through a rigorous pipeline and aligned with real-world repositories. This benchmark\\nalso provides comprehensive annotations and robust evaluation metrics.\\n‚Ä¢ SWE-bench [123]: SWE-bench is a dataset that tests a model‚Äôs ability to automatically solve\\nGitHub issues. The dataset has 2,294 Issue-Pull Request pairs from 12 popular Python reposi-\\ntories.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 22, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:23\\nTable 6. The detailed statistics of commonly-used benchmarks used in evaluating LLMs for code generation.\\nThe column labeled ‚Äò#PL‚Äô indicates the number of programming languages included in each dataset. For the\\nsake of brevity, we list the programming languages (PLs) for benchmarks that support fewer than or include\\nfive PLs. For benchmarks with six or more PLs, we provide only a numerical count of the PLs supported.\\nScenario\\nBenchmark\\nSize\\n#PL\\nDate\\nLink\\nGeneral\\nHumanEval [48]\\n164\\nPython\\n2021-07\\nhttps://huggingface.co/datasets/openai_humaneval\\nHumanEval+ [162]\\n164\\nPython\\n2023-05\\nhttps://huggingface.co/datasets/evalplus/humanevalplus\\nHumanEvalPack [187]\\n164\\n6\\n2023-08\\nhttps://huggingface.co/datasets/bigcode/humanevalpack\\nMBPP [17]\\n974\\nPython\\n2021-08\\nhttps://huggingface.co/datasets/mbpp\\nMBPP+ [162]\\n378\\nPython\\n2023-05\\nhttps://huggingface.co/datasets/evalplus/mbppplus\\nCoNaLa [297]\\n596.88K\\nPython\\n2018-05\\nhttps://huggingface.co/datasets/neulab/conala\\nSpider [300]\\n8,034\\nSQL\\n2018-09\\nhttps://huggingface.co/datasets/xlangai/spider\\nCONCODE [113]\\n104K\\nJava\\n2018-08\\nhttps://huggingface.co/datasets/AhmedSSoliman/CONCOD\\nODEX [273]\\n945\\nPython\\n2022-12\\nhttps://huggingface.co/datasets/neulab/odex\\nCoderEval [299]\\n460\\nPython, Java\\n2023-02\\nhttps://github.com/CoderEval/CoderEval\\nReCode [263]\\n1,138\\nPython\\n2022-12\\nhttps://github.com/amazon-science/recode\\nStudentEval [19]\\n1,749\\nPython\\n2023-06\\nhttps://huggingface.co/datasets/wellesley-easel/StudentEval\\nBigCodeBench [333]\\n1,140\\nPython\\n2024-06\\nhttps://huggingface.co/datasets/bigcode/bigcodebench\\nClassEval [72]\\n100\\nPython\\n2023-08\\nhttps://huggingface.co/datasets/FudanSELab/ClassEval\\nNaturalCodeBench [314]\\n402\\nPython, Java\\n2024-05\\nhttps://github.com/THUDM/NaturalCodeBench\\nCompetitions\\nAPPS [95]\\n10,000\\nPython\\n2021-05\\nhttps://huggingface.co/datasets/codeparrot/apps\\nCodeContests [151]\\n13,610\\nC++, Python,\\nJava\\n2022-02\\nhttps://huggingface.co/datasets/deepmind/code_contests\\nLiveCodeBench [188]\\n713\\nUpdating\\nPython\\n2024-03\\nhttps://github.com/LiveCodeBench/LiveCodeBench\\nData Science\\nDSP [41]\\n1,119\\nPython\\n2022-01\\nhttps://github.com/microsoft/DataScienceProblems\\nDS-1000 [136]\\n1,000\\nPython\\n2022-11\\nhttps://huggingface.co/datasets/xlangai/DS-1000\\nExeDS [107]\\n534\\nPython\\n2022-11\\nhttps://github.com/Jun-jie-Huang/ExeDS\\nMultilingual\\nMBXP [16]\\n12.4K\\n13\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/mbxp\\nMultilingual HumanEval [16]\\n1.9K\\n12\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/multi-humaneval\\nHumanEval-X [321]\\n820\\nPython, C++,\\nJava, JavaScript,\\nGo\\n2023-03\\nhttps://huggingface.co/datasets/THUDM/humaneval-x\\nMultiPL-E [39]\\n161\\n18\\n2022-08\\nhttps://huggingface.co/datasets/nuprl/MultiPL-E\\nxCodeEval [128]\\n5.5M\\n11\\n2023-03\\nhttps://github.com/ntunlp/xCodeEval\\nReasoning\\nMathQA-X [16]\\n5.6K\\nPython, Java,\\nJavaScript\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/mathqa-x\\nMathQA-Python [17]\\n23,914\\nPython\\n2021-08\\nhttps://github.com/google-research/google-research\\nGSM8K [58]\\n8.5K\\nPython\\n2021-10\\nhttps://huggingface.co/datasets/gsm8k\\nGSM-HARD [79]\\n1.32K\\nPython\\n2022-11\\nhttps://huggingface.co/datasets/reasoning-machines/gsm-hard\\nCRUXEval [82]\\n800\\nPython\\n2024-01\\nhttps://huggingface.co/datasets/cruxeval-org/cruxeval\\nRepository\\nRepoEval [309]\\n3,573\\nPython, Java\\n2023-03\\nhttps://paperswithcode.com/dataset/repoeval\\nStack-Repo [239]\\n200\\nJava\\n2023-06\\nhttps://huggingface.co/datasets/RepoFusion/Stack-Repo\\nRepobench [167]\\n27k\\nPython, Java\\n2023-01\\nhttps://github.com/Leolty/repobench\\nEvoCodeBench [144]\\n275\\nPython\\n2024-03\\nhttps://huggingface.co/datasets/LJ0815/EvoCodeBench\\nSWE-bench [123]\\n2,294\\nPython\\n2023-10\\nhttps://huggingface.co/datasets/princeton-nlp/SWE-bench\\nCrossCodeEval [68]\\n10K\\nPython, Java,\\nTypeScript, C#\\n2023-10\\nhttps://github.com/amazon-science/cceval\\nSketchEval [308]\\n20,355\\nPython\\n2024-03\\nhttps://github.com/nl2code/codes\\n‚Ä¢ CrossCodeEval [68]: CrossCodeEval is a diverse and multilingual scope completion dataset\\ncovering four languages: Python, Java, TypeScript, and C#. This benchmark tests the model‚Äôs\\nability to understand in-depth cross-file information and accurately complete the code.\\n‚Ä¢ SketchEval [308]: SketchEval is a repository-oriented benchmark that encompasses data from\\n19 repositories, each varying in complexity. In addition to the dataset, SketchEval introduces\\na metric, known as SketchBLEU, to measure the similarity between two repositories based\\non their structures and semantics.\\n5.2\\nData Synthesis\\nNumerous studies have demonstrated that high-quality datasets are integral to enhancing the\\nperformance of LLMs in various downstream tasks [33, 133, 179, 280, 286, 328]. For instance, the\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 23, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:24\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nSeed Problem\\n<Problem, Solution>\\nEvolved Problem\\nSelf-Instruct\\nEvol-Instruct\\nSolution\\nSeed Problem\\nEvolution Type\\n<Debiased Problem, Solution>\\nOSS-Instruct\\nSeed Code Snippet\\nùêøùêøùëÄ\\nùêøùêøùëÄ\\nùêøùêøùëÄ\\nFig. 8. The comparison among three representative data synthesis methods used for generating instruction\\ndata with LLMs. The Code Alpaca [43] employs the self-instruct method, whereas WizardCoder [173] and\\nMagicoder [278] utilize the Evol-Instruct and OSS-Instruct methods, respectively.\\nLIMA model, a 65B parameter LLaMa language model fine-tuned with a standard supervised loss\\non a mere 1,000 meticulously curated prompts and responses, achieved performance on par with, or\\neven superior to, GPT-4 in 43% of evaluated cases. This figure rose to 58% when compared to Bard\\nand 65% against DaVinci003, all without the use of reinforcement learning or human preference\\nmodeling [328]. The QuRating initiative strategically selects pre-training data embodying four key\\ntextual qualities ‚Äî writing style, facts & trivia, required expertise, and educational value ‚Äî that\\nresonate with human intuition. Training a 1.3B parameter model on such data resulted in reduced\\nperplexity and stronger in-context learning compared to baseline models [280].\\nDespite these advancements, acquiring quality data remains a significant challenge due to issues\\nsuch as data scarcity, privacy concerns, and prohibitive costs [165, 268]. Human-generated data is\\noften labor-intensive and expensive to produce, and it may lack the necessary scope and detail to\\nnavigate complex, rare, or ambiguous scenarios. As a resolution to these challenges, synthetic data\\nhas emerged as a viable alternative. By generating artificial datasets that replicate the intricacies\\nof real-world information, models such as GPT-3.5-turbo [196] and GPT-4 [5] have enabled the\\ncreation of rich datasets without the need for human annotation [92, 138, 165, 268]. This approach\\nis particularly beneficial in enhancing the instruction-following capabilities of LLMs, with a focus\\non generating synthetic instruction-based data.\\nA notable example of this approach is the Self-Instruct [268] framework, which employs an off-the-\\nshelf language model to generate a suite of instructions, inputs, and outputs. This data is then refined\\nby removing invalid or redundant entries before being used to fine-tune the model. The empirical\\nevidence supports the efficacy of this synthetic data generation methodology. Building upon this\\nconcept, the Alpaca [247] model, fine-tuned on 52k pieces of instruction-following data from a 7B\\nparameter LLaMa [252] model, exhibits performance comparable to the text-davinci-003 model.\\nWizardLM [289] introduced the Evol-Instruct technique, which incrementally transforms simple\\ninstructions into more complex variants. The fine-tuned LLaMa model using this technique has\\nshown promising results in comparison to established proprietary LLMs such as ChatGPT [196] and\\nGPT-4 [5], to some extent. Moreover, Microsoft has contributed to this field with their Phi series of\\nmodels, predominantly trained on synthetic high-quality data, which includes Phi-1 (1.3B) [84]\\nfor Python coding, Phi-1.5 (1.3B) [150] for common sense reasoning and language understanding,\\nPhi-2 (2.7B) [182] for advanced reasoning and language understanding, and Phi-3 (3.8B) [4] for\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 24, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:25\\ngeneral purposes. These models have consistently outperformed larger counterparts across various\\nbenchmarks, demonstrating the efficacy of synthetic data in model training.\\nDrawing on the successes of data synthesis for general-purpose LLMs, researchers have expanded\\nthe application of synthetic data to the realm of code generation. The Code Alpaca model, as de-\\nscribed in [43], has been fine-tuned on a 7B and 13B LLaMA model using a dataset of 20k instruction-\\nfollowing examples for code generation. This dataset was created by text-davinci-00310 and\\nemployed the Self-Instruct technique [268]. Building on this, the WizardCoder 15B [173] utilizes\\nthe Evol-Instruct technique to create an enhanced dataset of 78k evolved code instruction examples.\\nThis dataset originates from the initial 20k instruction-following dataset used by Code Alpaca\\n[43], which was also generated by text-davinci-003. The WizardCoder model, fine-tuned on\\nthe StarCoder [147] base model, achieved a 57.3% pass@1 on the HumanEval benchmarks. This\\nperformance not only surpasses all other open-source Code LLMs by a significant margin but also\\noutperforms leading closed LLMs such as Anthropic‚Äôs Claude and Google‚Äôs Bard. In a similar vein,\\nMagicoder [278] introduces a novel data synthesis approach termed OSS-INSTRUCT which enlight-\\nens LLMs with open-source code snippets to generate high-quality instruction data for coding tasks.\\nIt aims to address the inherent biases often present in synthetic data produced by LLMs. Building\\nupon CodeLlama [227], the MagicoderS-CL-7B model ‚Äî fine-tuned with 75k synthetic instruction\\ndata using the OSS-INSTRUCT technique and with gpt-3.5-turbo-1106 as the data generator ‚Äî\\nhas outperformed the prominent ChatGPT on the HumanEval Plus benchmark, achieving pass@1\\nof 66.5% versus 65.9%. In a noteworthy development, Microsoft has introduced the phi-1 model [84],\\na more compact LLM of only 1.3B parameters. Despite its smaller size, phi-1 has been trained on\\nhigh-quality textbook data sourced from the web (comprising 6 billion tokens) and supplemented\\nwith synthetic textbooks and exercises generated with GPT-3.5 (1 billion tokens). It has achieved\\npass@1 of 50.6% on HumanEval and 55.5% on MBPP, setting a new state-of-the-art for Python\\ncoding performance among existing small language models (SLMs). The latest contribution to\\nthis field is from the BigCode team, which has presented StarCoder2-15B-instruct [304], the first\\nentirely self-aligned code LLM trained with a transparent and permissive pipeline. This model\\naligns closely with the OSS-INSTRUCT principles established by Magicoder, generating instructions\\nbased on seed functions filtered from the Stack v1 dataset [132] and producing responses through\\nself-validation. Unlike Magicoder, StarCoder2-15B-instruct employs its base model, StarCoder2-15B,\\nas the data generator, thus avoiding reliance on large and proprietary LLMs like GPT-3.5-turbo\\n[196]. Figure 8 illustrates the comparison between Self-Instruct, Evol-Instruct, and OSS-Instruct\\ndata synthesis methods.\\nWhile synthetic data has demonstrated its potential across both small- and large-scale LMs for a\\nvariety of general and specialized tasks, including code generation, it also poses several challenges\\nthat must be addressed. These challenges include a lack of data diversity [280], the need to ensure\\nthe factuality and fidelity of the information [256, 281], and the potential to amplify existing biases\\nor introduce new ones [24, 89].\\n5.3\\nPre-Training\\n5.3.1\\nModel Architectures. Since the inception of the Transformer architecture for machine transla-\\ntion [257], it has become the de facto backbone for a multitude of LLMs that address a wide range of\\ndownstream tasks. The Transformer and its derivatives owe their prominence to their exceptional\\nability to parallelize computation and their powerful representational capacities [298, 319]. Through\\ninnovative scaling techniques, such as Mixture-of-Experts (MoE) [35, 233] and Depth-Up-Scaling\\n(DUS) [130], the capacity of Transformer-based LLMs has expanded to encompass hundreds of\\n10https://platform.openai.com\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 25, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:26\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 7. The overview of LLMs with encoder-decoder architectures for code generation.\\nModel\\nInstitution\\nSize\\nVocabulary\\nContext\\nWindow\\nDate\\nOpen Source\\nPyMT5[57]\\nMicrosoft\\n374M\\n50K\\n1024+1024\\n2020-10\\nPLBART[7]\\nUCLA\\n140M\\n50K\\n1024+1024\\n2021-03\\n\"\\nCodeT5 [271]\\nSalesforce\\n60M, 220M, 770M\\n32K\\n512+256\\n2021-09\\n\"\\nJuPyT5[41]\\nMicrosoft\\n350M\\n50K\\n1024+1024\\n2022-01\\nAlphaCode[151]\\nDeepMind\\n284M, 1.1B, 2.8B,\\n8.7B, 41.1B\\n8K\\n1536+768\\n2022-02\\nCodeRL[139]\\nSalesforce\\n770M\\n32K\\n512+256\\n2022-06\\n\"\\nERNIE-Code[40]\\nBaidu\\n560M\\n250K\\n1024+1024\\n2022-12\\n\"\\nPPOCoder[238]\\nVirginia Tech\\n770M\\n32K\\n512+256\\n2023-01\\nCodeT5+[269]\\nSalesforce\\n220M, 770M, 2B,\\n6B, 16B\\n50K\\n2048+2048\\n2023-05\\n\"\\nCodeFusion[241]\\nMicrosoft\\n75M\\n32k\\n128+128\\n2023-10\\n\"\\nAST-T5[81]\\nUC Berkeley\\n226M\\n32k\\n512+200/300\\n2024-01\\n\"\\nbillions or even trillions of parameters. These scaled-up models have exhibited a range of emergent\\nabilities [97, 127, 275], such as instruction following [200], in-context learning [70], and step-by-step\\nreasoning [105, 276] that were previously unforeseen.\\nIn the domain of code generation using LLMs, the architecture of contemporary models generally\\nfalls into one of two categories: encoder-decoder models, such as CodeT5 [271], CodeT5+ [269],\\nand CodeRL [139]; or decoder-only models, such as Codex [48], StarCoder [147], Code Llama [227],\\nand CodeGemma [59]. These architectures are depicted in Figure 2(b) and (c), respectively. For a\\ncomprehensive overview, Table 7 details the encoder-decoder architectures, while Table 8 focuses\\non the decoder-only models utilized in code generation.\\n5.3.2\\nPre-training Tasks. In the initial phase, language models for code generation are typically\\ntrained from scratch using datasets consisting of manually annotated pairs of natural language\\ndescriptions and corresponding code snippets, within a supervised learning framework. However,\\nmanual annotation is not only laborious and time-consuming, but the efficacy of the resulting\\nmodels is also constrained by both the volume and the quality of the available annotated data. This\\nlimitation is especially pronounced in the context of low-resource programming languages, such\\nas Swahili and Yoruba, where annotated examples are scarce [38, 46]. In light of these challenges,\\nthere has been a shift towards an alternative training strategy that involves pre-training models on\\nextensive and unlabelled code corpora. This method is aimed at imbuing the models with a broad\\nunderstanding of programming knowledge, encompassing elements like identifiers, code structure,\\nand underlying semantics [48]. In this regard, two pre-training tasks have gained prominence\\nfor their effectiveness, namely Causal Language Modeling (CLM), also known as unidirectional\\nlanguage modeling or next-token prediction, and Denoising Autoencoding (DAE). The CLM task\\ncan be applied to both decoder-only and encoder-decoder model architectures, while DAE tasks are\\nspecifically designed for encoder-decoder frameworks. It should also be noted that there is a variety\\nof additional auxiliary pre-training tasks that can further enhance model performance. These\\ninclude Masked Identifier Prediction, Identifier Tagging, Bimodal Dual Generation [271], Text-Code\\nMatching, and Text-Code Contrastive Learning [269]. These tasks contribute to a more nuanced\\nand comprehensive pre-training process, equipping the models with the capabilities necessary to\\nhandle a wide range of code generation scenarios.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 26, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:27\\nTable 8. The overview of LLMs with decoder-only architectures for code generation.\\nModel\\nInstitution\\nSize\\nVocabulary\\nContext\\nWindow\\nDate\\nOpen Source\\nGPT-C [244]\\nMicrosoft\\n366M\\n60K\\n1024\\n2020-05\\nCodeGPT [172]\\nMicrosoft\\n124M\\n50K\\n1024\\n2021-02\\n\"\\nGPT-Neo[30]\\nEleutherAI\\n125M, 1.3B, 2.7B\\n50k\\n2048\\n2021-03\\n\"\\nGPT-J [258]\\nEleutherAI\\n6B\\n50k\\n2048\\n2021-05\\n\"\\nCodex [48]\\nOpenAI\\n12M, 25M, 42M,\\n85M, 300M, 679M,\\n2.5B, 12B\\n-\\n4096\\n2021-07\\nCodeParrot [254]\\nHugging Face\\n110M, 1.5B\\n33k\\n1024\\n2021-11\\n\"\\nPolyCoder [290]\\nCMU\\n160M, 400M, 2.7B\\n50k\\n2048\\n2022-02\\n\"\\nCodeGen [193]\\nSalesforce\\n350M, 2.7B, 6.1B,\\n16.1B\\n51k\\n2048\\n2022-03\\n\"\\nGPT-NeoX [29]\\nEleutherAI\\n20B\\n50k\\n2048\\n2022-04\\n\"\\nPaLM-Coder [54]\\nGoogle\\n8B, 62B, 540B\\n256k\\n2048\\n2022-04\\nInCoder [77]\\nMeta\\n1.3B, 6.7B\\n50k\\n2049\\n2022-04\\n\"\\nPanGu-Coder [55]\\nHuawei\\n317M, 2.6B\\n42k\\n1024\\n2022-07\\nPyCodeGPT [306]\\nMicrosoft\\n110M\\n32k\\n1024\\n2022-06\\n\"\\nCodeGeeX [321]\\nTsinghua\\n13B\\n52k\\n2048\\n2022-09\\n\"\\nBLOOM [140]\\nBigScience\\n176B\\n251k\\n-\\n2022-11\\n\"\\nChatGPT [196]\\nOpenAI\\n-\\n-\\n16k\\n2022-11\\n\"\\nSantaCoder [9]\\nHugging Face\\n1.1B\\n49k\\n2048\\n2022-12\\n\"\\nLLaMA [252]\\nMeta\\n6.7B, 13.0B, 32.5B,\\n65.2B\\n32K\\n2048\\n2023-02\\n\"\\nGPT-4 [5]\\nOpenAI\\n-\\n-\\n32K\\n2023-03\\nCodeGen2 [192]\\nSalesforce\\n1B, 3.7B, 7B, 16B\\n51k\\n2048\\n2023-05\\n\"\\nreplit-code [223]\\nreplit\\n3B\\n33k\\n2048\\n2023-05\\n\"\\nStarCoder [147]\\nHugging Face\\n15.5B\\n49k\\n8192\\n2023-05\\n\"\\nWizardCoder [173]\\nMicrosoft\\n15B, 34B\\n49k\\n8192\\n2023-06\\n\"\\nphi-1 [84]\\nMicrosoft\\n1.3B\\n51k\\n2048\\n2023-06\\n\"\\nCodeGeeX2 [321]\\nTsinghua\\n6B\\n65k\\n8192\\n2023-07\\n\"\\nPanGu-Coder2 [234]\\nHuawei\\n15B\\n42k\\n1024\\n2023-07\\nLlama 2 [253]\\nMeta\\n7B, 13B, 70B\\n32K\\n4096\\n2023-07\\n\"\\nOctoCoder [187]\\nHugging Face\\n15.5B\\n49k\\n8192\\n2023-08\\n\"\\nCode Llama [227]\\nMeta\\n7B, 13B, 34B\\n32k\\n16384\\n2023-08\\n\"\\nCodeFuse [160]\\nAnt Group\\n350M, 13B, 34B\\n101k\\n4096\\n2023-09\\n\"\\nphi-1.5 [150]\\nMicrosoft\\n1.3B\\n51k\\n2048\\n2023-09\\n\"\\nCodeShell [285]\\nPeking University\\n7B\\n70k\\n8192\\n2023-10\\n\"\\nMagicoder [278]\\nUIUC\\n7B\\n32k\\n16384\\n2023-12\\n\"\\nAlphaCode 2 [11]\\nGoogle DeepMind\\n-\\n-\\n-\\n2023-12\\nStableCode [210]\\nStabilityAI\\n3B\\n50k\\n16384\\n2024-01\\n\"\\nWaveCoder [301]\\nMicrosoft\\n6.7B\\n32k\\n16384\\n2023-12\\n\"\\nphi-2 [182]\\nMicrosoft\\n2.7B\\n51k\\n2048\\n2023-12\\n\"\\nDeepSeek-Coder [88]\\nDeepSeek\\n1.3B, 6.7B, 33B\\n32k\\n16384\\n2023-11\\n\"\\nStarCoder 2 [170]\\nHugging Face\\n15B\\n49k\\n16384\\n2024-02\\n\"\\nClaude 3 [14]\\nAnthropic\\n-\\n-\\n200K\\n2024-03\\nCodeGemma [59]\\nGoogle\\n2B, 7B\\n25.6k\\n8192\\n2024-04\\n\"\\nCode-Qwen [249]\\nQwen Group\\n7B\\n92K\\n65536\\n2024-04\\n\"\\nLlama3 [180]\\nMeta\\n8B, 70B\\n128K\\n8192\\n2024-04\\n\"\\nStarCoder2-Instruct [304]\\nHugging Face\\n15.5B\\n49K\\n16384\\n2024-04\\n\"\\nCodestral [181]\\nMistral AI\\n22B\\n33k\\n32k\\n2024-05\\n\"\\nCausal Language Modeling. In decoder-only LLMs, given a sequence of tokens x = {ùë•1, . . . ,ùë•ùëõ},\\nthe CLM task refers to autoregressively predict the target tokens ùë•ùëñbased on the preceding tokens\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 27, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:28\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nùë•<ùëñin a sequence. The causal language modeling objective for training decoder LLMs is to minimize\\nthe following likelihood:\\nLùê∑ùëíùëêùëúùëëùëíùëü‚àíùëúùëõùëôùë¶\\nùê∂ùêøùëÄ\\n(x) = ‚àílog(\\nùëõ\\n√ñ\\nùëñ=1\\nùëÉùúÉ(ùë•ùëñ| x<ùëñ)) =\\nùëõ\\n‚àëÔ∏Å\\nùëñ=1\\n‚àílog ùëÉùúÉ(ùë•ùëñ| x<ùëñ)\\n(16)\\nwhere x<ùëñrepresents the sequence of preceding tokens {ùë•1, . . . ,ùë•ùëñ‚àí1} before xùëñin the input, ùúÉ\\ndenotes the model parameters. The conditional probability ùëÉùúÉ(ùë•ùëñ|x<ùëñ)) is modeled by adding a\\ncausal attention mask to the multi-head self-attention matrix of each Transformer block. To be\\nspecific, causal attention masking is implemented by setting the lower triangular part of the\\nmatrix to 0 and the remaining elements to ‚àí‚àû, ensuring that each token ùë•ùëñattends only to its\\npredecessors and itself. On the contrary, in encoder-decoder LLMs, a pivot token ùë•ùëòis randomly\\nselected in a sequence of tokens and then regarding the context before it as the source sequence\\nxùëñùëõ= {ùë•1, . . . ,ùë•ùëò} of the encoder and the sequence after it as the target output xùëúùë¢ùë°= {ùë•ùëò+1, . . . ,ùë•ùëõ}\\nof decoder. Formally, the causal language modeling objective for training encoder-decoder LLMs is\\nto minimize loss function as follows:\\nLùê∏ùëõùëêùëúùëëùëíùëü‚àíùê∑ùëíùëêùëúùëëùëíùëü\\nùê∂ùêøùëÄ\\n(x) = ‚àílog(\\nùëõ\\n√ñ\\nùëñ=ùëò+1\\nùëÉùúÉ(ùë•ùëñ| x‚â§ùëò, x<ùëñ)) =\\nùëõ\\n‚àëÔ∏Å\\nùëñ=ùëò+1\\n‚àílog ùëÉùúÉ(ùë•ùëñ| x‚â§ùëò, x<ùëñ)\\n(17)\\nwhere x‚â§ùëòis the source sequence input and x<ùëñdenotes the target sequence autoregressively\\ngenerated so far. During the inference phase, pre-trained LLMs that have been trained on large-\\nscale code corpus can generate code in a zero-shot manner without the need for fine-tuning. This\\nis achieved through the technique of prompt engineering, which guides the model to produce the\\ndesired output11 [33, 214]. Additionally, recent studies have explored the use of few-shot learning,\\nalso referred to as in-context learning, to enhance model performance further [145, 206].\\nDenoising Autoencoding. In addition to causal language modeling (CLM), the denoising\\nautoencoding (DAE) task has been extensively applied in pre-training encoder-decoder architectures\\nfor code generation, such as PLBART [7], CodeT5 [271], and its enhanced successor, CodeT5+ [269].\\nFollowing T5 [217] and CodeT5 [271], the DAE refers to initially perturbing the source sequence\\nby introducing randomly masked spans of varying lengths. This corrupted sequence serves as the\\ninput for the encoder. Subsequently, the decoder employs an autoregressive strategy to reconstruct\\nthe masked spans, integrating sentinel tokens to facilitate the generation process. This method\\nhas proven effective in improving the model‚Äôs ability to generate semantically and syntactically\\naccurate code by learning robust contextual representations [269, 271]. Formally, the denoising\\nautoencoding objective for training encoder-decoder LLMs is to minimize the following likelihood:\\nLùê∏ùëõùëêùëúùëëùëíùëü‚àíùê∑ùëíùëêùëúùëëùëíùëü\\nùê∑ùê¥ùê∏\\n(x) =\\nùëò\\n‚àëÔ∏Å\\nùëñ=1\\n‚àílog ùëÉùúÉ(xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\nùëñ\\n| x\\\\ùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†, xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\n<ùëñ\\n)\\n(18)\\nwhere ùúÉdenotes the model parameters, x\\\\ùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†is the noisy input with masked spans,\\nxùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†is the masked spans to predict from the decoder with ùëòdenoting the number of\\ntokens in xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†, and xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\n<ùëñ\\nis the span sequence autoregressively generated so far.\\nCompared with CLM, the DAE task presents a more challenging scenario, as it necessitates a deeper\\nunderstanding and capture of the intrinsic semantic relationships among token sequences by LLMs\\n[217].\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 28, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:29\\nModel learns to perform\\nmany code tasks via natural\\nlanguage instructions\\nInference on\\nunseen code\\ntask\\n(A) Train from Scratch (Transformer)\\n‚Ä¢ Randomly initialized\\nmodel parameters\\n‚Ä¢ Typically requires many\\ncode task-specific examples\\n‚Ä¢ One specialized code model\\nfor each code task\\nImprove performance via\\nfew-shot prompting or\\nprompt engineering\\nUse RL to align\\nwith human\\npreferences\\nInference on\\nunseen code\\ntask\\n(B) Prompting (StarCoder)\\n(C) Pretrain-Finetune (CodeBERT, CodeT5)\\n(E) RLHF (InstructGPT)\\n‚Ä¢ Typically requires many\\ncode task-specific examples\\n‚Ä¢ One specialized code model for\\neach code task\\n(D) Instruction Tuning (WizardCoder)\\nPretrained\\nLM\\nPretrained\\nLM\\nPretrained\\nLM\\nPretrained\\nLM\\nInference\\non task A\\nInference\\non task A\\nInference\\non task A\\nInference\\non task A\\nInference\\non task A\\nTrained LM\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nFinetune\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nRLHF\\nTask-specific Knowledge\\nWorld/General Knowledge\\nInstruction-following with\\nMulti-task Learning\\nHuman Preference Alignment\\nModel learns to perform\\nmany code tasks via natural\\nlanguage instructions\\nFig. 9. Comparison of instruction tuning with various fine-tuning strategies and prompting for code tasks,\\nadapted from [274]. For (A), which involves training a Transformer from scratch, please refer to [6] for its use\\nin source code summarization task. In the case of (E), we utilize a representative RLHF [200] as an example.\\nAdditional reinforcement learning methods, such as DPO [216], are also applicable at this stage.\\n5.4\\nInstruction Tuning\\nAfter pre-training LLMs on large-scale datasets, the next phase typically involves augmenting the\\nmodel‚Äôs ability to process and follow various instructions, known as instruction tuning. Instruction\\ntuning generally refers to the supervised fine-tuning of pre-trained LLMs using datasets comprised\\nof structured examples framed as various natural language instructions [114, 200, 274, 313]. The\\ncomparison of instruction tuning with various fine-tuning strategies and prompting for code tasks\\nis depicted in Figure 9. Two exemplars of instruction data sampled from Code Alpaca [43] are\\ndemonstrated in Figure 10. It capitalizes on the heterogeneity of instruction types, positioning\\ninstruction tuning as a form of multi-task prompted training that significantly enhances the model‚Äôs\\ngeneralization to unseen tasks [56, 200, 229, 274].\\nIn the realm of code generation, natural language descriptions serve as the instructions guiding\\nthe model to generate corresponding code snippets. Consequently, a line of research on instruction\\ntuning LLMs for code generation has garnered substantial interest across academia and industry.\\nTo perform instruction tuning, instruction data are typically compiled from source code with\\npermissive licenses [110, 132, 170] (refer to Section 5.1.2) or are constructed from synthetic code\\ndata [173, 278, 304] (refer to Section 5.2). These datasets are then utilized to fine-tune LLMs through\\na supervised learning paradigm. However, the substantial computational resources required for\\nfull parameter fine-tuning (FFT) LLM pose a notable challenge, particularly in scenarios with\\nconstrained resources [67, 153]. To mitigate this issue, parameter-efficient fine-tuning (PEFT) has\\nemerged as a compelling alternative strategy, gaining increasing attention for its potential to reduce\\nresource consumption [67]. In the following subsection, we categorize existing works based on\\ntheir instruction-tuning strategies to provide a comprehensive and systematic review.\\n11For more information on prompt engineering, visit https://www.promptingguide.ai\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 29, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:30\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\ndef find_primes(n): \\n    prime_list = [2] \\n    for number in range(2, n + 1): \\n        is_prime = True\\n        for k in range(2, number): \\n            if number % k == 0: \\n                is_prime = False \\n        if is_prime: \\n            prime_list.append(number) \\n    return prime_list\\nOutput:\\nInput:\\nInstruction:\\nWrite code to create a list of all \\nprime numbers between 2 and 100.\\nimport re\\nstring = \"This string contains some \\nurls such as https://www.google.com and \\nhttps://www.facebook.com.\"\\nurls = re.findall(\\'http[s]?://(?:[a-zA-\\nZ]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-\\nfA-F][0-9a-fA-F]))+\\', string) \\nprint(urls)\\nN/A\\nThis string contains some urls such as \\nhttps://www.google.com and \\nhttps://www.facebook.com.\\nGenerate a snippet of code to extract \\nall the URLs from the given string.\\nOutput:\\nInput:\\nInstruction:\\nFig. 10. Two exemplars of instruction data sampled from Code Alpaca [43] used to instruction-tune pre-\\ntrained code LLM to enhance their alignment with natural language instructions. The instruction corpus\\nencompasses a variety of tasks, each accompanied by distinct instructions, such as prime numbers generation\\nand URLs extraction.\\n5.4.1\\nFull Parameter Fine-tuning. Full parameter fine-tuning (FFT) involves updating all parameters\\nwithin a pre-trained model, as shown in Figure 11(a). This approach is often preferred when ample\\ncomputational resources and substantial training data are available, as it typically leads to better\\nperformance. [271] introduces an encoder-decoder pre-trained language model for code generation,\\nnamed CodeT5+. They instruction-tune this model on a dataset comprising 20k instruction samples\\nfrom Code Alpaca [43], resulting in an instruction-following model called InstructCodeT5+, which\\nexhibited improved capabilities in code generation. [173] leverages the Evol-Instruct data synthesis\\ntechnique from WizardLM [289] to evolve 20K code Alpaca [43] instruction samples into a 78K\\ncode instruction dataset. This enriched dataset is then used to fine-tune the StarCoder base model,\\nresulting in WizardCoder, which showcases notable advancements in code generation. In a similar\\nvein, inspired by the successes of WizardCoder [173] and RRHF [303], Pangu-Coder 2 [234] applies\\nthe Evol-Instruct method to generate 68k high-quality instruction samples from the initial 20k Code\\nAlpaca [43] instruction samples. Additionally, they introduces a novel reinforcement learning via\\nRank Responses to align Test & Teacher Feedback (RRTF), which further enhances the performance\\nof Pangu-Coder 2 in code generation. Diverging from synthetic instruction data generation methods,\\nOctoPack [187] utilizes real-world data by curating CommitPack from the natural structure of\\nGit commits, which inherently pair code changes with human-written instructions. This dataset,\\nconsisting of 4 terabytes of Git commits across 350 programming languages, is employed to fine-\\ntune StarCoder [147] and CodeGeeX2 [321], leading to the instruction-following code models of\\nOctoCoder and OctoGeeX for code generation, respectively. The most recent innovation comes\\nfrom Magicoder [278], who proposes OSS-INSTRUCT, a novel data synthesis method that leverages\\nopen-source code snippets to generate high-quality instruction data for code generation. This\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 30, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:31\\napproach seeks to reduce the bias often present in synthetic data generated by LLM. In line with\\nOSS-INSTRUCT, the BigCode team introduces StarCoder2-15B-instruct [304], which they claim\\nto be the first entirely self-aligned LLM for code generation, trained with a fully permissive and\\ntransparent pipeline. Moreover, [59] harnesses open-source mathematics datasets, such as MATH\\n[95] and GSM8k [58], along with synthetically generated code following the OSS-INSTRUCT\\n[278] paradigm, to instruction-tune CodeGemma 7B, yielding exceptional results in mathematical\\nreasoning and code generation tasks.\\n5.4.2\\nParameter-Efficient Fine-tuning. To mitigate the extensive computational and resource de-\\nmands inherent in fine-tuning LLMs, the concept of parameter-efficient fine-tuning (PEFT) has\\nemerged to focus on updating a minimal subset of parameters, which may either be a selection of\\nthe model‚Äôs parameters or an array of additional parameters specifically introduced for the tuning\\nprocess [67, 153]. The categorization of these methods is depicted in Figure 11(b), (c), and (d). A\\nplethora of innovative PEFT approaches have been developed, among which BitFit [305], Adapter\\n[102], Prompt tuning [142], Prefix-tuning [149], LoRA [103], IA3 [161], QLoRA [65], and AdaLoRA\\n[312] are particularly noteworthy. A seminal study in this field, LoRA [103], proposes a parameter\\nupdate mechanism for a pre-trained weight matrix ‚Äî such as those found in the key or value\\nprojection matrices of a Transformer block‚Äôs multi-head self-attention layer ‚Äî by factorizing the\\nupdate into two low-rank matrices. Crucially, all original model parameters remain frozen, with\\nonly the pair of low-rank matrices being trainable. After fine-tuning, the product of these low-rank\\nmatrices can be seamlessly incorporated into the existing weight matrix through an element-wise\\naddition. This process can be formally described as:\\n(W0 + ŒîW)ùë•= W0ùë•+ ŒîWùë•= Wùëìùëüùëúùëßùëíùëõ\\n0\\nùë•+ ùõº\\nùëüBùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùë¢ùëù\\nAùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùëëùëúùë§ùëõ\\n|                     {z                     }\\nŒîW\\nùë•\\n(19)\\nwhere W0 ‚ààRùëë√óùëòdenotes a pre-trained weight matrix, Bùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùë¢ùëù\\n‚ààRùëë√óùëüand Aùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùëëùëúùë§ùëõ\\n‚ààRùëü√óùëòare\\ntwo trainable low-rank matrixes and initialized by a zero matrix and a random Gaussian distribution\\nN (0, ùúé2) respectively, to ensure ŒîW = 0 at the beginning of training. The rank ùëü‚â™min(ùëë,ùëò), the\\nùõº\\nùëüis a scaling coefficient to balance the importance of the LoRA module, like a learning rate.\\nDespite the advancements in PEFT methods, their application in code generation remains limited.\\nFor instance, [121] pioneered the use of parameter-efficient instruction-tuning on a Llama 2 [253]\\nmodel with a single RTX 3090 GPU, leading to the development of a multilingual code generation\\nmodel called CodeUp. More recently, ASTRAIOS [334] conducted a thorough empirical examination\\nof parameter-efficient instruction tuning for code comprehension and generation tasks. This study\\nyielded several perceptive observations and conclusions, contributing valuable insights to the\\ndomain.\\n5.5\\nReinforcement Learning with Feedback\\nLLMs have exhibited remarkable instruction-following capabilities through instruction tuning.\\nHowever, they often produce outputs that are unexpected, toxic, biased, or hallucinated outputs that\\ndo not align with users‚Äô intentions or preferences [118, 200, 272]. Consequently, aligning LLMs with\\nhuman preference has emerged as a pivotal area of research. A notable work is InstructGPT [200],\\nwhich further fine-tunes an instruction-tuned model utilizing reinforcement learning with human\\nfeedback (RLHF) on a dataset where labelers have ranked model outputs in order of quality, from\\nbest to worst. This method has been instrumental in the development of advanced conversational\\nlanguage models, such as ChatGPT [196] and Bard [177]. Despite its success, acquiring high-quality\\nhuman preference ranking data is a resource-intensive process [141]. To address this, Reinforcement\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 31, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:32\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTrainable\\nInput\\nLayer N+1\\nüî•\\nLayer N\\nLayer 1\\nLayer 2\\n...\\nInput\\nLayer 2\\nüî•\\nLayer N\\nLayer 1\\n...\\nüî•\\nFrozen\\nInput\\n...\\nLayer 2\\nüî•\\nLayer 1\\nüî•\\nLayer N\\nüî•\\nInput\\nLayer N\\nLayer 1\\nLayer 2\\n‚ñ≥WN\\nüî•\\n‚ñ≥W2\\nüî•\\n‚ñ≥W1\\nüî•\\n...\\n...\\n(a) Full Fine-tuning\\n(b) Specification\\n(c) Addition\\n(d) Reparameterization\\nFig. 11. An illustration of full parameter fine-tuning (FFT) and parameter-efficient fine-tuning (PEFT) methods.\\n(a) refers to the Full Fine-tuning method, which updates all parameters of the base model during fine-tuning.\\n(b) stands for the Specification-based PEFT method that conditionally fine-tunes a small subset of the model\\nparameters while freezing the rest of the model, e.g. BitFit [305]. (c) represents the Addition-based PEFT\\nmethod that fine-tunes the incremental parameters introduced into the base model or input, e.g. Adapter\\n[102], Prefix-tuning [149], and Prompt-tuning [142]. (d) symbolizes the Reparameterization-based method\\nwhich reparameterizes existing model parameters by low-rank transformation, e.g. LoRA [103], QLoRA [65],\\nand AdaLoRA [312].\\nLearning from AI Feedback (RLAIF) [21, 141] has been proposed to leverage powerful off-the-shelf\\nLLMs (e.g., ChatGPT [196] and GPT-4 [5]) to simulate human annotators by generating preference\\ndata.\\nBuilding on RLHF‚Äôs success, researchers have explored reinforcement learning with feedback to\\nenhance code generation in LLMs. Unlike RLHF, which relies on human feedback, this approach\\nemploys compilers or interpreters to automatically provide feedback on code samples through code\\nexecution on unit test cases, catalyzing the advancement of this research domain. CodeRL [139]\\nintroduced an actor-critic reinforcement learning framework for code generation. In this setup, the\\nlanguage model serves as the actor-network, while a token-level functional correctness reward\\npredictor acts as the critic. Generated code is assessed through unit test signals from a compiler,\\nwhich can indicate compiler errors, runtime errors, unit test failures, or passes. CompCoder [266]\\nenhances code compilability by employing compiler feedback, including language model fine-\\ntuning, compilability reinforcement, and compilability discrimination strategies. Subsequently,\\nPPOCoder [238] integrates pre-trained code model CodeT5 [271] with Proximal Policy Optimization\\n(PPO) [230]. This integration not only utilizes execution (i.e., compilers or interpreters) feedback to\\nassess syntactic and functional correctness but also incorporates a reward function that evaluates\\nthe syntactic and semantic congruence between abstract syntax tree (AST) sub-trees and data flow\\ngraph (DFG) edges in the generated code against the ground truth. Additionally, the framework\\napplies a KL-divergence penalty to maintain fidelity between the actively learned policy and the\\nreferenced pre-trained model, enhancing the optimization process. More recently, RLTF [163] has\\nproposed an online reinforcement learning framework that provides fine-grained feedback based\\non compiler error information and location, along with adaptive feedback that considers the ratio\\nof passed test cases.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 32, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:33\\nDespite these successes, reinforcement learning algorithms face inherent limitations such as\\ninefficiency, instability, extensive resource requirements, and complex hyperparameter tuning,\\nwhich can impede the performance and scalability of LLMs. To overcome these challenges, recent\\nstudies have introduced various variants of RL methods that do not rely on PPO, including DPO\\n[216], RRHF [303], and sDPO [129]. In essence, these methods aim to maximize the likelihood\\nbetween the logarithm of conditional probabilities of preferred and rejected responses, which may\\nbe produced by LLMs with varying capabilities. Inspired by RRHF [303], PanGu-Coder 2 [234]\\nleverages a novel framework, Reinforcement Learning via Rank Responses to align Test & Teacher\\nFeedback (RRTF), significantly enhancing code generation capabilities, as evidenced by pass@1 of\\n62.20% on the HumanEval benchmark.\\nTaking a step forward, the integration of more non-differentiable code features, such as coding\\nstyle [44, 178] and readability [34], into the reinforcement learning feedback for LLM-based code\\ngeneration, presents an exciting avenue for future research.\\n5.6\\nPrompting Engineering\\nLarge-scale language models (LLMs) such as GPT-3 and its successors have been trained on large-\\nscale data corpora, endowing them with substantial world knowledge [33, 200, 274]. Despite this,\\ncrafting an effective prompting as a means of communicating with LLMs to harness their full\\npotential remains a long-standing challenge [164]. Recent advancements in prompting engineering\\nhave expanded the capabilities of LLMs, enabling more sophisticated task completion and enhancing\\nboth reliability and performance. Notable techniques include Chain-of-Thought (CoT) [276], Self-\\nConsistency [267], Tree-of-Thought (ToT) [295], Program of Thoughts (PoT) [50], Reasoning via\\nPlanning (RAP) [93], ReAct [296], Self-Refine [176], Reflexion [236], and LATS [327]. For instance,\\nCoT significantly improves the LLMs‚Äô ability to perform complex reasoning by providing a few\\nchain-of-thought demonstrations as exemplars in prompting.\\nPrompting engineering is particularly advantageous as it bypasses the need for additional training\\nand can significantly elevate performance. Consequently, numerous studies have leveraged this\\ntechnique for iterative and self-improving (refining) code generation within proprietary LLMs\\nsuch as ChatGPT and GPT-4. Figure 12 illustrates the general pipeline for self-improving code\\ngeneration with LLMs. For instance, Self-Debugging [51] involves prompting an LLM to iteratively\\nrefine a predicted program by utilizing feedback composed of code explanations combined with\\nexecution results, which assists in identifying and rectifying errors. When unit tests are unavailable,\\nthis feedback can rely solely on code explanations. Similarly, LDB [325] prompts LLMs to refine\\ngenerated code by incorporating debugging feedback, which consists of the evaluation of the\\ncorrectness of variable values throughout runtime execution, as assessed by the LLMs. In parallel,\\nSelfEvolve [122] employs a two-stage process where LLMs first generate domain-specific knowledge\\nfor a problem, followed by a trial code. This code is then iteratively refined through interactive\\nprompting and execution feedback. An empirical investigation by [195] provides a comprehensive\\nanalysis of the self-repairing capabilities for code generation in models like Code Llama, GPT-\\n3.5, and GPT-4, using problem sets from HumanEval and APPS. This study yields a series of\\ninsightful observations and findings, shedding light on the self-refinement effectiveness of these\\nLLMs. Moreover, Reflexion [236] introduces a general approach for code generation wherein LLM-\\npowered agents engage in verbal self-reflection on task feedback signals, storing these reflections\\nin an episodic memory buffer to inform and improve decision-making in subsequent interactions.\\nLATS [327] adopts a novel strategy, utilizing LLMs as agents, value functions, and optimizers. It\\nenhances decision-making by meticulously constructing trajectories through Monte Carlo Tree\\nSearch (MCTS) algorithms, integrating external feedback, and learning from experience. This\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 33, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content=\"1:34\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nStep 2: Trajectory\\nStep 3: Evaluation\\nStep 4: Self-Reflection\\nStep 1: Code Task\\nFeedback\\nCode LLM\\nExecutor\\nWrite a Python script to \\nprint all unique elements \\nin a list.\\ndef unique_elements(lst):\\nresult = set(lst)\\n    return list(result)\\nassert unique_elements\\n(['a', 'b', 'c', 'a', 'd']) \\n== ['a', 'b', 'c', 'd'] \\n[‚Ä¶] does not work as \\nexpected because it uses \\nthe built-in `set()` \\nfunction in Python, which \\ndoes not maintain the order \\nof elements.[‚Ä¶]\\n(Optional)\\nassert unique_elements([1, \\n2, 3, 4, 4]) == [1, 2, 3, 4]\\n(Code) LLM\\nFig. 12. An illustration of the self-improving code generation pipeline using prompts for LLMs. This process\\nincorporates iterative self-refinement by integrating execution outcomes and includes an optional self-\\nreflection mechanism to enhance generation quality.\\napproach has demonstrated remarkable results in code generation, achieving a pass@1 of 94.4% on\\nthe HumanEval benchmark with GPT-4.\\nDistinct from the aforementioned methods, CodeT [45] and LEVER [190] prompt LLMs to\\ngenerate numerous code samples, which are then re-ranked based on execution outcomes to select\\nthe optimal solution. Notably, these approaches do not incorporate a self-refinement step to further\\nimprove code generation.\\n5.7\\nRepository Level & Long Context\\nIn contemporary software engineering practices, modifications to a code repository are widespread\\nand encompass a range of activities, including package migration, temporary code edits, and the\\nresolution of GitHub issues. While LLMs showcase impressive prowess in function-level code\\ngeneration, they often falter when grappling with the broader context inherent to a repository,\\nsuch as import dependencies, parent classes, and files bearing similar names. These deficiencies\\nresult in suboptimal performance in repository-level code generation, as identified in recent studies\\n[239, 240]. The challenges faced by LLMs in this domain are primarily due to the following factors:\\n‚Ä¢ Code repositories typically contain intricate interdependencies scattered across various\\nfiles, including shared utilities, configurations, and cross-API invocations, which arise from\\nmodular design principles [22, 309].\\n‚Ä¢ Repositories are characterized by their unique structures, naming conventions, and coding\\nstyles, which are essential for maintaining clarity and facilitating ongoing maintenance [44].\\n‚Ä¢ The vast context of an entire repository often exceeds the context length limitations of LLMs,\\nthus hindering their ability to integrate comprehensive contextual information [22].\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.\"),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 34, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:35\\n‚Ä¢ LLMs may not have been adequately trained on extensive sets of repository data, such as\\nproprietary software or projects that are still in development [239].\\nGiven that the scope of a typical software repository encompasses hundreds of thousands of\\ntokens, it is imperative to enhance the capacity of LLMs to handle extensive contexts when they\\nare employed for repository-level code generation. Fortunately, recent advancements in positional\\nencoding techniques, such as ALiBi [211] and RoPE [243], have shown promise in improving the\\nTransformer‚Äôs ability to generalize from shorter training sequences to longer inference sequences\\n[318]. This progress addresses the third challenge mentioned above to a certain degree, thereby\\nenabling better contextualization of coding activities within full repositories.\\nTo further refine LLMs for repository-level code completion, several innovative approaches have\\nbeen introduced. RepoCoder [309] leverages a similarity-based retrieval system within an iterative\\nretrieval-generation paradigm to enrich the context and enhance code completion quality. In a\\nsimilar vein, CoCoMIC [69] employs a cross-file context finder named CCFINDER to pinpoint and\\nretrieve the most relevant cross-file contexts within a repository. RepoHyper [209] introduces a\\nsemantic graph structure, termed RSG, to encapsulate the expansive context of code repositories\\nand uses an ‚ÄúExpand and Refine‚Äù retrieval method to obtain relevant code snippets. Moreover, a\\nframework known as RLPG [240] has been proposed to generate repository-level prompts that\\nintegrate the repository‚Äôs structure with the relevant context across all files. However, the constant\\nreliance on retrieval mechanisms has raised concerns regarding efficiency and robustness, as some\\nretrieved contexts may prove unhelpful or harmful. In response, Repoformer [282] introduces a\\nselective Retrieval-Augmented Generation (RAG) framework that judiciously bypasses retrieval\\nwhen it is deemed redundant. This approach incorporates a self-supervised learning strategy that\\nequips a code LLM with the ability to perform a self-assessment on the utility of retrieval for\\nenhancing the quality of its output, thereby effectively utilizing potentially noisy retrieved contexts.\\nAdditionally, RepoFusion [239] has been developed to train models to combine multiple relevant\\ncontexts from a repository, aiming to produce more precise and context-aware code completions.\\nIn a novel approach, Microsoft‚Äôs CodePlan [22] frames repository-level coding tasks as a planning\\nproblem, generating a multi-step chain of edits (plan) where each step involves invoking an LLM on a\\nspecific code location, considering context from the entire repository, preceding code modifications,\\nand task-specific instructions.\\nAdvancing the state-of-the-art, [308] tackles the formidable challenge of NL2Repo, an endeavor\\nthat seeks to create a complete code repository from natural language requirements. To address\\nthis complex task, they introduce the CodeS framework, which strategically breaks down NL2Repo\\ninto a series of manageable sub-tasks using a multi-layer sketch approach. The CodeS framework\\ncomprises three distinct modules: 1) RepoSketcher, for creating a directory structure of the reposi-\\ntory based on given requirements; 2) FileSketcher, for sketching out each file within that structure;\\nand 3) SketchFiller, for fleshing out the specifics of each function within the file sketches [308].\\nAccordingly, a surge of benchmarks tailored for repository-level code generation has emerged,\\nsuch as RepoEval [309], Stack-Repo [239], Repobench [167], EvoCodeBench [144], SWE-bench\\n[123], CrossCodeEval [68], and SketchEval [308]. The detailed statistics and comparisons of these\\nbenchmarks are presented in Table 6.\\nDespite the progress made by these methods in repository-level code generation, significant chal-\\nlenges remain to be addressed. Programming developers are often required to invest considerable\\ntime in editing and debugging [25, 28, 186, 239, 255]. However, the advent of LLM-powered coding\\nagents, such as AutoCodeRover [316], SWE-Agent [124], and OpenDevin [199], has demonstrated\\ntheir potential to tackle complex problems, paving the way for future exploration in this field (for\\nmore details, see Section 5.9).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 35, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:36\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nEmbedding \\nModel\\nQuery\\nRetrieved Context\\nCreate a quick-\\nsort algorithm \\nin Python.\\nCombine Prompts and Context\\nCreate a quick-sort algorithm \\nin Python.\\nPlease solve the above problem \\nbased on the following context:\\n{context}\\ndef quick_sort(arr):\\n\"\"\"Sort a list of numbers in ascending \\norder using the Quick-Sort algorithm\"\"\"\\nif len(arr) == 0:\\nreturn []\\npivot = arr[0]\\nleft_arr = [x for x in arr if x < pivot]\\nright_arr = [x for x in arr if x > pivot]\\nreturn quick_sort(left_arr) + [pivot] + \\nquick_sort(right_arr)\\nCode LLM\\nEmbedding \\nModel\\nVector \\nDatabase\\nCode Solution\\nCode Data \\nChunks\\nOpen \\nSource\\nStage 1: Retrieval\\nStage 2: Generation\\nAlgorithm:\\n1. If the input \\narray...already \\nsorted....\\n5. Recursively \\ncall quicksort...\\nFig. 13. A workflow illustration of the Retrieval-Augmented Code Generation (RACG). Upon receiving a query\\n(instruction), the retriever selects the relevant contexts from a large-scale vector database. Subsequently, the\\nretrieved contexts are merged with the query, and this combined input is fed into the generator (LLM) to\\nproduce the target code solution.\\n5.8\\nRetrieval Augmented\\nLLMs have exhibited impressive capabilities but are hindered by several critical issues such as\\nhallucination [154, 315], obsolescence of knowledge [115], and non-transparent [32], untraceable\\nreasoning processes [80, 106, 276, 329]. While techniques like instruction-tuning (see Section 5.4)\\nand reinforcement learning with feedback (see Section 5.5) mitigate these issues, they also introduce\\nnew challenges, such as catastrophic forgetting and the requirement for substantial computational\\nresources during training [90, 201].\\nRecently, Retrieval-Augmented Generation (RAG) has emerged as an innovative approach to\\novercoming these limitations by integrating knowledge from external databases. Formally defined,\\nRAG denotes a model that, in response to queries, initially sources relevant information from\\nan extensive corpus of documents, and then leverages this retrieved information in conjunction\\nwith the original query to enhance the response‚Äôs quality and accuracy, especially for knowledge-\\nintensive tasks. The RAG framework typically consists of a vector database, a retriever, a re-ranker,\\nand a generator. It is commonly implemented using tools such as LangChain12 and LLamaIndex13.\\nBy performing continuous knowledge updates of the database and the incorporation of domain-\\nspecific data, RAG circumvents the need for re-training LLMs from scratch [80]. Consequently,\\nRAG has substantially advanced LLM performance across a variety of tasks [47, 143].\\nDue to the nature of code, code LLMs are also susceptible to the aforementioned issues that\\naffect general-purpose LLMs. For instance, they may exhibit a hallucination phenomenon when\\ninstructions fall outside the scope of their training data or necessitate the latest programming\\npackages. Given the dynamic nature of publicly available source-code libraries like PyTorch, which\\nundergo frequent expansion and updates, deprecated calling methods can become a significant\\nchallenge. If Code LLMs are not updated in tandem with the latest functions and APIs, this can\\nintroduce potential errors and safety risks. Retrieval-Augmented Code Generation (RACG) stands\\n12LangChain facilitates the development of LLM-powered applications. https://www.langchain.com\\n13LLamaIndex is a leading data framework for building LLM applications. https://www.llamaindex.ai\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 36, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:37\\nas a promising solution to these concerns. A workflow illustration of the RACG is depicted in\\nFigure 13.\\nDespite its potential, the adoption of RAG for code generation remains limited. Drawing in-\\nspiration from the common practice among programmers of referencing related code snippets,\\n[166] introduced a novel retrieval-augmented mechanism with graph neural networks (GNNs),\\ntermed HGNN, which unites the advantages of similar examples retrieval with the generalization\\ncapabilities of generative models for code summarization, which is the reverse process of code\\ngeneration. [205] pioneered a retrieval augmented framework named REDCODER for code gener-\\nation by retrieving and integrating relevant code snippets from a source-code database, thereby\\nproviding supplementary context for the generation process. Subsequently, a retrieval-augmented\\ncode completion framework termed ReACC [171] is proposed to leverage both lexical copying and\\nsemantic referencing of related code, achieving state-of-the-art performance on the CodeXGLUE\\nbenchmark [172]. In the spirit of how programmers often consult textual resources such as code\\nmanuals and documentation to comprehend functionalities, DocPrompting [330] explicitly utilizes\\ncode documentation by retrieving the relevant documentation pieces based on a natural language\\nquery and then generating the target code by blending the query with the retrieved information.\\nMore recently, RepoCoder [309], an iterative retrieval-generation framework, is proposed for\\nenhancing repository-level code completion by effectively utilizing code analogies across different\\nfiles within a repository to inform and improve code suggestions. Furthermore, breaking away\\nfrom reliance on a singular source of retrieval, [242] developed a multi-faceted ‚Äúknowledge soup‚Äù\\nthat integrates web searches, documentation, execution feedback, and evolved code snippets. Then,\\nit incorporates an active retrieval strategy that iteratively refines the query and enriches the\\nknowledge soup, expanding the scope of information available for code generation.\\nDespite these advancements, several limitations in retrieval-augmented code generation warrant\\nfurther exploration: 1) the quality of the retrieved information significantly impacts overall perfor-\\nmance; 2) the effective integration of retrieved code information with the query needs optimization;\\n3) an over-reliance on retrieved information may lead to inadequate responses that fail to address\\nthe query‚Äôs intent; 4) additional retrieved information necessitates larger context windows for the\\nLLM, resulting in increased computational demands.\\n5.9\\nAutonomous Coding Agents\\nThe advent of LLMs has marked the beginning of a new era of potential pathways toward artificial\\ngeneral intelligence (AGI), capturing significant attention in both academia and industry [108, 261,\\n279, 284]. A rapidly expanding array of applications for LLM-based autonomous agents, including\\nAutoGPT [2], AgentGPT [1], BabyAGI [3], and AutoGen [283], underlines the promise of this\\ntechnology.\\nLLM-powered autonomous agents are systems endowed with sophisticated reasoning abilities,\\nleveraging an LLM as a central computational engine or controller. This allows them to formulate\\nand execute problem-solving plans through a series of tool-enabled functions or API calls. Moreover,\\nthese agents are designed to function within a shared environment where they can communicate\\nand engage in cooperative, competitive, or negotiating interactions [104, 261, 283]. The typical\\narchitecture of such an agent encompasses an LLM-based Agent, a memory module, a planning\\ncomponent, and a tool utilization module, as depicted in Figure 14.\\nIn the realm of automated code generation, LLM-powered autonomous agents have demon-\\nstrated remarkable proficiency. For instance, AgentCoder [104] achieved a groundbreaking pass@1\\nof 96.3% on the HumanEval benchmark, forwarding a step closer to the future of automated soft-\\nware development [111]. The innovative meta-programming framework termed MetaGPT [100]\\nintegrates human workflow efficiencies into LLM-based multi-agent collaboration, as shown in\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 37, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:38\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nAgent\\nMemory\\nAction\\nTools\\nPlanning\\nShort-term Memory\\nLong-term Memory\\nCalendar ( )\\nCalculator ( )\\nCode Interpreter ( )\\nSearch ( )\\n...more\\nReflection\\nSelf-critics\\nChain of Thoughts\\nSubgoal Decomposition\\nFig. 14. The general architecture of an LLM-powered autonomous agent system, adapted from [279]. Plan-\\nning: The agent decomposes large tasks into smaller, manageable sub-goals or engages in self-criticism\\nand self-reflection on past actions to learn from mistakes and improve future performance. Memory: This\\ncomponent enables the agent to store and retrieve past information. Tools: The agent is trained to invoke\\nexternal functions or APIs. Action: The agent executes actions, with or without the use of tools, to interact\\nwith the environment. The gray dashed lines represent the data flow within the system.\\nFig. 15. MetaGPT integrates human workflow efficiencies into LLM-based multi-agent collaboration to break\\ndown complex code-related tasks into specific, actionable procedures. These procedures are then assigned to\\nvarious roles, such as Product Manager, Architect, and Engineer played by LLM. The image is sourced from\\nthe original paper [100].\\nFigure 15. Furthermore, [104] introduces AgentCoder, a multi-agent framework composed of three\\nspecialized agents, each with distinct roles and capabilities. These roles include a programmer agent\\nresponsible for code generation, a test designer agent tasked with generating unit test cases, and\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 38, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:39\\na test executor agent that executes the code and provides feedback. This division of labor within\\nAgentCoder promotes more efficient and effective code generation. CodeAct [265] distinguishes\\nitself by utilizing executable Python code to consolidate LLM agent actions within a unified action\\nspace, in contrast to the generation of JSON or textual formats. Additionally, AutoCodeRover [316]\\nis proposed to autonomously resolve GitHub issues for program enhancement.\\nTo address the complexity of tasks within software engineering, two innovative autonomous AI\\nsoftware engineers Devin14[61] and OpenDevin15[199], have been released and rapidly garnered\\nconsiderable interest within the software engineering (SE) and artificial general intelligence (AGI)\\ncommunity. Subsequently, an autonomous system, SWE-agent [124], leverages a language model\\nto interact with a computer to address software engineering tasks, successfully resolving 12.5% of\\nissues on the SWE-bench benchmark [123]. L2MAC [98] has been introduced as the first practical,\\nLLM-based, multi-agent, general-purpose stored-program automatic computer that utilizes a von\\nNeumann architecture, designed specifically for the generation of long and consistent outputs.\\nAt the time of writing this survey, OpenDevin has enhanced CodeAct with bash command-based\\ntools, leading to the release of OpenDevin CodeAct 1.0 [287], which sets a new state-of-the-art\\nperformance on the SWE-Bench Lite benchmark [123].\\nDespite these remarkable advancements, the journey toward fully realized AI software engineers\\nemploying LLM-powered autonomous agents is far from complete [261, 284]. Critical aspects\\nsuch as prompt design, context length, agent count, and toolsets call for further refinement and\\noptimization, especially as problem complexities escalate [111].\\n5.10\\nEvaluation\\nDespite the impressive capabilities of LLMs, they exhibit a range of behaviors that are both beneficial\\nand potentially risky. These behaviors can enhance performance across various downstream tasks\\nbut may also introduce reliability and trustworthiness concerns in LLM deployment [42, 48, 290].\\nConsequently, it is imperative to develop precise evaluation approaches to discern the qualitative\\nand quantitive differences between models, thereby encouraging further advancements in LLM\\ncapabilities.\\nEvaluation strategies for LLMs in code generation mirror those for general-purpose LLMs and\\ncan be divided into three principal categories: metrics-based, human-centered, and LLM-based\\napproaches. Detailed benchmarks for these evaluation strategies are presented in Section 5.1.3 and\\nsummarized in Table 6. Subsequent subsections will provide a thorough analysis of each approach.\\n5.10.1\\nMetrics. The pursuit of effective and reliable automatic evaluation metrics for generated\\ncontent is a long-standing challenge within the field of natural language processing (NLP) [49, 156,\\n203]. At the early stage, most works directly leverage token-matching-based metrics, such as Exact\\nMatch, BLEU [203], ROUGE [156], and METEOR [23], which are prevalent in text generation of\\nNLP, to assess the quality of code generation.\\nWhile these metrics offer a rapid and cost-effective approach for assessing the quality of gener-\\nated code, they often fall short of capturing the syntactical and functional correctness, as well as\\nthe semantic features of the code. To eliminate this limitation, CodeBLEU [221] was introduced, en-\\nhancing the traditional BLEU metric [203] by incorporating syntactic information through abstract\\nsyntax trees (AST) and semantic understanding via data-flow graph (DFG). Despite these improve-\\nments, the metric does not fully resolve issues pertaining to execution errors or discrepancies in\\nthe execution results of the generated code. In light of these challenges, execution-based metrics\\nhave gained prominence for evaluating code generation, including pass@k [48], n@k [151], test\\n14https://www.cognition.ai/introducing-devin\\n15https://github.com/OpenDevin/OpenDevin\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 39, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:40\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 9. The performance comparison of LLMs for code generation on the HumanEval [48] benchmark,\\nmeasured by Pass@1. For models with various sizes, we report only the largest size version of each model\\nwith a magnitude of B parameters. ‚Ä° denotes instruction-tuned models.\\nModel\\nSize\\npass@1 (%)\\nAvailability\\nClosed Source\\nGPT-4o-0513 [197]\\n-\\n91.0\\n[API Access]\\nGPT-4-Turbo-0409 [198]\\n-\\n88.2\\n[API Access]\\nGPT-4-1106 [5]\\n-\\n87.8\\n[API Access]\\nGPT-3.5-Turbo-0125 [196]\\n-\\n76.2\\n[API Access]\\nClaude-3.5-Sonnet [14]\\n-\\n92.0\\n[API Access]\\nClaude-3-Opus [14]\\n-\\n84.9\\n[API Access]\\nClaude-3-Sonnet [14]\\n-\\n73.0\\n[API Access]\\nClaude-3-Haiku [14]\\n-\\n75.9\\n[API Access]\\nGemini-1.5-Pro [220]\\n-\\n84.1\\n[API Access]\\nGemini-1.5-Flash [220]\\n-\\n74.3\\n[API Access]\\nGemini-1.0-Ultra [220]\\n-\\n74.4\\n[API Access]\\nGemini-1.0-Pro [220]\\n-\\n67.7\\n[API Access]\\n‚Ä°PanGu-Coder2 [234]\\n15B\\n61.64\\n-\\nPanGu-Coder [55]\\n2.6B\\n23.78\\n-\\nCodex [48]\\n12B\\n28.81\\nDeprecated\\nPaLM-Coder [54]\\n540B\\n36\\n-\\nAlphaCode [151]\\n1.1B\\n17.1\\n-\\nOpen Source\\n‚Ä°Codestral [181]\\n22B\\n81.1\\n[Checkpoint Download]\\n‚Ä°DeepSeek-Coder-V2-Instruct [331]\\n21B (236B)\\n90.2\\n[Checkpoint Download]\\n‚Ä°Qwen2.5-Coder-Instruct [109]\\n7B\\n88.4\\n[Checkpoint Download]\\nQwen2.5-Coder [109]\\n7B\\n61.6\\n[Checkpoint Download]\\n‚Ä°StarCoder2-Instruct [304]\\n15.5B\\n72.6\\n[Checkpoint Download]\\n‚Ä°CodeGemma-Instruct [59]\\n7B\\n56.1\\n[Checkpoint Download]\\nCodeGemma [59]\\n7B\\n44.5\\n[Checkpoint Download]\\nStarCoder 2 [170]\\n15B\\n46.3\\n[Checkpoint Download]\\n‚Ä°WaveCoder-Ultra [301]\\n6.7B\\n79.9\\n[Checkpoint Download]\\n‚Ä°WaveCoder-Pro [301]\\n6.7B\\n74.4\\n[Checkpoint Download]\\n‚Ä°WaveCoder-DS [301]\\n6.7B\\n65.8\\n[Checkpoint Download]\\nStableCode [210]\\n3B\\n29.3\\n[Checkpoint Download]\\nCodeShell [285]\\n7B\\n34.32\\n[Checkpoint Download]\\n‚Ä°CodeQwen1.5-Chat [249]\\n7B\\n83.5\\n[Checkpoint Download]\\nCodeQwen1.5 [249]\\n7B\\n51.8\\n[Checkpoint Download]\\n‚Ä°DeepSeek-Coder-Instruct [88]\\n33B\\n79.3\\n[Checkpoint Download]\\nDeepSeek-Coder [88]\\n33B\\n56.1\\n[Checkpoint Download]\\nreplit-code [223]\\n3B\\n20.12\\n[Checkpoint Download]\\n‚Ä°MagicoderùëÜ-CL [278]\\n7B\\n70.7\\n[Checkpoint Download]\\n‚Ä°Magicoder-CL [278]\\n7B\\n60.4\\n[Checkpoint Download]\\n‚Ä°WizardCoder [173]\\n33B\\n79.9\\n[Checkpoint Download]\\nCodeFuse [160]\\n34B\\n74.4\\n[Checkpoint Download]\\nPhi-1 [84]\\n1.3B\\n50.6\\n[Checkpoint Download]\\n‚Ä°Code Llama-Instruct [227]\\n70B\\n67.8\\n[Checkpoint Download]\\nCode Llama [227]\\n70B\\n53.0\\n[Checkpoint Download]\\n‚Ä°OctoCoder [187]\\n15.5B\\n46.2\\n[Checkpoint Download]\\nCodeGeeX2 [321]\\n6B\\n35.9\\n[Checkpoint Download]\\n‚Ä°InstructCodeT5+ [269]\\n16B\\n35.0\\n[Checkpoint Download]\\nCodeGen-NL [193]\\n16.1B\\n14.24\\n[Checkpoint Download]\\nCodeGen-Multi [193]\\n16.1B\\n18.32\\n[Checkpoint Download]\\nCodeGen-Mono [193]\\n16.1B\\n29.28\\n[Checkpoint Download]\\nStarCoder [147]\\n15B\\n33.60\\n[Checkpoint Download]\\nCodeT5+ [271]\\n16B\\n30.9\\n[Checkpoint Download]\\nCodeGen2 [192]\\n16B\\n20.46\\n[Checkpoint Download]\\nSantaCoder [9]\\n1.1B\\n14.0\\n[Checkpoint Download]\\nInCoder [77]\\n6.7B\\n15.2\\n[Checkpoint Download]\\nPolyCoder [290]\\n2.7B\\n5.59\\n[Checkpoint Download]\\nCodeParrot [254]\\n1.5B\\n3.99\\n[Checkpoint Download]\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 40, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:41\\ncase average [95], execution accuracy [218], and pass@t [195]. In particular, the pass@k, serving\\nas a principal evaluation metric, assesses the probability that at least one out of ùëòcode samples\\ngenerated by a model will pass all unit tests. An unbiased estimator for pass@k introduced by [48]\\nis defined as:\\npass@k B Etask\\n\"\\n1 ‚àí\\n\\x00ùëõ‚àíùëê\\nùëò\\n\\x01\\n\\x00ùëõ\\nùëò\\n\\x01\\n#\\n(20)\\nwhere ùëõis the total number of sampled candidate code solutions, ùëòis the number of randomly\\nselected code solutions from these candidates for each programming problem, with ùëõ‚â•ùëò, and ùëêis\\nthe count of correct samples within the ùëòselected.\\nNevertheless, these execution-based methods are heavily dependent on the quality of unit tests\\nand are limited to evaluating executable code [307]. Consequently, when unit tests are unavailable,\\ntoken-matching-based metrics are often employed as an alternative for evaluation. Furthermore, in\\nscenarios lacking a ground truth label, unsupervised metrics such as perplexity (PPL) [116] can\\nserve as evaluative tools. Perplexity quantifies an LLM‚Äôs uncertainty in predicting new content,\\nthus providing an indirect measure of the model‚Äôs generalization capabilities and the quality of the\\ngenerated code.\\nTaken together, while the aforementioned methods primarily focus on the functional correctness\\nof code, they do not provide a holistic evaluation that encompasses other critical dimensions such\\nas code vulnerability [189], maintainability [15], readability [34], complexity and efficiency [208],\\nstylistic consistency [178], and execution stability [215]. A comprehensive evaluation framework\\nthat integrates these aspects remains an open area for future research and development in the field\\nof code generation assessment.\\n5.10.2\\nHuman Evaluation. Given the intrinsic characteristics of code, the aforementioned automatic\\nevaluation metrics are inherently limited in their capacity to fully assess code quality. For instance,\\nmetrics specifically designed to measure code style consistency are challenging to develop and\\noften fail to capture this aspect adequately [44]. When it comes to repository-level code generation,\\nthe evaluation of overall code quality is substantially complicated due to the larger scale of the\\ntask, which involves cross-file designs and intricate internal as well as external dependencies, as\\ndiscussed by [22, 239].\\nTo overcome these challenges, conducting human evaluations becomes necessary, as it yields\\nrelatively robust and reliable results. Human assessments also offer greater adaptability across\\nvarious tasks, enabling the simplification of complex and multi-step evaluations. Moreover, human\\nevaluations are essential for demonstrating the effectiveness of certain token-matching-based\\nmetrics, such as CodeBLEU [221]. These studies typically conduct experiments to evaluate the\\ncorrelation coefficient between proposed metrics and quality scores assigned by actual users,\\ndemonstrating their superiority over existing metrics.\\nMoreover, in an effort to better align LLMs with human preferences and intentions, InstructGPT\\n[200] employs human-written prompts and demonstrations, and model output ranking in the\\nfine-tuning of LLMs using reinforcement learning from human feedback (RLHF). Although similar\\nalignment learning techniques have been applied to code generation, the feedback in this domain\\ntypically comes from a compiler or interpreter, which offers execution feedback, rather than from\\nhuman evaluators. Notable examples include CodeRL [139], PPOCoder [238], RLTF [163], and\\nPanGu-Coder2 [234]. Further information on this topic is available in Section 5.5.\\nNonetheless, human evaluations are not without drawbacks, as they can be prone to certain\\nissues that may compromise their accuracy and consistency. For instance, 1) personalized tastes\\nand varying levels of expertise among human evaluators can introduce biases and inconsistencies\\ninto the evaluation process; 2) conducting comprehensive and reliable human evaluations often\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 41, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:42\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nInstruction\\nCode LLM\\n(Generator)\\n(Code) LLM\\n(Judge)\\nWhich response is better?\\nInstruction: {instruction}\\nResponse 1: {response 1}\\nResponse 2: {response 2}\\nPairwise Comparison\\nRate the response on a scale\\nof 1 to 10.\\nInstruction: {instruction}\\nResponse: {response 1/2}\\nSingle Answer Grading\\nResponse 2 is better\\nThe score is 7\\nResponse 1\\nResponse 2\\nFig. 16. The pipeline of (Code) LLM-as-a-judge for evaluating generated code by Code LLMs. There are\\nprimarily two types of approaches: pairwise comparison and single answer grading.\\nnecessitates a substantial number of evaluators, leading to significant expenses and time-consuming;\\n3) the reproducibility of human evaluations is often limited, which presents challenges in extending\\nprevious evaluation outcomes or monitoring the progress of LLMs, as highlighted by [319].\\n5.10.3\\nLLM-as-a-Judge. The powerful instruction-following capabilities of LLMs have stimulated\\nresearchers to innovatively investigate the potential of LLM-based evaluations. The LLM-as-a-Judge\\n[320] refers to the application of advanced proprietary LLMs (e.g., GPT4, Gemini, and Claud 3)\\nas proxies for human evaluators. This involves designing prompts with specific requirements\\nto guide LLMs in conducting evaluations, as demonstrated by AlpacaEval [148] and MT-bench\\n[320]. This method reduces reliance on human participation, thereby facilitating more efficient\\nand scalable evaluations. Moreover, LLMs can offer insightful explanations for the assigned rating\\nscores, thereby augmenting the interpretability of evaluations [319].\\nNevertheless, the use of LLM-based evaluation for code generation remains relatively underex-\\nplored compared with general-purpose LLM. The pipeline of (Code) LLM-as-a-judge for evaluating\\ngenerated code by Code LLMs is depicted in Figure 16. A recent work [332] introduces the ICE-Score\\nevaluation metric, which instructs LLM for code assessments. This approach attains superior corre-\\nlations with functional correctness and human preferences, thereby eliminating the requirement\\nfor test oracles or references. As the capabilities of LLM continue to improve, we anticipate seeing\\nmore research in this direction.\\nDespite their scalability and explainability, the effectiveness of LLM-based evaluation is con-\\nstrained by the inherent limitations of the chosen LLM. Several studies have shown that most LLMs,\\nincluding GPT-4, suffer from several issues, including position, verbosity, and self-enhancement\\nbiases, as well as restricted reasoning ability [320]. Specifically, position bias refers to the tendency\\nof LLMs to disproportionately favor responses that are presented in certain positions, which can\\nskew the perceived quality of answers based on their order of presentation. Meanwhile, verbosity\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 42, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:43\\n1.1\\n5.5\\n13 15\\n33\\n70\\n# Parameters (B)\\n0\\n20\\n40\\n60\\n80\\nPass@1 (%)\\nGPT-3.5-Turbo\\nClaude-3-Sonnet\\nClaude-3-Haiku\\nClaude-3-Opus\\nQwen2.5-Coder\\nCodeGemma\\nStarCoder 2\\nWaveCoder\\nCodeFuse\\nCodeShell\\nCodeQwen1.5\\nDeepSeek-Coder\\nphi-1\\nCode Llama\\nCodeGeeX2\\nCodeGeeX\\nPanGu-Coder\\nCodeGen-NL\\nCodeGen-Multi\\nCodeGen-Mono\\nStarCoder\\nCodeT5+\\nSantaCoder\\nInCoder\\nPolyCoder\\nCodeParrot\\nCodestral\\nQwen2.5-Coder-Instruct\\nStarCoder2-Instruct\\nCodeGemma-Instruct\\nCodeQwen1.5-Chat\\nDeepSeek-Coder-Instruct\\nMagicoderS-CL\\nMagicoder-CL\\nWizardCoder\\nCode Llama-Instruct\\nBase\\nInstruct\\nFig. 17. The performance comparison of LLMs for code generation on the MBPP [17] benchmark, measured by\\nPass@1. For models with various sizes, we report only the largest size version of each model with a magnitude\\nof B parameters.\\nbias describes the inclination of LLMs to prefer lengthier responses, even when these are not\\nnecessarily of higher quality compared to more concise ones. Self-enhancement bias, on the other\\nhand, is observed when LLMs consistently overvalue the quality of the text they generate [319, 320].\\nMoreover, due to their inherent limitations in tackling complex reasoning challenges, LLMs may not\\nbe entirely reliable as evaluators for tasks that require intensive reasoning, such as those involving\\nmathematical problem-solving. However, these shortcomings can be partially addressed through\\nthe application of deliberate prompt engineering and fine-tuning techniques, as suggested by [320].\\n5.10.4\\nEmpirical Comparison. In this section, we present a performance comparison of LLMs for\\ncode generation using the well-regarded HumanEval, MBPP, and the more practical and chal-\\nlenging BigCodeBench benchmarks. This empirical comparison aims to highlight the progressive\\nenhancements in LLM capabilities for code generation. These benchmarks assess an LLM‚Äôs ability to\\ngenerate source code across various levels of difficulty and types of programming tasks. Specifically,\\nHumanEval focuses on complex code generation, MBPP targets basic programming tasks, and\\nBigCodeBench emphasizes practical and challenging programming tasks.\\nDue to the limitations in computational resources we faced, we have cited experimental results\\nfrom original papers or widely recognized open-source leaderboards within the research community,\\nsuch as the HumanEval Leaderboard 16, EvalPlus Leaderboard 17, Big Code Models Leaderboard\\n18, and BigCodeBench Leaderboard 19. We report performance on HumanEval using the pass@1\\nmetric, as shown in Table 9, while MBPP and BigCodeBench results are presented with pass@1 in\\nFigures 17 and 18, respectively.\\nWe offer the following insights:\\n16https://paperswithcode.com/sota/code-generation-on-humaneval\\n17https://evalplus.github.io/leaderboard.html\\n18https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard\\n19https://bigcode-bench.github.io/\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 43, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:44\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nCodeGemma-7B\\nStarCoder 2-15B\\nCodeGemma-Instruct-7B\\nCodeQwen1.5-Chat-7B\\nWaveCoder-Ultra-6.7B\\nCode Llama-70B\\nStarCoder2-Instruct-15B\\nCodeQwen1.5-7B\\nDeepSeek-Coder-33B\\nMagicoder-S-DS-7B\\nPhi-3-Medium-128K-Instruct-14B\\nQwen2.5-Coder-Instruct-7B\\nCodeGeeX4-9B\\nCode Llama-Instruct-70B\\nClaude-3-Haiku\\nGPT-3.5-Turbo-0125\\nDeepSeek-Coder-Instruct-33B\\nCodestral-22B\\nClaude-3-Sonnet\\nGemini-1.5-Flash\\nGPT-4-0613\\nClaude-3-Opus\\nGemini-1.5-Pro\\nGPT-4-Turbo-0409\\nClaude-3.5-Sonnet\\nDeepSeek-Coder-V2-Instruct-21B (236B)\\nGPT-4o-0513\\n40\\n45\\n50\\n55\\n60\\nPass@1 (%)\\n38.3\\n38.4\\n39.3\\n43.6\\n43.7\\n44\\n45.1\\n45.6\\n46.6\\n47.6\\n48.7\\n48.8\\n49\\n49.6\\n50.1\\n50.6\\n51.1\\n52.5\\n53.8\\n55.1\\n57.2\\n57.4\\n57.5\\n58.2\\n58.6\\n59.7\\n61.1\\nGPT-3.5-Turbo-0125\\nClosed Source\\nOpen Source\\nFig. 18. The performance comparison of LLMs for code generation on the BigCodeBench [333] benchmark,\\nmeasured by Pass@1. For models with various sizes, we report only the largest size version of each model\\nwith a magnitude of B parameters.\\n‚Ä¢ The performance gap between open-source and closed-source models across the three bench-\\nmarks is gradually narrowing. For instance, on the HumanEval benchmark, DeepSeek-Coder-\\nV2-Instruct with 21B activation parameters and Qwen2.5-Coder-Instruct 7B achieve 90% and\\n88.4% pass@1, respectively. These results are comparable to the much larger closed-source\\nLLMs, such as Claude-3.5-Sonnet, which achieves 92.0% pass@1. On the MBPP benchmark,\\nQwen2.5-Coder-Instruct 7B with 83.5% pass@1 significantly outperforms GPT-3.5-Turbo\\nwith 52.2% pass@1 and closely rivals the closed-source Claude-3-Opus with 86.4% pass@1.\\nOn the BigCodeBench, DeepSeek-Coder-V2-Instruct achieves 59.7%, surpassing all compared\\nclosed-source and open-source LLMs except for slightly falling behind GPT-4o-0513, which\\nachieves 61.1%.\\n‚Ä¢ Generally, as the number of model parameters increases, the performance of code LLMs\\nimproves. However, Qwen2.5-Coder-Instruct 7B achieves 88.4% pass@1, outperforming larger\\nmodels like StarCoder2-Instruct 15.5B with 72.6% pass@1, DeepSeek-Coder-Instruct 33B\\nwith 79.3% pass@1, and Code Llama-Instruct 70B with 67.8% pass@1 on the HumanEval\\nbenchmark. Similar trends are observed across the other two benchmarks, suggesting that\\ncode LLMs with 7B parameters may be sufficiently capable for code generation task.\\n‚Ä¢ Instruction-tuned models consistently outperform their base (pretrained) counterparts across\\nthe HumanEval and MBPP benchmarks. For instance, Qwen2.5-Coder-Instruct surpasses\\nQwen2.5-Coder by an average of 26.04%, StarCoder2-Instruct improves upon StarCoder 2\\nby an average of 35.20%, and CodeGemma-Instruct enhances CodeGemma by an average\\nof 11.26%. Additionally, DeepSeek-Coder-Instruct outperforms DeepSeek-Coder by an aver-\\nage of 23.71%, while Code Llama-Instruct shows a 13.80% improvement over Code Llama.\\nDetailed results can be found in Table 10. These findings underscore the effectiveness of\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 44, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:45\\nTable 10. The performance improvement of instruction-tuned models over their pretrained counterparts\\non the HumanEval, MBPP, and BigCodeBench benchmarks. The last two rows demonstrate the average\\nimprovement on the first two benchmarks and the three benchmarks, respectively.\\nQwen2.5-Coder-\\nInstruct 7B\\nStarCoder2-\\nInstruct 15.5B\\nCodeGemma-\\nInstruct 7B\\nDeepSeek-Coder-\\nInstruct 33B\\nCode Llama-\\nInstruct 70B\\nHumanEval\\n43.51%\\n56.80%\\n26.07%\\n41.35%\\n27.92%\\nMBPP\\n8.58%\\n13.60%\\n-3.56%\\n6.06%\\n-0.32%\\nBigCodeBench\\n-\\n17.45%\\n2.61%\\n9.66%\\n12.73%\\n# Avg. Imp. H. M.\\n26.04%\\n35.20%\\n11.26%\\n23.71%\\n13.80%\\n# Avg. Imp. H. M. B.\\n-\\n29.28%\\n8.37%\\n19.02%\\n13.44%\\ninstruction tuning, although the quality of the instruction tuning dataset plays a critical role\\nin determining model performance [173, 328].\\n‚Ä¢ Performance on the HumanEval benchmark is nearly saturated. However, MBPP, which\\ninvolves basic programming tasks, and BigCodeBench, which involves more practical and\\nchallenging programming tasks, demand more capable code LLMs. Additionally, while these\\nbenchmarks primarily evaluate the functional correctness of code, they do not provide\\na comprehensive assessment across other critical dimensions. Developing a more holistic\\nevaluation framework that integrates various aspects remains an open area for future research\\nand development in LLMs for code generation evaluation.\\nDiscussion: We discuss certain code LLMs in Table 9 for clarity: (1) General LLMs accessed via\\nAPI are not specifically trained on large code corpora but achieve state-of-the-art performance\\nin code generation, such as Claude-3.5-Sonnet with 92.0% pass@1 on HumanEval benchmark.\\n(2) AlphaCode targets code generation for more complex and unseen problems that require a\\ndeep understanding of algorithms and intricate natural language, such as those encountered in\\ncompetitive programming. The authors of AlphaCode found that large-scale model sampling to\\nnavigate the search space, such as 1M samples per problem for CodeContests, followed by filtering\\nbased on program behavior to produce a smaller set of submissions, is crucial for achieving good and\\nreliable performance on problems that necessitate advanced reasoning. (3) Phi-1 1.3B is a specialized\\nLLM for code, trained on ‚Äútextbook quality‚Äù data from the web (6B tokens) and synthetically\\ngenerated textbooks and exercises using GPT-3.5 (1B tokens). (4) Code Llama 70B is initialized with\\nLlama 2 model weights and continually pre-trained on 1T tokens from a code-heavy dataset and\\nlong-context fine-tuned with approximately 20B tokens. However, Code Llama-Instruct 70B is fine-\\ntuned from Code Llama-Python 70B without long-context fine-tuning, using an additional 260M\\ntokens to better follow human instructions. Surprisingly, these models underperform compared to\\nsmaller parameter Code LLMs like Qwen2.5-Coder-Instruct 7B, DeepSeek-Coder-V2-Instruct 21B,\\nand Codestral 22B across all three benchmarks. The underlying reasons for this discrepancy remain\\nunclear and warrant further exploration. (5) Unlike other open-source Code LLMs, DeepSeek-Coder-\\nV2-Instruct is further pre-trained on DeepSeek-V2 [159], which employs a Mixture-of-Experts (MoE)\\narchitecture with only 21B activation parameters out of 236B parameters, using an additional 6\\ntrillion tokens composed of 60% source code, 10% math corpus, and 30% natural language corpus.\\nFor a comprehensive understanding of MoE in LLMs, please refer to [36].\\n5.11\\nCode LLMs Alignment\\nThe pre-training of LLMs for next-token prediction, aimed at maximizing conditional generation\\nlikelihood across vast textual corpora, equips these models with extensive world knowledge and\\nemergent capabilities [33]. This training approach enables the generation of coherent and fluent\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 45, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:46\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\ntext in response to diverse instructions. Nonetheless, LLMs can sometimes misinterpret human\\ninstructions, produce biased content, or generate factually incorrect information (commonly referred\\nto as hallucinations), which may limit their practical utility [118, 272, 319].\\nAligning LLMs with human intentions and values, known as LLM alignment, has consequently\\nbecome a critical research focus [118, 272]. Key objectives frequently discussed in the context of LLM\\nalignment include robustness, interpretability, controllability, ethicality, trustworthiness, security,\\nprivacy, fairness, and safety. In recent years, significant efforts have been made by researchers\\nto achieve this alignment, employing techniques such as Reinforcement Learning with Human\\nFeedback (RLHF) [200].\\nHowever, the alignment of Code LLMs has not been extensively explored. Compared to text\\ngeneration, aligning code generation with human intentions and values is even more crucial. For\\ninstance, users without programming expertise might prompt Code LLM to generate source code\\nand subsequently execute it on their computers, potentially causing catastrophic damage. Some\\npotential risks include:\\n‚Ä¢ Malware Infection: The code could contain viruses, worms, or trojans that compromise our\\nsystem‚Äôs security.\\n‚Ä¢ Data Loss: It might delete or corrupt important files and data.\\n‚Ä¢ Unauthorized Access: It can create backdoors, allowing attackers to access our system\\nremotely.\\n‚Ä¢ Performance Issues: The code might consume excessive resources, slowing down our\\nsystem.\\n‚Ä¢ Privacy Breaches: Sensitive information, such as passwords or personal data, might be\\nstolen.\\n‚Ä¢ System Damage: It may alter system settings or damage hardware components.\\n‚Ä¢ Network Spread: It could propagate across networks, affecting other devices.\\n‚Ä¢ Financial Loss: If the code is ransomware, it might encrypt data and demand payment for\\ndecryption.\\n‚Ä¢ Legal Consequences: Running certain types of malicious code can lead to legal repercus-\\nsions.\\nAs illustrated, aligning Code LLMs to produce source code consistent with human preferences and\\nvalues is of paramount importance in software development. A recent study [293] provides the first\\nsystematic literature review identifying seven critical non-functional properties of LLMs for code,\\nbeyond accuracy, including robustness, security, privacy, explainability, efficiency, and usability.\\nThis study is highly pertinent to the alignment of Code LLMs. We recommend readers refer to this\\nsurvey for more detailed insights.\\nIn this survey, we identify five core principles that serve as the key objectives for aligning Code\\nLLMs: Green, Responsibility, Efficiency, Safety, and Trustworthiness (collectively referred to as\\nGREST). These principles are examined from a broader perspective. Each category encompasses\\nvarious concepts and properties, which are summarized in Table 11. In the following, we define\\neach principle and briefly introduce a few notable works to enhance understanding.\\nGreen: The Green principle underscores the importance of environmental sustainability in\\nthe development and deployment of LLMs for code generation. This involves optimizing energy\\nconsumption and reducing both the carbon footprint and financial costs associated with training\\nand inference processes. Currently, training, inference, and deployment of Code LLMs are notably\\nresource-intensive. For example, training GPT-3, with its 175 billion parameters, required the\\nequivalent of 355 years of single-processor computing time and consumed 284,000 kWh of energy,\\nresulting in an estimated 552.1 tons of CO2 emissions [228]. Furthermore, a ChatGPT-like application,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:47\\nTable 11. Five core principles serve as the key objectives for Code LLMs alignment: Green, Responsibility,\\nEfficiency, Safety, and Trustworthiness (collectively referred to as GREST).\\nPrinciples\\nInvolved Concepts and Properties\\nGreen\\nEnergy Efficiency: Minimizing computational energy use and reduce environmental impact and financial costs.\\nSustainable Materials: Leveraging eco-friendly infrastructure and servers for code generation, lowering long-term expenses.\\nCarbon Footprint: Reducing emissions associated with model training and inference to enhance efficiency and save costs.\\nResource Optimization: Efficiently utilizing computational resources to minimize waste and reduce expenses in code generation.\\nRecycling Management: Responsibly dispose of hardware used in model development to reduce waste management costs.\\nRenewable Energy: Utilizing renewable energy sources for powering training and inference processes to decrease energy costs.\\nLifecycle Assessment: Evaluating the environmental and financial impacts of models from creation to deployment and disposal.\\nResponsibility\\nEthical Considerations: Adhering to ethical guidelines to ensure responsible use and deployment of generated code.\\nAccountability: Establishing clear lines of responsibility for code generation outcomes and potential impacts.\\nUser Education: Providing resources and guidance to help users understand and responsibly use generated code.\\nImpact Assessment: Evaluating the social and technical implications of code generation to minimize negative effects.\\nRegulatory Compliance: Ensuring that generated code adheres to relevant laws (e.g., copyright) and industry regulations.\\nEfficiency\\nModel Optimization: Streamlining models to reduce computational load and improve speed.\\nPrompt Engineering: Designing effective prompts to generate accurate code efficiently.\\nResource Management: Allocating computational resources wisely to balance speed and cost.\\nInference Optimization: Enhancing the inference process to quickly generate code with minimal latency.\\nParallel Processing: Utilizing parallelism to speed up code generation tasks.\\nCaching Mechanisms: Implementing caching to reuse previous results and reduce redundant computations.\\nEvaluation Metrics: Using precise metrics to assess and improve the efficiency of code outputs.\\nSafety\\nInput Validation: Ensuring inputs (prompts) are safe and sanitized to prevent malicious exploitation.\\nSecurity Audits: Regularly reviewing generated code for vulnerabilities and potential exploits.\\nMonitoring and Logging: Keeping track of generation outputs to quickly identify and address safety issues.\\nUser Access Control: Limiting access to generation capabilities to trusted users to minimize risk.\\nContinuous Updates: Regularly updating models with the latest safety protocols and security patches.\\nEthical Guidelines: Implementing ethical standards to guide safe and responsible code generation.\\nTrustworthiness\\nReliability: Ensuring that generated code consistently meets functional requirements and performs as expected.\\nTransparency: Providing clear explanations of how code is generated to build user confidence.\\nVerification and Testing: Using rigorous testing frameworks to ensure the generated code accuracy and reliability.\\nBias Mitigation: Actively working to identify and reduce biases in code generation to ensure fairness and impartiality.\\nUser Feedback Integration: Continuously incorporating user feedback to refine and improve code generation processes.\\nDocumentation: Providing comprehensive documentation for generated code to enhance understanding and trust.\\nwith an estimated usage of 11 million requests per hour, can produce emissions of 12.8k metric\\ntons of CO2 per year, which is 25 times the carbon emissions associated with training GPT-3\\n[53]. To mitigate these costs, several techniques are often employed, such as the development of\\nspecialized hardware (e.g., Tensor Processing Units (TPUs) and Neural Processing Units (NPUs)),\\nmodel compression methods (e.g., quantization and knowledge distillation), parameter-efficient\\nfine-tuning (PEFT), and the use of renewable energy sources. For instance, Shi et al. [235] applied\\nknowledge distillation to reduce the size of CodeBERT [76] and GraphCodeBERT [86], resulting in\\noptimized models of just 3MB. These models are 160 times smaller than the original large models\\nand significantly reduce energy consumption by up to 184 times and carbon footprint by up to 157\\ntimes. Similarly, Wei et al. [277] utilized quantization techniques for Code LLMs such as CodeGen\\n[193] and Incoder [77] by employing lower-bit integers (e.g., int8). This approach reduced storage\\nrequirements by 67.3% to 70.8%, carbon footprint by 28.8% to 55.0%, and pricing costs by 28.9% to\\n55.0%.\\nResponsibility: The Responsibility principle in the context of Code LLMs underscores the\\nimportance of ethical considerations, fairness, and accountability throughout their lifecycle. This\\ninvolves addressing biases in training data, ensuring fairness and transparency in model decision-\\nmaking, maintaining accountability for outputs, adhering to applicable laws (e.g., copyright),\\nimplementing safeguards against misuse, and providing clear communication about the model‚Äôs\\ncapabilities and limitations. Specifically,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 47, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:48\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n‚Ä¢ Bias Mitigation. Biases in code generation can lead to flawed software and reinforce stereo-\\ntypes, potentially causing significant societal impacts. For example, an Code LLM that in-\\nherits biases from its training data may produce source code/software that inadvertently\\ndiscriminates against certain user groups. This can result in applications that fail to meet the\\ndiverse needs of users, promoting exclusionary practices and reinforcing existing stereotypes\\n[168, 185].\\n‚Ä¢ Fairness and Transparency. A lack of fairness and transparency in Code LLM decision-making\\ncan result in biased or suboptimal code solutions. If the model‚Äôs decision-making process is\\nopaque, developers might unknowingly introduce code that favors specific frameworks or\\nlibraries, thereby limiting innovation and diversity in software development. This opacity\\ncan create unfair advantages and hinder collaborative efforts within tech communities [31].\\n‚Ä¢ Legal Compliance. Compliance with relevant laws, such as licensing and copyright, is crucial\\nwhen using Code LLMs for code generation to avoid legal complications. If an Code LLM\\ngenerates code snippets that inadvertently infringe on existing copyrights, it can lead to\\nlegal disputes and financial liabilities for developers and organizations [292]. Such risks may\\ndiscourage the use of advanced AI tools, thus stifling innovation and affecting growth and\\ncollaboration within the tech community.\\n‚Ä¢ Accountability. Without accountability for code generated by Code LLMs, addressing bugs\\nor security vulnerabilities becomes challenging. If a model generates faulty code leading to\\na security breach, the absence of clear accountability can result in significant financial and\\nreputational damage for companies. This uncertainty can delay critical issue resolution and\\nimpede trust in AI-assisted development [155].\\n‚Ä¢ Misuse Prevention. Failing to implement mechanisms to prevent the misuse of Code LLMs can\\nenable the creation of harmful software. For example, models could be exploited to generate\\nmalware or unauthorized scripts, posing cybersecurity risks. Without proper safeguards,\\nthese models can facilitate malicious activities, threatening both individual and organizational\\nsecurity [184].\\n‚Ä¢ Clear Communication. Without clear communication about a model‚Äôs capabilities and limita-\\ntions, developers may misuse the model or overestimate its abilities. Relying on the model\\nto generate complex, mission-critical code without human oversight can lead to significant\\nsoftware failures. Misunderstanding its limitations can result in faulty implementations and\\nlost productivity [226].\\nTo adhere to this principle, potential mitigation methods include bias detection and mitigation,\\nquantification and evaluation, and adherence to ethical guidelines. Liu et al. [168] propose a new\\nparadigm for constructing code prompts, successfully uncovering social biases in code generation\\nmodels, and developing a dataset along with three metrics to evaluate overall social bias. Recently,\\nXu et al. [292] introduced LiCoEval, an evaluation benchmark for assessing the license compliance\\ncapabilities of LLMs. Additionally, incorporating diverse perspectives in development teams and\\nengaging with stakeholders from various communities can further align Code LLM outputs with\\nethical standards and societal values.\\nEfficiency: The Efficiency principle emphasizes optimizing the performance and speed of Code\\nLLMs for code generation while minimizing the computational resources required for training\\nand inference. For instance, training the GPT-3 model, which consists of 175 billion parameters,\\ndemands substantial resources. It requires approximately 1,024 NVIDIA V100 GPUs, costing around\\n4.6 million and taking approximately 34 days to complete the training process. To address these\\nchallenges, various techniques are employed, including model compression methods such as\\npruning, quantization, and knowledge distillation. Additionally, optimized algorithms like AdamW,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 48, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:49\\nparallel strategies such as tensor, pipeline, and data parallelism, and parameter-efficient fine-tuning\\n(PEFT) (see Section 5.4.2) are often utilized. For a comprehensive and detailed discussion on methods\\nto enhance the efficiency of Code LLMs for code generation, please refer to Section 4.5.2, ‚ÄúEfficiency\\nEnhancement‚Äù, in [293].\\nSafety: The Safety principle of Code LLMs is of utmost importance due to their potential\\nto introduce vulnerabilities, errors, or privacy breaches into software systems. Ensuring safety\\ninvolves comprehensive testing and validation processes to detect and mitigate these risks. For\\ninstance, attackers might compromise the training process of LLMs by injecting malicious examples\\ninto the training data, a method known as data poisoning attacks [231]. Even when attackers\\nlack access to the training process, they may employ techniques like the black-box inversion\\napproach introduced by Hajipour et al. [91]. This method uses few-shot prompting to identify\\nprompts that coax black-box code generation models into producing vulnerable code. Furthermore,\\nYang et al. [294] and Al-Kaswan et al. [8] reveals that Code LLMs, such as CodeParrot [73], can\\nmemorize training data, potentially outputting personally identifiable information like emails,\\nnames, and IP addresses, thereby posing significant privacy risks. Additionally, Yuan et al. [302]\\ndemonstrate that engaging with ChatGPT and GPT-4 in non-natural languages can circumvent\\nsafety alignment measures, leading to unsafe outcomes, such as ‚ÄúThe steps involved in stealing\\nmoney from a bank.‚Äù. To bolster the safety of LLMs in code generation, it is crucial to detect and\\neliminate privacy-related information from training datasets. For example, approaches outlined in\\n[77] and [9] utilize carefully crafted regular expressions to identify and remove private information\\nfrom training data. To counteract black-box inversion, implementing prompt filtering mechanisms\\nis recommended to identify and block prompts that might result in insecure code generation.\\nMoreover, adversarial training can enhance the model‚Äôs resilience to malicious prompts. Employing\\nreinforcement learning methods can further align Code LLMs with human preferences, thereby\\nreducing the likelihood of producing harmful outputs.\\nTrustworthiness: The Trustworthiness principle focuses on developing Code LLMs that users\\ncan depend on for accurate and reliable code generation, which is crucial for their acceptance and\\nwidespread adoption. Achieving this requires ensuring model transparency, providing explanations\\nfor decisions, and maintaining consistent performance across various scenarios. For instance,\\nJi et al. [120] propose a causal graph-based representation of prompts and generated code to\\nidentify the causal relationships between them. This approach offers insights into the effectiveness\\nof Code LLMs and assists end-users in understanding the generation. Similarly, Palacio et al.\\n[202] introduce ASTxplainer, a tool that extracts and aggregates normalized model logits within\\nAbstract Syntax Tree (AST) structures. This alignment of token predictions with AST nodes\\nprovides visualizations that enhance end-user understanding of Code LLM predictions. Therefore,\\nby prioritizing trustworthiness, we can bolster user confidence and facilitate the integration of\\nCode LLMs into diverse coding environments. By adhering to the aforementioned principles as key\\nobjectives for aligning Code LLMs, researchers and developers can create LLMs for code generation\\nthat are not only capable but also ethical, sustainable, and user-centric.\\n5.12\\nApplications\\nCode LLMs have been integrated with development tools and platforms, such as integrated de-\\nvelopment environments (IDEs) and version control systems, improving programming efficiency\\nsubstantially. In this section, we will briefly introduce several widely used applications as coding\\nassistants. The statistics of these applications are provided in Table 12.\\nGitHub Copilot. GitHub Copilot, powered by OpenAI‚Äôs Codex, is an AI pair programmer that\\nhelps you write better code faster. Copilot suggests whole lines or blocks of code as you type, based\\non the context provided by your existing code and comments. It‚Äôs trained on a dataset that includes\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 49, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:50\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 12. The overview of code assistant applications powered by LLMs. The column labeled ‚ÄòPLs‚Äô and ‚ÄòIDEs‚Äô\\nindicate programming languages and integrated development environments, respectively [307].\\nInstitution\\nProducts\\nModel\\nSupported Features\\nSupported PLs\\nSupported IDEs\\nGitHub & OpenAI\\nGitHub Copilot [48]\\nCodex\\nCode Completions, Code Generation,\\nCoding Questions Answering,\\nCode Refactoring, Code Issues Fix,\\nUnit Test Cases Generation,\\nCode Documentation Generation\\nJava, Python, JavaScript, TypeScript,\\nPerl, R, PowerShell, Rust, SQL, CSS,\\nRuby, Julia, C#, PHP, Swift, C++,Go,\\nHTML, JSON, SCSS, .NET, Less,\\nT-SQL, Markdown\\nVisual Studio, VS Code, Neovim,\\nJetBrains IDE\\nZhipu AI\\nCodeGeeX [321]\\nCodeGeeX\\nCode Generation, Code Translation,\\nCode Completion, Code Interpretation,\\nCode Bugs Fix, Comment Generation,\\nAI Chatbot\\nPHP, Go, C, C#, C++, Rust, Perl, CSS,\\nJava, Python, JavaScript, TypeScript,\\nObjective C++, Objective C, Pascal,\\nHTML, SQL, Kotlin, R, Shell, Cuda,\\nFortran, Tex, Lean, Scala\\nClion, RubyMine, AppCode, Aqua,\\nIntelliJ IDEA, VS Code, PyCharm,\\nAndroid Studio, WebStorm, Rider,\\nGoLand, DataGrip, DataSpell\\nAmazon\\nCodeWhisperer [12]\\n‚àí\\nCode Completion, Code Explanation,\\nCode Translation,\\nCode Security Identification,\\nCode Suggestion\\nJava, Python, TypeScript, JavaScript,\\nC#\\nJetBrains IDE, VS Code, AWS Cloud9,\\nAWS Lambda\\nCodeium\\nCodeium [60]\\n‚àí\\nCode Completion, Bug Detection,\\nCode Suggestions, AI Chatbot,\\nTest Type Generation,\\nTest Plan Creation,\\nCodebase Search\\nMore than 70 languages in total,\\nincluding but not limited to:\\nC, C#, C++, Dart, CSS, Go, Elixir,\\nHTML, Haskell, Julia, Java, JavaScript,\\nLisp, Kotlin, Lua, Objective-C,\\nPerl, Pascal, PHP, Protobuf,\\nR, Python, Ruby, Scala, Rust,\\nSwift, SQL, TS, Vue\\nJetBrains, VSCode, Visual Studio,\\nColab, Jupyter, Deepnote,\\nNotebooks, Databricks, Chrome,\\nVim, Neovim, Eclipse, Emacs,\\nVSCode Web IDEs, Sublime Text\\nHuawei\\nCodeArts Snap [234]\\nPanGu-Coder\\nCode Generation, Code Explanation\\nResearch and Development Knowledge\\nQuestion and Answer\\nCode Comment, Code Debug\\nUnit Test Case Generation\\nJava, Python\\nPyCharm, VS Code, IntelliJ\\nTabnine\\nTabNine [246]\\n‚àí\\nCode Generation, Code Completion,\\nCode Explanation, Bug Fix,\\nCode Recommendation, Code Refactoring,\\nCode Test Generation,\\nDocstring Generation\\nPython, Javascript, Java, TypeScript,\\nHTML, Haskell, Matlab, Kotlin, Sass,\\nGo, PHP, Ruby, C, C#, C++, Swift,\\nRust, CSS, Perl, Angular, Dart, React,\\nObjective C, NodeJS, Scala,\\nSublime, PyCharm, Neovim, Rider,\\nVS Code, IntelliJ IDE, Visual Studio,\\nPhpStorm, Vim, RubyMine, DataGrip,\\nAndroid Studio, WebStorm, Emacs,\\nClion, Jupyter Notebook, JupyterLab,\\nEclipse, GoLand, AppCode\\nReplit\\nReplit[222]\\nreplit-code\\nCode Completion, Code Editing,\\nCode Generation, Code Explanation,\\nCode Suggestion, Code Test Generation\\nC#, Bash, C, CSS, C++, Java, Go,\\nHTML, JavaScript, Perl, PHP,\\nRuby, Python, R, SQL, Rust\\n‚àí\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 50, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:51\\nFig. 19. An exemplar of GitHub Copilot to demonstrate how to use development tools powered by LLMs,\\nincluding powerful GPT 4o, o1-preview (Preview), and o1-mini (Preview). To illustrate its capabilities, we input\\nthe description of the ‚Äú5. Longest Palindromic Substring‚Äù problem from LeetCode into Copilot‚Äôs chat box. The\\ncode generated by Copilot is then submitted to the online judge platform, where it is successfully accepted.\\na significant portion of the public code available on GitHub, which enables it to understand a wide\\nrange of programming languages and coding styles. Copilot not only improves productivity but\\nalso serves as a learning tool by providing programmers with examples of how certain functions\\ncan be implemented or how specific problems can be solved.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 51, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:52\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nCodeGeeX. CodeGeeX stands out as a multifaceted programming assistant, proficient in code\\ncompletion, comment generation, code translation, and developer interactions. Its underlying code\\ngeneration LLM has been refined with extensive training on vast amounts of code data, exhibiting\\nsuperior performance on benchmarks like HumanEval, HumanEval-X, and DS1000. Renowned for\\nsupporting multilingual code generation, CodeGeeX plays a pivotal role in enhancing the efficiency\\nof code development.\\nCodeWhisperer. Amazon‚Äôs CodeWhisperer is a versatile, machine learning-driven code genera-\\ntor that offers on-the-fly code recommendations. Tailored to your coding patterns and comments,\\nCodeWhisperer provides personalized suggestions that range from succinct comments to complex\\nfunctions, all aimed at streamlining your coding workflow.\\nCodeium. Codeium is an AI-accelerated coding toolkit that offers a suite of functions, including\\ncode completion, explanation, translation, search, and user chatting. Compatible with over 70\\nprogramming languages, Codeium delivers fast and cutting-edge solutions to coding challenges,\\nsimplifying the development process for its users.\\nCodeArts Snap. Huawei‚Äôs CodeArts Snap is capable of generating comprehensive function-level\\ncode from both Chinese and English descriptions. This tool not only reduces the monotony of\\nmanual coding but also efficiently generates test code, in addition to providing automatic code\\nanalysis and repair services.\\nTabnine. Tabnine is an AI coding assistant that empowers development teams to leverage\\nAI for streamlining the software development lifecycle while maintaining strict standards for\\nprivacy, security, and compliance. With a focus on enhancing coding efficiency, code quality, and\\ndeveloper satisfaction, Tabnine offers AI-driven automation that is tailored to the needs of your\\nteam. Supporting over one million developers worldwide, Tabnine is applicable across various\\nindustries.\\nReplit. Replit is a multifunctional platform that caters to a diverse array of software development\\nneeds. As a complimentary online IDE, it facilitates code collaboration, and cloud services, and\\nfosters a thriving developer community. Replit also enables users to compile and execute code in\\nmore than 50 programming languages directly within a web browser, eliminating the need for local\\nsoftware installations.\\nTo illustrate the use of development tools powered by LLMs, we employ GitHub Copilot within\\nVisual Studio Code (VS Code) as our example. Note that\\n1‚óãFor details on using the GitHub Copilot extension in VS Code, please refer to the useful\\ndocument at https://code.visualstudio.com/docs/copilot/overview.\\n2‚óãIf you would like to get free access to Copilot as a student, teacher, or open-source maintainer,\\nplease refer to this tutorial at https://docs.github.com/en/copilot/managing-copilot/managing-\\ncopilot-as-an-individual-subscriber/managing-your-copilot-subscription/getting-free-access-\\nto-copilot-as-a-student-teacher-or-maintainer and GitHub education application portal at\\nhttps://education.github.com/discount_requests/application.\\nAs depicted in the upper section of Figure 19, users can interact with Copilot through the chat box in\\nthe lower left corner, where they can inquire about various coding-related tasks. This feature is now\\nsupported by the advanced capabilities of GPT-4o, o1-preview (Preview), and o1-mini (Preview).\\nFrom the generated content, Copilot demonstrates the ability to plan solutions to coding problems.\\nIt can write code and subsequently explain the generated code to enhance user comprehension.\\nWithin the right-side workspace, users can engage in inline chat conversations to generate or\\nrefactor source code, conduct code explanations, fix coding errors, resolve issues encountered\\nduring terminal command executions, produce documentation comments, and generate unit tests.\\nTo illustrate its capabilities, we input the description of the ‚Äú5. Longest Palindromic Substring‚Äù\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 52, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:53\\nproblem from LeetCode into Copilot‚Äôs chat box. The code generated by Copilot is then submitted\\nto the online judge platform, where it is successfully accepted, as shown at the lower section of\\nFigure 19.\\n6\\nCHALLENGES & OPPORTUNITIES\\nAccording to our investigations, the LLMs have revolutionized the paradigm of code generation\\nand achieved remarkable performance. Despite this promising progress, there are still numerous\\nchallenges that need to be addressed. These challenges are mainly caused by the gap between\\nacademia and practical development. For example, in academia, the HumanEval benchmark has\\nbeen established as a de facto standard for evaluating the coding proficiency of LLMs. However,\\nmany works have illustrated the evaluation of HumanEval can‚Äôt reflect the scenario of practical\\ndevelopment [68, 72, 123, 162]. In contrast, these serious challenges offer substantial opportunities\\nfor further research and applications. In this section, we pinpoint critical challenges and identify\\npromising opportunities, aiming to bridge the research-practicality divide.\\nEnhancing complex code generation at repository and software scale. In practical de-\\nvelopment scenarios, it often involves a large number of complex programming problems of\\nvarying difficulty levels [151, 311]. While LLMs have shown proficiency in generating function-\\nlevel code snippets, these models often struggle with more complex, unseen programming problems,\\nrepository- and software-level problems that are commonplace in real-world software develop-\\nment. To this end, it requires strong problem-solving skills in LLM beyond simply functional-level\\ncode generation. For example, AlphaCode [151] achieved an average ranking in the top 54.3% in\\nprogramming competitions where an understanding of algorithms and complex natural language is\\nrequired to solve competitive programming problems. [123] argues that existing LLMs can‚Äôt resolve\\nreal-world GitHub issues well since the best-performing model, Claude 2, is able to solve a mere\\n1.96% of the issues. The reason for poor performance is mainly attributed to the weak reasoning\\ncapabilities [105], complex internal- and external- dependencies [22], and context length limitation\\nof LLMs [22]. Therefore, the pursuit of models that can handle more complex, repository- and\\nsoftware-level code generation opens up new avenues for automation in software development\\nand makes programming more productive and accessible.\\nInnovating model architectures tuned to code structures. Due to their scalability and effec-\\ntiveness, Transformer-based LLM architectures have become dominant in solving code generation\\ntask. Nevertheless, they might not be optimally designed to capture the inherent structure and\\nsyntax of programming languages (PLs) [85, 86, 134, 175]. Code has a highly structured nature,\\nwith a syntax that is more rigid than natural language. This presents a unique challenge for LLMs,\\nwhich are often derived from models that were originally designed for natural language processing\\n(NLP). The development of novel model architectures that inherently understand and integrate the\\nstructural properties of code represents a significant opportunity to improve code generation and\\ncomprehension. Innovations such as tree-based neural networks [183], which mirror the abstract\\nsyntax tree (AST) representation of code, can offer a more natural way for models to learn and\\ngenerate programming languages. Additionally, leveraging techniques from the compiler theory,\\nsuch as intermediate representations (IR) [152], could enable models to operate on a more abstract\\nand generalizable level, making them effective across multiple programming languages [207]. By\\nexploring architectures beyond the traditional sequential models, researchers can unlock new\\npotentials in code generation.\\nCurating high-quality code data for pre-training and fine-tuning of LLMs. The efficacy\\nof LLMs largely depends on the quality and diversity of code datasets used during pre-training and\\nfine-tuning phases [133, 280, 328]. Currently, there is a scarcity of large, high-quality datasets that\\nencompass a wide range of programming tasks, styles, and languages. This limitation constrains the\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 53, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:54\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nability of LLMs to generalize across unseen programming tasks, different coding environments, and\\nreal-world software development scenarios. The development of more sophisticated data acquisition\\ntechniques, such as automated code repositories mining [158], advanced filtering algorithms, and\\ncode data synthesis [165] (see Section 5.2), can lead to the creation of richer datasets. Collaborations\\nwith industry partners (e.g., GitHub) could also facilitate access to proprietary codebases, thereby\\nenhancing the practical relevance of the training material. Furthermore, the adoption of open-source\\nmodels for dataset sharing can accelerate the collective effort to improve the breadth and depth of\\ncode data available for LLM research.\\nDeveloping comprehensive benchmarks and metrics for coding proficiency evaluation\\nin LLMs. Current benchmarks like HumanEval may not capture the full spectrum of coding\\nskills required for practical software development [191]. Additionally, metrics often focus on\\nsyntactic correctness or functional accuracy, neglecting aspects such as code efficiency [208],\\nstyle [44], readability [34], or maintainability [15]. The design of comprehensive benchmarks that\\nsimulate real-world software development challenges could provide a more accurate assessment\\nof LLMs‚Äô coding capabilities. These benchmarks should include diverse programming tasks of\\nvarying difficulty levels, such as debugging [325], refactoring [237], and optimization [112], and\\nshould be complemented by metrics that evaluate qualitative aspects of code. The establishment of\\ncommunity-driven benchmarking platforms could facilitate continuous evaluation and comparison\\nof LLMs for code generation across the industry and academia.\\nSupport for low-resource, low-level, and domain-specific programming languages. LLMs\\nare predominantly trained in popular high-level programming languages, leaving low-resource, low-\\nlevel, and domain-specific languages underrepresented. This lack of focus restricts the applicability\\nof LLMs in certain specialized fields and systems programming [250]. Intensifying research on\\ntransfer learning and meta-learning approaches may enable LLMs to leverage knowledge from\\nhigh-resource languages to enhance their performance on less common ones [38, 46]. Additionally,\\npartnerships with domain experts can guide the creation of targeted datasets and fine-tuning\\nstrategies to better serve niche markets. The development of LLMs with a capacity for multilingual\\ncode generation also presents a significant opportunity for broadening the scope of applications.\\nContinuous learning for LLMs to keep pace with evolving coding knowledge. The\\nsoftware development landscape is continuously evolving, with new languages, frameworks, and\\nbest practices emerging regularly. LLMs risk becoming outdated if they cannot adapt to these\\nchanges and incorporate the latest programming knowledge [115, 264]. While retrieval augmented\\ncode generation mitigates these issues, the performance is limited by the quality of the retrieval\\ncontext While retrieval-augmented code generation offers a partial solution to these issues, its\\neffectiveness is inherently constrained by the quality of retrieved context. [171, 309, 330]. Therefore,\\nestablishing mechanisms for continuous learning and updating of LLMs can help maintain their\\nrelevance over time. This could involve real-time monitoring of code repositories to identify trends\\nand innovations, as well as the creation of incremental learning systems that can assimilate new\\ninformation without forgetting previously acquired knowledge. Engaging the LLMs in active\\nlearning scenarios where they interact with human developers may also foster ongoing knowledge\\nacquisition.\\nEnsuring code safety and aligning LLM outputs with human coding preferences. Ensuring\\nthe safety and security of code generated by LLMs is a paramount concern, as is their ability to\\nalign with human preferences and ethical standards. Current models may inadvertently introduce\\nvulnerabilities or generate code that does not adhere to desired norms [48, 293]. Research into\\nthe integration of formal verification tools within the LLM pipeline can enhance the safety of the\\nproduced code. Additionally, developing frameworks for alignment learning that capture and reflect\\nhuman ethical preferences can ensure that the code generation process aligns with societal values\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 54, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:55\\n[200, 212]. Transparent and explainable AI methodologies can also contribute to building trust in\\nthe LLM-generated code by making the decision-making process more accessible to developers.\\n7\\nCONCLUSION\\nIn this survey, we provide a systematic literature review, serving as a valuable reference for\\nresearchers investigating the cutting-edge progress in LLMs for code generation. A thorough\\nintroduction and analysis for data curation, the latest advances, performance evaluation, ethical\\nimplications, environmental impact, and real-world applications are illustrated. In addition, we\\npresent a historical overview of the evolution of LLMs for code generation in recent years and\\noffer an empirical comparison using the widely recognized HumanEval, MBPP, and the more\\npractical and challenging BigCodeBench benchmarks to highlight the progressive enhancements\\nin LLM capabilities for code generation. Critical challenges and promising opportunities regarding\\nthe gap between academia and practical development are also identified for future investigation.\\nFurthermore, we have established a dedicated resource website to continuously document and\\ndisseminate the most recent advances in the field. We hope this survey can contribute to a compre-\\nhensive and systematic overview of LLM for code generation and promote its thriving evolution.\\nWe optimistically believe that LLM will ultimately change all aspects of coding and automatically\\nwrite safe, helpful, accurate, trustworthy, and controllable code, like professional programmers,\\nand even solve coding problems that currently cannot be solved by humans.\\nREFERENCES\\n[1] 2023. AgentGPT: Assemble, configure, and deploy autonomous AI Agents in your browser. https://github.com/\\nreworkd/AgentGPT.\\n[2] 2023. AutoGPT is the vision of accessible AI for everyone, to use and to build on. https://github.com/Significant-\\nGravitas/AutoGPT.\\n[3] 2023. BabyAGI. https://github.com/yoheinakajima/babyagi.\\n[4] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach,\\nAmit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: A highly capable language model\\nlocally on your phone. arXiv preprint arXiv:2404.14219 (2024).\\n[5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\\n(2023).\\n[6] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A Transformer-based Approach for\\nSource Code Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\\n4998‚Äì5007.\\n[7] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified pre-training for program\\nunderstanding and generation. arXiv preprint arXiv:2103.06333 (2021).\\n[8] Ali Al-Kaswan, Maliheh Izadi, and Arie Van Deursen. 2024. Traces of memorisation in large language models for\\ncode. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. 1‚Äì12.\\n[9] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas\\nMuennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. SantaCoder: don‚Äôt reach for the stars! arXiv preprint\\narXiv:2301.03988 (2023).\\n[10] Miltiadis Allamanis and Charles Sutton. 2014. Mining idioms from source code. In Proceedings of the 22nd acm sigsoft\\ninternational symposium on foundations of software engineering. 472‚Äì483.\\n[11] Google DeepMind AlphaCode Team. 2023. AlphaCode 2 Technical Report. https://storage.googleapis.com/deepmind-\\nmedia/AlphaCode2/AlphaCode2_Tech_Report.pdf.\\n[12] Amazon. 2022. What is CodeWhisperer? https://docs.aws.amazon.com/codewhisperer/latest/userguide/what-is-\\ncwspr.html.\\n[13] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019.\\nMathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. In Proceedings of the\\n2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers). 2357‚Äì2367.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:56\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[14] Anthropic. 2024.\\nThe Claude 3 Model Family: Opus, Sonnet, Haiku.\\nhttps://www-cdn.anthropic.com/\\nde8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.\\n[15] Luca Ardito, Riccardo Coppola, Luca Barbato, and Diego Verga. 2020. A tool-based perspective on software code\\nmaintainability metrics: a systematic literature review. Scientific Programming 2020 (2020), 1‚Äì26.\\n[16] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad,\\nShiqi Wang, Qing Sun, Mingyue Shang, et al. 2022. Multi-lingual evaluation of code generation models. arXiv preprint\\narXiv:2210.14868 (2022).\\n[17] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie\\nCai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732\\n(2021).\\n[18] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450\\n(2016).\\n[19] Hannah McLean Babe, Sydney Nguyen, Yangtian Zi, Arjun Guha, Molly Q Feldman, and Carolyn Jane Anderson. 2023.\\nStudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code. arXiv:2306.04556 [cs.LG]\\n[20] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang,\\net al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023).\\n[21] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna\\nGoldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv\\npreprint arXiv:2212.08073 (2022).\\n[22] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok,\\nShashank Shet, et al. 2023. Codeplan: Repository-level coding using llms and planning. arXiv preprint arXiv:2309.12499\\n(2023).\\n[23] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation\\nwith human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine\\ntranslation and/or summarization. 65‚Äì72.\\n[24] Enrico Barbierato, Marco L Della Vedova, Daniele Tessera, Daniele Toti, and Nicola Vanoli. 2022. A methodology for\\ncontrolling bias and fairness in synthetic data generation. Applied Sciences 12, 9 (2022), 4619.\\n[25] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with\\ncode-generating models. Proceedings of the ACM on Programming Languages 7, OOPSLA1 (2023), 85‚Äì111.\\n[26] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi\\nDu, Zhe Fu, et al. 2024. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint\\narXiv:2401.02954 (2024).\\n[27] Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan, Fangxin Lu, Zili Zhang, Yulei Sui, Xuanhua Shi,\\nand Hai Jin. 2024. Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler\\nFeedback. arXiv preprint arXiv:2403.16792 (2024).\\n[28] Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini Kalliamvakou, Travis Lowdermilk, and\\nIdan Gazit. 2022. Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools.\\nQueue 20, 6 (2022), 35‚Äì57.\\n[29] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy,\\nKyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. arXiv\\npreprint arXiv:2204.06745 (2022).\\n[30] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive\\nLanguage Modeling with Mesh-Tensorflow. https://doi.org/10.5281/zenodo.5297715 If you use this software, please\\ncite it using these metadata..\\n[31] Veronika Bogina, Alan Hartman, Tsvi Kuflik, and Avital Shulner-Tal. 2022. Educating software and AI stakeholders\\nabout algorithmic fairness, accountability, transparency and ethics. International Journal of Artificial Intelligence in\\nEducation (2022), 1‚Äì26.\\n[32] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,\\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models.\\narXiv preprint arXiv:2108.07258 (2021).\\n[33] Tom B Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).\\n[34] Raymond PL Buse and Westley R Weimer. 2009. Learning a metric for code readability. IEEE Transactions on software\\nengineering 36, 4 (2009), 546‚Äì558.\\n[35] Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, and Jiayi Huang. 2024. Shortcut-connected Expert\\nParallelism for Accelerating Mixture-of-Experts. arXiv preprint arXiv:2404.05019 (2024).\\n[36] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. A survey on mixture of experts.\\narXiv preprint arXiv:2407.06204 (2024).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:57\\n[37] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts,\\nTom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th\\nUSENIX Security Symposium (USENIX Security 21). 2633‚Äì2650.\\n[38] Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg,\\nAbhinav Jangda, and Arjun Guha. 2023. Knowledge Transfer from High-Resource to Low-Resource Programming\\nLanguages for Code LLMs. arXiv preprint arXiv:2308.09895 (2023).\\n[39] Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho\\nYee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. 2022. A scalable and extensible approach to\\nbenchmarking nl2code for 18 programming languages. arXiv preprint arXiv:2208.08227 (2022).\\n[40] Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, and Hua Wu. 2022. ERNIE-Code: Beyond english-centric\\ncross-lingual pretraining for programming languages. arXiv preprint arXiv:2212.06742 (2022).\\n[41] Shubham Chandel, Colin B Clement, Guillermo Serrato, and Neel Sundaresan. 2022. Training and evaluating a jupyter\\nnotebook data science assistant. arXiv preprint arXiv:2201.12901 (2022).\\n[42] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang,\\nYidong Wang, et al. 2024. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems\\nand Technology 15, 3 (2024), 1‚Äì45.\\n[43] Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA model for code generation. https://github.\\ncom/sahil280114/codealpaca.\\n[44] Binger Chen and Ziawasch Abedjan. 2023. DUETCS: Code Style Transfer through Generation and Retrieval. In 2023\\nIEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2362‚Äì2373.\\n[45] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. Codet:\\nCode generation with generated tests. arXiv preprint arXiv:2207.10397 (2022).\\n[46] Fuxiang Chen, Fatemeh H Fard, David Lo, and Timofey Bryksin. 2022. On the transferability of pre-trained language\\nmodels for low-resource programming languages. In Proceedings of the 30th IEEE/ACM International Conference on\\nProgram Comprehension. 401‚Äì412.\\n[47] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented\\ngeneration. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 17754‚Äì17762.\\n[48] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\\nYuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv\\npreprint arXiv:2107.03374 (2021).\\n[49] Stanley F Chen, Douglas Beeferman, and Roni Rosenfeld. 1998. Evaluation metrics for language models. (1998).\\n[50] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling\\ncomputation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588 (2022).\\n[51] Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, and Denny Zhou. 2023. Teaching large language models to self-debug.\\narXiv preprint arXiv:2304.05128 (2023).\\n[52] Xinyun Chen, Chang Liu, and Dawn Song. 2018. Tree-to-tree neural networks for program translation. Advances in\\nneural information processing systems 31 (2018).\\n[53] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. 2023. Reducing\\nthe Carbon Impact of Generative AI Inference (today and in 2035). In Proceedings of the 2nd workshop on sustainable\\ncomputer systems. 1‚Äì7.\\n[54] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.\\nJournal of Machine Learning Research 24, 240 (2023), 1‚Äì113.\\n[55] Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng\\nXiao, Bo Shen, Lin Li, et al. 2022. Pangu-coder: Program synthesis with function-level language modeling. arXiv\\npreprint arXiv:2207.11280 (2022).\\n[56] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa\\nDehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine\\nLearning Research 25, 70 (2024), 1‚Äì53.\\n[57] Colin B Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan. 2020. PyMT5:\\nmulti-mode translation of natural language and Python code with transformers. arXiv preprint arXiv:2010.03150\\n(2020).\\n[58] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\\nTworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint\\narXiv:2110.14168 (2021).\\n[59] CodeGemma Team, Ale Jakse Hartman, Andrea Hu, Christopher A. Choquette-Choo, Heri Zhao, Jane Fine, Jeffrey\\nHui, Jingyue Shen, Joe Kelley, Joshua Howland, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Nam Nguyen, Paul Michel,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:58\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nPeter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Siqi Zuo, Tris Warkentin, and Zhitao\\net al. Gong. 2024. CodeGemma: Open Code Models Based on Gemma. (2024). https://goo.gle/codegemma\\n[60] Codeium. 2023. Free, ultrafast Copilot alternative for Vim and Neovim. https://github.com/Exafunction/codeium.vim.\\n[61] Cognition. 2024. Introducing Devin, the first AI software engineer. https://www.cognition.ai/introducing-devin.\\n[62] Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. The Journal of\\nMachine Learning Research 11 (2010), 3053‚Äì3096.\\n[63] Cognitive Computations. 2023. oa_leet10k. https://huggingface.co/datasets/cognitivecomputations/oa_leet10k.\\n[64] Leonardo De Moura and Nikolaj Bj√∏rner. 2008. Z3: An efficient SMT solver. In International conference on Tools and\\nAlgorithms for the Construction and Analysis of Systems. Springer, 337‚Äì340.\\n[65] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized\\nllms. Advances in Neural Information Processing Systems 36 (2024).\\n[66] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\\n[67] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min\\nChan, Weize Chen, et al. 2022. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained\\nlanguage models. arXiv preprint arXiv:2203.06904 (2022).\\n[68] Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh\\nNallapati, Parminder Bhatia, Dan Roth, et al. 2024. Crosscodeeval: A diverse and multilingual benchmark for cross-file\\ncode completion. Advances in Neural Information Processing Systems 36 (2024).\\n[69] Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia,\\nDan Roth, and Bing Xiang. 2022. Cocomic: Code completion by jointly modeling in-file and cross-file context. arXiv\\npreprint arXiv:2212.10007 (2022).\\n[70] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022.\\nA survey on in-context learning. arXiv preprint arXiv:2301.00234 (2022).\\n[71] Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie Shan, Caishuang Huang, Wei Shen, Xiaoran Fan,\\nZhiheng Xi, et al. 2024. StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback.\\narXiv preprint arXiv:2402.01391 (2024).\\n[72] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng,\\nand Yiling Lou. 2024. Evaluating large language models in class-level code generation. In Proceedings of the IEEE/ACM\\n46th International Conference on Software Engineering. 1‚Äì13.\\n[73] Hugging Face. 2023. Training CodeParrot from Scratch. https://github.com/huggingface/blog/blob/main/codeparrot.\\nmd.\\n[74] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M Zhang. 2023. Large\\nlanguage models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on\\nSoftware Engineering: Future of Software Engineering (ICSE-FoSE). IEEE, 31‚Äì53.\\n[75] Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei Tan. 2023. Automated repair of programs\\nfrom large language models. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE,\\n1469‚Äì1481.\\n[76] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu,\\nDaxin Jiang, et al. 2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint\\narXiv:2002.08155 (2020).\\n[77] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke\\nZettlemoyer, and Mike Lewis. 2022. Incoder: A generative model for code infilling and synthesis. arXiv preprint\\narXiv:2204.05999 (2022).\\n[78] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish\\nThite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint\\narXiv:2101.00027 (2020).\\n[79] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.\\nPal: Program-aided language models. In International Conference on Machine Learning. PMLR, 10764‚Äì10799.\\n[80] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023.\\nRetrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023).\\n[81] Linyuan Gong, Mostafa Elhoushi, and Alvin Cheung. 2024. AST-T5: Structure-Aware Pretraining for Code Generation\\nand Understanding. arXiv preprint arXiv:2401.03003 (2024).\\n[82] Alex Gu, Baptiste Rozi√®re, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I Wang. 2024. Cruxeval:\\nA benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065 (2024).\\n[83] Sumit Gulwani. 2010. Dimensions in program synthesis. In Proceedings of the 12th international ACM SIGPLAN\\nsymposium on Principles and practice of declarative programming. 13‚Äì24.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:59\\n[84] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan\\nJavaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks are all you need. arXiv preprint\\narXiv:2306.11644 (2023).\\n[85] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022. UniXcoder: Unified Cross-Modal\\nPre-training for Code Representation. In Proceedings of the 60th Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers). 7212‚Äì7225.\\n[86] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svy-\\natkovskiy, Shengyu Fu, et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint\\narXiv:2009.08366 (2020).\\n[87] Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023. Longcoder: A long-range pre-trained language\\nmodel for code completion. In International Conference on Machine Learning. PMLR, 12098‚Äì12107.\\n[88] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li,\\net al. 2024. DeepSeek-Coder: When the Large Language Model Meets Programming‚ÄìThe Rise of Code Intelligence.\\narXiv preprint arXiv:2401.14196 (2024).\\n[89] Aman Gupta, Deepak Bhatt, and Anubha Pandey. 2021. Transitioning from Real to Synthetic data: Quantifying the\\nbias in model. arXiv preprint arXiv:2105.04144 (2021).\\n[90] Aman Gupta, Anup Shirgaonkar, Angels de Luis Balaguer, Bruno Silva, Daniel Holstein, Dawei Li, Jennifer Marsman,\\nLeonardo O Nunes, Mahsa Rouzbahman, Morris Sharp, et al. 2024. RAG vs Fine-tuning: Pipelines, Tradeoffs, and a\\nCase Study on Agriculture. arXiv preprint arXiv:2401.08406 (2024).\\n[91] Hossein Hajipour, Keno Hassler, Thorsten Holz, Lea Sch√∂nherr, and Mario Fritz. 2024. CodeLMSec Benchmark:\\nSystematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models. In 2024 IEEE\\nConference on Secure and Trustworthy Machine Learning (SaTML). IEEE, 684‚Äì709.\\n[92] Perttu H√§m√§l√§inen, Mikke Tavast, and Anton Kunnari. 2023. Evaluating large language models in generating synthetic\\nhci research data: a case study. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems.\\n1‚Äì19.\\n[93] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning\\nwith language model is planning with world model. arXiv preprint arXiv:2305.14992 (2023).\\n[94] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In\\nProceedings of the IEEE conference on computer vision and pattern recognition. 770‚Äì778.\\n[95] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\\nSteinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874\\n(2021).\\n[96] Felipe Hoffa. 2016. GitHub on BigQuery: Analyze all the open source code. URL: https://cloud.google.com/blog/topics/\\npublic-datasets/github-on-bigquery-analyze-all-the-open-source-code (2016).\\n[97] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las\\nCasas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language\\nmodels. arXiv preprint arXiv:2203.15556 (2022).\\n[98] Samuel Holt, Max Ruiz Luyten, and Mihaela van der Schaar. 2023. L2MAC: Large Language Model Automatic\\nComputer for Unbounded Code Generation. In The Twelfth International Conference on Learning Representations.\\n[99] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration.\\narXiv preprint arXiv:1904.09751 (2019).\\n[100] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing\\nYau, Zijuan Lin, Liyang Zhou, et al. 2023. Metagpt: Meta programming for multi-agent collaborative framework.\\narXiv preprint arXiv:2308.00352 (2023).\\n[101] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang.\\n2024. Large Language Models for Software Engineering: A Systematic Literature Review. arXiv:2308.10620 [cs.SE]\\n[102] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,\\nMona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International conference on\\nmachine learning. PMLR, 2790‚Äì2799.\\n[103] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\n2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).\\n[104] Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and Heming Cui. 2023. AgentCoder: Multi-Agent-based Code\\nGeneration with Iterative Testing and Optimisation. arXiv preprint arXiv:2312.13010 (2023).\\n[105] Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. arXiv preprint\\narXiv:2212.10403 (2022).\\n[106] Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards Reasoning in Large Language Models: A Survey. In 61st\\nAnnual Meeting of the Association for Computational Linguistics, ACL 2023. Association for Computational Linguistics\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:60\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n(ACL), 1049‚Äì1065.\\n[107] Junjie Huang, Chenglong Wang, Jipeng Zhang, Cong Yan, Haotian Cui, Jeevana Priya Inala, Colin Clement, Nan\\nDuan, and Jianfeng Gao. 2022. Execution-based evaluation for data science code generation models. arXiv preprint\\narXiv:2211.09374 (2022).\\n[108] Qiuyuan Huang, Naoki Wake, Bidipta Sarkar, Zane Durante, Ran Gong, Rohan Taori, Yusuke Noda, Demetri Ter-\\nzopoulos, Noboru Kuno, Ade Famoti, et al. 2024. Position Paper: Agent AI Towards a Holistic Intelligence. arXiv\\npreprint arXiv:2403.00833 (2024).\\n[109] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai\\nDang, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186 (2024).\\n[110] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet\\nchallenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).\\n[111] Yoichi Ishibashi and Yoshimasa Nishimura. 2024. Self-Organized Agents: A LLM Multi-Agent Framework toward\\nUltra Large-Scale Code Generation and Optimization. arXiv preprint arXiv:2404.02183 (2024).\\n[112] Shu Ishida, Gianluca Corrado, George Fedoseev, Hudson Yeo, Lloyd Russell, Jamie Shotton, Jo√£o F Henriques, and\\nAnthony Hu. 2024. LangProp: A code optimization framework using Language Models applied to driving. arXiv\\npreprint arXiv:2401.10314 (2024).\\n[113] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Mapping Language to Code in Pro-\\ngrammatic Context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.\\n1643‚Äì1652.\\n[114] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu\\nWang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: Scaling language model instruction meta learning through\\nthe lens of generalization. arXiv preprint arXiv:2212.12017 (2022).\\n[115] Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Jungkyu Choi, and Minjoon\\nSeo. 2022. Towards Continual Knowledge Learning of Language Models. In 10th International Conference on Learning\\nRepresentations, ICLR 2022. International Conference on Learning Representations.\\n[116] Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. 1977. Perplexity‚Äîa measure of the difficulty of speech\\nrecognition tasks. The Journal of the Acoustical Society of America 62, S1 (1977), S63‚ÄìS63.\\n[117] Susmit Jha, Sumit Gulwani, Sanjit A Seshia, and Ashish Tiwari. 2010. Oracle-guided component-based program\\nsynthesis. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 1. 215‚Äì224.\\n[118] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi\\nZhou, Zhaowei Zhang, et al. 2023. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852 (2023).\\n[119] Ruyi Ji, Jingjing Liang, Yingfei Xiong, Lu Zhang, and Zhenjiang Hu. 2020. Question selection for interactive program\\nsynthesis. In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation.\\n1143‚Äì1158.\\n[120] Zhenlan Ji, Pingchuan Ma, Zongjie Li, and Shuai Wang. 2023. Benchmarking and explaining large language model-\\nbased code generation: A causality-centric approach. arXiv preprint arXiv:2310.06680 (2023).\\n[121] Juyong Jiang and Sunghun Kim. 2023. CodeUp: A Multilingual Code Generation Llama2 Model with Parameter-\\nEfficient Instruction-Tuning. https://github.com/juyongjiang/CodeUp.\\n[122] Shuyang Jiang, Yuhao Wang, and Yu Wang. 2023. Selfevolve: A code evolution framework via large language models.\\narXiv preprint arXiv:2306.02907 (2023).\\n[123] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. 2023.\\nSWE-bench: Can Language Models Resolve Real-world Github Issues?. In The Twelfth International Conference on\\nLearning Representations.\\n[124] Alexander Wettig Kilian Lieret Shunyu Yao Karthik Narasimhan Ofir Press John Yang, Carlos E. Jimenez. 2024.\\nSWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING. (2024).\\nhttps://swe-agent.com/\\n[125] Aravind Joshi and Owen Rambow. 2003. A formalism for dependency grammar based on tree adjoining grammar. In\\nProceedings of the Conference on Meaning-text Theory. MTT Paris, France, 207‚Äì216.\\n[126] Harshit Joshi, Jos√© Cambronero Sanchez, Sumit Gulwani, Vu Le, Gust Verbruggen, and Ivan Radiƒçek. 2023. Repair\\nis nearly generation: Multilingual program repair with llms. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, Vol. 37. 5131‚Äì5140.\\n[127] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford,\\nJeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\\n[128] Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty.\\n2023. xcodeeval: A large scale multilingual multitask benchmark for code understanding, generation, translation and\\nretrieval. arXiv preprint arXiv:2303.03004 (2023).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:61\\n[129] Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, and Chanjun Park. 2024. sDPO:\\nDon‚Äôt Use Your Data All at Once. arXiv preprint arXiv:2403.19270 (2024).\\n[130] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim,\\nHyeonju Lee, Jihoo Kim, et al. 2023. Solar 10.7 b: Scaling large language models with simple yet effective depth\\nup-scaling. arXiv preprint arXiv:2312.15166 (2023).\\n[131] Barbara Kitchenham, O Pearl Brereton, David Budgen, Mark Turner, John Bailey, and Stephen Linkman. 2009.\\nSystematic literature reviews in software engineering‚Äìa systematic literature review. Information and software\\ntechnology 51, 1 (2009), 7‚Äì15.\\n[132] Denis Kocetkov, Raymond Li, LI Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Mu√±oz Ferrandis,\\nSean Hughes, Thomas Wolf, Dzmitry Bahdanau, et al. 2022. The Stack: 3 TB of permissively licensed source code.\\nTransactions on Machine Learning Research (2022).\\n[133] Andreas K√∂pf, Yannic Kilcher, Dimitri von R√ºtte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum,\\nDuc Nguyen, Oliver Stanley, Rich√°rd Nagyfi, et al. 2024. Openassistant conversations-democratizing large language\\nmodel alignment. Advances in Neural Information Processing Systems 36 (2024).\\n[134] Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang. 2023. Is model attention aligned with human\\nattention? an empirical study on large language models for code generation. arXiv preprint arXiv:2306.01220 (2023).\\n[135] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised translation of\\nprogramming languages. arXiv preprint arXiv:2006.03511 (2020).\\n[136] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida\\nWang, and Tao Yu. 2023. DS-1000: A natural and reliable benchmark for data science code generation. In International\\nConference on Machine Learning. PMLR, 18319‚Äì18345.\\n[137] Hugo Lauren√ßon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao,\\nLeandro Von Werra, Chenghao Mou, Eduardo Gonz√°lez Ponferrada, Huu Nguyen, et al. 2022. The bigscience\\nroots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems 35 (2022),\\n31809‚Äì31826.\\n[138] Moritz Laurer. 2024. Synthetic data: save money, time and carbon with open source. https://huggingface.co/blog/\\nsynthetic-data-save-costs.\\n[139] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering\\ncode generation through pretrained models and deep reinforcement learning. Advances in Neural Information\\nProcessing Systems 35 (2022), 21314‚Äì21328.\\n[140] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Iliƒá, Daniel Hesslow, Roman Castagn√©, Alexan-\\ndra Sasha Luccioni, Fran√ßois Yvon, Matthias Gall√©, et al. 2023. Bloom: A 176b-parameter open-access multilingual\\nlanguage model. (2023).\\n[141] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and\\nAbhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint\\narXiv:2309.00267 (2023).\\n[142] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning.\\narXiv preprint arXiv:2104.08691 (2021).\\n[143] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler,\\nMike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp\\ntasks. Advances in Neural Information Processing Systems 33 (2020), 9459‚Äì9474.\\n[144] Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. 2024. EvoCodeBench: An Evolving Code Generation\\nBenchmark Aligned with Real-World Code Repositories. arXiv preprint arXiv:2404.00599 (2024).\\n[145] Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. 2023. Towards enhancing in-context learning for code generation.\\narXiv preprint arXiv:2303.17780 (2023).\\n[146] Li Li, Tegawend√© F Bissyand√©, Mike Papadakis, Siegfried Rasthofer, Alexandre Bartel, Damien Octeau, Jacques Klein,\\nand Le Traon. 2017. Static analysis of android apps: A systematic literature review. Information and Software\\nTechnology 88 (2017), 67‚Äì95.\\n[147] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\\nChristopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161\\n(2023).\\n[148] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B.\\nHashimoto. 2023. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-\\nlab/alpaca_eval.\\n[149] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint\\narXiv:2101.00190 (2021).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 61, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:62\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[150] Yuanzhi Li, S√©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks are\\nall you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 (2023).\\n[151] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R√©mi Leblond, Tom Eccles, James Keeling,\\nFelix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science 378, 6624\\n(2022), 1092‚Äì1097.\\n[152] Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang, Qiyi Tang, Sen Nie, and Shi Wu. 2022. Unleashing the power of\\ncompiler intermediate representation to enhance neural program embeddings. In Proceedings of the 44th International\\nConference on Software Engineering. 2253‚Äì2265.\\n[153] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameter-efficient\\nfine-tuning. arXiv preprint arXiv:2303.15647 (2023).\\n[154] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak\\nNarayanan, Yuhuai Wu, Ananya Kumar, et al. 2023. Holistic Evaluation of Language Models. Transactions on Machine\\nLearning Research (2023).\\n[155] Andreas Liesenfeld, Alianda Lopez, and Mark Dingemanse. 2023. Opening up ChatGPT: Tracking openness, trans-\\nparency, and accountability in instruction-tuned text generators. In Proceedings of the 5th international conference on\\nconversational user interfaces. 1‚Äì6.\\n[156] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.\\n74‚Äì81.\\n[157] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022. A survey of transformers. AI open 3 (2022),\\n111‚Äì132.\\n[158] Erik Linstead, Paul Rigor, Sushil Bajracharya, Cristina Lopes, and Pierre Baldi. 2007. Mining internet-scale software\\nrepositories. Advances in neural information processing systems 20 (2007).\\n[159] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai,\\nDaya Guo, et al. 2024. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv\\npreprint arXiv:2405.04434 (2024).\\n[160] Bingchang Liu, Chaoyu Chen, Cong Liao, Zi Gong, Huan Wang, Zhichao Lei, Ming Liang, Dajun Chen, Min Shen,\\nHailian Zhou, et al. 2023. Mftcoder: Boosting code llms with multitask fine-tuning. arXiv preprint arXiv:2311.02303\\n(2023).\\n[161] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel.\\n2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural\\nInformation Processing Systems 35 (2022), 1950‚Äì1965.\\n[162] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is your code generated by chatgpt really\\ncorrect? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing\\nSystems 36 (2024).\\n[163] Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. 2023. Rltf: Reinforcement learning\\nfrom unit test feedback. arXiv preprint arXiv:2307.04349 (2023).\\n[164] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt,\\nand predict: A systematic survey of prompting methods in natural language processing. Comput. Surveys 55, 9 (2023),\\n1‚Äì35.\\n[165] Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang,\\nDenny Zhou, et al. 2024. Best Practices and Lessons Learned on Synthetic Data for Language Models. arXiv preprint\\narXiv:2404.07503 (2024).\\n[166] Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, and Yang Liu. 2020. Retrieval-Augmented Generation for Code\\nSummarization via Hybrid GNN. In International Conference on Learning Representations.\\n[167] Tianyang Liu, Canwen Xu, and Julian McAuley. 2023.\\nRepobench: Benchmarking repository-level code auto-\\ncompletion systems. arXiv preprint arXiv:2306.03091 (2023).\\n[168] Yan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji Zhang, Daoguang Zan, Jian-Guang Lou, Pin-Yu Chen, and Tsung-Yi\\nHo. 2023. Uncovering and quantifying social biases in code generation. Advances in Neural Information Processing\\nSystems 36 (2023), 2368‚Äì2380.\\n[169] Yue Liu, Chakkrit Tantithamthavorn, Li Li, and Yepang Liu. 2022. Deep learning for android malware defenses: a\\nsystematic literature review. Comput. Surveys 55, 8 (2022), 1‚Äì36.\\n[170] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang,\\nDmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. 2024. StarCoder 2 and The Stack v2: The Next Generation. arXiv\\npreprint arXiv:2402.19173 (2024).\\n[171] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy. 2022. ReACC: A Retrieval-\\nAugmented Code Completion Framework. In Proceedings of the 60th Annual Meeting of the Association for Computa-\\ntional Linguistics (Volume 1: Long Papers). 6227‚Äì6240.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:63\\n[172] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain,\\nDaxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and\\ngeneration. arXiv preprint arXiv:2102.04664 (2021).\\n[173] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin,\\nand Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. In The Twelfth\\nInternational Conference on Learning Representations.\\n[174] Michael R Lyu, Baishakhi Ray, Abhik Roychoudhury, Shin Hwei Tan, and Patanamon Thongtanunam. 2024. Automatic\\nProgramming: Large Language Models and Beyond. arXiv preprint arXiv:2405.02213 (2024).\\n[175] Wei Ma, Mengjie Zhao, Xiaofei Xie, Qiang Hu, Shangqing Liu, Jie Zhang, Wenhan Wang, and Yang Liu. 2022. Are\\nCode Pre-trained Models Powerful to Learn Code Syntax and Semantics? arXiv preprint arXiv:2212.10017 (2022).\\n[176] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\\nShrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. Advances in\\nNeural Information Processing Systems 36 (2024).\\n[177] James Manyika and Sissie Hsiao. 2023. An overview of Bard: an early experiment with generative AI. AI. Google\\nStatic Documents 2 (2023).\\n[178] Vadim Markovtsev, Waren Long, Hugo Mougard, Konstantin Slavnov, and Egor Bulychev. 2019. STYLE-ANALYZER:\\nfixing code style inconsistencies with interpretable unsupervised algorithms. In 2019 IEEE/ACM 16th International\\nConference on Mining Software Repositories (MSR). IEEE, 468‚Äì478.\\n[179] Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. 2022. Generating training data with language models: Towards\\nzero-shot language understanding. Advances in Neural Information Processing Systems 35 (2022), 462‚Äì477.\\n[180] Meta. 2024. Introducing Meta Llama 3: The most capable openly available LLM to date. https://ai.meta.com/blog/meta-\\nllama-3/.\\n[181] MistralAI. 2024. Codestral. https://mistral.ai/news/codestral/.\\n[182] S√©bastien Bubeck Mojan Javaheripi. 2023. Phi-2: The surprising power of small language models. https://www.\\nmicrosoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models.\\n[183] Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang. 2014. TBCNN: A tree-based convolutional neural network for\\nprogramming language processing. arXiv preprint arXiv:1409.5718 (2014).\\n[184] Zahra Mousavi, Chadni Islam, Kristen Moore, Alsharif Abuadbba, and M Ali Babar. 2024. An investigation into\\nmisuse of java security apis by large language models. In Proceedings of the 19th ACM Asia Conference on Computer\\nand Communications Security. 1299‚Äì1315.\\n[185] Spyridon Mouselinos, Mateusz Malinowski, and Henryk Michalewski. 2022. A simple, yet effective approach to\\nfinding biases in code generation. arXiv preprint arXiv:2211.00609 (2022).\\n[186] Hussein Mozannar, Gagan Bansal, Adam Fourney, and Eric Horvitz. 2022. Reading between the lines: Modeling user\\nbehavior and costs in AI-assisted programming. arXiv preprint arXiv:2210.14306 (2022).\\n[187] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru\\nTang, Leandro Von Werra, and Shayne Longpre. 2023. Octopack: Instruction tuning code large language models.\\narXiv preprint arXiv:2308.07124 (2023).\\n[188] King Han Naman Jain, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik\\nSen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for\\ncode. arXiv preprint arXiv:2403.07974 (2024).\\n[189] Antonio Nappa, Richard Johnson, Leyla Bilge, Juan Caballero, and Tudor Dumitras. 2015. The attack of the clones: A\\nstudy of the impact of shared code on vulnerability patching. In 2015 IEEE symposium on security and privacy. IEEE,\\n692‚Äì708.\\n[190] Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. 2023. Lever:\\nLearning to verify language-to-code generation with execution. In International Conference on Machine Learning.\\nPMLR, 26106‚Äì26128.\\n[191] Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz,\\nCaiming Xiong, et al. 2023. L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language\\nModels. arXiv preprint arXiv:2309.17446 (2023).\\n[192] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. 2023. Codegen2: Lessons for\\ntraining llms on programming and natural languages. arXiv preprint arXiv:2305.02309 (2023).\\n[193] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022.\\nCodegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474\\n(2022).\\n[194] Changan Niu, Chuanyi Li, Bin Luo, and Vincent Ng. 2022. Deep learning meets software engineering: A survey on\\npre-trained models of source code. arXiv preprint arXiv:2205.11739 (2022).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:64\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[195] Theo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. 2023. Is Self-Repair\\na Silver Bullet for Code Generation?. In The Twelfth International Conference on Learning Representations.\\n[196] OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/chatgpt.\\n[197] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/.\\n[198] OpenAI. 2024. New models and developer products announced at DevDay. https://openai.com/index/new-models-\\nand-developer-products-announced-at-devday/.\\n[199] OpenDevin. 2024. OpenDevin: Code Less, Make More. https://github.com/OpenDevin/OpenDevin.\\n[200] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.\\nAdvances in neural information processing systems 35 (2022), 27730‚Äì27744.\\n[201] Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023. Fine-tuning or retrieval? comparing\\nknowledge injection in llms. arXiv preprint arXiv:2312.05934 (2023).\\n[202] David N Palacio, Alejandro Velasco, Daniel Rodriguez-Cardenas, Kevin Moran, and Denys Poshyvanyk. 2023. Eval-\\nuating and explaining large language models for code using syntactic structures. arXiv preprint arXiv:2308.03873\\n(2023).\\n[203] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation\\nof machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics.\\n311‚Äì318.\\n[204] Nikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr, and Sergey Mechtaev.\\n2024. The Fact Selection Problem in LLM-Based Program Repair. arXiv preprint arXiv:2404.05520 (2024).\\n[205] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval Augmented\\nCode Generation and Summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021.\\n2719‚Äì2734.\\n[206] Arkil Patel, Siva Reddy, Dzmitry Bahdanau, and Pradeep Dasigi. 2023. Evaluating In-Context Learning of Libraries\\nfor Code Generation. arXiv preprint arXiv:2311.09635 (2023).\\n[207] Indraneil Paul, Jun Luo, Goran Glava≈°, and Iryna Gurevych. 2024. IRCoder: Intermediate Representations Make\\nLanguage Models Robust Multilingual Code Generators. arXiv preprint arXiv:2403.03894 (2024).\\n[208] Norman Peitek, Sven Apel, Chris Parnin, Andr√© Brechmann, and Janet Siegmund. 2021. Program comprehension and\\ncode complexity metrics: An fmri study. In 2021 IEEE/ACM 43rd International Conference on Software Engineering\\n(ICSE). IEEE, 524‚Äì536.\\n[209] Huy N Phan, Hoang N Phan, Tien N Nguyen, and Nghi DQ Bui. 2024. RepoHyper: Better Context Retrieval Is All You\\nNeed for Repository-Level Code Completion. arXiv preprint arXiv:2403.06095 (2024).\\n[210] Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, Ashish Datta, Maksym Zhu-\\nravinskyi, Dakota Mahan, Marco Bellagente, Carlos Riquelme, et al. 2024. Stable Code Technical Report. arXiv\\npreprint arXiv:2404.01226 (2024).\\n[211] Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input\\nlength extrapolation. arXiv preprint arXiv:2108.12409 (2021).\\n[212] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023. Fine-tuning\\naligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693\\n(2023).\\n[213] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by\\ngenerative pre-training. (2018).\\n[214] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are\\nunsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.\\n[215] Steven Raemaekers, Arie Van Deursen, and Joost Visser. 2012. Measuring software library stability through historical\\nversion analysis. In 2012 28th IEEE international conference on software maintenance (ICSM). IEEE, 378‚Äì387.\\n[216] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct\\npreference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing\\nSystems 36 (2024).\\n[217] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\nPeter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine\\nlearning research 21, 140 (2020), 1‚Äì67.\\n[218] Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. 2022. Evaluating the text-to-sql capabilities of large\\nlanguage models. arXiv preprint arXiv:2204.00498 (2022).\\n[219] Aurora Ramirez, Jose Raul Romero, and Christopher L Simons. 2018. A systematic review of interaction in search-based\\nsoftware engineering. IEEE Transactions on Software Engineering 45, 8 (2018), 760‚Äì781.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:65\\n[220] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Sori-\\ncut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding\\nacross millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024).\\n[221] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco,\\nand Shuai Ma. 2020. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297\\n(2020).\\n[222] Replit. 2016. Idea to software, fast. https://replit.com.\\n[223] Replit. 2023. replit-code-v1-3b. https://huggingface.co/replit/replit-code-v1-3b.\\n[224] Tal Ridnik, Dedy Kredo, and Itamar Friedman. 2024. Code Generation with AlphaCodium: From Prompt Engineering\\nto Flow Engineering. arXiv preprint arXiv:2401.08500 (2024).\\n[225] Nick Roshdieh. 2023. Evol-Instruct-Code-80k. https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1.\\n[226] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D Weisz. 2023. The programmer‚Äôs\\nassistant: Conversational interaction with a large language model for software development. In Proceedings of the\\n28th International Conference on Intelligent User Interfaces. 491‚Äì514.\\n[227] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal\\nRemez, J√©r√©my Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950\\n(2023).\\n[228] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy\\nKepner, Devesh Tiwari, and Vijay Gadepally. 2023. From words to watts: Benchmarking the energy costs of large\\nlanguage model inference. In 2023 IEEE High Performance Extreme Computing Conference (HPEC). IEEE, 1‚Äì9.\\n[229] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud\\nStiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask Prompted Training Enables Zero-Shot Task Generalization.\\nIn ICLR 2022-Tenth International Conference on Learning Representations.\\n[230] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization\\nalgorithms. arXiv preprint arXiv:1707.06347 (2017).\\n[231] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. 2021. You autocomplete me: Poisoning vulnera-\\nbilities in neural code completion. In 30th USENIX Security Symposium (USENIX Security 21). 1559‚Äì1575.\\n[232] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv\\npreprint arXiv:1803.02155 (2018).\\n[233] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017.\\nOutrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538\\n(2017).\\n[234] Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang\\nZhao, et al. 2023. Pangu-coder2: Boosting large language models for code with ranking feedback. arXiv preprint\\narXiv:2307.14936 (2023).\\n[235] Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, and David Lo. 2024. Greening large language models\\nof code. In Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society.\\n142‚Äì153.\\n[236] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language\\nagents with verbal reinforcement learning. Advances in Neural Information Processing Systems 36 (2024).\\n[237] Atsushi Shirafuji, Yusuke Oda, Jun Suzuki, Makoto Morishita, and Yutaka Watanobe. 2023. Refactoring Programs\\nUsing Large Language Models with Few-Shot Examples. arXiv preprint arXiv:2311.11690 (2023).\\n[238] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based code generation using\\ndeep reinforcement learning. arXiv preprint arXiv:2301.13816 (2023).\\n[239] Disha Shrivastava, Denis Kocetkov, Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. 2023. RepoFusion:\\nTraining Code Models to Understand Your Repository. arXiv preprint arXiv:2306.10998 (2023).\\n[240] Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. 2023. Repository-level prompt generation for large language\\nmodels of code. In International Conference on Machine Learning. PMLR, 31693‚Äì31715.\\n[241] Mukul Singh, Jos√© Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, and Gust Verbruggen. 2023. Codefusion: A\\npre-trained diffusion model for code generation. arXiv preprint arXiv:2310.17680 (2023).\\n[242] Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, and Tao Yu. 2024. ARKS: Active\\nRetrieval in Knowledge Soup for Code Generation. arXiv preprint arXiv:2402.12317 (2024).\\n[243] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer\\nwith rotary position embedding. Neurocomputing 568 (2024), 127063.\\n[244] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: Code generation\\nusing transformer. In Proceedings of the 28th ACM joint meeting on European software engineering conference and\\nsymposium on the foundations of software engineering. 1433‚Äì1443.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:66\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[245] Marc Szafraniec, Baptiste Roziere, Hugh Leather, Francois Charton, Patrick Labatut, and Gabriel Synnaeve. 2022.\\nCode translation with compiler representations. In Proceedings of the Eleventh International Conference on Learning\\nRepresentations: ICLR.\\n[246] TabNine. 2018. AI Code Completions. https://github.com/codota/TabNine.\\n[247] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.\\nHashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_\\nalpaca.\\n[248] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\\nMorgane Rivi√®re, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and\\ntechnology. arXiv preprint arXiv:2403.08295 (2024).\\n[249] Qwen Team. 2024. Code with CodeQwen1.5. https://qwenlm.github.io/blog/codeqwen1.5.\\n[250] Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt,\\nand Siddharth Garg. 2023. Benchmarking large language models for automated verilog rtl code generation. In 2023\\nDesign, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 1‚Äì6.\\n[251] theblackcat102. 2023.\\nThe evolved code alpaca dataset.\\nhttps://huggingface.co/datasets/theblackcat102/evol-\\ncodealpaca-v1.\\n[252] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste\\nRozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.\\narXiv preprint arXiv:2302.13971 (2023).\\n[253] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.\\narXiv preprint arXiv:2307.09288 (2023).\\n[254] Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. 2022. Natural language processing with transformers. \" O‚ÄôReilly\\nMedia, Inc.\".\\n[255] Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022. Expectation vs. experience: Evaluating the usability\\nof code generation tools powered by large language models. In Chi conference on human factors in computing systems\\nextended abstracts. 1‚Äì7.\\n[256] Boris Van Breugel, Zhaozhi Qian, and Mihaela Van Der Schaar. 2023. Synthetic data, real errors: how (not) to publish\\nand use synthetic data. In International Conference on Machine Learning. PMLR, 34793‚Äì34808.\\n[257] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\\nPolosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).\\n[258] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https:\\n//github.com/kingoflolz/mesh-transformer-jax.\\n[259] Chong Wang, Jian Zhang, Yebo Feng, Tianlin Li, Weisong Sun, Yang Liu, and Xin Peng. 2024. Teaching Code LLMs to\\nUse Autocompletion Tools in Repository-Level Code Generation. arXiv preprint arXiv:2401.06391 (2024).\\n[260] Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. 2024. Software testing with\\nlarge language models: Survey, landscape, and vision. IEEE Transactions on Software Engineering (2024).\\n[261] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen,\\nYankai Lin, et al. 2024. A survey on large language model based autonomous agents. Frontiers of Computer Science 18,\\n6 (2024), 1‚Äì26.\\n[262] Simin Wang, Liguo Huang, Amiao Gao, Jidong Ge, Tengfei Zhang, Haitao Feng, Ishna Satyarth, Ming Li, He Zhang, and\\nVincent Ng. 2022. Machine/deep learning for software engineering: A systematic literature review. IEEE Transactions\\non Software Engineering 49, 3 (2022), 1188‚Äì1231.\\n[263] Shiqi Wang, Li Zheng, Haifeng Qian, Chenghao Yang, Zijian Wang, Varun Kumar, Mingyue Shang, Samson Tan,\\nBaishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, and Bing Xiang. 2022.\\nReCode: Robustness Evaluation of Code Generation Models. (2022). https://doi.org/10.48550/arXiv.2212.10264\\n[264] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. 2023. Knowledge editing for large language\\nmodels: A survey. arXiv preprint arXiv:2310.16218 (2023).\\n[265] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024. Executable code\\nactions elicit better llm agents. arXiv preprint arXiv:2402.01030 (2024).\\n[266] Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, and Qun Liu.\\n2022. Compilable Neural Code Generation with Compiler Feedback. In Findings of the Association for Computational\\nLinguistics: ACL 2022. 9‚Äì19.\\n[267] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny\\nZhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171\\n(2022).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:67\\n[268] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.\\n2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. In The 61st Annual Meeting Of The\\nAssociation For Computational Linguistics.\\n[269] Yue Wang, Hung Le, Akhilesh Gotmare, Nghi Bui, Junnan Li, and Steven Hoi. 2023. CodeT5+: Open Code Large\\nLanguage Models for Code Understanding and Generation. In Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing. 1069‚Äì1088.\\n[270] Yanlin Wang and Hui Li. 2021. Code completion by modeling flattened abstract syntax trees as graphs. In Proceedings\\nof the AAAI conference on artificial intelligence, Vol. 35. 14015‚Äì14023.\\n[271] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-\\nDecoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing. 8696‚Äì8708.\\n[272] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and\\nQun Liu. 2023. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966 (2023).\\n[273] Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. 2022. Execution-based evaluation for open-domain\\ncode generation. arXiv preprint arXiv:2212.10481 (2022).\\n[274] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\\nQuoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).\\n[275] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\\nDenny Zhou, Donald Metzler, et al. 2022. Emergent Abilities of Large Language Models. Transactions on Machine\\nLearning Research (2022).\\n[276] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.\\nChain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing\\nsystems 35 (2022), 24824‚Äì24837.\\n[277] Xiaokai Wei, Sujan Kumar Gonugondla, Shiqi Wang, Wasi Ahmad, Baishakhi Ray, Haifeng Qian, Xiaopeng Li, Varun\\nKumar, Zijian Wang, Yuchen Tian, et al. 2023. Towards greener yet powerful code generation via quantization: An\\nempirical study. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the\\nFoundations of Software Engineering. 224‚Äì236.\\n[278] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need.\\narXiv preprint arXiv:2312.02120 (2023).\\n[279] Lilian Weng. 2023. LLM-powered Autonomous Agents. lilianweng.github.io (Jun 2023). https://lilianweng.github.io/\\nposts/2023-06-23-agent/\\n[280] Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024. QuRating: Selecting High-Quality Data for\\nTraining Language Models. arXiv preprint arXiv:2402.09739 (2024).\\n[281] Erroll Wood, Tadas Baltru≈°aitis, Charlie Hewitt, Sebastian Dziadzio, Thomas J Cashman, and Jamie Shotton. 2021.\\nFake it till you make it: face analysis in the wild using synthetic data alone. In Proceedings of the IEEE/CVF international\\nconference on computer vision. 3681‚Äì3691.\\n[282] Di Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Krishna Ramanathan, and Xiaofei Ma. 2024. Repoformer: Selective\\nRetrieval for Repository-Level Code Completion. arXiv preprint arXiv:2403.10059 (2024).\\n[283] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang,\\nand Chi Wang. 2023. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv\\npreprint arXiv:2308.08155 (2023).\\n[284] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin,\\nEnyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint\\narXiv:2309.07864 (2023).\\n[285] Rui Xie, Zhengran Zeng, Zhuohao Yu, Chang Gao, Shikun Zhang, and Wei Ye. 2024. CodeShell Technical Report.\\narXiv preprint arXiv:2403.15747 (2024).\\n[286] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S Liang. 2023. Data selection for language models via\\nimportance resampling. Advances in Neural Information Processing Systems 36 (2023), 34201‚Äì34227.\\n[287] Bowen Li Xingyao Wang and Graham Neubig. 2024. Introducing OpenDevin CodeAct 1.0, a new State-of-the-art in\\nCoding Agents. https://www.cognition.ai/introducing-devin.\\n[288] Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang Huang, and Lu Zhang. 2017. Precise condition\\nsynthesis for program repair. In 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE,\\n416‚Äì426.\\n[289] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.\\nWizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 (2023).\\n[290] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large\\nlanguage models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:68\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n1‚Äì10.\\n[291] Junjielong Xu, Ying Fu, Shin Hwei Tan, and Pinjia He. 2024. Aligning LLMs for FL-free Program Repair. arXiv preprint\\narXiv:2404.08877 (2024).\\n[292] Weiwei Xu, Kai Gao, Hao He, and Minghui Zhou. 2024. A First Look at License Compliance Capability of LLMs in\\nCode Generation. arXiv preprint arXiv:2408.02487 (2024).\\n[293] Zhou Yang, Zhensu Sun, Terry Zhuo Yue, Premkumar Devanbu, and David Lo. 2024. Robustness, security, privacy,\\nexplainability, efficiency, and usability of large language models for code. arXiv preprint arXiv:2403.07506 (2024).\\n[294] Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsun Kim, Donggyun Han, and David Lo. 2024. Unveiling\\nmemorization in code models. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering.\\n1‚Äì13.\\n[295] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of\\nthoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems\\n36 (2024).\\n[296] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct:\\nSynergizing Reasoning and Acting in Language Models. In International Conference on Learning Representations\\n(ICLR).\\n[297] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned\\ncode and natural language pairs from stack overflow. In Proceedings of the 15th international conference on mining\\nsoftware repositories. 476‚Äì486.\\n[298] Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim,\\nMunhyong Kim, Sungju Kim, et al. 2024. HyperCLOVA X Technical Report. arXiv preprint arXiv:2404.01954 (2024).\\n[299] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao\\nXie. 2024. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings\\nof the 46th IEEE/ACM International Conference on Software Engineering. 1‚Äì12.\\n[300] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle\\nRoman, et al. 2018. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing\\nand Text-to-SQL Task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.\\n3911‚Äì3921.\\n[301] Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng Yin. 2023.\\nWavecoder: Widespread and versatile enhanced instruction tuning with refined data generation. arXiv preprint\\narXiv:2312.14187 (2023).\\n[302] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2023.\\nGpt-4 is too smart to be safe: Stealthy chat with llms via cipher. arXiv preprint arXiv:2308.06463 (2023).\\n[303] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. Rrhf: Rank responses to\\nalign language models with human feedback without tears. arXiv preprint arXiv:2304.05302 (2023).\\n[304] Jiawei Liu Yifeng Ding Naman Jain Harm de Vries Leandro von Werra Arjun Guha Lingming Zhang Yuxiang Wei,\\nFederico Cassano. 2024. StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation.\\nhttps://github.com/bigcode-project/starcoder2-self-align.\\n[305] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021.\\nBitfit: Simple parameter-efficient fine-tuning for\\ntransformer-based masked language-models. arXiv preprint arXiv:2106.10199 (2021).\\n[306] Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-\\nGuang Lou. 2022. CERT: continual pre-training on sketches for library-oriented code generation. arXiv preprint\\narXiv:2206.06888 (2022).\\n[307] Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, and Jian-Guang Lou. 2023.\\nLarge Language Models Meet NL2Code: A Survey. In Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers). 7443‚Äì7464.\\n[308] Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan,\\net al. 2024. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. arXiv preprint arXiv:2403.16443\\n(2024).\\n[309] Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\\n2023. RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. In Proceedings of\\nthe 2023 Conference on Empirical Methods in Natural Language Processing. 2471‚Äì2484.\\n[310] Jialu Zhang, Jos√© Pablo Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, and Gust Verbruggen.\\n2024. Pydex: Repairing bugs in introductory python assignments using llms. Proceedings of the ACM on Programming\\nLanguages 8, OOPSLA1 (2024), 1100‚Äì1124.\\n[311] Jialu Zhang, De Li, John Charles Kolesar, Hanyuan Shi, and Ruzica Piskac. 2022. Automated feedback generation\\nfor competition-level code. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:69\\nEngineering. 1‚Äì13.\\n[312] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. 2023.\\nAdaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning\\nRepresentations.\\n[313] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang,\\nFei Wu, et al. 2023. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792 (2023).\\n[314] Shudan Zhang, Hanlin Zhao, Xiao Liu, Qinkai Zheng, Zehan Qi, Xiaotao Gu, Xiaohan Zhang, Yuxiao Dong, and Jie\\nTang. 2024. NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts.\\narXiv preprint arXiv:2405.04520 (2024).\\n[315] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong\\nChen, et al. 2023. Siren‚Äôs song in the AI ocean: a survey on hallucination in large language models. arXiv preprint\\narXiv:2309.01219 (2023).\\n[316] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. 2024. AutoCodeRover: Autonomous Program\\nImprovement. arXiv preprint arXiv:2404.05427 (2024).\\n[317] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023. Unifying the\\nperspectives of nlp and software engineering: A survey on language models for code. arXiv preprint arXiv:2311.07989\\n(2023).\\n[318] Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bin Qin, and Ting Liu. 2023. Length Extrapolation of Transformers: A\\nSurvey from the Perspective of Position Encoding. arXiv preprint arXiv:2312.17044 (2023).\\n[319] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\\nZhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).\\n[320] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,\\nDacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\\nInformation Processing Systems 36 (2024).\\n[321] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li,\\net al. 2023. Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x. In\\nProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 5673‚Äì5684.\\n[322] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. 2024.\\nOpenCodeInterpreter: Integrating Code Generation with Execution and Refinement. arXiv preprint arXiv:2402.14658\\n(2024).\\n[323] Wenqing Zheng, SP Sharan, Ajay Kumar Jaiswal, Kevin Wang, Yihan Xi, Dejia Xu, and Zhangyang Wang. 2023.\\nOutline, then details: Syntactically guided coarse-to-fine code generation. In International Conference on Machine\\nLearning. PMLR, 42403‚Äì42419.\\n[324] Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen. 2023. A survey of\\nlarge language models for code: Evolution, benchmarking, and future trends. arXiv preprint arXiv:2311.10372 (2023).\\n[325] Li Zhong, Zilong Wang, and Jingbo Shang. 2024. LDB: A Large Language Model Debugger via Verifying Runtime\\nExecution Step-by-step. arXiv preprint arXiv:2402.16906 (2024).\\n[326] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie\\nZhan, et al. 2023. Solving challenging math word problems using gpt-4 code interpreter with code-based self-\\nverification. arXiv preprint arXiv:2308.07921 (2023).\\n[327] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023. Language agent tree\\nsearch unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406 (2023).\\n[328] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili\\nYu, et al. 2024. Lima: Less is more for alignment. Advances in Neural Information Processing Systems 36 (2024).\\n[329] Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui,\\nOlivier Bousquet, Quoc V Le, et al. 2022. Least-to-Most Prompting Enables Complex Reasoning in Large Language\\nModels. In The Eleventh International Conference on Learning Representations.\\n[330] Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang, and Graham Neubig. 2022. DocPrompting: Generating Code by\\nRetrieving the Docs. In The Eleventh International Conference on Learning Representations.\\n[331] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong\\nMa, et al. 2024. DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence. arXiv\\npreprint arXiv:2406.11931 (2024).\\n[332] Terry Yue Zhuo. 2024. ICE-Score: Instructing Large Language Models to Evaluate Code. In Findings of the Association\\nfor Computational Linguistics: EACL 2024. 2232‚Äì2242.\\n[333] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf,\\nHaolan Zhan, Junda He, Indraneil Paul, et al. 2024. Bigcodebench: Benchmarking code generation with diverse\\nfunction calls and complex instructions. arXiv preprint arXiv:2406.15877 (2024).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 69, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:70\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[334] Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian Liu, and Niklas\\nMuennighoff. 2024. Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models. arXiv preprint\\narXiv:2401.00788 (2024).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 0, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33\\nFlashFill++: Scaling Programming by Example by Cutting to\\nthe Chase\\nJOS√â CAMBRONERO‚àó, Microsoft, USA\\nSUMIT GULWANI‚àó, Microsoft, USA\\nVU LE‚àó, Microsoft, USA\\nDANIEL PERELMAN‚àó, Microsoft, USA\\nARJUN RADHAKRISHNA‚àó, Microsoft, USA\\nCLINT SIMON‚àó, Microsoft, USA\\nASHISH TIWARI‚àó, Microsoft, USA\\nProgramming-by-Examples (PBE) involves synthesizing an intended program from a small set of user-provided\\ninput-output examples. A key PBE strategy has been to restrict the search to a carefully designed small\\ndomain-specific language (DSL) with effectively-invertible (EI) operators at the top and effectively-enumerable\\n(EE) operators at the bottom. This facilitates an effective combination of top-down synthesis strategy (which\\nbackpropagates outputs over various paths in the DSL using inverse functions) with a bottom-up synthesis\\nstrategy (which propagates inputs over various paths in the DSL). We address the problem of scaling synthesis\\nto large DSLs with several non-EI/EE operators. This is motivated by the need to support a richer class of\\ntransformations and the need for readable code generation. We propose a novel solution strategy that relies\\non propagating fewer values and over fewer paths.\\nOur first key idea is that of cut functions that prune the set of values being propagated by using knowledge\\nof the sub-DSL on the other side. Cuts can be designed to preserve completeness of synthesis; however, DSL\\ndesigners may use incomplete cuts to have finer control over the kind of programs synthesized. In either case,\\ncuts make search feasible for non-EI/EE operators and efficient for deep DSLs. Our second key idea is that of\\nguarded DSLs that allow a precedence on DSL operators, which dynamically controls exploration of various\\npaths in the DSL. This makes search efficient over grammars with large fanouts without losing recall. It also\\nmakes ranking simpler yet more effective in learning an intended program from very few examples. Both\\ncuts and precedence provide a mechanism to the DSL designer to restrict search to a reasonable, and possibly\\nincomplete, space of programs.\\nUsing cuts and gDSLs, we have built FlashFill++, an industrial-strength PBE engine for performing rich\\nstring transformations, including datetime and number manipulations. The FlashFill++ gDSL is designed to\\nenable readable code generation in different target languages including Excel‚Äôs formula language, PowerFx,\\nand Python. We show FlashFill++ is more expressive, more performant, and generates better quality code than\\ncomparable existing PBE systems. FlashFill++ is being deployed in several mass-market products ranging\\nfrom spreadsheet software to notebooks and business intelligence applications, each with millions of users.\\nCCS Concepts: ‚Ä¢ Software and its engineering ‚ÜíProgramming by example; Domain specific languages.\\n‚àóAuthors in alphabetic order\\nAuthors‚Äô addresses: Jos√© Cambronero, Microsoft, USA, jcambronero@microsoft.com; Sumit Gulwani, Microsoft, USA,\\nsumitg@microsoft.com; Vu Le, Microsoft, USA, levu@microsoft.com; Daniel Perelman, Microsoft, USA, danpere@microsoft.\\ncom; Arjun Radhakrishna, Microsoft, USA, arradha@microsoft.com; Clint Simon, Microsoft, USA, clint.simon@microsoft.\\ncom; Ashish Tiwari, Microsoft, USA, astiwar@microsoft.com.\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,\\ncontact the owner/author(s).\\n¬© 2023 Copyright held by the owner/author(s).\\n2475-1421/2023/1-ART33\\nhttps://doi.org/10.1145/3571226\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 1, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:2\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nAdditional Key Words and Phrases: programming by example, domain-specific languages, string transforma-\\ntions\\nACM Reference Format:\\nJos√© Cambronero, Sumit Gulwani, Vu Le, Daniel Perelman, Arjun Radhakrishna, Clint Simon, and Ashish\\nTiwari. 2023. FlashFill++: Scaling Programming by Example by Cutting to the Chase. Proc. ACM Program.\\nLang. 7, POPL, Article 33 (January 2023), 30 pages. https://doi.org/10.1145/3571226\\n1\\nINTRODUCTION\\nProgramming-by-examples (PBE) has seen tremendous interest and progress in the last decade [Gul-\\nwani et al. 2017]. A variety of approaches have been proposed targeting various applications. Starting\\nfrom purely symbolic techniques, the field has explored neural [Devlin et al. 2017] and neurosym-\\nbolic approaches [Chaudhuri et al. 2021; Kalyan et al. 2018; Rahmani et al. 2021; Verbruggen et al.\\n2021]. In this paper, we present novel symbolic techniques to improve the scalability of PBE systems.\\nPBE applications range from enabling non-experts to author programs for spreadsheet data\\nmanipulation [Gulwani et al. 2012] or application creation in a low-code/no-code setting [Lukes\\net al. 2021], to improving productivity of data scientists for data wrangling tasks [Le and Gulwani\\n2014; Miltner et al. 2018] and even automating professional developers‚Äô repeated edits [Pan et al.\\n2021; Rolim et al. 2017]. A flagship application for PBE is that of string transformations, for\\ninstance, converting ‚ÄòAlan Turing‚Äô to ‚Äòturing, alan‚Äô‚Äîsuch tasks are very common and are\\nwell described by examples [Gulwani 2011]. We focus on such string transformations, though our\\ntechnical contributions are more generally applicable to any grammar-based synthesis setting.\\nMost PBE engines work by defining a program search space, and then employing some strategy\\nto search over it. The program space is often defined by a domain-specific language (DSL), which\\nfixes a finite set of operators and all the different ways they can be composed to create programs. A\\nkey challenge in PBE is scaling the search to very large program spaces. DSL designers have to\\nbuild DSLs that are expressive enough to be useful, yet small enough to keep the program search\\nspace small. This tension in DSL design has hindered broader applications of program synthesis.\\nUsers implicitly advocate for larger DSLs as they want synthesizers to produce programs that are\\ncloser to the ones they would manually write, i.e., ones that use a large variety of functions that\\nare available in general purpose programming languages. On the other hand, these larger DSLs\\n(a) make the search space large and the synthesis slow, and (b) more importantly, allow the large\\nnumber of functions to be combined in unintuitive ways to produce undesirable programs. Program\\nsynthesis research has mainly focused on completeness, i.e., ensuring that we find a program when\\none exists, and insisting on completeness for large DSLs exacerbates these problems. We introduce\\ntwo new mechanisms, cuts and precedence, by which DSL designers can control the program search\\nspace even as the DSL itself grows in size. This not only eases the job of the DSL designer, but also\\nenables them to build synthesizers based on very expressive DSLs.\\nChallenges. Let us say we are given an input-output example: a tuple of input values and one\\noutput value. There are two commonly used search strategies to find programs that would generate\\nthe output value using the input values: bottom-up and top-down.\\nBottom-up (BU) search starts with the inputs and generates all possible values that can be computed\\nfrom the inputs using all possible (partial) programs in the search space. It does so by applying the\\nexecutable semantics functions of the DSL operators. Thus, information flows from the inputs, and all\\ncomputed intermediate results are completely oblivious to the output. The BU strategy is effective\\nonly when the sets of values generated in each intermediate stage remains small. This happens\\nwhen there are only a small number of leaf constants and each operator is effectively-enumerable\\n(EE) ‚Äì namely, it has a small arity and many-to-one semantics, thus having a small dynamic fan-out.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 2, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:3\\nTop-down (TD) search starts with the output and applies the inverse semantics of the operators,\\nso-called witness functions, to generate intermediate values that can generate the output value. Thus,\\ninformation flows from the output value, and at every stage, the intermediate values are computed\\nsolely based on the output and are completely oblivious to the input values. The TD strategy is\\neffective only when these intermediate sets are small. This happens only when every operator is\\neffectively invertible (EI) ‚Äì namely, it allows for effective (inverse) computation of various inputs\\nthat can yield a given output.\\nIf the DSL has both non-EE and non-EI operators, then neither bottom-up nor top-down strategies\\nare effective. Recently, it was observed that one could scale synthesis to larger DSLs by combining\\nthe two strategies [Lee 2021]. If there is a partition of the DSL such that the sub-DSL closer to\\nthe start symbol has EI operators, and the rest contains EE operators, then the two strategies\\ncan be combined to yield a meet-in-the-middle strategy at that cut [Lee 2021]. However, this new\\nstrategy still has only a limited form of information flow between the inputs and the output. In\\nfact, the DSLs used in Duet [Lee 2021] are relatively small, albeit larger than those in FlashFill and\\nFlashMeta [Gulwani 2011; Polozov and Gulwani 2015]. These latter systems are based on a TD\\nstrategy over very small DSLs.\\nAn alternate way to scale PBE synthesis is based on abstraction and refinement types [Feng et al.\\n2017; Guo et al. 2020; Polikarpova et al. 2016; Wang et al. 2017]. The idea behind abstraction is that\\ninstead of computing the exact set of values that can be generated (in either top-down or bottom-up\\nstrategy), we compute overapproximations of the sets of values. This approach works when high\\nquality abstractions can be quickly generated. Typically, it is still ‚Äúone-sided‚Äù ‚Äì either the inputs flow\\nto intermediate values or the output value flows backwards to intermediate values. Furthermore,\\nabstractions of compositions of operators are computed by composing the abstractions of operators,\\nwhich loses accuracy as the composition depth grows. We overcome some of these shortcomings\\nin our work; however, abstraction-based approaches are inherently complementary.1\\nOur Contribution. In this paper, we present two novel techniques - cuts and precedence - to\\neffectively address the scalability challenges of PBE synthesis. The executable semantics functions\\n(that are the basis of BU) and the witness functions (that are basis of TD) are defined to allow\\ninformation to flow in one direction. We introduce cuts that prune values generated by witness\\nfunctions guided by the values that sub-DSLs could possibly compute on the inputs. This concept\\ninherently builds in bi-directional information flow in its definition, and in fact, generalizes the\\nsemantics functions and witness functions. A DSL designer can author a cut function based on their\\nintuition of the form of values that can be computed at a nonterminal using the inputs, and then\\nrestricting them to those that would be relevant for the output. Unlike abstractions, cuts are not\\ncomputed compositionally. They are provided for whole sub-DSLs; thus, they avoid information loss\\naccumulated by composing lossy abstractions. This is similar to how accelerations avoid information\\nloss in program analysis by capturing the effect (composition or transitive closure) of multiple state\\ntransitions by a single ‚Äúmeta transition‚Äù [Finkel 1987; Karp and Miller 1969].\\nA top-down strategy would get stuck at a non-EI operator. However, a cut function for an\\nargument of that non-EI operator can help unblock TD synthesis. As a special case, a cut for that\\nargument can be generated using bottom-up enumeration, in which case we get the meet-in-the-\\nmiddle strategy [Lee 2021]. However, cuts may be generated by other means based on the DSL\\ndesigner‚Äôs insight. In general, we get a novel search strategy, middle-out synthesis, which uses cuts\\n1In this paper, we focus exclusively on synthesis approaches based on concrete values: the specification is a concrete IO\\nexample, and the semantics functions (and the inverse semantics) are given on concrete values (and not abstract values or\\nrefinement types). More specifically, we are in the context of version-space algebra (VSA) driven synthesis, and hence the\\nterms top-down and bottom-up are always used in that context.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 3, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:4\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nto reduce the original PBE problem over a large DSL (with potentially non-EI and non-EE operators)\\ninto simpler PBE problems over (smaller depth) sub-DSLs with only EI or only EE operators.\\nOur second key idea is to introduce precedence over operators in the grammar of domain-specific\\nlanguages. Precedence is a natural concept in grammars and arises naturally when the DSL designer\\nwants to prefer certain operators over others. We show that if the precedence is a series-parallel\\npartial order, then it can be encoded as an ordering on grammar rules to create a guarded DSL (gDSL),\\nand program search can be performed directly on the gDSL without compromising soundness or\\ncompleteness, while gaining efficiency. The ordering on rules in a gDSL is interpreted as a mandate\\nto explore a certain branch only when higher-ordered branches have failed to return a result. This\\nhas two major advantages. First, it makes the search more scalable by dynamically using different\\nunderapproximations of the DSL to be explored. Second, it makes ranking simpler to write for DSL\\ndesigners because the precedence already builds in a default ranking over programs.\\nCuts and precedence provide DSL designers two new mechanisms to control the program search\\nspace, beyond what they get through designing DSLs. Our contributions include:\\n‚Ä¢ A new algorithmic approach for PBE (middle-out synthesis) that leverages a novel cut rule to\\nspeed up synthesis over large DSLs and to handle non-EI and non-EE operators.\\n‚Ä¢ A new formalism of guarded DSLs that supports operator precedence, and an extension of\\nour synthesis approach to gDSLs that scales synthesis to large DSLs and gives ranking based\\non path orderings [Dershowitz and Jouannaud 1990] for free.\\n‚Ä¢ A new and expressive system FlashFill++ for string transformations that supports datetime &\\nnumber manipulations and is designed for readable code generation.\\n‚Ä¢ An extensive comparison of FlashFill++ with existing state-of-the-art PBE systems for string\\ntransformations (FlashFill [Gulwani 2011], SmartFill [Chen et al. 2021a], and Duet [Lee 2021])\\nthat shows improvements in expressiveness, learning performance, and code readability.\\n2\\nOVERVIEW\\n2.1\\nNew Challenges in PBE for String Transformations\\nWe first discuss some challenges faced by the current generation of PBE tools for string trans-\\nformations. These challenges were compiled in collaboration with two key industrial deployers\\nof PBE: the Microsoft Excel and the Microsoft PowerBI teams. To compile these challenges, we\\ninterviewed several product managers in these teams, interacted with both expert and novice users\\nof the FlashFill feature, and analyzed online help posts.\\nGenerating Readable Code. Consider the task of transforming the input pair (\"David Walker\",\\n\"623179\") to the output string \"D-6231#walker\". Any string processing library would contain\\nmany redundant methods for extracting \"Walker\" from \"David Walker\". For example, in Python,\\nwe could use the split method to accomplish the task. Alternatively, we could use the find method\\nalong with string slicing, or use regular expressions.\\nIn contrast to the design of string processing libraries, the prevailing wisdom in DSL design for\\nsynthesis has been to work with a minimal number of operators [Gulwani 2016]. For example,\\nFlashFill [Gulwani 2011] and the more recent Duet [Lee 2021] DSLs contain only 3 and 5 functions\\nthat directly operate on strings, respectively. Smaller DSLs lead to smaller program search spaces,\\nyielding better synthesis performance and effective ranking [Polozov and Gulwani 2015]. Following\\nthis minimalism to an extreme can lead to a DSL whose programs translate to very unnatural and\\nunreadable programs in target languages like Python (see Figure 1) or PowerFx (see Figure 2).\\nOne straightforward approach to readability is writing a good translator from the synthesis DSL\\nto the target language. However, if the semantic gap between the DSL and the target languages‚Äô\\noperators is large, then ‚Äúreadable translation‚Äù itself becomes a new and nontrivial synthesis problem.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 4, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:5\\n# Python program generated by FlashFill\\nimport regex\\ndef transformation_text_program(input1 , input2 ):\\ncomputed_value_83 = regex.search(r\"\\\\p{Lu}+\", input1 ). group (0)\\nindex1_83 = regex.search(r\"[-.\\\\p{Lu}\\\\p{Ll}0 -9]+\", input2 ). start ()\\ncomputed_value_0_83 = input2[index1_83 :( len(input2) + -2)]\\nkth_match_83 =\\nl i s t (regex.finditer(r\"[-.\\\\p{Lu}\\\\p{Ll}0 -9]+\", input1 ))[ -1]\\ncomputed_value_1_83 = kth_match_83.group (0). lower ()\\nreturn computed_value_83+\"-\"+computed_value_0_83+\"#\"+computed_value_1_83\\n# Python program generated by FlashFill ++\\ndef formula(i1 , i2):\\ns1 = i1[:1]\\ns2 = i2[:4]\\ns3 = i1.split(\" \")[1]. lower()\\nreturn s1 + \"-\" + s2 + \"#\" + s3\\n# Python program generated by FlashFill ++ and renamed by Codex\\ndef formula(name , number ):\\nfirst_initial = name [:1]\\nnumber_prefix = number [:4]\\nlast_name = name.split(\" \")[1]. lower()\\nreturn first_initial + \"-\" + number_prefix + \"#\" + last_name\\nFig. 1. The python programs generated by FlashFill and FlashFill++ for the task of transforming (\"David\\nWalker\", \"623179\") to \"D-6231#walker\". FlashFill++‚Äôs program is much more readable and its readability\\nis further improved by renaming variables using a pretrained large language model, as shown.\\nOur insight is that to effectively generate readable code the DSLs should not be designed with the\\nsingle-minded goal of efficient learning, but also pay heed to the target languages.\\nWhile generating readable code is challenging, the need is sorely felt in industrial PBE tools‚Äî\\nusers are more likely to trust and use PBE tools if they produce idiomatic, readable code. Quoting\\none study participant in [Drosos et al. 2020]: ‚Äúdon‚Äôt know what is going on there, so I don‚Äôt know if I\\ncan trust it if I want to extend it to other tasks. I saw my examples were correctly transformed, but\\nbecause the code is hard to read, I would not be able to trust what it is doing‚Äù. The lack of readable\\ncode is one of the primary challenges preventing a broader adoption of PBE technologies.\\nMultiple Target Languages. The need for readable code generation is compounded by the\\nproliferation of different target languages, each with their own set of operations; see Figure 3.\\nThese target languages range across standard programming languages (e.g., Python, R), individual\\nlibraries (e.g., Pandas, PySpark), data query languages (e.g., SQL), and custom application-specific\\nlanguages (e.g., Google Sheets & Excel formula languages, PowerBI‚Äôs M language). Apart from the\\nobvious benefit, multiple target support can also help with learning: seeing the same program in\\nmultiple languages helps with cross-language knowledge transfer [Shrestha et al. 2018].\\nDate-Time and Numeric Transformations. Most string PBE technologies cannot natively\\nhandle date-time and numeric operations efficiently, leading to situations like transforming ‚Äòjan‚Äô\\nto ‚ÄòJanember‚Äô given the input-output example ‚Äònov‚Äô ‚Ü¶‚Üí‚ÄòNovember‚Äô. Duet [Lee 2021] does allow\\nfor limited numeric operators, but still lacks support for important data-processing operations\\nsuch as rounding and bucketing. According to the Microsoft Excel team, date-time and numeric\\noperations (of the kind shown in Figure 3) are among the most requested FlashFill features. However,\\nas illustrated in Section 2.2, these operations are not amenable to standard synthesis techniques.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 5, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:6\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\n# PowerFx formula generated by FlashFill\\nConcatenate(\\nMid(Left(input1 , Match(input1 , \"\\\\p{Lu}+\"). StartMatch\\n+ Len(Match(input1 , \"\\\\p{Lu}+\"). FullMatch) - 1),\\nMatch(input1 , \"\\\\p{Lu}+\"). StartMatch),\\nConcatenate(\"-\",\\nConcatenate(Mid(Left(input2 ,Len(input2)-2), Match(input2 ,\"[0 -9]+\"). StartMatch),\\nConcatenate(\"#\",\\nLower(Mid(\\nLeft(input1 ,\\nFirst(LastN(MatchAll(input1 , \"[\\\\p{Lu}\\\\p{Ll}]+\"), 1)). StartMatch\\n+ Len(First(LastN(MatchAll(input1 , \"[\\\\p{Lu}\\\\p{Ll}]+\"), 1)). FullMatch )-1),\\nLast(MatchAll(input1 , \"[\\\\p{Lu}\\\\p{Ll}]+\")). StartMatch ))))))\\n# PowerFx formula generated by FlashFill ++\\nLeft(input1 , 1) & \"-\" & Left(input2 , 4) & \"#\"\\n& Lower(Last(FirstN(Split(input1 , \" \"), 2)). Result)\\nFig. 2. PowerFx formulas generated by FlashFill and FlashFill++ to transform (\"David Walker\", \"623179\") into\\n\"D-6231#walker\". The latter is much more readable than the former.\\nPerforming operations over datetimes and numbers allows our system to handle use cases like\\nthe one detailed in Fig. 4(a), which presents 911 call records that need to be transformed. Each call\\nlog, shown in the Input column, contains an (optional) address, the township, the call date and time,\\nfollowed by possible annotations indicating the specific 911 station that addressed the call. Let us\\nsuppose that a data scientist wants to extract the date (2015-12-11) and time (13:34:52) from each log,\\nand map it to the corresponding weekday (Fri) and the 3-hour window (12PM - 3PM), as shown in\\nthe Output column. Performing this transformation requires string processing to extract candidate\\ndates and times, parsing these substrings into appropriate datatypes, performing type-specific\\ntransformations on the extracted values, and then formatting them into an appropriate output\\nstring value. This is beyond the capabilities of current synthesizers. Our system can synthesize the\\nintended program (shown in Fig. 4(b)) from just the first example. This program is readable and\\nalso serves educational value (e.g. teaching the API of the popular datetime Python library).\\n2.2\\nOverview of FlashFill++\\nWe now show how our novel techniques address the various challenges from subsection 2.1.\\nExtended Domain-Specific Language. The main strength of FlashFill++ compared to previous\\nsystems is its expanded DSL containing over 40 operators, including 25 for just strings and the rest\\nfor datetime and numbers, such as for rounding and bucketing; see Figure 7. Contrast this with the\\nnumbers 3 and 5 mentioned previously for FlashFill and Duet.\\nThis extended DSL supports more expressive and more readable programs. Contrast the code\\ngenerated by FlashFill++ in Fig. 1 and 2 to that generated by FlashFill to see the clear difference an\\nextended DSL makes. However, expanding the DSL comes with its own set of challenges: (a) Given\\nthe larger search space, standard synthesis techniques fall short on efficiency. (b) The larger search\\nspace also complicates ranking‚Äîthe problem of picking the best (or intended) program among all\\nthe ones consistent with the examples. (c) Handling numeric and date-time operators requires new\\nsynthesis techniques. Next, we discuss some novel strategies to address these challenges.\\nCuts and Middle-Out Synthesis. Our new DSL contains several non-EI operators (required for\\nnumber and datetime operations) that inhibit use of a top-down synthesis strategy across those\\noperators. Furthermore, bottom-up synthesis is not feasible for the sub-DSLs below those operators\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 6, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:7\\nRound to last day of month: 2/5/2020 =‚áí2/29/2020\\nfrom datetime import datetime\\nfrom dateutil.relativedelta import *\\ndef formula(i1):\\nmonth_start = datetime(i1.year ,i1.month ,1)\\nmonth_end = month_start\\n+ relativedelta(months =1)\\nreturn month_end - relativedelta(days =1)\\nEOMONTH(A1)\\nWith({ monthStart:\\nDate(Year(i1), Month(i1), 1)\\n},\\nDateAdd(\\nDateAdd(\\nmonthStart , 1, \"\" Months \"\"),\\n-1, \"\"Days\"\"\\n))\\nRound to start of quarter: 2/5/2020 =‚áí1/1/2020\\nfrom datetime import datetime\\ndef formula(i1):\\nquarter = (i1.month - 1) // 3 + 1\\nreturn datetime(i1.year ,3* quarter -2,1)\\nEOMONTH(\\nDATE(YEAR(A1),\\nROUNDUP(MONTH(A1)/3,\\n0)*3,1),\\n0)\\nWith({ quarter:\\nRoundUp(Month(i1) / 3, 0)\\n},\\nDate(Year(i1),quarter *3-2,1)\\n+ Time(0, 0, 0))\\nDays since start of year: 2/5/2029 =‚áí36\\ndef formula(i1):\\nreturn i1.timetuple (). tm_yday\\nA1 - DATE(YEAR(A1), 1, 1) + 1\\nDateDiff(\\nDate(Year(i1),1,1),i1) + 1\\nCreate year-quarter string: 4/5/1983 =‚áí‚Äò1983-Q2‚Äô\\nfrom datetime import datetime\\ndef formula(i1):\\nquarter = (i1.month -1)//3 + 1\\nreturn i1.strftime(\"%Y\") +\\n\"-Q\"+f\"{quarter :01.0f}\"\\nYEAR(A1) & \"-Q\" &\\n& ROUNDUP(MONTH(A1)/3, 0)\\nText(i1 , \"yyyy\", \"en -US\")\\n& \"-Q\" &\\nText(\\nRoundUp(Month(i1) /3, 0),\\n\"0\",\\n\"en -US\")\\nExtract number, convert and round: ‚ÄòYour Total: $1,2564.45‚Äô =‚áí12564.5ùëë\\nfrom decimal import *\\ndef formula(i1):\\nsource = Decimal( str ( float (\\ni1.split(\"$\")[-1]. replace(\",\", \"\"))))\\ndelta = Decimal(\"0.5\")\\nreturn\\nfloat (( source / delta)\\n.quantize(0, ROUND_CEILING) * delta)\\nROUNDUP(\\nNUMBERVALUE(\\nRIGHT(A1 ,\\nLEN(A1)-FIND(\"$\", A1))\\n) / 0.5,\\n0\\n) * 0.5\\nRoundUp(Value(\\nLast(\\nSplit(i1 , \"$\")\\n).Result ,\\n\"en -US\"\\n) * 2, 0) / 2\\nFig. 3. Code produced by FlashFill++ for various date-time and rounding scenarios in Python (left), Excel\\n(center), and PowerFx (right) respectively.\\ndue to enumeration blowup, rendering a meet-in-the-middle strategy infeasible. We propose a\\nnovel middle-out synthesis strategy that uses cuts to deal with such non-EI operators.\\nConsider the number parsing, rounding, and formatting subset of the FlashFill++ DSL below\\ndecimal roundNumber := RoundNumber(parseNumber, roundNumDesc)\\ndecimal parseNumber := ParseNumber(substr, locale) | ...\\nstring substr\\n:= ...\\nFix the input-output example ‚ü®‚ÄúThe price is $24.58 and 46 units are available.‚Äù ‚Ü¶‚Üí\\n24.00‚ü©for the non-terminal roundNumber. We first discuss the short-comings of both bottom-\\nup and top-down synthesis in this case.\\nTop-Down Synthesis using Witness Functions. In FlashMeta-style programming-by-example, the\\nprimary deductive tools are witness functions. Given a specification in the form of an input-output\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 7, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:8\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\n(a)\\nInput\\nOutput\\nCEDAR AVE & COTTAGE AVE; HORSHAM; 2015-12-11 @ 13:34:52;\\nFri, 12PM - 3PM\\nRT202 PKWY; MONTGOMERY; 2016-01-13 @ 09:05:41-Station:STA18;\\nWed, 9AM - 12PM\\n; UPPER GWYNEDD; 2015-12-11 @ 21:11:18;\\nFri, 9PM - 12AM\\n(b)\\ndef derive_value(_input ):\\ntext = _input.split(\";\")[2]\\npart_0 = text.split(\" \")[0]; part_1 = text.split(\" \")[2][:8]\\ndate = datetime.datetime.strptime(part_0 , \"%Y-%m-%d\")\\ntime = datetime.datetime.strptime(part_1 , \"%H:%M:%S\")\\nbase_value = datetime.timedelta(hours=time.hour , minutes=time.minute ,\\nseconds=time.second , microseconds=time.microsecond)\\ndelta_value = datetime.timedelta(hours =3)\\ntime_str = (time - base_value % delta_value ). strftime(\"%#I%p\")\\nrounded_up_next = (time - base_value % delta_value) + delta_value\\ncomputed_value = time_str + \"-\" + rounded_up_next.strftime(\"%#I%p\")\\nreturn date.strftime(\"%a\") + \", \" + computed_value\\nFig. 4. (a) A task to map the 911-call logs in the Input column to weekday/time buckets in the Output column.\\n(b) Python function synthesized by our approach for this task from just one example.\\nexample ùëñ‚Ü¶‚Üíùëúand a top-level operator ùêπ, a witness function for position ùëògenerates a sub-\\nspecification for the ùëòùë°‚Ñéparameter for ùêπ. For example, if the top-level operator is concat(ùëÅ1, ùëÅ2)\\nand the input-output example is ùëñ‚Ü¶‚Üí‚Äúabc‚Äù, the witness function for the 1ùë†ùë°position will return\\n{‚Äúa‚Äù, ‚Äúab‚Äù} (assuming we do not consider the trivial case of appending an empty string). In the\\nexample we are considering, writing a witness functions for the RoundNumber operator is not as\\nstraight-forward‚Äîthere are an infinite set of numbers that can round to 24.00. A standard top-down\\nprocedure cannot handle this infinite-width witness function.\\nBottom-Up Synthesis. The other major paradigm for programming-by-example is bottom-up syn-\\nthesis: here, the synthesizer starts enumerating programs ‚Äì starting from constants and iteratively\\napplying operators from the grammar on the previously generated programs ‚Äì and checks if any\\nof the enumerated programs satisfies the given input-output example. An efficient bottom-up\\nsynthesizer will avoid enumerating all programs using observational equivalence‚Äîthat is, it only\\ngenerate programs that produce different outputs for the given input. In our running example, the\\nsynthesizer will begin by generating substr sub-programs and concrete values for locale and\\nroundNumberDesc. However, enumerating such sub-programs is expensive‚Äîthe fragment of the\\nDSL reachable from the non-terminal substr is large. In fact, string operations like substr are\\nbest handled using witness functions.\\nMiddle-Out Synthesis using Cuts. Examining our input-output example by hand, it is easy to see that\\nin any valid program the output of ParseNumber should be derived from a numerical substring\\nin the input. Intuitively, it does not matter what or how complex the substr sub-program is; we\\ncan be confident that the output of the ParseNumber will be either 24.58 or 46. Cuts capture this\\nsimple intuition‚Äîfor a given input-output example ùëñ‚Ü¶‚Üíùëúand a non-terminal ùëÅthat expands to\\nùëì(ùëÅ1, ùëÅ2), the cut for ùëÅ1 (say) in the context of ùëÅwill be a set of values {ùëú1,ùëú2, . . . ,ùëúùëõ} such that\\nin any desired program ùëÉgenerated by ùëÅ, the output of the sub-program corresponding to ùëÅ1 will\\nbe one of the ùëúùëòfor ùëò‚àà{1, 2, . . . ,ùëõ}.\\nGiven that the output of ParseNumber will be either 24.58 or 46, the synthesizer has two sub-\\ntasks for the 24.58 case (the 46 case will be similar): (a) synthesizing a program for ùëùùëéùëüùë†ùëíùëÅùë¢ùëöùëèùëíùëü\\nfor the example ùëñ‚Ü¶‚Üí24.58, and (b) synthesizing a program for ùëüùëúùë¢ùëõùëëùëÅùë¢ùëöùëèùëíùëüfor the example\\nùëñ‚Ü¶‚Üí24.00 using a modified DSL, which is generated dynamically in middle-out synthesis, where\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 8, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:9\\nparseNumber ‚Üí24.58 is the only rule whose head is parseNumber. Both sub-tasks can now be\\nrecursively solved, possibly using either the top-down strategy or the bottom-up strategy.\\nGuarded Context-Free Grammars. The larger program space resulting from an extended DSL\\nwith a wide range of operators poses both efficiency and ranking challenges. However, human\\nprogrammers often encounter the same challenge when writing their own implementations and\\ndecide between these operators and programs using simple rules of thumb, which can be leveraged\\nboth for improving search efficiency and ranking. For example, ‚Äúif a task can be done using a date-\\ntime function, do not use string transformation functions‚Äù or ‚Äúif a task can be done using string\\nindexing, do not use regular expressions‚Äù. To mimic this kind of coarse reasoning, we introduce\\nthe notion of gDSLs. In a gDSL, the production rules for each non-terminal are ordered (with a\\npartial order |‚ä≤), with production rules earlier in the order preferred to ones later in the order. For\\nexample, the rule concat := segment |‚ä≤Concat(segment, concat) expresses that we always\\nprefer programs that do not use the Concat operation to ones that do. During synthesis for a gDSL\\nrule ùëÅ‚Üíùõº|‚ä≤ùõΩthe branch ùõΩis explored only if the branch ùõºfails to produce a program. This\\ngreatly improves the performance of synthesis and the FlashFill++ synthesis times are competitive\\nwith other synthesis techniques that work with significantly smaller DSLs.\\nApart from improving the efficiency of search, gDSLs also simplify the task of writing ranking\\nfunctions. Intuitively, the precedence in the guarded rules induce a ranking on programs, and any\\nadditional ranking function only needs to order the remaining incomparable programs. Precedences\\nrank programs by a lexicographic path ordering (LPO) [Dershowitz and Jouannaud 1990], and our\\nfinal ranker will be a lexicographic combination of LPO and base arithmetic ranker ‚Äì such program\\nrankers have not been used in program synthesis before.\\n3\\nBACKGROUND: PROGRAMMING BY EXAMPLE\\nWe now define the problem of programming-by-example and discuss common solutions.\\n3.1\\nDomain-Specific Languages\\nWe use domain-specific languages (DSLs) to specify the set of target programs for a synthesizer.\\nFormally, a DSL D is given by ‚ü®N, T, F, R, Vin, ùë£out‚ü©where:\\n‚Ä¢ N is a finite set of non-terminal symbols (or non-terminals). The symbol ùë£out is a special start\\nnon-terminal in N that represents the output of a program in the DSL.\\n‚Ä¢ T is a set of terminal symbols (or terminals) that is partitioned as Vin ‚à™O into inputs Vin and\\nvalues O. The set Vin contains special terminals, in1, in2, . . ., that represent the input symbols\\nin a program of the DSL. The set O contains constant values.\\n‚Ä¢ F is a finite set of function symbols (or operations). Each operation f ‚ààF has a fixed arity\\nArity(f). The semantics of f, denoted by JfK, is a mapping from OArity(f) to O.\\n‚Ä¢ R is a set of rules of the form ùëÅ‚Üíf(ùë£1, . . . , ùë£ùëò) or ùëÅ‚Üíùë£0 where ùëÅ‚ààN, f ‚ààF , ùë£0 ‚ààT,\\nand ùë£1, . . . , ùë£ùëò‚ààN ‚à™T.\\nThe formalism above is untyped for ease of reading. In practice, the FlashFill++ DSL is typed‚Äî values\\ncan be integers, floats, strings, Booleans, and date-time objects, and each non-terminal, terminal,\\nand operator have specific type signatures.\\nEvery terminal or nonterminal ùë£generates a set L(ùë£) of programs defined recursively as follows:\\n(a) L(inùëò) = {inùëò} for all input symbols inùëò‚ààVin, (b) L(ùëú) = {ùëú} for all values ùëú‚ààO, and\\n(c) L(ùëÅ) =\\n\\x08f(ùëÉ1, . . . , ùëÉùëõ) | ùëÅ‚Üíf(ùë£1, . . . , ùë£ùëõ) ‚ààR, ‚àÄùëñ.ùëÉùëñ‚ààL(ùë£ùëñ)\\n\\t\\n‚à™\\n\\x08\\nùëÉ| ùëÅ‚Üíùë£‚ààR, ùë£‚ààT, ùëÉ‚àà\\nL(ùë£)\\n\\t\\n. The set of programs defined by the whole DSL L(D) is L(ùë£out).\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 9, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:10\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nExample 3.1. The following is a simple DSL Dùê¥for affine arithmetic expressions over naturals\\nN: (a) Here, the terminals T are {input1, input2, 0, 1, 2, . . .}, with input1, input2 being the spe-\\ncial input terminals in Vin. (b) The non-terminals N are {output, addend, const}, with output\\nbeing the output symbol ùë£out. (c) The operators F are Plus and Times. (d) The rules R are given\\nby {output ‚ÜíPlus(addend, output), output ‚Üíconst, addend ‚ÜíTimes(const, input1),\\naddend ‚ÜíTimes(const, input2), const ‚Üí0 | 1 | 2 | . . .}. Note that the declaration uint\\nconst; in the listing is shorthand for a set of rules for the form const ‚Üíùëòfor each ùëò‚ààN.\\nPlus(Times(5, input1), 3) is a sample program in L(Dùê¥).\\n‚ñ°\\n@input uint input1, input2;\\n@start uint output := Plus(addend, output) | const;\\nuint addend := Times(const, input1) | Times(const, input2);\\nuint const;\\n3.2\\nSynthesis Tasks and Solutions\\nA state ùëÜis a valuation of all input symbols in a DSL, i.e., ùëÜ= {in1 ‚Ü¶‚Üíùëú1, . . . , inùëò‚Ü¶‚Üíùëúùëò} where inùëñ\\nare input symbols and ùëúùëñare values. An example Ex is a pair ùëÜ‚Ü¶‚Üíùëúof a state ùëÜand value ùëú.\\nA synthesis task ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©is given by: (a) a term ùõº, which is either a nonterminal, terminal, or\\nright-hand side of a rule, (b) a set R of rules, and (c) an example ùëÜ‚Ü¶‚Üíùëú. A solution of the synthesis\\ntask ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©is a program ùëÉsuch that: (a) JùëÉK(ùëÜ) = ùëú, and (b) ùëÉ‚ààL(ùõº). Here, JùëÉK : OVin ‚Ü¶‚ÜíO\\nrepresents the standard semantics of a program. Formally, JùëÉK(ùëÜ) is recursively defined as: (1)\\nJinùëñK(ùëÜ) = ùëÜ(inùëñ), (2) JùëúK(ùëÜ) = ùëúfor every value ùëú, (3) Jf(ùë£1, ùë£2)K(ùëÜ) = JfK(Jùë£1K(ùëÜ), Jùë£2K(ùëÜ)) for every\\noperator f. To keep the presentation simple, the synthesis task is defined to contain one input-output\\nexample. In practice, a synthesis task often involves multiple examples. It is straight-forward to\\nextend our technique for this, as done in our implementation.\\nExample 3.2. A sample synthesis task for the affine expression DSL Dùê¥is ‚ü®output, R,ùëÜ‚Ü¶‚Üí7‚ü©\\nwhere ùëÜ= {input1 ‚Ü¶‚Üí2, input2 ‚Ü¶‚Üí0}. The program Plus(Times(3, input1), 1) is a solution of\\nthis task. Here Times and Plus have their usual arithmetic semantics.\\n‚ñ°\\nGiven a synthesis task ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, a synthesizer generates a program set PS such that every\\nprogram in the set PS is a solution of the synthesis task, which we denote by the assertion PS |=\\n‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. Note that it is vacuously true that ‚àÖ|= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, and so practical synthesizers\\nstrive to establish the above assertion for nonempty sets PS. The notation Ã∏|= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©denotes\\nthat there is no nonempty set PS that is a solution for the synthesis task.\\n3.3\\nBottom-Up and Top-Down Synthesis\\nThere are two main approaches for solving the synthesis task: bottom-up and top-down.\\nThe bottom-up (BU) approach enumerates programs generated by different nonterminals of the\\ngrammar and collects the values that those programs compute on the input state ùëÜ. More precisely,\\nfor each nonterminal ùëÅ‚Ä≤, the BU approach computes the bottom-up value set buùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) given by\\n{JùëÉ‚Ä≤K(ùëÜ) | ùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤)}. These sets are computed starting from the leaf (terminals) of the grammar\\nand moving up to the root (start symbol ùë£out). Success is declared if the output value ùëúis found to\\nbe in the set buùë£out (ùëÜ‚Ü¶‚Üíùëú). Note that the BU procedure is not guided by the output value ùëú.\\nThe top-down (TD) approach starts with the output value ùëúthat needs to be generated at start\\nsymbol ùë£out, and for every nonterminal ùëÅ‚Ä≤, it computes the set of values that flow to ùëúat ùë£out.\\nMore formally, we say a value ùëú‚Ä≤ at ùëÅ‚Ä≤ flows to value ùëúat ùëÅif either (a) ùëÅ‚Üíùëì(. . . , ùëÅ‚Ä≤, . . .) is a\\ngrammar rule and JùëìK(ùëú1, . . . ,ùëú‚Ä≤, . . . ,ùëúùëò) = ùëúfor some values ùëú1, . . . ,ùëúùëò, or (b) there exist ùëú‚Ä≤‚Ä≤ and\\nùëÅ‚Ä≤‚Ä≤ s.t. ùëú‚Ä≤ at ùëÅ‚Ä≤ flows to ùëú‚Ä≤‚Ä≤ at ùëÅ‚Ä≤‚Ä≤ and ùëú‚Ä≤‚Ä≤ at ùëÅ‚Ä≤‚Ä≤ flows to ùëúat ùëÅ. For each nonterminal ùëÅ‚Ä≤, the\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 10, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:11\\nTD approach computes a top-down value set tdùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) that contains the values ùëú‚Ä≤ at ùëÅ‚Ä≤ that\\nflow into the value ùëúat ùë£out. The top-down value sets are computed using witness functions. An\\noperator f with arity ùëõis associated with ùëõwitness functions - one for each argument position. The\\nùëò-th witness function, WFf,ùëò: Oùëò‚Ü¶‚Üí2O, for operator f maps the desired output and ùëò‚àí1 values\\nfor previous arguments to possible values for the ùëò-th argument. Such a parameterized collection\\nof witness functions is sound (complete) if ùëúùëò‚ààWFf,ùëò(ùëú,ùëú1, . . . ,ùëúùëò‚àí1) for ùëò= 1, . . . ,ùëõimplies (is\\nimplied by) JfK(ùëú1, . . . ,ùëúùëõ) = ùëú. For example, if the top-level operator is Plus and the output is 7, the\\npossible arguments for Plus would be (0, 7), (1, 6), (2, 5), . . ., and hence, WFPlus,1(7) = {0, 1, 2, . . .}\\nand WFPlus,2(7,ùë•) = {7 ‚àíùë•}.\\nTop-down and bottom-up synthesis are both efficient and practical in different scenarios. TD\\nsynthesis performs well when operators are effectively invertible (EI), i.e., the witness functions\\nWFf,ùëòfor each operator f produces sets that are finite and manageable in size. BU synthesis performs\\nwell when the grammar is effectively enumerable (EE), i.e., both the number of constants in the\\ngrammar is bounded and the number of different intermediate values produced is manageable.\\nNote that the focus in this paper is exclusively on synthesis approaches based on concrete values:\\nthe semantics and witness functions are given on concrete values, and not abstract values or types.\\nAbstractions and types can make top-down strategies work on grammars with non-EI operators,\\nfor example [Feng et al. 2017; Polikarpova et al. 2016], but require additional machinery, such as\\ntype systems, abstract domains, and constraint solvers.\\nExample 3.3. The affine expression DSL from Example 3.1 is neither effectively invertible nor\\neffectively enumerable. The operator Plus has a witness function WFPlus,1 which produces a set\\nof size ùëõ+ 1 for an example ùëÜ‚Ü¶‚Üíùëõ. Further, it contains an infinite number of constants (i.e., the\\nnon-negative integers) which make bottom-up approach infeasible. In the FlashFill++ DSL, many\\nstring operators are not effectively invertible. E.g., the LowerCase operator‚Äôs witness function that\\ncan produce a set that is exponential in the input‚Äôs length: WFLowerCase,1(‚Äòabc‚Äô) produces a set of\\nsize 8, i.e., {‚ÄòABC‚Äô, ‚ÄòABc‚Äô, ‚ÄòAbC‚Äô, ‚ÄòaBC‚Äô, ‚ÄòabC‚Äô, ‚ÄòaBc‚Äô, ‚ÄòAbc‚Äô, ‚Äòabc‚Äô}.\\n‚ñ°\\nIn Section 4, we introduce cuts that can be used to decompose the synthesis problem to enable\\nmiddle-out synthesis, which can learn over deep DSLs ‚Äì DSLs that can generate programs with\\nlarge depth ‚Äì where neither bottom-up nor top-down is feasible. In Section 5, we introduce gDSLs,\\nwhich provide a precedence-based mechanism to help learning scale to broad DSLs ‚Äì DSLs with\\nseveral options for a single nonterminal.\\n4\\nCUTS AND MIDDLE-OUT SYNTHESIS\\nTop-down and bottom-up approaches, as well as their combination, struggle to scale to large DSLs.\\nCuts can help scale synthesis. If we want to generate a value ùëúat nonterminal ùëÅ, and another\\nnonterminal ùëÅ‚Ä≤ is on the path from ùëÅto the terminals, then a cut at ùëÅ‚Ä≤ returns values to generate\\nat ùëÅ‚Ä≤ that can help with generating ùëúat ùëÅ.\\nDefinition 4.1 (Cuts). Given a synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and a non-terminal ùëÅ‚Ä≤ ‚ààN, a cut\\nCutùëÅ‚Ä≤,ùëÅfor ùëÅ‚Ä≤ in the context ùëÅ, maps an example, ùëÜ‚Ü¶‚Üíùëú, to a set of values. Such a function is\\ncomplete if for every solution ùëÉfor the task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, whenever ùëÉcontains a sub-program\\nùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤), then JùëÉ‚Ä≤K(ùëÜ) ‚ààCutùëÅ‚Ä≤,ùëÅ(ùëÜ‚Ü¶‚Üíùëú).\\nNote that ùëÅneed not be the start symbol of the grammar and ùëúneed not be the original output\\nvalue in the input-output example. Typically, we define cuts CutùëÅ‚Ä≤,ùëÅwhen ùëÅ‚Üíùëì(ùëÅ‚Ä≤, ùëÅ‚Ä≤‚Ä≤) is a\\ngrammar rule. Such a cut can be used in place of the witness function for the first argument of ùëì.\\nLet us illustrate cuts through an example.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 11, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:12\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nMO.Cut\\nPS1 |= ‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©\\nPS2 |= ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©\\n√ò\\nùëú1\\nPS2[ùëú1 ‚Ü¶‚ÜíPS1] |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nif ùëú1 ‚ààCutùëÅ1,ùëÅ(ùëÜ‚Ü¶‚Üíùëú)\\nFig. 5. The cut inference rule that enables middle-out program synthesis.\\nExample 4.2. Consider again the synthesis task from Section 2.2, where the input-output example\\nwas ùëÜ‚Ü¶‚Üí24.00 and the input state ùëÜwas ‚ü®in1 ‚Ü¶‚Üí‚ÄúThe price is $24.58 and 46 units are\\navailable.‚Äù ‚ü©. Suppose we want to synthesize a program from this example starting from the\\nnonterminal roundNumber of the FlashFill++ DSL (Figure 7). One potential cut for parseNumber in\\nthe context roundNumber could work by scanning the input for any maximal substrings that are\\nnumerical constants and returning them (as a number). Here, it would return {24.58, 46}. A more\\nsophisticated cut could additionally look at the output 24.00 and only return the set {24.58}, as it is\\nthe only value in the string that can be rounded down to 24.00. These cuts are not complete as they\\ndo not include 24.5, which can also be extracted from the input and rounded to 24.\\n‚ñ°\\nRecall that we model synthesizers as generating nonempty program sets PS and asserting\\nPS |= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. Figure 5 presents a new inference rule, called the cut rule, that can be used\\nto generate such assertions. This rule can be used in conjunction with any synthesizer (such\\nas those based on top-down or bottom-up approach). The cut rule uses a cut for a nonterminal\\nùëÅ1 in the context of ùëÅto decompose the overall synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©into two subtasks\\n‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©and ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©. The first, or inner, subtask tries to find a program\\nin L(ùëÅ1) that maps ùëÜto ùëú1, whereas the second, or outer, subtask tries to find a program in L(ùëÅ)\\nthat maps ùëÜto ùëúassuming that we have a program to maps ùëÜto ùëú1. The notation R with ùëÅ1 ‚Üíùëú1\\nsimply means we remove all old rules in R of the form ùëÅ1 ‚Üíùõºand only have one rule ùëÅ1 ‚Üíùëú1.\\nThe cut rule also shows how the solutions to the two subtasks are combined to generate a solution\\nfor the original synthesis task.\\nTheorem 4.3. [Soundness] If program sets PS1 and PS2 are such that PS1 |= ‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©and\\nPS2 |= ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©, then PS2[ùëú1 ‚Ü¶‚ÜíPS1] |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. Furthermore, [complete-\\nness] if a program ùëÉis a solution for the synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and the program ùëÉcontains a\\nsubprogram ùëÉ1 ‚ààL(ùëÅ‚Ä≤) that maps the input ùëÜto a value ùëú1 (i.e., JùëÉ1K(ùëÜ) = ùëú1), then ùëú1 will be in the\\ncut CutùëÅ1,ùëÅ(ùëÜ‚Ü¶‚Üíùëú) assuming that the cut is complete, and moreover, the program ùëÉ[ùëÉ1 ‚Ü¶‚Üíùëú1] is a\\nsolution for the task ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©.\\nWe use the term middle-out synthesis to describe the synthesis approach that uses the Rule MO.Cut\\nto perform synthesis. Note that the subproblems created by Rule MO.Cut can be solved using either\\nthe top-down approach, or the bottom-up approach, or the middle-out approach, or a hybrid\\ncombination of the approaches. One common strategy is: after applying Rule MO.Cut, we solve\\nthe outer subtask using bottom-up or hybrid approach, and for each ùëú1 for which the outer has a\\nsolution, we solve the inner subtask using the top-down or hybrid approach. Note that the cut rule\\ncan be used multiple times to solve a synthesis task.\\nExample 4.4. Consider the synthesis task ‚ü®roundNumber, R,ùëÜ‚Ü¶‚Üí24.00‚ü©from Example 4.2. Us-\\ning the fact that 24.58 was in the cut for parseNumber, we can use MO.Cut rule from Figure 5\\nand get the subtasks ‚ü®parseNumber, R,ùëÜ‚Ü¶‚Üí24.58‚ü©and ‚ü®roundNumber, R‚Ä≤,ùëÜ‚Ü¶‚Üí24.00‚ü©, where R‚Ä≤\\nis R with parseNumber ‚Üí24.58. The second subproblem now has only one rule for parseNumber,\\nwhich directly generates 24.58. The first subproblem now has to generate 24.58 from the input, and\\nthe second subproblem has to round 24.58 to 24.00.\\n‚ñ°\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 12, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:13\\n4.1\\nGeneralizing Top-Down and Bottom-Up Synthesis\\nExamining the top-down and bottom-up synthesis approaches closely, it can be seen that top-\\ndown synthesis is purely output driven and bottom-up synthesis is purely input driven. Cuts neatly\\ngeneralize both these approaches, allowing us the possibility to use a set of values influenced by\\nboth: (a) the set ùêµof values reachable through forward semantics from the input values (bottom-up\\nsearch), and (b) the set ùëáof values reachable through inverse semantics from the output values\\n(top-down search).\\nRecall that the set buùëÅ‚Ä≤ is the set of values that bottom-up enumeration generates corresponding\\nto nonterminal ùëÅ‚Ä≤ and the set tdùëÅ,ùëÅ‚Ä≤ is the set of values that arise at ùëÅ‚Ä≤ by repeatedly applying\\n(precise) witness functions starting from ùëÅ. Let real value set rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) be the set of values\\nJùëÉ‚Ä≤K(ùëÜ) where ùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤) is a sub-program of a program ùëÉ‚ààL(ùëÅ) such that JùëÉK(ùëÜ) = ùëú. Clearly,\\nit can be seen that rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) ‚äÜtdùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) ‚à©buùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú). Restating the definition of\\ncomplete cuts, a cut is complete if and only if the set it returns is a superset of rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú). Both\\ntop-down and bottom-up search for synthesis are special cases of cut-based middle-out synthesis.\\nTheorem 4.5. [Cuts generalize top-down and bottom-up value sets.] Given a synthesis problem\\n‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and a non-terminal ùëÅ‚Ä≤, both the functions buùëÅ‚Ä≤ and tdùëÅ,ùëÅ‚Ä≤ are complete cuts for the\\nnon-terminal ùëÅ‚Ä≤ in the context of ùëÅ.\\nIn the light of this theorem, restricting the cut in the middle-out synthesis rule from Figure 5 to\\nonly being buùëÅ‚Ä≤ produces the state-of-the-art combination of top-down and bottom-up synthesis\\nDuet [Lee 2021]. While the above theorem states that TD and BU analyses produce complete cuts,\\nnot all complete cuts are (overapproximations of) top-down value set or bottom-up value set.\\nExample 4.6. Building on Example 4.2, consider now the example ùëÜ‚Ü¶‚Üí24.58, but we want\\nto synthesize a program from this example starting from the nonterminal parseNumber, which\\nhas a rule parseNumber ‚ÜíParseNumber(substr, locale). One potential cut for substr in the\\ncontext parseNumber could be obtained by scanning the input string for any substrings that are\\nnumerical constants and returning them (as a string). Here it returns a set containing ‚Äú24.58‚Äù and\\n‚Äú46‚Äù and all substrings of these two strings that are valid numbers. This complete cut is not an\\noverapproximation of the bottom-up values that substr can generate since there are many more\\nsubstrings in an input. However, it is complete in the context parseNumber because these are the\\nonly strings that can be parsed as numbers.\\n‚ñ°\\n4.2\\nComputing Cuts\\nWe can use top-down value sets or bottom-up value sets as the cuts, as shown in Theorem 4.5;\\nhowever, if we do that, we only replicate top-down, bottom-up, and Duet‚Äôs meet-in-the-middle\\nsynthesis [Lee 2021]. DSL designers can provide more refined cuts that would enable going beyond\\ncurrent methods. How can designers author more refined cuts? For most operators, using witness\\nfunctions to perform top-down synthesis might suffice. Specialized cuts are only required when we\\nhave not-effectively-invertible operators that have either no witness function or very inefficient\\nwitness functions.\\nIn practice, DSL designers do not necessarily have to use complete cuts. Looking back at Exam-\\nple 4.6, the set {‚Äú24.58‚Äù, ‚Äú46‚Äù} is a reasonable, but incomplete cut, because any program that extracts\\na number from a strict substring, say ‚Äú4.5‚Äù, of these two strings would be a contrived program. Cuts\\nare reminiscent of interpolants or invariants from program analysis: a cut at ùëÅ‚Ä≤ in the context of ùëÅ\\nis the denotation of an invariant that holds true at ùëÅ‚Ä≤ for all programs that compute the desired\\noutput at ùëÅ. We next describe a few cut functions used in FlashFill++ along with the DSL designer‚Äôs\\nintuition about the DSL that helped construct that cut function.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 13, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:14\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nCut for LowerCase. When computing a cut for a nonterminal ùëÅ‚Ä≤ in context of nonterminal ùëÅ, we\\nneed to consider all paths from ùëÅ‚Ä≤ to ùëÅin the grammar. We focus on one path at a time, and take a\\nunion if there are multiple paths. Consider the grammar rule single := LowerCase(concat) in the\\nFlashFill++ DSL (Figure 7), and ignore the other paths between nonterminals single and concat\\nfor now. Clearly, the witness function is given as:\\nWFLowerCase,1(ùë¶) = {ùë•| lowercase(ùë•) = ùë¶}\\nThe returned set contains 2len(ùë¶) strings. In contrast, the cut could be much smaller. Ignoring other\\npaths between the two nonterminals, one possible cut is:\\nCutconcat,single(ùëÜ‚Ü¶‚Üíùë¶) = {ùë•| lowercase(ùë•) = ùë¶and each char of ùë•is in some input in ùëÜ}.\\nHere the DSL designer uses their knowledge that the non-terminal concat can only generate strings\\nwhose characters are in some input. One exception to this invariance rule are strings that are\\ngenerated as ‚Äúconstant strings\", and hence this cut is not complete, but it may be a reasonable\\ncompromise between completeness and efficiency. An alternative cut could be:\\n{ùë•| lowercase(ùë•) = ùë¶and for each char ùë•[ùëñ] of ùë•, ùë•[ùëñ] is in some input or ùë•[ùëñ] == ùë¶[ùëñ]}.\\nFinally, the designer can further refine the cut to only include those strings that contain large\\nchunks of substrings of some input. Consider input ‚ÄòAmal Ahmed <AMAL@CCS.NEU.EDU>‚Äô and\\noutput ‚Äòamal‚Äô. The witness function would return 16 values, all variations of the string ‚Äòamal‚Äô\\nwith each letter optionally capitalized. However, a cut would only return 2 values ‚ÄòAmal‚Äô and\\n‚ÄòAMAL‚Äô, i.e., those variations that occur contiguously in the input. The same kind of reasoning\\ncan be used on all other paths from concat to single that go via the operators UpperCase and\\nProperCase, and thus we can get a cut for concat in the context of single.\\nCut for Concat. Consider the grammar rule concat := segment|Concat(segment, concat) in the\\nFlashFill++ DSL (Figure 7). The witness function for (the first argument of) Concat is given by\\nWFConcat,1(ùë¶) = {ùë•| ùë•is a prefix of ùë¶}\\nThe DSL designer knows the invariant that every string generated by segment is either a substring\\nof an input, or a string representation of date or number, or a constant string. So, a possible cut,\\nCutsegment,concat(ùëÜ‚Ü¶‚Üíùë¶), could be:\\n{ùë•| ùë•is maximal prefix of ùë¶either contained in some input or a number or a date}.\\nThis cut is not complete since segment could generate a constant string, but the DSL designer may\\nprefer to use this cut and fallback on the witness function only if the above choices fail to work.\\nCut for RoundNumber and FormatDateTime. Example 4.2 presented the cut for parseNumber\\nin the context roundNumber. The witness function for roundNumber on the input ùëÜ‚Ü¶‚Üí24.00 will\\nhave to return an infinite set of floating point values that can all round to 24.00. However, the DSL\\ndesigner knows that parseNumber can only generate a number that occurs in the input, i.e., 24.58 or\\n46, and furthermore, numbers such as 4.5 are not reasonable choices. Hence, the designer can pick the\\ncut CutparseNumber,roundNumber(ùëÜ‚Ü¶‚Üíùë¶):\\n{ùë•| ùë•is a maximally long number extracted from a substring of an input}.\\nHere the DSL designer used their knowledge about the form of values that a nonterminal can\\ngenerate to construct a cut. Another such example is the cut, CutasDate,formatDate(ùëÜ‚Ü¶‚Üíùë¶), which\\ncan be computed as:\\n{ùë•| ùë•is a maximally long datetime value extracted from a substring of an input}.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 14, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:15\\nCut for Const in the Affine Grammar. The DSL designer of arithmetic expressions given in\\nExample 3.1 can provide a cut for const in the context output by noting the monotonicity invariant:\\nvalues computed by subexpressions are smaller than values computed by the whole expression. Consider\\nthe input stateùëÜ= ‚ü®in1 ‚Ü¶‚Üí2, in2 ‚Ü¶‚Üí0‚ü©and the input-output exampleùëÜ‚Ü¶‚Üí7. Since the output is 7, we\\ncan restrict the potential values for the coefficient of in1 (which is 2) to at most 3 values, i.e., {0, 1, 2, 3}\\nas any larger value will make the product exceed the value 7. Thus, the cut, Cutconst,output(ùëÜ‚Ü¶‚Üíùë¶),\\nis computed as:\\n{ùë•‚ààN | 0 ‚â§ùë•‚â§‚åä\\nùë¶\\nùëÜ[in1] ‚åã}.\\nUsing this cut we can now perform bottom-up synthesis, whereas it wasn‚Äôt possible before since\\nthere are an infinite set of possible values for the non-terminal const and the grammar is not\\neffectively enumerable.\\nWhenever there is a nontrivial invariant that holds for all values that can be generated at a\\ncertain nonterminal, we can usually exploit that invariant to design a cut for that nonterminal.\\nWhile we have focused mainly on the FlashFill++ DSL to illustrate this process of designing good\\ncuts, the ideas extend to DSLs for any other domain.\\n5\\nPRECEDENCE IN DOMAIN-SPECIFIC LANGUAGES\\nWe first define the problem of synthesis in presence of precedence over operators. We then introduce\\ngDSLs, which extend the notion of DSL (Section 3.1) with precedence. We then present the gDSL\\nfor FlashFill++ and inference rules that solve the PBE synthesis problem over gDSLs.\\n5.1\\nSynthesis with Preference\\nDSL designers who want to translate programs generated in a DSL into a popular target language,\\nsuch as Python, typically want the DSL to contain all operators from the target language libraries.\\nThese operators are often redundant. For example, substrings of a string can be extracted using\\nabsolute index positions, or regular expressions, or finding locations of constant substrings in the\\nstring, or splitting a string by certain delimiters. In such cases, whenever a task is achievable in\\nmany different ways, we get a so-called broad DSL. For broad DSLs, DSL designers often have a\\npreference for which operators to use. For example, they may prefer split over find, which they\\nmay prefer in turn over using regular expressions or absolute indices. As another example, DSL\\ndesigners may prefer transforming a string containing ‚ÄúJan‚Äù to ‚ÄúJanuary‚Äù by treating the substring\\naround ‚ÄúJan‚Äù in the input as a datetime object and working with them, rather than generating\\n‚ÄúJanuary‚Äù by concatenating ‚ÄúJan‚Äù with a constant string ‚Äúuary‚Äù.\\nExample 5.1. Consider the task of extracting the second column from a line in a comma-separated\\nvalues (CSV) file, specified by the input-output example ‚ÄòWA, Olympia, UTC-8‚Äô ‚Ü¶‚Üí‚ÄòOlympia‚Äô.\\nIn the FlashFill++ DSL, this can be done using the program Split(x, ‚Äò,‚Äô, 2), where ùë•is the input,\\nor using the program Slice(x, Find(x, ‚Äò,‚Äô, 1, 0), Find(x, ‚Äò,‚Äô, 2, 0)). A traditional VSA-based syn-\\nthesizer would (possibly implicitly) produce both programs, assign scores to both using a ranking\\nfunction, and return the better ranked one. However, typically we strictly prefer programs that use\\nthe Split operator over the Slice operator. Ideally, a synthesizer should not even examine Slice\\nprograms when an equivalent Split program exists. Similar preference is also seen in Python\\nprogrammers who often prefer to use str.split over regex.find or str.find.\\n‚ñ°\\nThis motivates the need to perform synthesis over a broad DSL where there is preference over\\noperators. Suppose a DSL designer has a DSL and a preference over operators and terminals. Let\\nŒ£ := F ‚à™T be the collection of all operators and terminal symbols in the DSL and let ‚âªŒ£ be a\\nprecedence relation on the symbols f ‚ààŒ£. We make the assumption that (A1) the DSL designer\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 15, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:16\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nonly provides precedence over operators that occur as alternates for the same nonterminal; that is,\\nif the designer sets f1 ‚âªŒ£ f2, then ùëÅ‚Üíf1(. . .) ‚ààR and ùëÅ‚Üíf2(. . .) ‚ààR are two rules with the\\nsame nonterminal ùëÅin the grammar. We also assume that (A2) the relation ‚âªŒ£ is a strict partial\\norder (irreflexive, asymmetric, transitive) to ensure that the DSL designers preference is consistent.\\nWe first need to formalize what it means for a synthesizer to satisfy the operator precedence ‚âªŒ£\\nprovided by the DSL designer. For this, we need to lift ‚âªŒ£ to a precedence on the set of programs ùëÉ\\ngenerated by the DSL. However, this is not easy since it is not clear which of f1(f2(in)) or f‚Ä≤\\n1(f‚Ä≤\\n2(in))\\nto prefer if the DSL designer says f1 ‚âªŒ£ f‚Ä≤\\n1 and f‚Ä≤\\n2 ‚âªŒ£ f2. We resolve this issue by saying that the\\npreference for the operator occurring ‚Äúabove‚Äù in the program is more important than anything\\nbelow. In the above example, we give more weight to f1 ‚âªŒ£ f‚Ä≤\\n1 and hence we want f1(f2(in)) to be\\npreferred over f‚Ä≤\\n1(f‚Ä≤\\n2(in)). This follows the intuition that operators at the top in, say, the FlashFill++\\nDSL, such as Concat, FormatNumber, or FormatDateTime, are more influential in determining the\\nhigh-level strategy for solving a task than operators at the bottom, such as Split or Slice. Hence,\\nwe extend the user provided ‚âªŒ£ to ‚âªùëí\\nŒ£‚äá‚âªŒ£ so that whenever ùëÅ0 ‚Üíf1(. . . , ùëÅ1, . . .) and ùëÅ1 ‚Üíf2(. . .)\\nare rules in the DSL, then f1 ‚âªùëí\\nŒ£ f2.\\nExample 5.2. Consider the synthesis task from Example 4.2 of generating ‚Äò24.00‚Äô from the input\\nstring. One possible program is Concat(ùëù1, ‚Äò.00‚Äô), where ùëù1 is a subprogram that extracts ‚Äò24‚Äô\\nfrom the input string. A second possible program is Segment(FormatNumber(ùëù2, fmt_desc) that\\ngenerates ‚Äò24.00‚Äô by formatting a number computed by program ùëù2. Here, Segment is a dummy\\nidentity operator having higher preference than Concat in the FlashFill++ gDSL (Figure 7). In the\\nFlashFill++ DSL, the second program is preferred since it does not use concatenation, irrespective of\\nhow ùëù1 and ùëù2 work‚Äîat the top-level concatenation is strictly less preferred. Note that here ùëù1 is\\nlikely to be significantly smaller and simpler than ùëù2 as it is just extracting the string ‚Äò24‚Äô, while\\nùëù2 is extracting a number and then rounding it. A traditional arithmetic ranking function (as used\\nin FlashFill and FlashMeta) intuitively computes the score of programs as a weighted sum of the\\nscore of its sub-programs, and hence, will need to be tuned carefully to ensure that the smaller\\nconcat program is scored worse than the larger segment program.\\n‚ñ°\\nWe want the relation ‚âªùëí\\nŒ£ to be a strict partial order. However, in general, it may not be a strict\\npartial order due to cycles in the grammar (where some ùëÅ0 generates a term containing ùëÅ0), which\\nagain makes ‚âªùëí\\nŒ£ violate irreflexivity or transitivity. We make the reasonable assumption that there\\nare no cycles since we often limit the depth of terms being synthesized and then the assumption\\ncan be satisfied by renaming the nonterminals. Thus, without loss of much generality, we can\\nassume that the extended precedence ‚âªùëí\\nŒ£ on Œ£ is a strict partial order.\\nNow we formalize synthesizing in presence of precedence ‚âªŒ£ by lifting the precedence ‚âªŒ£ on\\nŒ£ to ‚âªùëí\\nŒ£, and then to a preference on programs (trees) over Œ£ using the well-known lexicographic\\npath ordering (LPO), ‚âªùëôùëùùëú, which is defined as follows [Dershowitz and Jouannaud 1990]: Given\\nprograms ùëÉ1 := f1(ùëÉ11, . . . , ùëÉ1ùëò) and ùëÉ2 := f2(ùëÉ21, . . . , ùëÉ2ùëô), we have ùëÉ1 ‚âªùëôùëùùëúùëÉ2 if either (a) f1 ‚âªùëí\\nŒ£ f2\\nand ùëÉ1 ‚âªùëôùëùùëúùëÉ2ùëñfor all ùëñ, or (b) f1 = f2 and there exists a ùëös.t. ùëÉ1ùëñ= ùëÉ2ùëñfor ùëñ< ùëöand ùëÉ1ùëö‚âªùëôùëùùëúùëÉ2ùëö,\\nor (c) ùëÉ1ùëñ‚âªùëôùëùùëúùëÉ2 for some ùëñ.\\nDefinition 5.3. Let ‚™∞base be the base ordering to rank programs that are unordered by ‚âªùëôùëùùëú. Given\\na DSL D with precedence ‚âªŒ£ on the set Œ£ := F ‚à™T of all operators and terminal symbols in D, and\\n‚™∞base, the PBE synthesis with precedence problem is to find the maximally ranked program by the\\nlexicographic combination of ‚âªùëôùëùùëúand ‚™∞base that satisfies a given example, where ‚âªùëôùëùùëúis the LPO\\ninduced by the extended precedence ‚âªùëí\\nŒ£.2\\n2Given orderings ‚âª1 and ‚âª2, the lexicographic combination ‚âª‚âª1,‚âª2 is defined as follows: ùë†‚âª‚âª1,‚âª2 ùë°if either (a) ùë†‚âª1 ùë°, or\\n(b) ùë†‚äÅ1 ùë°and ùë°‚äÅ1 ùë†and ùë†‚âª2 ùë°.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 16, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:17\\nWe solve the PBE synthesis with precedence problem by extending DSLs to gDSLs and modifying\\nthe inference rules to correctly handle the precedence.\\n5.2\\nGuarded Domain-Specific Languages\\nA guarded domain-specific language (gDSL) is a DSL D = ‚ü®N, T, F, R, Vin, ùë£out‚ü©where the set R of\\nrules can additionally contain guarded rules of the form ùëÅ‚Üíùõº1 |‚ä≤ùõº2 |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤ùõºùëòwhere each ùõºùëñis\\neither f(ùë£1, . . . , ùë£ùëõ) or ùë£0, where f ‚ààF , ùë£0 ‚ààT, and ùë£1, . . . , ùë£ùëõ‚ààN ‚à™T. A non-terminal ùëÅcan have\\nany number of guarded rules associated with it, each with possibly different values of ùëò‚â•1.\\nThe rules in a regular DSL can be viewed as a special case of guarded rules where ùëò= 1 (in the\\ndefinition of guarded production rules above.) When ùëò> 1, a guarded rule ùëÅ‚Üíùõº1 |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤ùõºùëò\\nassociated with the nonterminal ùëÅhas ùëòalternates on the right-hand side that are ordered. The ùëñ-th\\nalternate ùõºùëñyields a (regular) rule ùëÅ‚Üíùõºùëñ. We call ùëÅ‚Üíùõºùëñthe ùëñ-th constituent rule of the original\\nguarded rule. Define Rùëêas the collection of all constituent rules of rules in R; that is, Rùëê:= {ùëÅ‚Üí\\nùõºùëñ| ùëÅ‚Üí(. . . |‚ä≤ùõºùëñ|‚ä≤. . .) ‚ààR}. We call the (regular) DSL Dùëê:= ‚ü®N, T, F, Rùëê, Vin, ùë£out‚ü©obtained\\nfrom a gDSL D := ‚ü®N, T, F, R, Vin, ùë£out‚ü©a constituent DSL of the gDSL D.\\nGiven an instance of the PBE synthesis with precedence problem, we can annotate the given DSL\\nwith precedence to get a gDSL. We assume that the precedence relation ‚âªŒ£ satisfies Assumptions (A1)\\nand (A2). Furthermore, we also assume that (A3) the precedence is a series-parallel partial order\\n(SPPO) [B√©chet et al. 1997]. Under Assumption (A3), the precedence can be encoded in gDSL\\nby introducing new nonterminals where necessary. Specifically, if we have a maximal chain\\nf1 ‚âªŒ£ f2 ‚âªŒ£ . . . ‚âªŒ£ fùëòover alternate operators ùëÅ‚Üíf1(. . .)| ¬∑ ¬∑ ¬∑ |fùëò(. . .) in the DSL, then we\\nadd a guarded rule ùëÅ‚Üíf1(. . .) |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤fùëò(. . .). However, if certain alternate operators are left\\nincomparable by the DSL designers, then we introduce a new nonterminal. For example, if f1 ‚âªŒ£ f3\\nand f2 ‚âªŒ£ f3, but there is no preference between f1 and f2, then we introduce a new nonterminal\\nùëÅ‚Ä≤ and have ùëÅ‚ÜíùëÅ‚Ä≤ |‚ä≤f3(. . .) and ùëÅ‚Ä≤ ‚Üíf1(. . .)|f2(. . .) in the gDSL. Any SPPO ‚âªŒ£ can thus be\\nencoded in the gDSL.\\nExample 5.4. Consider the DSL for affine arithmetic from Example 3.1. Suppose the DSL designer\\nwants the precedence input1 ‚âªŒ£ input2. Then, we can replace the two rules for nonterminal\\naddend by a single guarded rule: addend ‚ÜíTimes(const, input1) |‚ä≤Times(const, input2) to\\nget a gDSL, Dùëî\\nùê¥. In the LPO induced by the extension ‚âªùëí\\nŒ£ of this preference, the program ùëÉ1 :=\\nPlus(Times(3, input1), 1) is preferred over ùëÉ2 := Plus(Times(3, input2), 7).\\n‚ñ°\\nLet ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©be a synthesis task, where R is the rules of a gDSL D. We say a program ùëÉis\\na solution for this task if\\n(a) ùëÉis a solution for the task ‚ü®ùë£out, Rùëê,ùëÜ‚Ü¶‚Üíùëú‚ü©(in the constituent DSL Dùëê), and\\n(b) for any other ùëÉ‚Ä≤ that is a solution in the constituent DSL, ùëÉ‚Ä≤ ‚äÅùëôùëùùëúùëÉ, where ‚âªùëôùëùùëúis the LPO\\ninduced by the extended precedence ‚âªùëí\\nŒ£ coming from the guarded rules.\\nCondition (a) says that ùëÉshould be a solution for the synthesis problem ignoring the precedence.\\nCondition (b) says that the ordering in the rules should be interpreted as an ordering on programs\\nusing the induced LPO, and we should ignore programs smaller in this ordering.\\nExample 5.5. Consider the gDSL Dùëî\\nùê¥and programs ùëÉ1, ùëÉ2 from Example 5.4. Consider the synthesis\\ntask ‚ü®ùë£out, R, ‚ü®input1 ‚Ü¶‚Üí2, input2 ‚Ü¶‚Üí0‚ü©‚Ü¶‚Üí7‚ü©from Example 3.2, but with R now coming from the\\ngDSL Dùëî\\nùê¥. Both programs, ùëÉ1 and ùëÉ2, map the input state to 7. However, ùëÉ2 is now not a solution in\\nDùëî\\nùê¥because there exists ùëÉ1 that is preferred. The program ùëÉ3 := 7 also maps the input state to 7.\\nThe (derivations of) ùëÉ1 and ùëÉ3 are incomparable; and in fact, both are solutions in Dùëî\\nùê¥.\\n‚ñ°\\nIf a solution for the unguarded DSL exists, then there will be a solution that is maximal and\\nhence a solution for the gDSL will exist.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 17, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:18\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nGuarded.If\\nPS1 |= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nùëÅ‚Üíùõº1 |‚ä≤ùõº2 ‚ààR\\nPS1 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nGuarded.Else\\nÃ∏|= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nùëÅ‚Üíùõº1 |‚ä≤ùõº2 ‚ààR\\nPS2 |= ‚ü®ùõº2, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nPS2 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nFig. 6. Extending top-down synthesis for guarded rules.\\nTheorem 5.6 (Precedence preserves solvability.). Let D be a gDSL based on precedence ‚âªŒ£\\nthat is a strict partial order. Let ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùë£‚ü©be a synthesis task. This task has a solution in D iff\\nthere is a solution in Dùëê.\\nIf we can compute all solutions for a synthesis task over a gDSL, we can order them by any ‚™∞base\\nordering to solve the PBE synthesis with precedence problem.\\n5.3\\nPBE Synthesis Rules for Guarded DSLs\\nFigure 6 contains two inference rules that describe how guarded rules are handled in top-down,\\nbottom-up, or middle-out synthesis. If ùëÅ‚Üíùõº1 |‚ä≤ùõº2 is a guarded rule in R and we can (recursively)\\nprove ùëÉùëÜ1 |= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, then Rule Guarded.If can be used to assert ùëÉùëÜ1 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. (This\\nassertion will be nontrivial only if ùëÉùëÜ1 ‚â†‚àÖ.) In case there is no program that solves ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©,\\nand we can (recursively) prove ùëÉùëÜ2 |= ‚ü®ùõº2, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, then Rule Guarded.Else can be used to assert\\nùëÉùëÜ2 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©.\\nRecall that the notation ùëÉùëÜ|= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©simply asserts that every program in ùëÉùëÜsolves the\\ngiven synthesis task, and does not require ùëÉùëÜto contain all solutions. The Rule Guarded.Else has a\\ncondition that a certain synthesis task be unsolvable. If we have the ability to compute all solutions\\n(such as, using version-space algebras, or VSAs), then that can help in determining when a problem\\nis unsolvable, but other synthesis approaches that can establish infeasibility can also be used.\\nExample 5.7. Continuing from Example 5.5, consider the task of generating 7 from inputs 2\\nand 0. Say we reduce the original task to the proof obligation ùëã|= ‚ü®addend, R,ùëÜ‚Ü¶‚Üíùë•‚ü©, where\\nwe pick say 6 for ùë•. Now, we have a guarded rule for addend and we can use Rule Guarded.If\\nto get the proof obligation ùëã|= ‚ü®Times(const, input1), R,ùëÜ‚Ü¶‚Üí6‚ü©. This subtask has a solution\\nùëã= {Times(3, input1)}, and hence we will not consider the option Times(const, input2).\\n‚ñ°\\nPerforming synthesis over the gDSL is equivalent to solving PBE synthesis with precedence.\\nTheorem 5.8. Let ‚âªŒ£ be a precedence on Œ£ := F ‚à™T that satisfies Assumptions (A1), (A2), and (A3).\\nLet D := ‚ü®N, T, F, R, Vin, ùë£out‚ü©be a gDSL that encodes ‚âªŒ£. Let Dùëê:= ‚ü®N, T, F, Rùëê, Vin, ùë£out‚ü©be\\nits (unguarded) constituent DSL. Let ‚™∞base be an ordering on programs and let ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©be a\\nsynthesis task. Then, the following are equivalent:\\n(1) {ùëÉ} |= ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and ùëÉis maximal w.r.t ‚™∞base among all such solutions.\\n(2) The program ùëÉis a solution in Dùëêfor the task ‚ü®ùë£out, Rùëê,ùëÜ‚Ü¶‚Üíùëú‚ü©that is maximal w.r.t a lexicographic\\ncombination of ‚âªùëôùëùùëú(induced by ‚âªŒ£) and ‚™∞base.\\nTheorem 5.8 shows that using a ranker ‚™∞base with a gDSL D has the same effect as using a\\ncomplex ranker (lexicographic combination of ‚âªùëôùëùùëúand ‚™∞base) with a regular DSL Dùëê. This shows\\nthat our gDSL-based approach solves the PBE synthesis with precedence problem (under some\\nassumptions). Theorem 5.8 also explains why the ranker ‚™∞base used with a gDSL D can be very\\nsimple compared to what is needed with a regular DSL Dùëê. Designing a good complex ranking\\nfunction has traditionally been very challenging in PBE, taking many developer-months to converge\\non a good ranking function [Kalyan et al. 2018; Natarajan et al. 2019; Singh and Gulwani 2015]. In\\ncontrast, FlashFill++ uses the power of gDSLs (Theorem 5.8) to reduce the requirements on its base\\nranker, which was developed significantly faster.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 18, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:19\\nlanguage FlashFillPP;\\n@start string output := single |> If(cond, single, output)\\n// conditions\\nbool cond := pred |> And(pred, cond);\\nbool pred := StartsWith(x, matchPattern) |> EndsWith(x, matchPattern)\\n|> Contains(x, matchPattern) |> ...;\\n// single branch\\nstring single := concat | LowerCase(concat) | UpperCase(concat) | ProperCase(concat);\\n// optional concatenation of substrings\\nstring concat := segment |> Concat(segment, concat)\\n// substring logic\\nstring segment := substr | formatNumber | formatDate | constStr;\\n// format substring as a number\\nstring formatNumber := FormatNumber(roundNumber, numFmtDesc);\\ndecimal roundNumber := parseNumber |> RoundNumber(parseNumber, roundNumDesc);\\ndecimal parseNumber := AsNumber(row, columnName) |> ParseNumber(substr, locale);\\n// format substring as a date\\nstring formatDate := FormatDateTime(asDate, dateFmtDesc);\\nDateTime asDate := AsDateTime(row, columnName) |> ParseDateTime(substr, parseDateFmtDesc);\\n// find a substring within the input x\\nstring substr := Split(x, splitDelimiter, splitInstance)\\n|> Slice(x, pos, pos)\\n|> MatchFull(x, matchPattern, matchInstance);\\n// find a position within the input x\\nint pos := End(x) |> Abs(x, absPos)\\n|> Find(x, findDelimiter, findInstance, findOffset)\\n|> Match(x, matchPattern, matchInstance)\\n|> MatchEnd(x, matchPattern, matchInstance);\\n// literal terminals\\nFmtNumDescriptor numFmtDesc; RndNumDescriptor roundNumDesc;\\nFmtDateTimeDescriptor dateFmtDes, parseDateFmtDesc;\\nstring constStr, splitDelimiter, findDelimiter;\\nint splitInstance, findInstance, matchInstance, findOffset;\\nRegex matchPattern; int absPos;\\nFig. 7. A fragment of the gDSL for FlashFill++. | choices are unguarded, |> choices are guarded.\\n5.4\\nGuarded DSL for FlashFill++\\nWe now describe the FlashFill++ gDSL and compare it to FlashFill. Figure 7 shows a major part of the\\nDSL. FlashFill++ shares the top level rules that perform conditional statements, case conversion, and\\nstring concatenation with FlashFill. Conditionals enable if-then-else logic. The condition is one or\\nmore conjunctive predicates based on properties of the input string. Case conversion transforms a\\nstring into lower-, upper-, or proper-case. Concatenation concatenates two strings.\\nAlthough FlashFill can perform some datetime and number operations using text manipulation\\n(such as \"01/01/2020\" ‚Üí\"2020\" or \"10.01\" ‚Üí\"10\"), it is unable to express other sophisticated\\ndatetime and number operations as it does not incorporate operations over those datatypes, but\\nrather treats them as standard strings. For instance, FlashFill cannot get the day of week from a date\\n(such as \"01/01/2020\" ‚Üí\"Wednesday\"), or round up a number (e.g., \"10.49\" ‚Üí\"10.5\"). This\\nmotivates us to add new rules to support richer datetime (rules parseDate and formatDate) and\\nnumber (rules parseNumber and formatNumber) transformations.\\nThe next major differences are in the substr and pos rules. FlashFill has a single Slice operator\\nwhich selects a substring defined by its start and end positions. These positions can be defined\\neither as absolute positions or with the complex RegPos operator which finds the ùëòth place in the\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 19, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:20\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nstring where the left- and right substrings match the two given regular expressions. While this is\\nexpressive enough to cover any desired substring selection and all of the operators in FlashFill++\\ncan technically be expressed in terms of it, this introduces a challenge for an industrial synthesizer\\nthat targets different languages for code generation: not all platforms of interest support regular\\nexpressions natively (e.g. Microsoft Excel‚Äôs formula language does not support regular expressions).\\nIn contrast, when designing FlashFill++ we chose a wider collection of more natural operators that\\nare closer to what developers use in practice when working with target languages ‚Äì this removes\\nthe mismatch between synthesis DSL and code generation target language.\\nIn particular, instead of only allowing substrings to be defined as a Slice with their start and\\nend positions, FlashFill++ adds a Split operator to select the ùëòth element in a sequence generated\\nby splitting the input string by some delimiters. We also add a MatchFull operator to find the ùëòth\\nmatch of a regular expression. Additionally, in FlashFill++ the pos rule replaces the operator RegPos\\n(which relies on a pair of regular expressions to identify a position) with a Find of a constant string\\nin the input and a Match/MatchEnd of a regular expression.\\nExample Guarded Rules in the FlashFill++ DSL. We go over a few cases of guarded rules in FlashFill++\\nto show they capture natural intuitions and rules of thumb in the string transformation domain.\\n‚Ä¢ Single segments over concats. The guarded rule segment |‚ä≤Concat(segment, concat) ensures\\nthat we try to synthesize programs that generate the whole output at once, before generating\\nprograms that produce a prefix and a suffix of the output and then combine them. Whenever\\nsuch a program exists, this guarded rule potentially saves the exploration of a huge portion\\nof the program space. This single guarded rule plays a crucial role in keeping FlashFill++\\nperformant since witness function of the concat operator produces 2(ùëõ‚àí1) subtasks, one\\neach for the prefix and suffix at each point where the output string can be split.\\n‚Ä¢ Splits over slices. As illustrated Example 5.1, FlashFill++ strictly prefers programs that use\\nthe Split operator over programs that use the Slice operator. Program in data wrangling\\nscenarios very commonly begin by extracting the appropriate part of the input from a\\ndelimited file record (CSVs, TSVs, etc). In all such cases, a split program more closely follows\\nthe human intuition of ‚Äúextract the ùëõùë°‚Ñécolumn delimited by‚Äù as compared to a slice program.\\nNote that the split operator is a ‚Äúhigher level‚Äù construct preferred over the ‚Äúlow-level‚Äù slice.\\n‚Ä¢ Input numbers over rounded numbers. The RoundNumber operator in Figure 7 is guarded by\\nparseNumber, meaning that we can only generate a program that rounds a number if no\\nnumber in the input can be used directly to produce the same output.\\n6\\nEXPERIMENTAL EVALUATION\\nWe now present our experimental results, including an ablation study and survey-based user study.\\n6.1\\nMethodology\\nWe used 3 publicly available benchmark sets ‚Äì Duet string benchmarks [Lee 2021], Playgol [Cropper\\n2019], and Prose [PROSE 2022] ‚Äì covering a range of string transformations, including datetime\\nand number formatting.\\nWe compare FlashFill++ with three systems: two publicly available state-of-the-art synthesis\\nsystems that are deployed in productions, namely FlashFill [Gulwani 2011] and SmartFill [Chen\\net al. 2021a; Google 2021], and one, Duet, from a recent publication [Lee 2021]. To experiment with\\nFlashFill, we implemented it on top of the FlashMeta framework [Polozov and Gulwani 2015]. We\\ncarry out our FlashFill, FlashFill++, and Duet experiments on a machine with 2 CPUs & 8GB RAM. To\\nexperiment with SmartFill, we employ Google Sheets in Chrome and use it to solve the subset of\\ntasks that are suitable for its spreadsheet environment.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 20, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:21\\nTable 1. Comparing different tools (rows) on the 3 public benchmarks (columns) based on (1) number of\\nbenchmarks correctly solved (Columns 2‚Äì5) and (2) average number of examples required on the successful\\nbenchmarks (Columns 6‚Äì8). The total number of benchmarks attempted are shown in brackets.\\nNumber benchmarks solved\\nAverage #examples required\\nDuet (205)\\nPlaygol (327)\\nProse (354)\\nTotal (886)\\nDuet\\nPlaygol\\nProse\\nFlashFill\\n139\\n264\\n172\\n575\\n1.41\\n1.26\\n1.54\\nFlashFill++\\n159\\n307\\n353\\n819\\n1.69\\n1.46\\n1.52\\nDuet\\n102\\n211\\n166\\n479\\n1.97\\n2.11\\n2.01\\nSmartFill\\n37 (158)\\n34 (327)\\n18 (296)\\n89 (781)\\n2.75\\n2.85\\n2.83\\nSince SmartFill is not exposed in the Google Sheets API, we rely on browser-automation using\\nSelenium [Selenium 2022] for SmartFill evaluation. Moreover, we remove problematic benchmark\\ntasks and use 158, 327, and 296 tasks from the Duet, Playgol, and Prose benchmarks, respectively,\\nfor SmartFill evaluation. The tasks removed were unsuitable for browser automation (e.g. too many\\nrows or new line characters).\\nThe Duet tool [Lee 2021] accepts a DSL as part of its input. Different Duet benchmarks used\\nslightly different DSLs. For fair comparison, we set a single fixed DSL. We obtained the fixed DSL\\nby taking all commonly-used rules in the string transformation benchmarks of Duet. Second, Duet\\nhas some hyperparameters, for which we picked the best setting after some experimentation.\\n6.2\\nExpressiveness and Performance\\nA key feature of FlashFill++ is improved expressiveness. Table 1 (Columns 2‚Äì5) shows the number\\nof benchmark tasks that the various tools (rows) can correctly solve. FlashFill++ produces a correct\\nprogram for most number of benchmarks. FlashFill is limited due to a lack of datetime and number\\nformatting. The DSL used in the Duet tool has no datetime support, and only limited support for\\nnumber and string formatting. The SmartFill tool is a neural-based general purpose tool and has\\nthe weakest numbers here. Since our SmartFill experiments rely on browser-based interaction, it is\\npossible that the underlying synthesizer can solve more benchmarks but these are not exposed to\\nthe UI. However, our setup reflects the experience that a user would face.\\nOur DSL is expressive, covering a large class of string, datetime and number transformations;\\nthus showing the added value from using cuts and guarded rules.\\nWhile increasing expressiveness enables users to accomplish more tasks, there is a risk of reducing\\nperformance on existing tasks. To this end, we consider the minimum number of examples required\\nfor a synthesizer to learn a correct program. To find that number, we use a counter-example guided\\n(CEGIS) loop which provides the next failing I/O example in every iteration to the synthesizer.\\nWe use a time out of 20 seconds. Table 1 Columns 6‚Äì8 present the average number of examples\\nthe various tools used across the benchmarks where they were successful. Here FlashFill has the\\nbest numbers, which indicates that when it works (for string benchmarks), it learns with very\\nfew examples. Our tool FlashFill++ takes only slightly more examples on average, partly because\\ndatetime and number formatting typically requires more examples for intent disambiguation. For\\nexample, the string ‚Äò2/3/2020‚Äô can either be the 2ùëõùëëof March or the 3ùëõùëëof February. The Duet and\\nSmartFill tools take more examples in general. We emphasize that the performance of FlashFill++ is\\ngood here because it solves more problems (presumably much harder instances) and yet it doesn‚Äôt\\nuse too many more examples (the harder instances did not make the averages much worse).\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 21, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:22\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\n0\\n2500\\n5000\\n7500\\n10000 12500\\nBest Duet or FF Synthesis Time (ms)\\n‚àí2\\n‚àí1\\n0\\n1\\n2\\nlog10(Best / FlashFill++)\\nOutcome\\nFlashFill++ better\\nFlashFill++ worse\\n(a) log10( min(FlashFill,Duet)\\nFlashFill++\\n) vs min(FlashFill, Duet).\\nBenchmark classes\\nDuet\\nPlaygol\\nProse\\nOverall\\nFlashFill\\n689.7\\n1757.0\\n734.1\\n1116.4\\nFlashFill++\\n203.0\\n217.1\\n246.4\\n228.5\\nDuet\\n264.0\\n558.4\\n1351.7\\n766.74\\n(b) Average time (in ms) taken by the tools\\n(rows) over successful benchmarks from the three\\nsources (columns), and the average for each tool\\nover the entire suite (last column).\\nFig. 8. FlashFill++ is generally faster ‚Äì up to two orders of magnitude ‚Äì than the best of FlashFill and Duet,\\nand the slowdowns are mostly on fast benchmarks.\\nDespite solving more tasks, FlashFill++ continues to require a reasonable number of examples\\ncompared to baselines, showing that it generalizes to unseen examples and does not overfit.\\nWe next compare synthesis times (averaged over 5 runs). In particular, we compute the synthesis\\ntime when the tools are given the minimum number of examples they required to produce the\\ncorrect program. Figure 8 shows the results. We restrict this experiment to FlashFill, Duet, and\\nFlashFill++, as synthesis time for SmartFill is unobservable through the browser.\\nFigure 8a compares FlashFill++ with the faster of FlashFill and Duet. We focus on benchmarks that\\nare solved by FlashFill++, and by either FlashFill or Duet. The y-axis displays the log base 10 of the\\nratio of the best of FlashFill and Duet synthesis time to the FlashFill++ synthesis time. A higher value\\nrepresents a larger reduction in synthesis time. On the x-axis we display the best of FlashFill and\\nDuet synthesis time (in milliseconds) for that benchmark task. FlashFill++ reduces synthesis time\\nin 63% of the benchmarks, and the remaining 37% happen to be benchmarks where synthesis is\\nfast (< 500ùëöùë†in most cases) and slowdown is likely not observable in a user-facing application. In\\n37.3% cases, FlashFill++ is at least one order of magnitude faster and in 18% cases it is at least two\\norders of magnitude faster. Table 8b shows the average synthesis time for various tools across the\\nbenchmark classes. We averaged over the benchmarks that the tool solved. We see that FlashFill++\\nhas better averages despite solving more (presumably harder) benchmarks.\\nFlashFill++ is faster on average despite solving more (presumably harder) benchmarks and better\\nthan the best of the baselines on most hard instances.\\nFinally, we evaluate the gain from using gDSLs. We create FlashFill++‚àíby replacing gDSL used\\nin FlashFill++ by a regular DSL ( |‚ä≤operator is treated as the usual |). We compare FlashFill++ and\\nFlashFill++‚àíon synthesis time and minimum examples required metrics. Figure 9b summarize the\\nresults. We note that gDSLs reduce synthesis time in 91% of the benchmark tasks, give more than\\n3x speedup in 20% of cases, and generate better performance across the benchmarks and metrics.\\nPrecedences in gDSLs consistently help FlashFill++ in improving both search (synthesis time)\\nand ranking (minimum number of examples).\\n6.3\\nCode Readability\\nTraditionally DSL design has focused on efficacy of the learning and ranking process, and not on\\nreadable code generation. We evaluated the extent to which FlashFill++ enables such readable code.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 22, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:23\\nDuet\\nPlaygol\\nProse\\nOverall\\nAverage Synthesis Time\\nFlashFill++\\n203.0\\n217.1\\n246.4\\n228.5\\nFlashFill++ ‚àí\\n255.0\\n350.6\\n410.3\\n361.1\\nAverage #examples required\\nFlashFill++\\n1.69\\n1.46\\n1.52\\n1.53\\nFlashFill++ ‚àí\\n1.76\\n1.54\\n1.56\\n1.59\\n(a) Synthesis time and no. of examples required to learn.\\n0\\n2000\\n4000\\n6000\\n8000\\nSynthesis Time FF++ (Ablated) (ms)\\n‚àí0.50\\n‚àí0.25\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\nlog10(FF++ (Ablated) / FlashFill++)\\nOutcome\\nFlashFill++ worse\\nFlashFill++ better\\n(b) Synthesis Time Ratios.\\nFig. 9. FlashFill++ improves over an ablation that removes the use of gDSLs, denoted FlashFill++‚àí.\\nFirst, we compared the code complexity for programs synthesized by FlashFill++ and FlashFill for\\ntwo target languages: Python & PowerFx, the low/no-code language of the Power platform [PowerFx\\n2021]. On average, FlashFill++‚Äôs Python is 76% shorter and uses 83% fewer functions, whereas\\nFlashFill++‚Äôs PowerFx is 57% shorter and uses 55% fewer functions. We next compared the Google\\nSheets formula language code generated by SmartFill with Excel code generated by FlashFill++. We\\nfound that FlashFill++ does better in ‚âà60% of the formulas generated and is at parity for ‚âà20% of\\nthe formulas. We did not compare with Duet since it generates programs only in its DSL and not in\\nany target language.\\nNext, we carried out a survey-based user study where we asked users to read and compare\\ndifferent Python functions synthesized by alternative approaches. In our study, we further augment\\nFlashFill++ with a procedure to rename variables. This part of the system is optional, and is only\\nadded to further underscore the benefits of readable code generation. To perform variable renaming\\nwe use the few-shot learning capability of Codex [Chen et al. 2021b], a pretrained large language\\nmodel, and iteratively provide the following prompt [Gao et al. 2021]: (1) two samples of the\\nrenaming task, where each sample contains I/O examples, FlashFill++ program, and the renamed\\nprogram, (2) the current renaming task, which contains the examples and the FlashFill++ program\\nto be renamed, and (3) partially renamed program up to the next non-renamed variable.\\nWe sampled 10 tasks from our benchmarks, with probability proportional to the number of\\nidentifier renaming calls made to Codex. For each task, we displayed the Python code generated by\\nFlashFill, FlashFill++, and FlashFill++ with Codex (anonymized as A, B, and C). For the first 5 tasks,\\nthe participants were asked (on a 7-point Likert scale) the extent to which they agreed with the\\nstatements ‚ÄúFlashFill++ is more readable than FlashFill‚Äù and ‚ÄúFlashFill++ with Codex is more readable\\nthan FlashFill++‚Äù. For the last 5 tasks, the participants answered (on a 7-point Likert scale) the extent\\nto which they agreed with the statement ‚ÄúX is similar to the code I write‚Äù, where X was replaced\\nwith the corresponding (anonymized) system name.\\nFigure 10a shows that participants found code generated by FlashFill++ (without identifier renam-\\ning) was more readable than code generated by FlashFill for the same task. Adding Codex-based\\nrenaming further improved readability with most participants at least somewhat agreeing.\\nFigure 10b shows that participants strongly disagreed that FlashFill code is similar to the code\\nthey write. In contrast, most participants at least somewhat agreed that FlashFill++ code is similar\\nto the code they write. Adding identifier renaming resulted in improvements, across all five tasks.\\nWe also provided an open-ended text box for additional feedback with each task. Here are some\\nillustrative excerpts, where we have replaced the anonymized system names (A,B,C) with meaningful\\ncounterparts: FlashFill: ‚Äúis a mess‚Äù, FlashFill++: ‚Äúvery readable‚Äù, FlashFill++ +Codex: \"parameter name is\\nmore self describing‚Äù; ‚ÄúFlashFill is just confusing while FlashFill++ and/or FlashFill++ with Codex are\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 23, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:24\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nD3 D2 D1 N S1 S2 S3\\nResponse\\n0\\n50\\n100\\n150\\n200\\n# of Participants\\ncomparison\\nFF++ > FF\\nFF++ w/Codex > FF++\\n(a) Statement: X is more readable than Y (denoted\\nas ùëã> ùëå). Most participants found FlashFill++\\nbetter than FlashFill. Adding Codex further im-\\nproved it.\\nD3 D2 D1 N S1 S2 S3\\nResponse\\n0\\n50\\n100\\n150\\n# of Participants\\nApproach\\nFF\\nFF++\\nFF++ w/Codex\\n(b) Statement: X is similar to the code I write. Par-\\nticipants did not find FlashFill similar. FlashFill++\\nwas closer, but FlashFill++ +Codex most similar.\\nFig. 10. Survey: D3-S3 Strongly disagree/agree. FF=FlashFill, FF++=FlashFill++, FF++ w/Codex=FlashFill++\\nwith Codex.\\nquite simple and direct‚Äù; and ‚ÄúFlashFill is very badly written, and FlashFill++ with Codex‚Äôs parameter\\nname tells a much better story‚Äù.\\n7\\nDISCUSSION\\n7.1\\nRelated Work\\nCuts are closely related to the widely-studied concept of accelerations in program analysis. Accel-\\nerations are used to capture the effect (transitive closure) of multiple state transitions by a single\\n‚Äúmeta transition‚Äù [Finkel 1987; Karp and Miller 1969]. It has often been used to handle numerical\\nupdates in programs, especially when more classical abstract interpretation techniques either do\\nnot converge or become very imprecise [Bardin et al. 2008; Boigelot 2003; Jeannet et al. 2014].\\nOur use of cuts inherits its motivation and purpose from these works, but applies them to PBE.\\nWhereas in program analysis, accelerations had success mostly on numerical domains, in PBE we\\nfind cuts helpful more generally. Currently, we assume cuts are provided by the DSL designer, but\\nautomatically generating them remains an interesting topic for future research.\\nIn PBE, cuts help speed-up search (by guiding top-down propagation across non-EI operators\\nbased on abstracting behavior of inner sub-DSLs). Other ways to speed-up search include using\\ntypes and other forms of abstractions [Guo et al. 2020; Osera and Zdancewic 2015; Wang et al.\\n2018], or combining search strategies [Lee 2021]. The difference between cuts and abstraction-based\\nmethods in synthesis is the same as the difference between accelerations and abstraction in program\\nanalysis. We need cuts for only some operators, whereas abstract transformers are required for\\nall operators. Moreover, the methods are not incompatible ‚Äì a promising direction would be to\\ncombine them.\\nMiddle-out synthesis, enabled by cuts, is a new way to combine bottom-up [Alur et al. 2013,\\n2017] and top-down [Gulwani 2011; Polozov and Gulwani 2015] synthesis. It is very different from\\nthe meet-in-the-middle way of combining them where search starts simultaneously from the bottom\\n(enumerating subprograms that generate new values) and from the top (back propagating the\\noutput) until we find values that connect the two [Gulwani et al. 2011; Lee 2021]. Helped by the\\njump provided by cuts, middle-out synthesis starts at the middle creating two subproblems that can\\nbe solved using either approach. Meet-in-the-middle approach in [Lee 2021] guides the top-down\\nsearch based on the values propagated by bottom-up, similar to middle-out synthesis; however, our\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 24, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:25\\ncuts are more general and can handle more scenarios because they overcome issues (not effectively\\nenumerable operators and large number of constants) that may make partial bottom-up fail.\\nMiddle-out synthesis can be viewed as a divide-and-conquer strategy for synthesis. The coopera-\\ntive synthesis framework in [Huang et al. 2020] proposes 3 such strategies that are used when the\\noriginal synthesis problem remains unsolved. However, [Huang et al. 2020] works on complete\\nlogical specifications, and not on input-output examples.\\nAt a very abstract level, top-down synthesis and middle-out synthesis can both be viewed as\\nan approach that synthesizes a sketch and then fills the sketch in a PBE setting [Feng et al. 2017;\\nPolikarpova et al. 2016; Wang et al. 2017, 2020]. In this context, Scythe [Wang et al. 2017] uses\\noverapproximation of the set of values that are computed by partial programs to synthesize a sketch.\\nUnlike our work, [Wang et al. 2017] is exclusively focused on bottom-up synthesis. Since [Wang\\net al. 2017] is bottom-up, its approximations get coarser as the program gets deeper. Scythe, tends\\nto do well for shallow programs. FlashFill++‚Äôs use of cuts is more ‚Äúon-demand‚Äù and thus not affected\\nby depth of programs. In fact, FlashFill++ can synthesize deep programs, with the largest solution in\\nour benchmark suite having a depth of 24, with over 10% of our benchmarks requiring programs\\nwith a depth of at least 10. Furthermore, [Wang et al. 2017] is exclusively focused on SQL ‚Äì its\\nmain contribution is an approach to overapproximate SQL queries that abstracts carefully selected\\nnonterminals. In contrast, our formalization is not fixed to any particular DSL, but relies on the\\nDSL designer to provide the cuts.\\nMorpheus [Feng et al. 2017] and Synquid [Polikarpova et al. 2016] overapproximate each com-\\nponent to prune partial programs to synthesize sketches that are subsequently filled. Morpheus\\nis specialized to tables and uses the distinction between value and table transformations. In con-\\ntrast, our framework is more general as it allows the use of approximations (cuts) for only certain\\nfunctions (wherever they are provided); however, we cannot (and do not) prune partial programs.\\nWe always work on concrete values ‚Äì there is no abstract domain involved. We do not use SMT\\nsolvers, whereas SMT solvers are a key component of [Feng et al. 2017; Polikarpova et al. 2016].\\nPrecedence and gDSLs. Precedences or priorities have been used in many grammar formalisms,\\nbut mainly for achieving disambiguation while parsing in ambiguous grammars [Aasa 1995; Aho\\net al. 1973; Earley 1974; Heering et al. 1989]. Disambiguation here refers to preferring the parse\\nùëé+(ùëè‚àóùëê) over (ùëé+ùëè)‚àóùëêfor the same string ùëé+ùëè‚àóùëê. In contrast, we use gDSLs to compare derivations\\nof different strings (programs). Furthermore, in the work on filters and SDF [Heering et al. 1989],\\nthe semantics of the precedence relation ‚âªon rules is different: there ùëÜ1 ‚Üíùë§1 ‚âªùëÜ2 ‚Üíùë§2 means\\nthat one can not use ùëÜ2 ‚Üíùë§2 as a child of ùëÜ1 ‚Üíùë§1 in a parse tree [van den Brand et al. 2002]. In\\nour case, we disallow precedence on rules with different left-hand nonterminals. Nevertheless, our\\nprecedence can be viewed as a specialized filter in the terminology of [van den Brand et al. 2002].\\nFarzan and Nicolet [Farzan and Nicolet 2021] use a sub-grammar to optimize search. This can be\\nmodeled using our precedence operator. They use constraint-based synthesis (using SMT solvers)\\nwhere the interest is in any one correct program. Ranking is not of interest there, whereas we use\\nprecedence in the context of top-down synthesis where we synthesize program sets and rank them.\\nThe interaction of precedence in the grammar and the program ranking is one of our contributions.\\nCasper [Ahmad and Cheung 2018] uses a hierarchy of grammars growing in size for synthesis,\\nmaking search efficient. This hierarchy is dynamically generated - guided by the input-output\\nexample. Our precedence-based approach provides a different mechanism to constrain search. The\\nvalue of our approach is that it is easily integrated with the underlying synthesis framework, giving\\nsynthesis-engine-builders more flexibility in controlling search and ranking. Precedence is also\\nintuitive for DSL designers because they must think only locally to decide if the operators need a\\nprecedence relation. Technically speaking, our notion of precedence implicitly represents a lattice\\nof grammars rather than a strict linear hierarchy.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 25, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:26\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nEarlier work on ‚ÄúParsing expression grammars‚Äù [Ford 2004] introduces grammars that are like\\nCFGs, but contain features such as prioritized choice (precedence), greedy repetitions, etc., but\\nit does so for parsing, whereas our focus is on top-down, bottom-up, and combination synthesis\\ntechniques. Our novelty is in supporting precedence in our synthesis framework, and formally\\nworking out how it impacts ranking and search.\\nUsing precedence is one way to handle potentially redundant operators in the DSL and prune\\nsearch space. The other way is to explicitly write the equivalence relation on programs and\\nonly consider programs that are canonical representatives of each equivalence class [Osera and\\nZdancewic 2015; Smith and Albarghouthi 2019; Udupa et al. 2013]. The gDSL approach is low\\noverhead, but may consider equivalent programs during search. However, this is by design as our\\ngoal is to generate whatever program leads to most naturally readable code.\\nPrecedence of grammar rules can be viewed as a specialized case of probabilistic context-free\\ngrammars (pCFGs) that have been used to bias the enumeration of programs through their gram-\\nmar [Lee et al. 2018; Liang et al. 2010; Menon et al. 2013]. While specialized, precedence is actually\\npreferable in many scenarios where we want to synthesize not just any program that works on the\\ninput-output examples, but one that has other desirable properties, such as, the program generalizes\\nto unseen inputs and has a readable translation in target language. As such, precedence in gDSLs\\ngive designers of synthesizers a clean way to state their ranking preference. Weights in a pCFG\\nhave to be learned from data or manually set ‚Äì both are daunting tasks compared to writing a gDSL.\\nThe contrast between pCFGs and gDSLs is akin to that between neural and symbolic approaches.\\nFlashFill [Gulwani 2011] demonstrated the effectiveness of inductive synthesis at tackling\\ncomplex string transformation. FlashMeta [Polozov and Gulwani 2015] recognized that many\\npopular inductive synthesizers [Gulwani 2011; Le and Gulwani 2014] could be decomposed into\\ndomain-specific features, such as the DSL operators and their semantics, and general (shareable)\\ndeductive steps. We used the FlashMeta framework to implement FlashFill, based on the original\\npaper [Gulwani 2011], for our experiments. We also built FlashFill++ the same way, which extends\\nthe capabilities of FlashFill to include new operations like date and number formatting, and also\\nfocuses on generating readable code.\\nIn recent work [Verbruggen et al. 2021], FlashFill has been combined with a pre-trained language\\nmodel (LM), GPT3, to facilitate semantic transformations, such as converting the city \"San Francisco\"\\nto the state \"CA\". Complementing FlashFill‚Äôs syntactic effectiveness with GPT3‚Äôs ability to do\\nsemantic transformation is interesting, but orthogonal to the problem here. However, we do exploit\\na LM to (optionally) generate meaningful variable names for our user study on code readability.\\nTrust and readability. Wrex [Drosos et al. 2020] argues that readable code is indispensable\\nfor users of synthesis technology. Wrex employed hand-written rewrite rules to produce readable\\ncode for their user study. However, this is not an approach that easily extends to all scenarios and\\nlarger languages. FlashFill++ is inspired by Wrex [Drosos et al. 2020] to address the readable code\\ngeneration challenge in a more general and scalable way: redesigning the DSL used with a focus\\non enabling readable code generation, rather than post-processing.\\nZhang et al [Zhang et al. 2021, 2020] introduced a system for interpretable synthesis, where the\\nuser interacts with the synthesizer. This approach is complementary to FlashFill++.\\n7.2\\nLimitations\\nThe concepts of cuts and precedence have been developed exclusively for synthesis approaches\\nbased on concrete values, so-called version space algebra (VSA) based methods, in this paper. The\\nterm top-down, respectively bottom-up, has been used for techniques that are based on propagation\\nof concrete output (respectively, input) values through partial sketches (respectively, programs)\\ntypically represented using VSAs. In systems that represent approximations of the sets of values\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 26, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:27\\n(at each component) using abstractions or refinement types (e.g., Synquid, Morpheus, etc), cuts\\ncan potentially address the issue of abstractions becoming too coarse as they are composed from\\nabstractions of sub-components. This is similar to how interpolants can be used to compute tight\\ninvariants in program verification when successive application of abstract transformers lead to\\noverly general invariants. Studying broader implications of these ideas is left for future work.\\nNontrivial cuts that go beyond bottom-up or top-down approaches can only be computed if there\\nare nontrivial invariants that hold (about the values that are generated) at certain intermediate\\nnonterminals in the grammar. Moreover, the DSL designer must be aware of these invariants. The\\nDSL designer can choose to write cuts that are incomplete in theory, but reasonable in practice,\\nguided by their understanding of the domain. Nontrivial cuts are likely to exist in large DSLs where\\ndifferent forms of values flow on different paths. We have not done a formal study of how difficult\\nit is for a DSL designer to write useful cut functions‚Äîthis is beyond the scope of the current paper\\nwhich just lays down the foundations for cuts.\\nDesigning a guarded DSL requires the DSL designer to have clear preference for certain operators\\nover other alternatives, and moreover, any precedence on operators closer to the start symbol (in the\\ngrammar) should override any precedence on operators closer to the leaves of the program tree. The\\n‚Äúhigher-up‚Äù operators in any DSL typically establish the high-level tactic of the program, and hence\\nthis requirement often holds. Precedences in a grammar are likely to exist if it contains redundant\\noperators that are some preferred compositions of other low-level operators in the grammar. Further,\\nthere are certain technical assumptions we make about precedences. The assumption that the\\nprecedence is a series-parallel partial order is not required if we start with the gDSL (rather than\\nstart with precedences), which is what we do in practice. The assumption that the reachability\\nrelation on the grammar nonterminals be acyclic is required only to provide a clean mathematical\\ninterpretation of the ranking induced by gDSL on programs (terms) as a path ordering. In the\\npresence of cycles, the synthesis rule and the full system can still be used without problems, but\\nranking cannot be described in a simple way.\\n8\\nCONCLUSION\\nWe introduced two techniques, cuts and precedence through guarded DSLs, that DSL designers can\\nuse to prune search in programming by example. Cuts enable a novel synthesis strategy: middle-\\nout synthesis. This strategy allows FlashFill++ to support synthesis tasks that require non-EI/EE\\noperators, such as datetime and numeric transformations. The use of precedence through gDSLs\\nallows us to increase the size of our DSL, by incorporating redundant operators, which facilitate\\nreadable code generation in different target languages. We compare our tool to existing state-of-\\nthe-art PBE systems, FlashFill, Duet, and SmartFill, on three public benchmark datasets and show\\nthat FlashFill++ can solve more tasks, in less time, and without substantially increasing the number\\nof examples required. We also perform a survey-based study on code readability, confirming that\\nthe programs synthesized by FlashFill++ are more readable than those generated by FlashFill.\\nREFERENCES\\nAnnika Aasa. 1995. Precedences in specifications and implementations of programming languages. Theoretical Computer\\nScience 142, 1 (1995), 3‚Äì26.\\nMaaz Bin Safeer Ahmad and Alvin Cheung. 2018. Automatically Leveraging MapReduce Frameworks for Data-Intensive\\nApplications. In Proc. 2018 International Conference on Management of Data, SIGMOD Conference. ACM, 1205‚Äì1220.\\nhttps://doi.org/10.1145/3183713.3196891\\nAlfred Aho, S. Johnson, and Jeffrey Ullman. 1973. Deterministic parsing of ambiguous grammars. Commun. ACM 18 (01\\n1973), 441‚Äì452. https://doi.org/10.1145/360933.360969\\nRajeev Alur, Rastislav Bod√≠k, Garvit Juniwal, Milo M. K. Martin, Mukund Raghothaman, Sanjit A. Seshia, Rishabh Singh,\\nArmando Solar-Lezama, Emina Torlak, and Abhishek Udupa. 2013. Syntax-Guided Synthesis. In Formal Methods in\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:28\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nComputer-Aided Design, FMCAD 2013. 1‚Äì8.\\nRajeev Alur, Arjun Radhakrishna, and Abhishek Udupa. 2017. Scaling Enumerative Program Synthesis via Divide and\\nConquer. In TACAS. 319‚Äì336.\\nS√©bastien Bardin, Alain Finkel, J√©r√¥me Leroux, and Laure Petrucci. 2008. FAST: acceleration from theory to practice. Int. J.\\nSoftw. Tools Technol. Transf. 10, 5 (2008), 401‚Äì424. https://doi.org/10.1007/s10009-008-0064-3\\nDenis B√©chet, Philippe de Groote, and Christian Retor√©. 1997. A Complete Axiomatisation for the Inclusion of Series-Parallel\\nPartial Orders. In Rewriting Techniques and Applications, 8th Int. Conf., RTA-97 (Lecture Notes in Computer Science,\\nVol. 1232). Springer, 230‚Äì240. https://doi.org/10.1007/3-540-62950-5_74\\nBernard Boigelot. 2003. On iterating linear transformations over recognizable sets of integers. Theor. Comput. Sci. 309, 1-3\\n(2003), 413‚Äì468. https://doi.org/10.1016/S0304-3975(03)00314-1\\nSwarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, and Yisong Yue. 2021. Neu-\\nrosymbolic Programming. Found. Trends Program. Lang. 7, 3 (2021), 158‚Äì243.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards,\\nYuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser,\\nMohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,\\nIgor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua\\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter\\nWelinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021b. Evaluating Large\\nLanguage Models Trained on Code. CoRR abs/2107.03374 (2021). arXiv:2107.03374 https://arxiv.org/abs/2107.03374\\nXinyun Chen, Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, and Denny Zhou. 2021a. Spreadsheet-\\nCoder: Formula Prediction from Semi-structured Context. In Proceedings of the 38th International Conference on Machine\\nLearning (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 1661‚Äì1672.\\nhttps://proceedings.mlr.press/v139/chen21m.html\\nAndrew Cropper. 2019. Playgol: Learning Programs Through Play. In Proceedings of the Twenty-Eighth International Joint\\nConference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, Sarit Kraus (Ed.). ijcai.org, 6074‚Äì6080.\\nhttps://doi.org/10.24963/ijcai.2019/841 https://github.com/andrewcropper/ijcai19-playgol.\\nNachum Dershowitz and Jean-Pierre Jouannaud. 1990. Rewrite Systems. In Handbook of Theoretical Computer Science,\\nVolume B: Formal Models and Semantics. Elsevier and MIT Press, 243‚Äì320.\\nJacob Devlin, Rudy Bunel, Rishabh Singh, Matthew J. Hausknecht, and Pushmeet Kohli. 2017. Neural Program Meta-Induction.\\nIn NIPS, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and\\nRoman Garnett (Eds.). 2080‚Äì2088.\\nIan Drosos, Titus Barik, Philip J. Guo, Robert DeLine, and Sumit Gulwani. 2020. Wrex: A Unified Programming-by-Example\\nInteraction for Synthesizing Readable Code for Data Scientists. In Proceedings of the 2020 CHI Conference on Human\\nFactors in Computing Systems (Honolulu, HI, USA) (CHI ‚Äô20). Association for Computing Machinery, New York, NY, USA,\\n1‚Äì12. https://doi.org/10.1145/3313831.3376442\\nJay Earley. 1974. Ambiguity and Precedence in Syntax Description. Acta Informatica 4 (1974), 183‚Äì192.\\nAzadeh Farzan and Victor Nicolet. 2021. Phased synthesis of divide and conquer programs. In PLDI ‚Äô21: 42nd ACM SIGPLAN\\nInternational Conference on Programming Language Design and Implementation. ACM, 974‚Äì986. https://doi.org/10.1145/\\n3453483.3454089\\nYu Feng, Ruben Martins, Jacob Van Geffen, Isil Dillig, and Swarat Chaudhuri. 2017. Component-based synthesis of table\\nconsolidation and transformation tasks from examples. In Proc. 38th ACM SIGPLAN Conference on Programming Language\\nDesign and Implementation, PLDI. ACM, 422‚Äì436. https://doi.org/10.1145/3062341.3062351\\nAlain Finkel. 1987. A Generalization of the Procedure of Karp and Miller to Well Structured Transition Systems. In Proc.\\n14th Intl. Colloquium on Automata, Languages and Programming, ICALP87 (Lecture Notes in Computer Science, Vol. 267),\\nThomas Ottmann (Ed.). Springer, 499‚Äì508. https://doi.org/10.1007/3-540-18088-5_43\\nBryan Ford. 2004. Parsing expression grammars: a recognition-based syntactic foundation. In Proc. 31st ACM SIGPLAN-\\nSIGACT Symposium on Principles of Programming Languages, POPL. ACM, 111‚Äì122. https://doi.org/10.1145/964001.964011\\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained Language Models Better Few-shot Learners. In\\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\\nConference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, Online,\\n3816‚Äì3830. https://doi.org/10.18653/v1/2021.acl-long.295\\nGoogle. 2021. SpreadSheetCoder. https://github.com/google-research/google-research/tree/master/spreadsheet_coder\\nSumit Gulwani. 2011. Automating string processing in spreadsheets using input-output examples. In POPL. 317‚Äì330.\\nSumit Gulwani. 2016. Programming by Examples - and its applications in Data Wrangling. In Dependable Software Systems\\nEngineering. 137‚Äì158.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:29\\nSumit Gulwani, William R. Harris, and Rishabh Singh. 2012. Spreadsheet data manipulation using examples. Commun.\\nACM 55, 8 (2012), 97‚Äì105.\\nS. Gulwani, V. Korthikanti, and A. Tiwari. 2011. Synthesizing geometry constructions. In Proc. ACM Conf. on Prgm. Lang.\\nDesgn. and Impl. PLDI. 50‚Äì61.\\nSumit Gulwani, Oleksandr Polozov, and Rishabh Singh. 2017. Program Synthesis. Foundations and Trends in Programming\\nLanguages 4, 1-2 (2017), 1‚Äì119.\\nZheng Guo, Michael James, David Justo, Jiaxiao Zhou, Ziteng Wang, Ranjit Jhala, and Nadia Polikarpova. 2020. Program\\nsynthesis by type-guided abstraction refinement. Proc. ACM Program. Lang. 4, POPL (2020), 12:1‚Äì12:28. https://doi.org/\\n10.1145/3371080\\nJan Heering, P. R. H. Hendriks, Paul Klint, and J. Rekers. 1989. The syntax definition formalism SDF - reference manual.\\nACM SIGPLAN Notices 24, 11 (1989), 43‚Äì75.\\nKangjing Huang, Xiaokang Qiu, Peiyuan Shen, and Yanjun Wang. 2020. Reconciling enumerative and deductive program\\nsynthesis. In Proc. 41st ACM SIGPLAN Intl. Conf. on Programming Language Design and Implementation, PLDI, Alastair F.\\nDonaldson and Emina Torlak (Eds.). ACM, 1159‚Äì1174. https://doi.org/10.1145/3385412.3386027\\nBertrand Jeannet, Peter Schrammel, and Sriram Sankaranarayanan. 2014. Abstract acceleration of general linear loops.\\nIn The 41st Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL. ACM, 529‚Äì540.\\nhttps://doi.org/10.1145/2535838.2535843\\nAshwin Kalyan, Abhishek Mohta, Alex Polozov, Dhruv Batra, Prateek Jain, and Sumit Gulwani. 2018. Neural-Guided\\nDeductive Search for Real-Time Program Synthesis from Examples. In 6th International Conference on Learning Repre-\\nsentations (ICLR) (6th international conference on learning representations (iclr) ed.). https://www.microsoft.com/en-\\nus/research/publication/neural-guided-deductive-search-real-time-program-synthesis-examples/\\nRichard M. Karp and Raymond E. Miller. 1969. Parallel Program Schemata. J. Comput. Syst. Sci. 3, 2 (1969), 147‚Äì195.\\nhttps://doi.org/10.1016/S0022-0000(69)80011-5\\nVu Le and Sumit Gulwani. 2014. FlashExtract: A Framework for Data Extraction by Examples. In PLDI. 542‚Äì553.\\nWoosuk Lee. 2021. Combining the top-down propagation and bottom-up enumeration for inductive program synthesis.\\nProc. ACM Program. Lang. 5, POPL (2021), 1‚Äì28. https://doi.org/10.1145/3434335 https://github.com/wslee/duet.\\nWoosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. 2018. Accelerating search-based program synthesis using\\nlearned probabilistic models. In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and\\nImplementation, PLDI, Jeffrey S. Foster and Dan Grossman (Eds.). ACM, 436‚Äì449. https://doi.org/10.1145/3192366.3192410\\nPercy Liang, Michael I. Jordan, and Dan Klein. 2010. Learning Programs: A Hierarchical Bayesian Approach. In Proceedings\\nof the 27th International Conference on Machine Learning (ICML-10), Johannes F√ºrnkranz and Thorsten Joachims (Eds.).\\nOmnipress, 639‚Äì646.\\nDylan Lukes, John Sarracino, Cora Coleman, Hila Peleg, Sorin Lerner, and Nadia Polikarpova. 2021. Synthesis of web\\nlayouts from examples. In ESEC/FSE ‚Äô21: 29th ACM Joint European Software Engineering Conference and Symposium on the\\nFoundations of Software Engineering, Athens, Greece, August 23-28, 2021, Diomidis Spinellis, Georgios Gousios, Marsha\\nChechik, and Massimiliano Di Penta (Eds.). ACM, 651‚Äì663.\\nAditya Krishna Menon, Omer Tamuz, Sumit Gulwani, Butler W. Lampson, and Adam Kalai. 2013. A Machine Learning\\nFramework for Programming by Example. In Proceedings of the 30th International Conference on Machine Learning, ICML\\n(JMLR Workshop and Conference Proceedings, Vol. 28). JMLR.org, 187‚Äì195. http://proceedings.mlr.press/v28/menon13.html\\nAnders Miltner, Kathleen Fisher, Benjamin C. Pierce, David Walker, and Steve Zdancewic. 2018. Synthesizing bijective\\nlenses. Proc. ACM Program. Lang. 2, POPL (2018), 1:1‚Äì1:30.\\nNagarajan Natarajan, Danny Simmons, Naren Datha, Prateek Jain, and Sumit Gulwani. 2019. Learning Natural Programs\\nfrom a Few Examples in Real-Time. In AIStats. https://www.microsoft.com/en-us/research/publication/learning-natural-\\nprograms-from-a-few-examples-in-real-time/\\nPeter-Michael Osera and Steve Zdancewic. 2015. Type-and-example-directed program synthesis. In Proc. 36th ACM SIGPLAN\\nConf. on Programming Language Design and Implementation. ACM, 619‚Äì630. https://doi.org/10.1145/2737924.2738007\\nRangeet Pan, Vu Le, Nachiappan Nagappan, Sumit Gulwani, Shuvendu K. Lahiri, and Mike Kaufman. 2021. Can Program\\nSynthesis be Used to Learn Merge Conflict Resolutions? An Empirical Analysis. In 43rd IEEE/ACM International Conference\\non Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021. IEEE, 785‚Äì796.\\nNadia Polikarpova, Ivan Kuraj, and Armando Solar-Lezama. 2016. Program synthesis from polymorphic refinement\\ntypes. In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation,\\nPLDI 2016, Santa Barbara, CA, USA, June 13-17, 2016, Chandra Krintz and Emery D. Berger (Eds.). ACM, 522‚Äì538.\\nhttps://doi.org/10.1145/2908080.2908093\\nOleksandr Polozov and Sumit Gulwani. 2015. FlashMeta: A Framework for Inductive Program synthesis. In OOPSLA/SPLASH.\\n107‚Äì126.\\nPowerFx 2021. PowerFx: The low code programming language. https://powerapps.microsoft.com/en-us/blog/introducing-\\nmicrosoft-power-fx-the-low-code-programming-language-for-everyone/. Accessed: 2021-11-19.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 29, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:30\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nMicrosoft PROSE. 2022. PROSE public benchmark suite. https://github.com/microsoft/prose-benchmarks.\\nKia Rahmani, Mohammad Raza, Sumit Gulwani, Vu Le, Daniel Morris, Arjun Radhakrishna, Gustavo Soares, and Ashish\\nTiwari. 2021. Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis.\\nProc. ACM Program. Lang. 5, OOPSLA (2021), 1‚Äì29.\\nReudismam Rolim, Gustavo Soares, Loris D‚ÄôAntoni, Oleksandr Polozov, Sumit Gulwani, Rohit Gheyi, Ryo Suzuki, and Bj√∂rn\\nHartmann. 2017. Learning syntactic program transformations from examples. In ICSE. IEEE / ACM, 404‚Äì415.\\nSelenium. 2022. Selenium. https://github.com/SeleniumHQ/selenium\\nNischal Shrestha, Titus Barik, and Chris Parnin. 2018. It‚Äôs Like Python But: Towards Supporting Transfer of Programming\\nLanguage Knowledge. In 2018 IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC, J√°come\\nCunha, Jo√£o Paulo Fernandes, Caitlin Kelleher, Gregor Engels, and Jorge Mendes (Eds.). IEEE Computer Society, 177‚Äì185.\\nhttps://doi.org/10.1109/VLHCC.2018.8506508\\nRishabh Singh and Sumit Gulwani. 2015. Predicting a Correct Program in Programming by Example. In CAV. 398‚Äì414.\\nCalvin Smith and Aws Albarghouthi. 2019. Program Synthesis with Equivalence Reduction. In VMCAI, Constantin Enea\\nand Ruzica Piskac (Eds.).\\nAbhishek Udupa, Arun Raghavan, Jyotirmoy V. Deshmukh, Sela Mador-Haim, Milo M. K. Martin, and Rajeev Alur. 2013.\\nTRANSIT: specifying protocols with concolic snippets. In ACM SIGPLAN Conference on Programming Language Design\\nand Implementation, PLDI, Hans-Juergen Boehm and Cormac Flanagan (Eds.). ACM, 287‚Äì296. https://doi.org/10.1145/\\n2491956.2462174\\nMark van den Brand, Jeroen Scheerder, Jurgen J. Vinju, and Eelco Visser. 2002. Disambiguation Filters for Scannerless\\nGeneralized LR Parsers. In Compiler Construction, 11th Intl. Conf, CC 2002, Part of ETAPS, Proceedings (Lecture Notes in\\nComputer Science, Vol. 2304). Springer, 143‚Äì158.\\nGust Verbruggen, Vu Le, and Sumit Gulwani. 2021. Semantic programming by example with pre-trained models. Proc. ACM\\nProgram. Lang. 5, OOPSLA (2021), 1‚Äì25. https://doi.org/10.1145/3485477\\nChenglong Wang, Alvin Cheung, and Rastislav Bod√≠k. 2017. Synthesizing highly expressive SQL queries from input-output\\nexamples. In Proc. 38th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI. ACM,\\n452‚Äì466. https://doi.org/10.1145/3062341.3062365\\nXinyu Wang, Isil Dillig, and Rishabh Singh. 2018. Program synthesis using abstraction refinement. Proc. ACM Program.\\nLang. 2, POPL (2018), 63:1‚Äì63:30. https://doi.org/10.1145/3158151\\nYuepeng Wang, Rushi Shah, Abby Criswell, Rong Pan, and Isil Dillig. 2020. Data Migration using Datalog Program Synthesis.\\nProc. VLDB Endow. 13, 7 (2020), 1006‚Äì1019. https://doi.org/10.14778/3384345.3384350\\nTianyi Zhang, Zhiyang Chen, Yuanli Zhu, Priyan Vaithilingam, Xinyu Wang, and Elena L. Glassman. 2021. Interpretable\\nProgram Synthesis. Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3411764.3445646\\nTianyi Zhang, London Lowmanstone, Xinyu Wang, and Elena L. Glassman. 2020. Interactive Program Synthesis by\\nAugmented Examples. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology\\n(Virtual Event, USA) (UIST ‚Äô20). Association for Computing Machinery, New York, NY, USA, 627‚Äì648. https://doi.org/10.\\n1145/3379337.3415900\\nReceived 2022-07-07; accepted 2022-11-07\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 0, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nContents lists available at ScienceDirect \\nComputer Languages, Systems & Structures \\njournal homepage: www.elsevier.com/locate/cl \\nSystematic mapping study of template-based code generation \\nEugene Syriani ‚àó, Lechanceux Luhunu , Houari Sahraoui \\nDepartment of computer science and operations research (DIRO), University of Montreal, Montreal, Quebec, Canada \\na r t i c l e \\ni n f o \\nArticle history: \\nReceived 18 August 2017 \\nRevised 24 November 2017 \\nAccepted 30 November 2017 \\nAvailable online 7 December 2017 \\nKeywords: \\nCode generation \\nSystematic mapping study \\nModel-driven engineering \\na b s t r a c t \\nContext: Template-based code generation (TBCG) is a synthesis technique that produces \\ncode from high-level speciÔ¨Åcations, called templates. TBCG is a popular technique in \\nmodel-driven engineering (MDE) given that they both emphasize abstraction and automa- \\ntion. Given the diversity of tools and approaches, it is necessary to classify existing TBCG \\ntechniques to better guide developers in their choices. \\nObjective: The goal of this article is to better understand the characteristics of TBCG tech- \\nniques and associated tools, identify research trends, and assess the importance of the role \\nof MDE in this code synthesis approach. \\nMethod: We survey the literature to paint an interesting picture about the trends and uses \\nof TBCG in research. To this end, we follow a systematic mapping study process. \\nResults: Our study shows, among other observations, that the research community has \\nbeen diversely using TBCG over the past 16 years. An important observation is that TBCG \\nhas greatly beneÔ¨Åted from MDE. It has favored a template style that is output-based and \\nhigh-level modeling languages as input. TBCG is mainly used to generate source code and \\nhas been applied to many domains. \\nConclusion: TBCG is now a mature technique and much research work is still conducted \\nin this area. However, some issues remain to be addressed, such as support for template \\ndeÔ¨Ånition and assessment of the correctness and quality of the generated code. \\n¬© 2017 Elsevier Ltd. All rights reserved. \\n1. Introduction \\nCode generation has been around since the 1950s, taking its origin in early compilers [1] . Since then, software organi- \\nzations have been relying on code synthesis techniques in order to reduce development time and increase productivity [2] . \\nAutomatically generating code is an approach where the same generator can be reused to produce many different artifacts \\naccording to the varying inputs it receives. It also helps detecting errors in the input artifact early on before the generated \\ncode is compiled, when the output is source code. \\nThere are many techniques to generate code, such as programmatically [3] , using a meta-object protocol [4] , or aspect- \\noriented programming [5] . Since the mid-1990‚Äôs, template-based code generation (TBCG) emerged as an approach requiring \\nless effort for the programmers to develop code generators. Templates favor reuse following the principle of write once, \\nproduce many . The concept was heavily used in web designer software (such as Dreamweaver) to generate web pages \\nand Computer Aided Software Engineering (CASE) tools to generate source code from UML diagrams. Many development \\n‚àóCorresponding author. \\nE-mail addresses: syriani@iro.umontreal.ca (E. Syriani), luhunukl@iro.umontreal.ca (L. Luhunu), sahraoui@iro.umontreal.ca (H. Sahraoui). \\nhttps://doi.org/10.1016/j.cl.2017.11.003 \\n1477-8424/¬© 2017 Elsevier Ltd. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='44 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nenvironments started to include a template mechanism in their framework such as Microsoft Text Template Transformation \\nToolkit (T4) 1 for .NET and Velocity 2 for Apache. \\nModel-driven engineering (MDE) has advocated the use of model-to-text transformations as a core component of its \\nparadigm [6] . TBCG is a popular technique in MDE given that they both emphasize abstraction and automation. MDE tools, \\nsuch as Acceleo 3 and Xpand 4 , allow developers to generate code from high-level models without worrying on how to parse \\nand traverse input models. We can Ô¨Ånd today TBCG applied in a plethora of computer science and engineering research. \\nThe software engineering research community has focused essentially on primary studies proposing new TBCG tech- \\nniques, tools and applications. However, to the best of our knowledge, there is no classiÔ¨Åcation, characterization, or \\nassessment of these studies available yet. Therefore, in this paper, we conducted a systematic mapping study (SMS) of \\nthe literature in order to understand the trends, identify the characteristics of TBCG, assess the popularity of existing tools \\nwithin the research community, and determine the inÔ¨Çuence that MDE has had on TBCG. We are interested in various \\nfacets of TBCG, such as characterizing the templates, the inputs, and outputs, along with the evolution of the amount of \\npublications using TBCG over the past 16 years. \\nThe remainder of this paper is organized as follows. In Section 2 , we introduce the necessary background on TBCG \\nand discuss related work. In Section 3 , we elaborate on the methodology we followed for this SMS, and on the results of \\nthe paper selection phase. The following sections report the results: the trends and evolution of TBCG in Section 4 , the \\ncharacteristics of TBCG according to our classiÔ¨Åcation scheme in Section 5 , the relationships between the different facets \\nin Section 6 , how TBCG tools have been used in primary studies in Section 7 , and the relation between MDE and TBCG \\nin Section 8 . In Section 9 , we answer our research questions and discuss limitations of the study. Finally, we conclude in \\nSection 10 . \\n2. Background and related work \\nWe brieÔ¨Çy explain TBCG in the context of MDE and discuss related work on secondary studies about code generation. \\n2.1. Code generation \\nIn this paper, we view code generation as in automatic programming [1] rather than compilers. The underlying principle \\nof automatic programming is that a user deÔ¨Ånes what he expects from the program and the program should be automat- \\nically generated by a software without any assistance by the user. This generative approach is different from a compiler \\napproach. Compilers produce code executable by a computer from a speciÔ¨Åcation conforming to a programming language, \\nwhereas automatic programming transforms user speciÔ¨Åcations into code which often conforms to a programming language. \\nCompilers have a phase called code generation that retrieves an abstract syntax tree produced by a parser and translates it \\ninto machine code or bytecode executable by a virtual machine. Compared to code generation as in automatic programming, \\ncompilers can be regarded as tasks or services that are incorporated in or post-positioned to code generators [7] . \\nCode generation is an important model-to-text transformation that ensures the automatic transformation of a model into \\ncode. Organization are adopting the use of code generation since it reduces the development process time and increases \\nthe productivity. Generating the code using the most appropriate technique is even more crucial since it is the key to \\nbeneÔ¨Åt from all the advantages code synthesis offers to an organization. Nowadays, TBCG has raised to be the most popular \\nsynthesis technique available. Using templates can quickly become a complex task especially when the model should satisfy \\na certain condition before a template fragment is executed. \\nAs Balzer [8] states, there are many advantages to code generation. The effort of the user is reduced as he has fewer \\nlines to write: speciÔ¨Åcations are shorter than the program that implements them. SpeciÔ¨Åcations are easier to write and \\nto understand for a user, given that they are closer to the application and domain concepts. Writing speciÔ¨Åcations is less \\nerror-prone than writing the program directly, since the expert is the one who writes the speciÔ¨Åcation rather than another \\nprogrammer. \\nThese advantages are in fact the pillar principles of MDE and domain-speciÔ¨Åc modeling. Floch et al. [9] observed many \\nsimilarities between MDE and compilers research and principles. Thus, it is not surprising to see that many, though not \\nexclusively, code generation tools came out of the MDE community. The advantages of code generation should be contrasted \\nwith some of its limitations. For example, there are issues related to integration of generated code with manually written \\ncode and to evolving speciÔ¨Åcations that require to re-generate the code [10] . Sometimes, relying too much on code genera- \\ntors may produce an overly general solution that may not necessarily be optimal for a speciÔ¨Åc problem. \\n1 https://msdn.microsoft.com/en-us/library/bb126445.aspx \\n2 http://velocity.apache.org/ \\n3 http://www.eclipse.org/acceleo/ \\n4 http://wiki.eclipse.org/Xpand'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 2, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n45 \\nconforms to\\nuses\\ninput\\ninput\\ngenerates\\nTemplate\\n<%context class%>\\npublic class <%name%> { String id; }\\nDesign-time input\\nClass\\nname:string\\nRuntime input\\nPerson\\nOutput\\npublic class Person { String id; }\\nTemplate engine\\nFig. 1. Components of TBCG. \\n2.2. Model-to-text transformations \\nIn MDE, model transformations can have different purposes [11] , such as translating, simulating, or reÔ¨Åning models. One \\nparticular kind of model transformation is devoted to code generation with model-to-text transformations (M2T) [12] . In \\ngeneral, M2T transforms a model (often in the form of a graph structure) into a linearized textual representation. M2T is \\nused to produce various text artifacts, e.g., to generate code, to serialize models, to generate documentation and reports, or \\nto visualize and explore models. As such, code generation is a special case of M2T, where the output artifacts are executable \\nsource code. There are commonly Ô¨Åve different code generation approaches present in the literature [7,12] : \\nVisitor based approaches consist of programmatically traversing the internal representation of the input, while relying \\non an API dedicated to manipulate the input, to write the output to a text stream. This requires to program directly \\nthe creation of the output Ô¨Åles and storing the strings while navigating through the data structure of the input. The \\nimplementation typically makes use of the visitor design pattern [13] . This approach is used in [3] to generate Java \\ncode from a software architecture description language. \\nMeta-programming is a language extension approach, such as using a meta-object protocol, that relies on introspection \\nin order to access the abstract syntax tree of the source program. For example, in OpenJava [4] , a Java meta-program \\ncreates a Java Ô¨Åle, compiles it on the Ô¨Çy, and loads the generated program in its own run-time. \\nIn-line generation relies on a preprocessor that generates additional code to expand the existing one, such as with the \\nC++ standard template library or C macro preprocessor instructions. The generation instructions can be deÔ¨Åned at a \\nhigher-level of abstraction, either using a dedicated language distinct from the base language (e.g., for macros) or as \\na dedicated library as in [14] . \\nCode annotations are in-line descriptions that added to statement declarations (e.g., class deÔ¨Ånition) that can either be \\ninternally transformed into more expanded code (e.g., attributes in C#) or that are processed by a tool other than the \\ncompiler of the language (e.g., the speciÔ¨Åcation of documentation comments processed by Javadoc). This approach is \\nused in [15] . \\nTemplate based is described below. \\n2.3. Template-based code generation \\nThe literature agrees on a general deÔ¨Ånition of M2T code generation [12] and on templates. J√∂rges [7] identiÔ¨Åes three \\ncomponents in TBCG: the data, the template, and the output. However, there is another component that is not mentioned, \\nwhich is the meta-information the generation logic of the template relies on. Therefore, we conducted this study according \\nto the following notion of TBCG. \\nFigure 1 summarizes the main concepts of TBCG. We consider TBCG as a synthesis technique that uses templates in order \\nto produce a textual artifact, such as source code, called the output . A template is an abstract and generalized representation \\nof the textual output it describes. It has a static part , text fragments that appear in the output ‚Äúas is‚Äù. It also has a dynamic \\npart embedded with splices of meta-code that encode the generation logic. Templates are executed by the template engine \\n(sometimes refereed to as template processor ) to compute the dynamic part and replace meta-codes by static text according \\nto run-time input . The design-time input deÔ¨Ånes the meta-information which the run-time input conforms to. The dynamic \\npart of a template relies on the design-time input to query the run-time input by Ô¨Åltering the information retrieved and \\nperforming iterative expansions on it. Therefore, TBCG relies on a design-time input that is used to deÔ¨Åne the template \\nand a run-time input on which the template is applied to produce the output. For example, a TBCG engine that takes as \\nrun-time input an XML document relies on an XML schema as design-time input. DeÔ¨Ånition 1 summarizes our deÔ¨Ånition of \\nTBCG.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='46 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nDeÔ¨Ånition 1. A synthesis technique is a TBCG if it consists of a set of templates speciÔ¨Åed in a formalism, where the speciÔ¨Å- \\ncation of a template is based on a design-time input such that, when executed, it reads a run-time input to produce a textual \\noutput . \\nFor example, the work in [16] generates a C# API from Ecore models using Xpand. According to DeÔ¨Ånition 1 , the tem- \\nplates of this TBCG example are Xpand templates, the design-time input is the metamodel of Ecore, the run-time input is \\nan Ecore model, and the output is a C# project Ô¨Åle and C# classes. \\n2.4. Literature reviews on code generation \\nIn evidence-based software engineering [17] , a systematic literature review is a secondary study that reviews primary \\nstudies with the aim of synthesizing evidence related to a speciÔ¨Åc research question. Several forms of systematic reviews \\nexist depending on the depth of reviewing primary studies and on the speciÔ¨Åcities of research questions. Unlike conven- \\ntional systematic literature reviews that attempt to answer a speciÔ¨Åc question, a SMS aim at classifying and performing a \\nthematic analysis on a topic [18] . SMS is a secondary study method that has been adapted from other disciplines to soft- \\nware engineering in [19] and later evolved by Petersen et al. in [20] . A SMS is designed to provide a wide overview of \\na research area, establish if research evidence exists on a speciÔ¨Åc topic, and provide an indication of the quantity of the \\nevidence speciÔ¨Åc to the domain. \\nOver the years, there have been many primary studies on code generation. However, we could not Ô¨Ånd any secondary \\nstudy on TBCG explicitly. Still, the following are closely related secondary studies. \\nMehmood et al. [21] performed a SMS regarding the use of aspect-oriented modeling for code generation, which is not \\nbased on templates. They analyzed 65 papers mainly based on three main categories: the focus area, the type of research, \\nand the type of contribution. The authors concluded that this synthesis technique is still immature. The study shows that \\nno work has been reported to use or evaluate any of the techniques proposed. \\nGurunule et al. [22] presented a comparison of aspect orientation and MDE techniques to investigate how they can each \\nbe used for code generation. The authors found that further research in these areas can lead to signiÔ¨Åcant advancements in \\nthe development of software systems. Unlike Mehmood et al. [21] , they did not follow a systematic and repeatable process. \\nDominguez et al. [23] performed a systematic literature review of studies that focus on code generation from state ma- \\nchine speciÔ¨Åcations. The study is based on a set of 53 papers, which have been classiÔ¨Åed into two groups: pattern-based \\nand not pattern-based. The authors do not take template-based approaches into consideration. \\nBatot et al. [24] performed a systematic mapping study on model transformations solving a concrete problem that have \\nbeen published in the literature. They analyzed 82 papers based on a classiÔ¨Åcation scheme that is general to any model \\ntransformation approach, which includes M2T. They conclude that concrete model transformations have been pulling out \\nfrom the research literature since 2009 and are being considered as development tasks. They also found that 22% of their \\ncorpus solve concrete problems using reÔ¨Ånement and code synthesis techniques. Finally, they found that research in model \\ntransformations is heading for a more stable and grounded validation. \\nThere are other studies that attempted to classify code generation techniques. However, they did not follow a systematic \\nand repeatable process. For example, Czarnecki et al. [12] proposed a feature model providing a terminology to characterize \\nmodel transformation approaches. They distinguished two categories for M2T approaches: those that are visitor-based and \\nthose that are template-based; the latter being in line with DeÔ¨Ånition 1 . The authors found that many new approaches \\nto model-to-model transformation have been proposed recently, but relatively little experience is available to assess their \\neffectiveness in practical applications. \\nRose et al. [25] extended the feature model of Czarnecki et al. to focus on template-based M2T tools. Their classiÔ¨Åcation \\nis centered exclusively on tool-dependent features. Their goal is to help developers when they are faced to choose between \\ndifferent tools. This study is close to the work of Czarnecki in [12] but focuses only on a feature model for M2T. The \\ndifference with our study is that it focuses on a feature diagram and deals with tool-dependent features only. \\nThere are also other systematic reviews performed on other related topics. For example, the study in [26] performed a \\nSMS on domain-speciÔ¨Åc modeling languages (DSL). They analyzed 390 papers to portray the published literature on DSLs, \\nwhich is comparable to the size of our corpus. Also, the study in [27] presents a systematic literature review on software \\nproduct lines engineering in the context of DSLs. Similar to our study, they propose a deÔ¨Ånition for the life-cycle of language \\nproduct lines and use it to analyze how the literature has an impact this life-cycle. Another SMS recently published in \\n[28] presents a taxonomy of source code labeling. \\n3. Research methods \\nIn order to analyze the topic of TBCG, we conducted a SMS following the process deÔ¨Åned by Petersen et al. in [20] and \\nsummarized in Fig. 2 . The deÔ¨Ånition of research question is discussed in Section 3.1 . The search conduction is described \\nin Section 3.2 . We present the screening of papers in Section 3.3 . The relevant papers are obtained based on the criteria \\npresented in Section 3.3.1 and Section 3.3.2 . The elaboration of the classiÔ¨Åcation scheme is described in Section 3.4 . Finally, \\nwe detail the selection of the papers in Section 3.5 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 4, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n47 \\nProcess Steps\\nOutcomes\\nDefinition of\\nResearch Question\\nConduct Search\\nScreening of Papers\\nKeywording using\\nAbstracts\\nData Extraction and\\nMapping Process\\nSystematic Map\\nClassification\\nScheme\\nRelevant Papers\\nAll Papers\\nReview Scope\\nFig. 2. The systematic mapping study process we followed from Peterson et al. \\n3.1. Objectives \\nThe objective of this study is to obtain an overview of the current research in the area of TBCG and to characterize the \\ndifferent approaches that have been developed. We deÔ¨Åned four research questions to set the scope of this study: \\n1. What are the trends in template-based code generation? We are interested to know how this technique has evolved over \\nthe years through research publications. \\n2. What are the characteristics of template-based code generation approaches? We want to identify major characteristics of \\nthese techniques and their tendencies. \\n3. To what extent are template-based code generation tools being used in research? We are interested in identifying popular \\ntools and their uses. \\n4. What is the place of MDE in template-based code generation? We seek to determine whether and how MDE has inÔ¨Çuenced \\nTBCG. \\n3.2. Selection of source \\nWe delimited the scope of the search to be regular publications that mention TBCG as at least one of the approaches \\nused for code generation and published between 20 0 0‚Äì2016. Therefore, this includes publications where code generation is \\nnot necessarily the main contribution. For example, Buchmann et al. [29] used TBCG to obtain ATL code while their main \\nfocus was implementing a higher-order transformation. Given that not all publications have the term ‚Äúcode generation‚Äù in \\ntheir title, we retrieved publications based on their title, abstract, or full text (when available) mentioning the keywords \\n‚Äútemplate‚Äù and its variations, ‚Äúcode‚Äù, and ‚Äúgeneration‚Äù and synonyms with their variations (e.g., ‚Äúsynthesis‚Äù). We used the \\nfollowing search query for all digital libraries, using their respective syntax: \\ntemplat ‚àóAND ‚Äúcode generat ‚àó‚Äù OR ‚Äúcode synthesi ‚àó‚Äù\\nWe validated our search with a sample of 100 pilot papers we preselected. These papers were chosen from papers we \\napriori knew should be included and resulting from a preliminary pilot search online. We iterated over different versions of \\nthe search strings until all pilot papers could be retrieved from the digital libraries. \\n3.3. Screening procedure \\nScreening is the most crucial phase in a SMS [20] . We followed a two-stage screening procedure: automatic Ô¨Åltering, \\nthen title and abstract screening. In order to avoid the exclusion of papers that should be part of the Ô¨Ånal corpus, we \\nfollowed a strict screening procedure. With four reviewers at our disposal, each article is screened by at least two reviewers \\nindependently. When both reviewers of a paper disagree upon the inclusion or exclusion of the paper, a physical discussion \\nis required. If the conÔ¨Çict is still unresolved, an additional senior reviewer is involved in the discussion until a consensus \\nis reached. To determine a fair exclusion process, a senior reviewer reviews a sample of no less than 20% of the excluded \\npapers at the end of the screening phase, to make sure that no potential paper is missed. \\n3.3.1. Inclusion criteria \\nA paper is included if it explicitly indicates the use of TBCG or if it proposes a TBCG technique. We also include papers \\nif the name of a TBCG tool appears in the title, abstract, or content. ‚ÄúUse‚Äù is taken in a large sense when it is explicit that \\na TBCG contributes to the core of the paper (not in the related work). \\n3.3.2. Exclusion criteria \\nResults from the search were Ô¨Årst Ô¨Åltered automatically to discard records that were outside the scope of this study: \\npapers not in computer science, not in the software engineering domain, with less than two pages of length (e.g., proceed-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='48 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nings preface), not peer-reviewed (e.g., white papers), not written in English, or not published between the years 20 0 0 and \\n2016. Then, papers were excluded through manual inspection based on the following criteria: \\n‚Ä¢ No code generation. There is no code generation technique used. \\n‚Ä¢ Not template-based code generation. Code generation is mentioned, but the considered technique is not template-based \\naccording to DeÔ¨Ånition 1 . \\n‚Ä¢ Not a paper. This exclusion criterion spans papers that were not caught by the automatic Ô¨Åltering. For example, some \\npapers had only the abstract written in English and the content of the paper in another language. Additionally, there \\nwere 24 papers where the full text was not accessible online. \\nFor the Ô¨Årst two criteria, when the abstract did not give enough details about the code generation approach, a quick \\nlook at the full text helped clear any doubts on whether to exclude the paper or not. Reviewers were conservative on that \\nmatter. \\n3.4. ClassiÔ¨Åcation scheme \\nWe elaborated a classiÔ¨Åcation scheme by combining our general knowledge with the information extracted from the \\nabstracts during the screening phase. We classify all papers along different categories that are of interest in order to answer \\nour research questions. This helps analyzing the overall results and gives an overview of the trends and characteristics of \\nTBCG. The categories we classiÔ¨Åed the corpus with are the following: \\n‚Ä¢ Template style : We characterize the level of customization and expressiveness of the templates used in the code gen- \\neration approach. PredeÔ¨Åned style is reserved for approaches where the template used for code generation is deÔ¨Åned \\ninternally to the tool. However, a subset of the static part of the template is customizable to vary slightly the gener- \\nated output. This is, for example, the case for common CASE tools where there is a predeÔ¨Åned template to synthesize \\na class diagram into a number of programming languages. Nevertheless, the user can specify what construct to use for \\nmany-cardinality associations, e.g., Array or ArrayList for Java templates. Output-based style covers templates that \\nare syntactically based on the actual target output. In contrast with the previous style, output-based templates offer full \\ncontrol on how the code is generated, both on the static and dynamic parts. The generation logic is typically encoded in \\nmeta-code as in the example of Fig. 1 . Rule-based style puts the focus of the template on computing the dynamic part \\nwith the static part being implicit. The template lists declarative production rules that are applied on-demand by the \\ntemplate engine to obtain the Ô¨Ånal target output. For example, this is used to render the concrete textual syntax from \\nthe abstract syntax of a model using a grammar. \\n‚Ä¢ Input type : We characterize the language of the design-time input that is necessary to develop templates. The run-time \\ninput is an instance that conforms to it. General-purpose modeling language is for generic languages reusable across dif- \\nferent domains that are not programming languages, such as UML. Domain-speciÔ¨Åc modeling language is for languages \\ntargeted for a particular domain, for example, where the run-time input is a Simulink model. Schema is for structured \\ndata deÔ¨Ånitions, such as XML or database schema. Programming language is for well-deÔ¨Åned programming languages, \\nwhere the run-time input is source code. \\n‚Ä¢ Output type : We characterize the output of the code generator (more than one category can be selected when multiple \\nartifacts are generated). Source code is executable code conforming to a speciÔ¨Åc programming language. Structured data \\nis for code that is not executable, such as HTML. \\n‚Ä¢ Application scale : We characterize the scale of the artifact on which the TBCG approach is applied with respect to the \\nevaluation or illustration cases in the paper. The presence of a formal case study that either relied on multiple data \\nsources or subjects qualiÔ¨Åed as large scale application. Small scale was used mainly when there was only one example or \\ntoy examples in the paper that illustrated the TBCG. No application if for when the code generation was not applied on \\nany example. \\n‚Ä¢ Application domain : We classify the general domain TBCG has been applied on. For example, this includes Software engi- \\nneering, Embedded systems, Compilers, Bio-medicine , etc. \\n‚Ä¢ Orientation : We distinguish industrial papers, where at least one author is aÔ¨Éliated to industry, from academic papers \\notherwise. \\n‚Ä¢ Tool : We capture the tool used for TBCG. If a tool is not clearly identiÔ¨Åed in a paper or the TBCG is programmed directly, \\nwe classify the tool as unspeciÔ¨Åed . We consider a tool to be popular within the research community when it is used in \\nat least 1% of the papers. Otherwise, we classify it as other . \\n‚Ä¢ MDE : We determine whether the part of the solution where TBCG is applied in the paper follows MDE techniques and \\nprinciples. A good indication is if the design-time input is a metamodel. \\n3.5. Paper selection \\nTable 1 summarizes the Ô¨Çow of information through the selection process of this study. This section explains how we \\nobtained the Ô¨Ånal corpus of papers to classify and analyze.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 6, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n49 \\nTable 1 \\nEvolution of paper corpus during the study process. \\nPhase \\nNumber of papers \\nCollection \\nEngineering Village \\n4043 \\nScopus \\n932 \\nSpringerLink \\n2671 \\nInitial corpus \\n5131 \\nScreening \\nExcluded during screening \\n4553 \\nIncluded \\n578 \\nClassiÔ¨Åcation \\nExcluded during classiÔ¨Åcation \\n99 \\nFinal corpus \\n481 \\n3.5.1. Paper collection \\nThe paper collection step was done in two phases: querying and automatic duplicates removal. There are several \\nonline databases that index software engineering literature. For this study, we considered three main databases to maxi- \\nmize coverage: Engineering Village 5 , Scopus 6 , and SpringerLink 7 . The Ô¨Årst two cover typical software engineering editors \\n( IEEE Xplore , ACM Digital Library , Elsevier ). However, from past experiences [24] , they do not include all of Springer pub- \\nlications. We used the search string from Section 3.2 to retrieve all papers from these three databases. We obtained 7 646 \\ncandidate papers that satisfy the query and the options of the search stated in Section 3.3.2 . We then removed automatically \\nall duplicates using EndNote software. This resulted in 5 131 candidate papers for the screening phase. \\n3.5.2. Screening \\nBased on the exclusion criteria stated in Section 3.3.2 , each candidate paper was screened by at least two reviewers to \\ndecide on its inclusion. To make the screening phase more eÔ¨Écient, we used a home-made tool [30] . After all the reviewers \\ncompleted screening the papers they were assigned, the tool calculates an inter-rater agreement coeÔ¨Écient. In our case, the \\nCohens Kappa coeÔ¨Écient was 0.813. This high value shows that the reviewers were in almost perfect agreement. \\nAmong the initial corpus of candidate papers, 4556 were excluded, 551 were included and 24 received conÔ¨Çicting ratings. \\nDuring the screening, the senior reviewer systematically veriÔ¨Åed each set of 100 rejected papers for sanity check. A total \\nof 7 more papers were included back hence the rejected papers were reduced to 4549. Almost all cases of conÔ¨Çicts were \\nabout a disagreement on whether the code generation technique of a paper was using templates or not. These conÔ¨Çicts were \\nresolved in physical meetings and 20 of them were Ô¨Ånally included for a total of 578 papers and 4553 excluded. \\nAmong the excluded papers, 52% were rejected because no code generation was used. We were expecting such a high \\nrate because terms such as ‚Äútemplates‚Äù are used in many other Ô¨Åelds, like biometrics. Also, many of these papers were \\nreferring to the C++ standard template library [31] , which is not about code generation. We counted 34% papers excluded \\nbecause they were not using templates . Examples of such papers are cited in Section 2.2 . Also, more than a quarter of the \\npapers were in the compilers or embedded system domains, where code generation is programmed imperatively rather than \\ndeclaratively speciÔ¨Åed using a template mechanism. Finally, 5% of the papers were considered as not a paper . In fact, this \\ncriterion was in place to catch papers that escaped the automatic Ô¨Åltering from the databases. \\n3.5.3. Eligibility during classiÔ¨Åcation \\nOnce the screening phase over, we thoroughly analyzed the full text of the remaining 578 papers to classify them \\naccording to our classiÔ¨Åcation scheme. Doing so allowed us to conÔ¨Årm that the code generation approach was effectively \\ntemplate-based according to DeÔ¨Ånition 1 . We encountered papers that used multiple TBCG tools: they either compared tools \\nor adopted different tools for different tasks. We classiÔ¨Åed each of these papers as a single publication, but incremented the \\noccurrence corresponding to the tools referred to in the paper. This is the case of [32] where the authors use Velocity and \\nXSLT for code generation. Velocity generates Java and SQL code, while XSLT generates the control code. \\nWe excluded 99 additional papers. During screening, we detected situations where the abstract suggested the imple- \\nmentation of TBCG, whereas the full text proved otherwise. In most of the cases, the meaning of TBCG differed from the \\ndescription presented in Section 2.3 . As shown in [33] the terms template-based and generation are used in the context of \\nnetworking and distributed systems. We also encountered circumstances where the tool mentioned in the abstract requires \\nthe explicit use of another component to be considered as TBCG, such as Simulink TLC, as in [34] . \\nThe Ô¨Ånal corpus 8 considered for this study contains 481 papers. \\n5 https://www.engineeringvillage.com/ \\n6 https://www.scopus.com/ \\n7 http://link.springer.com/ \\n8 The complete list of papers is available online at http://www-ens.iro.umontreal.ca/ ‚àºluhunukl/survey/classiÔ¨Åcation.html'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 7, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='50 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\n# of papers\\nFig. 3. Evolution of papers in the corpus. \\nTable 2 \\nMost popular venues. \\nVenue \\nVenue & publication type \\n# Papers \\nModel Driven Engineering Languages and Systems ( Models ) \\nMDE \\nConference \\n26 \\nSoftware and Systems Modeling ( Sosym ) \\nMDE \\nJournal \\n26 \\nEuropean Conference on Modeling Foundations and Applications ( Ecmfa ) \\nMDE \\nConference \\n19 \\nGenerative and Transformational Techniques in Software Engineering ( Gttse ) \\nSoft. eng. \\nConference \\n11 \\nGenerative Programming: Concepts & Experience ( Gpce ) \\nSoft. eng. \\nConference \\n8 \\nInternational Conference on Computational Science and Applications ( Iccsa ) \\nOther \\nConference \\n8 \\nSoftware Language Engineering ( Sle ) \\nMDE \\nConference \\n7 \\nLeveraging Applications of Formal Methods, VeriÔ¨Åcation and Validation ( Isola ) \\nOther \\nConference \\n7 \\nAutomated Software Engineering ( Ase ) \\nSoft. eng. \\nJournal + Conference \\n5 + 2 \\nInternational Conference on Web Engineering ( Icwe ) \\nOther \\nConference \\n6 \\nEvaluation of Novel Approaches to Software Engineering ( Enase ) \\nSoft. eng. \\nConference \\n5 \\n4. Evolution of TBCG \\nWe start with a thorough analysis of the trends in TBCG in order to answer the Ô¨Årst research question. \\n4.1. General trend \\nFigure 3 reports the number of papers per year, averaging around 28. The general trend indicates that the number of \\npublications with at least one TBCG method started increasing in 2002 to reach a Ô¨Årst local maximum in 2005 and then \\nremained relatively constant until 2012. This increase coincides with the early stages of MDE and the Ô¨Årst edition of the \\nModels conference, previous called Uml conference. This is a typical trend where a research community gets carried away \\nby the enthusiasm of a new potentially interesting domain, which leads to more publications. However, the most proliÔ¨Åc \\nperiod was in 2013, where we notice a signiÔ¨Åcant peak with 2.4 times the average numbers of publications observed in the \\nprevious years. Fig. 3 then shows sudden a decrease in 2015. \\nResorting to statistical methods, the high coeÔ¨Écient of variability and modiÔ¨Åed Thompson Tau test indicate that 2013 \\nand 2015 are outliers in the range 2005‚Äì2016, where the average is 37 papers per year. The sudden isolated peak in 2013 \\nis the result of a special event or popularity of TBCG. The following decrease in the amount of papers published should not \\nbe interpreted as a decline in interest in TBCG, but that some event happened around 2013 which boosted publications, \\nand then it went back to the steady rate of publication as previous years. In fact, 2016 is one standard deviation above the \\naverage. \\n4.2. Publications and venues \\nWe analyzed the papers based on the type of publication and the venue of their publication. MDE venues account for \\nonly 22% of the publications, so are software engineering venues, while the majority (56%) were published in other venues. \\nTable 2 shows the most popular venues that have at least Ô¨Åve papers. These top venues account for just more than a quarter \\nof the total number of publications. Among them, MDE venues account for 60% of the papers. Models , Sosym , and Ecmfa \\nare the three most popular venues 9 with a total of 66 publications between them. This is very signiÔ¨Åcant given that the \\n9 We grouped Uml conference with Models and Ecmda-fa with Ecmfa .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 8, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n51 \\nTable 3 \\nDistribution of the template style facet. \\nOutput-based \\nPredeÔ¨Åned \\nRule-based \\n72% \\n24% \\n4% \\n0\\n10\\n20\\n30\\n40\\n50\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nOutput-based\\nPredefined\\nRule-based\\n# of papers\\nFig. 4. Template style evolution. \\naverage is only 1.67 paper per venue with a standard deviation of 2.63. Also, 43% of venues had only one paper using TBCG, \\nwhich is the case for most of the other venues. \\nThe peak in 2013 was mainly inÔ¨Çuenced by MDE and software engineering venues. However the drop in 2015 is the result \\nof an accumulation of the small variations among the other venues. Since 2014, MDE venues account for 10‚Äì12 papers per \\nyear, while only 6‚Äì7 in software engineering. \\nAs for the publication type, conference publications have been dominating at 64%. Journal article account for 24% of all \\npapers. The remaining papers were published as book chapters, workshops or other publication type. Interestingly, we notice \\na steady increase in journal articles, reaching a maximum of 15 in 2016. \\n5. Characteristics of template-based code generation \\nWe examine the characteristics of TBCG using the classiÔ¨Åcation scheme presented in Section 3.4 . \\n5.1. Template style \\nAs Table 3 illustrates, the vast majority of the publications follow the output-based style. This consists of papers like [35] , \\nwhere Xpand is used to generate workÔ¨Çow code used to automate modeling tools. There, it is the Ô¨Ånal output target text \\nthat drives the development of the template. This high score is expected since output-based style is the original template \\nstyle for TBCG as depicted in Fig. 4 . This style has always been the most popular style since 20 0 0. \\nThe predeÔ¨Åned style is the second most popular. Most of these papers generate code using a CASE tool, such as [36] that \\nuses Rhapsody to generate code to map UML2 semantics to Java code with respect to association ends. Apart from CASE \\ntools, we also classiÔ¨Åed papers like [37] as predeÔ¨Åned style since the output code is already Ô¨Åxed as HTML and the pro- \\ngrammer uses the tags to change some values based on the model. Each year, around 28% of the papers were using the \\npredeÔ¨Åned style, except for a peak of 39% in 2005, given the popularity of CASE tools then. \\nWe found 19 publications that used rule-based style templates. This includes papers like [38] which generates Java code \\nwith Stratego from a DSL. A possible explanation of such a low score is that this is the most diÔ¨Écult template style to \\nimplement. It had a maximum of two papers per year throughout the study period. \\n5.2. Input type \\nGeneral purpose languages account for almost half of the design-time input of the publications, as depicted in Table 4 . \\nUML (class) diagrams, which are used as metamodels for code generation, are the most used for 87% of these papers as in \\n[35] . Other popular general-purpose languages that were used are, for example, the architecture analysis and design lan- \\nguage [39] and feature diagrams [40] . The schema category comes second with 21% of the papers. For example, a database \\nTable 4 \\nDistribution of the input type facet. \\nGeneral purpose \\nSchema \\nDomain speciÔ¨Åc \\nProgramming language \\n48% \\n22% \\n20% \\n10%'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 9, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='52 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nGeneral purpose\\nSchema\\nDomain Specific\\nProgramming Language\\n# of papers\\nFig. 5. Evolution of the design-time input type. \\nschema is used as input at design-time in [41] to generate Java for a system that demonstrates that template can im- \\nprove software development. Also, an XML schema is used in [42] as design-time input to produce C programs in order to \\nimplement an approach that can eÔ¨Éciently support all the conÔ¨Åguration options of an application in embedded systems. \\nDSLs are almost at par with schema. They have been gaining popularity and gradually reducing the gap with general-purpose \\nlanguages. For example in [43] , a custom language is given as the design input in order to generate C and C++ to develop \\na TBCG approach dedicated to real-time systems. The least popular design-time input type is programming language . This \\nincludes papers like [44] where T4 is used to generate hardware description (VHDL) code for conÔ¨Ågurable hardware. In this \\ncase, the input is another program on which the template depends. \\nOver the years, the general-purpose category has dominated the input type facet, as depicted in Fig. 5 . 2003 and 2006 \\nwere the only exceptions where schema obtained slightly more publications. We also notice a shift from schema to domain- \\nspeciÔ¨Åc design-time input types. Domain-speciÔ¨Åc input started increasing in 2009 but never reached the same level as \\ngeneral purpose. Programming language input maintained a constant level, with an average of 1% per year. Interestingly, in \\n2011, there were more programming languages used than DSLs. \\n5.3. Output type \\nAn overwhelming majority of the papers use TBCG to generate source code (81%), in contrast with 19% was structured \\ndata (though some papers output both types). Table 5 shows the distribution of the output languages that appeared in more \\nthan Ô¨Åve papers, representing 74% of the corpus. This includes papers like [45] where Java code is generated an adaptable \\naccess control tool for electronic medical records. Java and C are the most targeted programming languages. Writing a \\nprogram manually often requires proved abilities, especially with system and hardware languages, such as VHDL [46] . This \\nis why 8% of all papers generate low level source codes. Generation of structured data includes TBCG of mainly XML and \\nHTML Ô¨Åles. For example [47] produces both HTML and XML as parts of the web component to ease regression testing. In \\naddition, we found that around 4% of the papers generate combinations of at least two output types. This includes papers \\nsuch as [48] that generate both C# and HTML from a domain-speciÔ¨Åc model. \\nStructured data output remained constant over the years, unlike source code which follows the general trend. \\n5.4. Application scale \\nAs depicted in Table 6 , most papers applied TBCG on large scale examples. This result indicates that TBCG is a technique \\nwhich scales with larger amounts of data. This includes papers like [49] that uses Acceleo to generate hundreds of lines \\nTable 5 \\nDistribution of the most popular output languages. \\nJava \\nC \\nHTML \\nC ++ \\nC# \\nXML \\nVHDL \\nSQL \\nAspectJ \\nSystemC \\n33% \\n11% \\n7% \\n7% \\n4% \\n3% \\n3% \\n2% \\n2% \\n2% \\nTable 6 \\nDistribution of application scale facet \\nLarge scale \\nSmall scale \\nNo application \\n63% \\n32% \\n5%'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 10, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n53 \\n0\\n10\\n20\\n30\\n40\\n50\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nLarge scale\\nSmall scale\\nNo application\\n# of papers\\nFig. 6. Application scale evolution. \\nFig. 7. Distribution of application domain facet. \\nof aspect-oriented programming code. Small scale obtains 32% of the papers. This is commonly found in research papers \\nthat only need a small and simple example to illustrate their solution. This is the case in [50] in which a small concocted \\nexample shows the generation process with the Epsilon Generation Language (EGL) 10 . No application was used in 5% of the \\npublications. This includes papers like [51] where authors just mention that code synthesis is performed using a tool named \\nMako-template. Even though the number of publications without an actual application is very low, this demonstrates that \\nsome authors have still not adopted good practice to show an example of the implementation. This is important, especially \\nwhen the TBCG approach is performed with a newly developed tool. \\nAs depicted in Fig. 6 , most papers illustrated their TBCG using large scale applications up to 2015. In this period, while \\nit follows the general trend of papers, the other two categories remained constant over the years. However, in 2016, we \\nobserve a major increase of small scale for TBCG. \\n5.5. Application domain \\nThe tree map in Fig. 7 highlights the fact that TBCG is used in many different areas. Software engineering obtains more \\nthan half of the papers with 55% of the publications. We have grouped in this category other related areas like ontolo- \\ngies, information systems or software product lines. This is expected given that the goal of TBCG is to synthesize software \\napplications. For example, the work in [52] uses the Rational CASE tool to generate Java programs in order to implement an \\napproach that transforms UML state machine to behavioral code. The next category is embedded systems which obtains 13% \\nof papers. Embedded systems often require low level hardware code diÔ¨Écult to write. Some even consider code generation \\nto VHDL as a compilation rather than automatic programming. In this category, we found papers like [53] in which Velocity \\nis used to produce Verilog code to increase the speed of simulation. Web technology related application domains account for \\n8% of the papers. It consists of papers like [54] where the authors worked to enhance the development dynamic web sites. \\nNetworking obtains 4% of the papers, such as [55] where code is generated for a telephony service network. Compiler obtains \\n1% of the papers, such as [56] where a C code is generated and optimized for an Intel C compiler. It is interesting to note \\nthat several papers were applied in domains such as bio-medicine [57] , artiÔ¨Åcial intelligence [58] , and graphics [59] . \\n10 http://www.eclipse.org/epsilon/doc/egl/'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 11, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='54 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nWe combined application domains with a single paper into the other category. This regroups domains such as agronomy, \\neducation, and Ô¨Ånance. It is important to mention that the domain discussed in this category corresponds to the domain of \\napplication of TBCG employed, which differs from the publication venue. \\n5.6. Orientation \\nA quarter (24%) of the papers in the corpus are authored by a researcher from industry . The remaining 76% are written \\nonly by academics . This is a typical distribution since industrials tend to not publish their work. This result shows that TBCG \\nis used in industry as in [16] . Industry oriented papers have gradually increased since 2003 until they reached a peak in \\n2013. \\n6. Relations between characteristics \\nTo further characterize the trends observed in Section 5 , we identiÔ¨Åed signiÔ¨Åcant and interesting relations between the \\ndifferent facets of the classiÔ¨Åcation scheme, using SPSS. \\n6.1. Statistical correlations \\nA Shapiro‚ÄìWilk test of each category determined that the none of them are normally distributed. Therefore, we opted \\nfor the Spearman two-tailed test of non-parametric correlations with a signiÔ¨Åcance value of 0.05 to identify correlations \\nbetween the trends of each category. The only signiÔ¨Åcantly strong correlations we found statistically are between the two \\ninput types, and between MDE and input type. \\nWith no surprise, the correlation between run-time and design time input is the strongest among all, with a correlation \\ncoeÔ¨Écient of 0.944 and a p- value of less than 0.001. This concurs with the results found in Section 5.2 . An example is \\nwhen the design-time input is UML, the run-time input is always a UML diagram as in [57] . Such a strong relationship is \\nalso noticeable in [60] with programming languages and source code, as well as in [58] when a schema design is used for \\nstructured data. As a result, all run-time input categories are correlated to the same categories as for design-time input. We \\nwill therefore treat these two facets together as input type . \\nThere is a strong correlation of coeÔ¨Écient of 0.738 and a p-value of less than 0.001 between input type and MDE . As \\nexpected, more than 90% of the papers using general purpose and domain speciÔ¨Åc inputs are follow the MDE approach. \\n6.2. Other interesting relations \\nWe also found weak but statistically signiÔ¨Åcant correlations between the remaining facets. We discuss the result here. \\n6.2.1. Template style \\nFigure 8 shows the relationship between template style, design-time input, and output types. We found that for \\nthe predeÔ¨Åned templates, there are twice as many papers that use schema input than domain speciÔ¨Åc. However, for \\n168\\n72\\n79\\n26\\n62\\n13\\n24\\n18\\n13\\n3\\n3\\nOutput-based\\nPredefined\\nRule-based\\nGeneral \\npurpose\\nDomain \\nspecific\\nSchema\\nProg. \\nlanguage\\nSource \\ncode\\nStructured \\ndata\\nDesign-time input type\\nOutput type\\nFig. 8. Relation between template style (vertical) and input/output types (horizontal).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 12, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n55 \\nSource \\ncode\\nStructured \\ndata\\nGeneral \\npurpose\\nDomain \\nspecific\\nSchema\\nProg. \\nlanguage\\nFig. 9. Relation between output (vertical) and design-time input (horizontal) types showing the number of papers in each intersection. \\nTable 7 \\nDistribution of the tool facet. \\nNamed MDE \\nUnspeciÔ¨Åed \\nOther \\nNamed not MDE \\n41% \\n28% \\n23% \\n8% \\noutput-based, domain speciÔ¨Åc inputs are used slightly more often. We also notice that general purpose input is never used \\nwith rule-based templates. The output type follows the same general distribution regardless of the template style. \\nAll rule-based style approaches have included a sample application. Meanwhile, the proportion of small scale was twice \\nmore important for predeÔ¨Åned templates (51%) then for output-based (27%). \\nWe found that popular tools were used twice as often on output-based templates (58%) than on predeÔ¨Åned templates \\n(23%). Rule-based templates never employed a tool that satisÔ¨Åed our popularity threshold, but used other tools such as \\nStratego. \\nWe found that all papers using a rule-based style template do not follow an MDE approach. On the contrary, 70% of the \\noutput-based style papers and 56% of the predeÔ¨Åned ones follow an MDE approach. \\nFinally, we found that for each template style, the number of papers authored by an industry researcher Ô¨Çuctuated \\nbetween 22‚Äì30%. \\n6.2.2. Input type \\nThe bubble chart in Fig. 9 illustrates the tendencies between input and output types. It is clear that source code is \\nthe dominant generated artifact regardless of the input type. Source code is more often generated from general purpose \\nand domain speciÔ¨Åc inputs than from schema and programming languages. Also, the largest portion of structured data is \\ngenerated from a schema input. \\nMoving on to input type and application scale, we found that small scales are used 40% of the time when the input is a \\nprogramming language. The number of papers with no sample application is very low (5%) regardless of the template style. \\nFinally, 74% of papers using large scale applications use a domain speciÔ¨Åc input, which is slightly higher than those using a \\ngeneral purpose input with 71%. \\n6.2.3. Output type, application scale, and orientation \\nAs we compared output type to orientation, we found that industrials generate slightly more source code than academics: \\n89% vs. 80%. However, academics generate more structured data than industrials: 18% vs. 6% and 3% vs. 1%., respectively. We \\nfound that 65% of the papers without application are from the academy. \\n7. Template-based code generation tools \\nTable 7 shows that half of the papers used a popular TBCG tool ( named ). The other half used less popular tools (the other \\ncategory), did not mention any TBCG tool, or implemented the code generation directly for the purpose of the paper. \\n7.1. Popular tools in research \\nFigure 10 shows the distribution of popular tools used in at least 1% of the papers, i.e., Ô¨Åve papers. We see that only \\n6/14 popular tools follow MDE approaches. Acceleo and Xpand are the most popular with respectively 16% and 15% of the \\npapers using them. Their popularity is probably due to their simple syntax and ease of use [61] and the fact that they are \\nMDE tools [16] . They both have an OCL-like language for the dynamic part and rely on a metamodel speciÔ¨Åed in Ecore as \\ndesign-time input.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 13, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='56 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nFig. 10. Popular tools. \\nEGL also has a structure similar to the other model-based tools. It is natively integrated with languages from the Epsilon \\nfamily, thus relies on the Epsilon Object Language for its dynamic part. MOFScript is another popular model-based tool that \\nonly differs in syntax from the others. Xtend2 is the least used popular model-based tool. It is both an advanced form of \\nXpand and a simpliÔ¨Åed syntactical version of Java. \\nXSLT is the third most popular tool used. It is suitable for XML documents only. Some use it for models represented in \\ntheir XMI format, as it is the case in [62] . XSLT follows the template and Ô¨Åltering strategy. It matches each tag of the input \\ndocument and applies the corresponding template. \\nJET [63] and Velocity [53] are used as often as each other on top of being quite similar. The main difference is that JET \\nuses an underlying programming language (Java) for the dynamic part. In JET, templates are used to developers generate the \\nJava code speciÔ¨Åc for the synthesis of code to help developers implement the code generation. \\nStringTemplate [64] has its own template structure. It can be embedded into a Java code where strings to be output are \\ndeÔ¨Åned using templates. Note that all the tools mentioned above use an output-based template style. \\nThe most popular CASE tools for TBCG are Fujaba [65] , Rational [66] , and Rhapsody [67] . One of the features they offer is \\nto generate different target languages from individual UML elements. All CASE tools (even counting the other category) have \\nbeen used in a total of 39 papers, which puts them at par with Xpand. CASE tools are mostly popular for design activities; \\ncode generation is only one of their many features. CASE tools have a predeÔ¨Åned template style. \\nSimulink TLC is the only rule-based tool among the most popular ones. As a rule-based approach, it has a different \\nstructure compared to the above mentioned tools. Its main difference is that the developer writes the directives to be \\nfollowed by Simulink in order to render the Ô¨Ånal C code from S-functions. \\nWe notice that the most popular tools are evenly distributed between model-based tools (Acceleo, Xpand) and code- \\nbased tools (JET, XSLT). Surprisingly, XSLT, which has been around the longest, is less popular than Xpand. This is undoubt- \\nedly explained by the advantages that MDE has to offer [7,8] . \\n7.2. UnspeciÔ¨Åed and other tools \\nAs depicted in Table 7 , 28% of the papers did not specify the tool that was used, as in [68] where the authors introduce \\nthe concept of a meta-framework to resolve issues involved in extending the life of applications. Furthermore, 23% of the \\npapers used less popular tools, present in less than Ô¨Åve papers, such as T4 [44] and Cheetah [56] , a python powered template \\nmainly used for web developing. Like JET, Cheetah templates generate Python classes, while T4 is integrated with .NET \\ntechnology. Some CASE tools were also in this category, such as AndroMDA [69] . Other examples of less popular tools are \\nGroovy template [47] , Meta-Aspect-J [70] , and Jinja2 [71] . The fact that new or less popular tools are still abundantly used \\nsuggests that research in TBCG is still active with new tools being developed or evolved. \\n7.3. Trends of tools used \\nEach one of these tools had a different evolution over the years. UnspeciÔ¨Åed tools were prevalent before 2004 and then \\nkept a constant rate of usage until a drop since 2014. We notice a similar trend for CASE tools that were the most popular \\nin 2005 before decreasing until 2009. They only appear in at most three papers per year after 2010. The use of the most \\npopular tool, Xpand, gradually increased since 2005 to reach the peak in 2013 before decreasing. The other category main- \\ntained an increasing trend until 2014. Yet, a few other popular tools appeared later on. For example, EGL started appearing \\nin 2008 and had its peak in 2013. Acceleo appeared a year later and was the most popular TBCG tool in 2013‚Äì2014. Finally,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n57 \\nMOFScript had no more than a paper per year since 2005. StringTemplate and T4 were used scarcely since 2006 and 2009, \\nrespectively. \\n7.4. Characteristics of tools \\nWe have also analyzed each popular tool with respect to the characteristics presented in Section 5 . As mentioned earlier, \\nmost of the popular tools implement output-based template technique except the CASE tools which are designed following \\nthe predeÔ¨Åned style. \\nTools such as Acceleo, Xpand, EGL, MOFScript and 97% of the CASE tools papers are only used based on an MDE ap- \\nproach, given that they were created by this community. Nevertheless, there are tools that were never used with MDE \\nprinciples, like JET and Cheetah. Such tools can handle a program code or a schema as metamodel but have no internal \\nsupport for modeling languages. Moreover, the programmer has to write his own stream reader to parse the input, but they \\nallow for a broader range of artifacts as inputs that do not have to be modeled explicitly. A few code-based tools provide \\ninternal support for model-based approaches. For instance, Velocity, XSLT, and StringTemplate can handle both UML and pro- \\ngrammed metamodel as design-time input. T4 can be integrated with MDE artifacts (e.g., generate code based on a domain- \\nspeciÔ¨Åc language in Visual studio). However, all four papers in the corpus that implemented TBCG in T4 did not use an MDE \\napproach. \\nA surprising result we found is that EGL is the only MDE tool that has its papers mostly published in MDE venues like \\nSosym , Models , and Ecmfa . All the other tools are mostly published in other venues like Icssa , whereas software engineering \\nvenues, like Ase or Icse , and MDE venues account for 26‚Äì33% of the papers for each of the rest of the MDE tools. \\nCASE tools, MOFScript, Velocity, and Simulink TLC mostly generate program code. The latter is always used in the domain \\nof embedded systems. Papers that use StringTemplate do not include any validation process, so is Velocity in 93% of the \\npapers using it. XSLT has been only used to generate structured data as anticipated. \\nOther tools are the most used TBCG in the industry. This is because the tool is often internal to the company [72] . Among \\nthe most popular tools, Xpand is the most in the industry. \\n7.5. Application scale \\nBetween application scale and tools, we found that 74% of the papers that make use of a popular tool used large scale \\napplication to illustrate their approach. Also, 62% of the papers using unpopular tools 11 use large scale applications. Small \\nscale is likely to be used in unpopular tools rather than popular tools. \\n7.6. Tool support \\nIn total, we found 82 different tools named in the corpus. Among them 54% are no longer supported (i.e., no release, \\nupdate or commit since the past two years). Interestingly, 55% of the tools have been developed by the industry. 70% of \\nthese industry tools are still supported, in contrast with 51% for the academic tools. As one would expect, the tendency \\nshows that tools that are frequently mentioned in research papers are still supported and are developed by industry. \\n8. MDE and template-based code generation \\nOverall, 64% of the publications followed MDE techniques and principles. For example in [73] , the authors propose a sim- \\nulation environment with an architecture that aims at integrating tools for modeling, simulation, analysis, and collaboration. \\nAs expected, most of the publications using output-based and predeÔ¨Åned techniques are classiÔ¨Åed as model-based papers. \\nThe remaining 36% of the publications did not use MDE. This includes all papers that use a rule-based template style as \\nreported in Section 6 . For example, the authors in [40] developed a system that handles the implementation of dependable \\napplications and offers a better certiÔ¨Åcation process for the fault-tolerance mechanisms. \\nAs Fig. 11 shows, the evolution of the MDE category reveals that MDE-based approach started over passing non MDE- \\nbased techniques in 2005, except for 2006. It increased to reach a peak in 2013 and then started decreasing as the general \\ntrend of the corpus. Overall, MDE-based techniques for TBCG have been dominating other techniques in the past 12 years. \\nWe also analyzed the classiÔ¨Åcation of only MDE papers with respect to the characteristics presented in Section 3 . We \\nonly focus here on facets with different results compared to the general trend of papers. We found that only half of the \\ntotal number of papers using unspeciÔ¨Åed and other tools are MDE-based papers. We only found one paper that uses a \\nprogramming language as design-time input with MDE [74] . This analysis also shows that the year 2005 clearly marked the \\nshift from schema to domain-speciÔ¨Åc design-time inputs, as witnessed in Section 5.2 . Thus after general purpose, which \\nobtains 69% of the publications, domain speciÔ¨Åc accounts for a better score of 26%, while schema obtains only 4%. With \\nrespect to the run-time category, the use of domain-speciÔ¨Åc models increased to reach a peak in 2013. As expected, no \\nprogram code is used for MDE papers, because MDE typically does not consider them as models, unless a metamodel of the \\nprogramming language is used. \\n11 Refers to the union of other and unspeciÔ¨Åed categories of the tool facet.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 15, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='58 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nUsing MDE\\nNot using MDE\\n# of papers\\nFig. 11. Evolution of the MDE facet. \\nInterestingly, MDE venues are only the second most popular after other venues for MDE approaches. Finally, MDE journal \\npapers maintained a linear increase over the years, while MDE conference papers had a heterogeneous evolution similar to \\nthe general trend of papers. \\n9. Discussion \\n9.1. RQ1: What are the trends in TBCG? \\nThe statistical results from this signiÔ¨Åcantly large sample of papers clearly suggest that TBCG has received suÔ¨Écient \\nattention from the research community. The community has maintained a production rate in-line with the last 11 years \\naverage, especially with a constant rate of appearance in journal articles. The only exceptions were a signiÔ¨Åcant boost in \\n2013 and a dip in 2015. The lack of retention of papers appearing in non MDE may indicate that TBCG is now applied \\nin development projects rather than being a critical research problem to solve. Also, conference papers as well as venues \\noutside MDE and software engineering had a signiÔ¨Åcant impact on the evolution of TBCG. Given that TBCG seems to have \\nreached a steady publication rate since 2005, we can expect contributions from the research community to continue in that \\ntrend. \\n9.2. RQ2: What are the characteristics of TBCG approaches? \\nOur classiÔ¨Åcation scheme constitutes the main source to answer this question. The results clearly indicate the preferences \\nthe research community has regarding TBCG. Output-based templates have always been the most popular style from the \\nbeginning. Nevertheless, there have been some attempts to propose other template styles, like the rule-based style, but they \\ndid not catch on. Because of its simplicity to use, the predeÔ¨Åned style is probably still popular in practice, but it is less \\nmentioned in research papers. TBCG has been used to synthesize a variety of application code or documents. As expected, \\nthe study shows that modeling language inputs have prevailed over any other type. SpeciÔ¨Åcally for MDE approaches to TBCG, \\nthe input to transform is moving from general purpose to domain-speciÔ¨Åc models. Academic researchers have contributed \\nmost, as expected with a literature review, but we found that industry is actively and continuously using TBCG as well. The \\nstudy also shows that the community is moving from large-scale applications to smaller-sized examples in research papers. \\nThis concurs with the level of maturity of this synthesis approach. The study conÔ¨Årms that the community uses TBCG to \\ngenerate mainly source code. This trend is set to continue since the automation of computerized tasks is continuing to gain \\nground in all Ô¨Åelds. Finally, TBCG has been implemented in many domains, software engineering and embedded systems \\nbeing the most popular, but also unexpectedly in unrelated domains, such as bio-medicine and Ô¨Ånance. \\n9.3. RQ3: To what extent are TBCG tools being used in research? \\nIn this study, we discovered a total of 82 different tools for TBCG that are mentioned in research papers. Many studies \\nimplemented code generation with a custom-made tool that was never or seldom reused. This indicates that the develop- \\nment of new tools is still very active. MDE tools are the most popular. Since the research community has favored output- \\nbased template style, this has particularly inÔ¨Çuenced the tools implementation. This template style allows for more Ô¨Åne- \\ngrained customization of the synthesis logic which seems to be what users have favored. This particular aspect is also \\ninÔ¨Çuencing the expansion of TBCG into industry. Most popular tool are actively supported by industry. Well-known tools \\nlike Acceleo, Xpand and Velocity are moving from being simple research material to effective development resources in in- \\ndustry. Finally, the study There are many TBCG tools that are popular in industry that fall under the ‚ÄúOther tools‚Äù category \\nbecause they are rarely reported in the scientiÔ¨Åc literature (under 1% of the papers in our corpus). Since this study is a lit- \\nerature review, the presence of a tool in this study is biased towards the what is published, and may not reÔ¨Çect the reality \\nin industry. This is a common threat of SMS.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n59 \\n9.4. RQ4: What is the place of MDE in TBCG? \\nAll this analysis clearly concludes that the advent of MDE has been driving TBCG research. In fact, MDE has led to \\nincrease the average number of publications by a factor of four. There are many advantages to code generation, such as \\nreduced development effort, easier to write and understand domain/application concepts and less error-prone [8] . These are, \\nin fact, the pillar principles of MDE and domain-speciÔ¨Åc modeling [2] . Thus, it is not surprising to see that many, though \\nnot exclusively, code generation tools came out from the MDE community. As TBCG became a commonplace in general, \\nthe research in this area is now mostly conducted by the MDE community. Furthermore, MDE has brought very popular \\ntools that have encountered a great success, and they are also contributing to the expansion of TBCG across industry. It \\nis important to mention that the MDE community publishes in speciÔ¨Åc venues like Models , Sosym , or Ecmfa unlike other \\nresearch communities where the venues are very diversiÔ¨Åed. This resulted in three MDE venues at the top of the ranking. \\n9.5. IdentiÔ¨Åed challenges \\nAfter thoroughly analyzing each paper in the corpus, we noted several problems that the research community has ne- \\nglected in TBCG. First, we found 66% of the papers did not provide any assessment of the code generation process or the \\ngenerated output. We only found one paper with a formal veriÔ¨Åcation of the generated code using non-functional require- \\nment analysis [75] . Furthermore, the TBCG can be veriÔ¨Åed through benchmarks as in [55] . Second, we found no paper that \\ninvestigates eÔ¨Éciency of code generation. Researchers may be inspired from other related communities, such as compiler \\noptimization [60] . Third, designing templates requires skillful engineering. Good practices, design patterns and other forms \\nof good reusable idioms would be of great value to developers [76] . \\n9.6. Threats to validity \\nThe results presented in this survey have depended on many factors that could potentially threaten its validity. \\n9.6.1. Construction validity \\nIn a strict sense, our Ô¨Åndings are valid only for our sample that we collected from 20 0 0‚Äì2016. This leads to determine \\nwhether the primary studies used in our survey are a good representation of the whole population. From Fig. 3 , we can \\nobserve that our sample can be attributed as a representative sample of the whole population. In particular, the average \\nnumber of identiÔ¨Åed primary studies per year is 28 with standard deviation 15.76. A more systematic selection process \\nwould have been diÔ¨Écult to be exhaustive about TBCG. We chose to obtain the best possible coverage at the cost of du- \\nplications. Nevertheless, the size of the corpus we classiÔ¨Åed is about ten times larger than other systematic reviews related \\nto code generation (see Section 2.4 ). We are, therefore, conÔ¨Ådent that this sample is a representative subset of all relevant \\npublications on TBCG. \\nAnother potential limitation is the query formulation for the keyword search. It is diÔ¨Écult to encode a query that is \\nrestrictive enough to discard unrelated publications, but at the same time retrieves all the relevant ones. In order to obtain \\na satisfactory balance, we included synonyms and captured possible declinations. In this study, we are only interested in \\ncode generation. Therefore we discarded articles where TBCG was used for reporting or mass mailing, for example. We \\nbelieve that our sample is large enough that additional papers will not signiÔ¨Åcantly affect the general trends and results. \\nWe are fully aware that, since TBCG is widely used in practice, industries have their own tools and many have not been \\npublished in academic venues. Our goal was not to be exhaustive, but to get a representative sample. \\n9.6.2. Internal validity \\nA potential limitation is related to data extraction. It is diÔ¨Écult to extract data from relevant publications, especially \\nwhen the quality of the paper is low, when code generation is not the primary contribution of the paper, or when critical \\ninformation for the classiÔ¨Åcation is not directly available in the paper. For example in [77] , the authors only mention the \\nname of the tool used to generate the code. In order to mitigate this threat, we had to resort to searching for additional \\ninformation about the tool: reading other publications that use the tool, traversing the website of the tool, installing the \\ntool, or discussing with the tools experts. \\nAnother possible threat is the screening of papers based on inclusion and exclusion criteria that we deÔ¨Åned before the \\nstudy was conducted. During this process, we examined only the title, the abstract. Therefore, there is a probability that we \\nexcluded relevant publications such as [55] , that do not include any TBCG terms. In order to mitigate this threat, whenever \\nwe were unsure whether a publication should be excluded or not we conservatively opted to include it. However, during \\nclassiÔ¨Åcation when reading the whole content of the paper, we may still have excluded it. \\n9.6.3. External validity \\nThe results we obtained are based on TBCG only. Even though our classiÔ¨Åcation scheme includes facets like orientation, \\napplication domain, that are not related to the area, we followed a topic based classiÔ¨Åcation. The core characteristics of our \\nstudy are strictly related to this particular code synthesis technique. We have deÔ¨Åned characteristics like template style and \\nthe two levels of inputs that we believe are exclusive to TBCG. Therefore, the results cannot be generalized to other code \\ngeneration techniques mentioned in Section 2.2 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='60 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n9.6.4. Conclusion validity \\nOur study is based on a large number of primary studies. This helps us mitigate the potential threats related to the \\nconclusions of our study. A missing paper or a wrongly classiÔ¨Åed paper would have a very low impact on the statistics \\ncompared to a smaller number of primary studies. In addition, as a senior reviewer did a sanity check on the rejected \\npapers, we are conÔ¨Ådent that we did not miss a signiÔ¨Åcant number of papers. Hence, the chances for wrong conclusions are \\nsmall. Replication of this study can be achieved as we provided all the details of our research method in Section 3 . Also, our \\nstudy follows the methodology described in [20] . \\n10. Conclusion \\nThis paper reports the results of a large survey we conducted on the topic of TBCG, which has been missing in the \\nliterature. The objectives of this study are to better understand the characteristics of TBCG techniques and associated tools, \\nidentify research trends, and assess the importance of the role that MDE plays. The analysis of this corpus is organized into \\nfacets of a novel classiÔ¨Åcation scheme, which is of great value to modeling and software engineering researchers who are \\ninterested in painting an overview of the literature on TBCG. \\nOur study shows that the community has been diversely using TBCG over the past 16 years, and that research and \\ndevelopment is still very active. TBCG has greatly beneÔ¨Åted from MDE in 2005 and 2013 which mark the two peaks of \\nthe evolution of this area, tripling the average number of publications. In addition, TBCG has favored a template style that \\nis output-based and modeling languages as input. It has been applied in a variety of domains. The community has been \\nfavoring the use of custom tools for code generation over popular ones. Most research using TBCG follows an MDE approach. \\nFurthermore, both MDE and non-MDE tools are becoming effective development resources in industry. \\nThe study also revealed that, although the research in TBCG is mature enough, there are many open issues that can \\nbe addressed in the future, upstream and downstream the generation itself. Upstream, the deÔ¨Ånition of templates is not \\na trivial task. Supporting the developers in such a deÔ¨Ånition is a must. Downstream, methods and techniques need to be \\ndeÔ¨Åned to assess the correctness and quality of the generated code. \\nWe believe this survey will be of beneÔ¨Åt to someone familiar with code generation by knowing how their favorite tool \\nranks in popularity within the research community, the relevance and importance of the use of templates, and in which \\ncontext TBCG has been applied (application domain). The relations across categories in Section VI show non-intuitive results \\nas well. The paper also promotes MDE in Ô¨Åelds that have not been traditionally exposed to it. \\nSupplementary material \\nSupplementary material associated with this article can be found, in the online version, at 10.1016/j.cl.2017.11.003 . \\nReferences \\n[1] Rich C , Waters RC . Automatic programming: myths and prospects. Computer 1988;21(8):40‚Äì51 . \\n[2] Kelly S , Tolvanen J-P . Domain-speciÔ¨Åc modeling: enabling full code generation. John Wiley & Sons; 2008 . \\n[3] Bonta E , Bernardo M . Padl2java: a Java code generator for process algebraic architectural descriptions. In: Proceedings of the european conference on \\nsoftware architecture. IEEE; 2009. p. 161‚Äì70 . \\n[4] Tatsubori M , Chiba S , Killijian M-O , Itano K . OpenJava: a class-based macro system for Java. In: ReÔ¨Çection and software engineering. In: LNCS, 1826. \\nSpringer; 20 0 0. p. 117‚Äì33 . \\n[5] Lohmann D , Blaschke G , Spinczyk O . Generic advice: on the combination of AOP with generative programming in AspectC++. In: Proceedings of \\ninternational conference on generative programming and component engineering. In: LNCS, 3286. Berlin Heidelberg: Springer; 2004. p. 55‚Äì74 . \\n[6] Kleppe AG , Warmer J , Bast W . MDA explained. The model driven architecture: practice and promise. Addison-Wesley; 2003 . \\n[7] J√∂rges S . Construction and evolution of code generators 7747. Ch 2 The state of the art in code generation. Berlin Heidelberg: Springer; 2013. p. 11‚Äì38 . \\n[8] Balzer R . A 15 year perspective on automatic programming. Trans Softw Eng 1985;11(11):1257‚Äì68 . \\n[9] Floch A , Yuki T , Guy C , Derrien S , Combemale B , Rajopadhye S , et al. Model-driven engineering and optimizing compilers: a bridge too far?. In: Model \\nDriven Engineering Languages and Systems. In: LNCS, 6981. Springer Berlin Heidelberg; 2011. p. 608‚Äì22 . \\n[10] Stahl T , Voelter M , Czarnecki K . Model-driven software development ‚Äì technology, engineering, management. John Wiley & Sons; 2006 . \\n[11] L√∫cio L , Amrani M , Dingel J , Lambers L , Salay R , Selim GM , et al. Model transformation intents and their properties. Softw Syst Model \\n2014;15(3):685‚Äì705 . \\n[12] Czarnecki K , Helsen S . Feature-based survey of model transformation approaches. IBM Syst J 2006;45(3):621‚Äì45 . \\n[13] Gamma E , Helm R , Johnson R , Vlissides J . Design patterns: elements of reusable object-oriented software. Addison Wesley Professional; 1994 . \\n[14] Beckmann O , Houghton A , Mellor M , Kelly PH . Runtime code generation in C++ as a foundation for domain-speciÔ¨Åc optimisation. In: Domain-SpeciÔ¨Åc \\nProgram Generation. In: LNCS, 3016. Berlin Heidelberg: Springer; 2004. p. 291‚Äì306 . \\n[15] C√≥rdoba I , de Lara J . ANN: a domain-speciÔ¨Åc language for the effective design and validation of Java annotations. Comput Lang Syst Struct \\n2016;45:164‚Äì90 . \\n[16] Jugel U , Preu√üner A . A case study on API generation. In: System analysis and modeling: about models. In: LNCS, 6598. Springer; 2011. p. 156‚Äì72 . \\n[17] Kitchenham BA , Dyba T , Jorgensen M . Evidence-based software engineering. In: Proceedings of international conference on software engineering. \\nWashington, DC, USA: IEEE Computer Society; 2004. p. 273‚Äì81 . \\n[18] Kitchenham BA , Budgen D , Brereton OP . Using mapping studies as the basis for further research - a participant-observer case study. Inf Softw Technol \\n2011;53(6):638‚Äì51 . \\n[19] Brereton P , Kitchenham BA , Budgen D , Turner M , Khalil M . Lessons from applying the systematic literature review process within the software engi- \\nneering domain. J Syst Softw 2007;80(4):571‚Äì83 . \\n[20] Petersen K , Feldt R , Mujtaba S , Mattsson M . Systematic mapping studies in software engineering. In: Proceedings of the 12th international conference \\non evaluation and assessment in software engineering, EASE‚Äô08, 17. British Computer Society; 2008. p. 68‚Äì77 . \\n[21] Mehmood A , Jawawi DN . Aspect-oriented model-driven code generation: a systematic mapping study. Inf Softw Technol 2013;55(2):395‚Äì411 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n61 \\n[22] Gurunule D , Nashipudimath M . A review: analysis of aspect orientation and model driven engineering for code generation. Procedia Comput Sci \\n2015;45:852‚Äì61 . \\n[23] Dom√≠guez E , P√©rez B , Rubio AL , Zapata MA . A systematic review of code generation proposals from state machine speciÔ¨Åcations. Inf Softw Technol \\n2012;54(10):1045‚Äì66 . \\n[24] Batot E , Sahraoui H , Syriani E , Molins P , Sboui W . Systematic mapping study of model transformations for concrete problems. In: Model-driven \\nengineering and software development. IEEE; 2016. p. 176‚Äì83 . \\n[25] Rose LM , Matragkas N , Kolovos DS , Paige RF . A feature model for model-to-text transformation languages. In: Modeling in software engineering. IEEE; \\n2012. p. 57‚Äì63 . \\n[26] Kosar T , Bohra S , Mernik M . Domain-speciÔ¨Åc languages: a systematic mapping study. Inf Softw Technol 2016;71:77‚Äì91 . \\n[27] M√©ndez-Acu√±a D , Galindo JA , Degueule T , Combemale B , Baudry B . Leveraging software product lines engineering in the development of external \\nDSLs: a systematic literature review. Comput Lang Syst Struct 2016;46:206‚Äì35 . \\n[28] Mat√∫s Sul√≠r JP . Labeling source code with metadata: a survey and taxonomy. In: Proceedings of federated conference on computer science and infor- \\nmation systems. In: Workshop on Advances in Programming Languages (WAPL‚Äô17), CFP1785N-ART. IEEE; 2017. p. 721‚Äì9 . \\n[29] Buchmann T , Schw√§gerl F . Using meta-code generation to realize higher-order model transformations. In: Proceedings of international conference on \\nsoftware technologies; 2013. p. 536‚Äì41 . \\n[30] Seriai A , Benomar O , Cerat B , Sahraoui H . Validation of software visualization tools: a systematic mapping study. In: Proceedings of IEEE working \\nconference on software visualization. VISSOFT; 2014. p. 60‚Äì9 . \\n[31] Liu Q . C++ techniques for high performance Ô¨Ånancial modelling. WIT Trans Model Simul 2006;43:1‚Äì8 . \\n[32] Fang M , Ying J , Wu M . A template engineering based framework for automated software development. In: Proceeding of the 10th international con- \\nference on computer supported cooperative work in design. IEEE; 2006. p. 1‚Äì6 . \\n[33] Singh A , Schaeffer J , Green M . A template-based approach to the generation of distributed applications using a network of workstations. IEEE Trans \\nParallel Distrib Syst 1991;2(1):52‚Äì67 . \\n[34] O‚ÄôHalloran C . Automated veriÔ¨Åcation of code automatically generated from simulink ¬Æ. Autom Softw Eng 2013;20(2):237‚Äì64 . \\n[35] Dahman W , Grabowski J . UML-based speciÔ¨Åcation and generation of executable web services. In: System analysis and modeling. In: LNCS, 6598. \\nSpringer; 2011. p. 91‚Äì107 . \\n[36] Gessenharter D . Mapping the UML2 semantics of associations to a Java code generation model. In: Proceedings of international conference on model \\ndriven engineering languages and systems. In: LNCS, 5301. Springer; 2008. p. 813‚Äì27 . \\n[37] Valderas P , Pelechano V , Pastor O . Towards an end-user development approach for web engineering methods. In: Proceedings of international confer- \\nence on advanced information systems engineering, 4001. Springer; 2006. p. 528‚Äì43 . \\n[38] Hemel Z , Kats LC , Groenewegen DM , Visser E . Code generation by model transformation: a case study in transformation modularity. Softw Syst Model \\n2010;9(3):375‚Äì402 . \\n[39] Brun M , Delatour J , Trinquet Y . Code generation from AADL to a real-time operating system: an experimentation feedback on the use of model \\ntransformation. In: Engineering of complex computer systems. IEEE; 2008. p. 257‚Äì62 . \\n[40] Buckl C , Knoll A , Schrott G . Development of dependable real-time systems with Zerberus. In: Proceedings of the 11th IEEE paciÔ¨Åc rim international \\nsymposium on dependable computing; 2005. p. 404‚Äì8 . \\n[41] Li J , Xiao H , Yi D . Designing universal template for database application system based on abstract factory. In: Computer science and information \\nprocessing. IEEE; 2012. p. 1167‚Äì70 . \\n[42] Gopinath VS , Sprinkle J , Lysecky R . Modeling of data adaptable reconÔ¨Ågurable embedded systems. In: International conference and workshops on \\nengineering of computer based systems. IEEE; 2011. p. 276‚Äì83 . \\n[43] Buckl C , Regensburger M , Knoll A , Schrott G . Models for automatic generation of safety-critical real-time systems. In: Availability, reliability and \\nsecurity. IEEE; 2007. p. 580‚Äì7 . \\n[44] Fischer T , Kollner C , Hardle M , Muller Glaser KD . Product line development for modular FPGA-based embedded systems. In: Proceedings of symposium \\non rapid system prototyping. IEEE; 2014. p. 9‚Äì15 . \\n[45] Chen K , Chang Y-C , Wang D-W . Aspect-oriented design and implementation of adaptable access control for electronic medical records. Int J Med \\nInform 2010;79(3):181‚Äì203 . \\n[46] Brox M , S√°nchez-Solano S , del Toro E , Brox P , Moreno-Velo FJ . CAD tools for hardware implementation of embedded fuzzy systems on FPGAs. IEEE \\nTrans Ind Inform 2013;9(3):1635‚Äì44 . \\n[47] Fraternali P , Tisi M . A higher order generative framework for weaving traceability links into a code generator for web application testing. In: Proceed- \\nings of international conference on web engineering. In: LNCS, 5648. Springer; 2009. p. 340‚Äì54 . \\n[48] Vok√°Àác M , Glattetre JM . Using a domain-speciÔ¨Åc language and custom tools to model a multi-tier service-oriented application experiences and chal- \\nlenges. In: Model Driven Engineering Languages and Systems, 3713. Springer; 2005. p. 492‚Äì506 . \\n[49] Kokar M , Baclawski K , Gao H . Category theory-based synthesis of a higher-level fusion algorithm: an example. In: Proceedings of international confer- \\nence on information fusion; 2006. p. 1‚Äì8 . \\n[50] Hoisl B , Sobernig S , Strembeck M . Higher-order rewriting of model-to-text templates for integrating domain-speciÔ¨Åc modeling languages. In: Model‚Äì\\nDriven Engineering and Software Development. SCITEPRESS; 2013. p. 49‚Äì61 . \\n[51] Ecker W , Velten M , Zafari L , Goyal A . The metamodeling approach to system level synthesis. In: Proceedings of design, automation & test in Europe \\nconference & exhibition. IEEE; 2014. p. 1‚Äì2 . \\n[52] Behrens T , Richards S . Statelator-behavioral code generation as an instance of a model transformation. In: Proceedings of international conference on \\nadvanced information systems engineering. In: LNCS, 1789. Springer; 20 0 0. p. 401‚Äì16 . \\n[53] Durand SH , Bonato V . A tool to support Bluespec SystemVerilog coding based on UML diagrams. In: Proceedings of annual conference on IEEE indus- \\ntrial electronics society. IEEE; 2012. p. 4670‚Äì5 . \\n[54] Schattkowsky T , Lohmann M . Rapid development of modular dynamic web sites using UML. In: Proceedings of international conference on the uniÔ¨Åed \\nmodeling language. In: LNCS, 2460. Springer; 2002. p. 336‚Äì50 . \\n[55] Buezas N , Guerra E , de Lara J , Mart√≠n J , Monforte M , Mori F , et al. Umbra designer: graphical modelling for telephony services. In: Proceedings of \\neuropean conference on modelling foundations and applications. In: LNCS, 7949. Berlin Heidelberg: Springer; 2013. p. 179‚Äì91 . \\n[56] Manley R , Gregg D . A program generator for intel AES-NI instructions. In: Proceedings of international conference on cryptology. In: LNCS, 6498. \\nSpringer; 2010. p. 311‚Äì27 . \\n[57] Phillips J , Chilukuri R , Fragoso G , Warzel D , Covitz PA . The caCORE software development kit: streamlining construction of interoperable biomedical \\ninformation services. BMC Med Inform Decis Mak 2006;6(2):1‚Äì16 . \\n[58] Fu J , Bastani FB , Yen I-L . Automated AI planning and code pattern based code synthesis. In: Proceedings of international conference on tools with \\nartiÔ¨Åcial intelligence. IEEE; 2006. p. 540‚Äì6 . \\n[59] Possatto MA , Lucr√©dio D . Automatically propagating changes from reference implementations to code generation templates. Inf Softw Technol \\n2015;67:65‚Äì78 . \\n[60] Ghodrat MA , Givargis T , Nicolau A . Control Ô¨Çow optimization in loops using interval analysis. In: Proceedings of international conference on compilers, \\narchitectures and synthesis for embedded systems. ACM; 2008. p. 157‚Äì66 . \\n[61] Guduvan A-R , Waeselynck H , Wiels V , Durrieu G , Fusero Y , Schieber M . A meta-model for tests of avionics embedded systems. In: Proceedings of \\ninternational conference on model-driven engineering and software development; 2013. p. 5‚Äì13 . \\n[62] Adamko A . Modeling data-oriented web applications using UML. In: Proceedings of the international conference on computer as a tool, EUROCON \\n2005, 1. IEEE; 2005. p. 752‚Äì5 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 19, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='62 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n[63] K√∂vi A , Varr√≥ D . An eclipse-based framework for AIS service conÔ¨Ågurations. In: Proceedings of the 4th international symposium on service availability, \\nISAS. In: LNCS, 4526. Springer; 2007. p. 110‚Äì26 . \\n[64] Anjorin A , Saller K , Rose S , Sch√ºrr A . A framework for bidirectional model-to-platform transformations. In: Proceedings of the 5th international con- \\nference on software language engineering, SLE 2012. In: LNCS, 7745. Berlin Heidelberg: Springer; 2013. p. 124‚Äì43 . \\n[65] Burmester S , Giese H , Sch√§fer W . Model-driven architecture for hard real-time systems: from platform independent models to code. In: Proceedings \\nof European conference on model driven architecture-foundations and applications. In: LNCS, 3748. Berlin Heidelberg: Springer; 2005. p. 25‚Äì40 . \\n[66] Brown AW , Conallen J , Tropeano D . Introduction: models, modeling, and model-driven architecture (MDA). In: Proceedings of international conference \\non model-driven software development. Berlin Heidelberg: Springer; 2005. p. 1‚Äì16 . \\n[67] Basu AS , Lajolo M , Prevostini M . A methodology for bridging the gap between UML and codesign. In: UML for SOC design. US: Springer; 2005. \\np. 119‚Äì46 . \\n[68] Furusawa T . Attempting to increase longevity of applications based on new SaaS/cloud technology. Fujitsu Sci Tech J 2010;46:223‚Äì8 . \\n[69] Muller P-A , Studer P , Fondement F , B√©zivin J . Platform independent web application modeling and development with Netsilon. Softw Syst Model \\n2005;4(4):424‚Äì42 . \\n[70] Antkiewicz M , Czarnecki K . Framework-speciÔ¨Åc modeling languages with round-trip engineering. In: Model driven engineering languages and systems. \\nIn: LNCS, 4199. Berlin Heidelberg: Springer; 2006. p. 692‚Äì706 . \\n[71] Hinkel G , Denninger O , Krach S , Groenda H . Experiences with model-driven engineering in neurorobotics. In: Proceedings of the 12th european con- \\nference on modelling foundations and applications, ECMFA 2016. Cham: Springer International Publishing; 2016. p. 217‚Äì28 . \\n[72] Kulkarni V , Barat S , Ramteerthkar U . Early experience with agile methodology in a model-driven approach. In: Model driven engineering languages \\nand systems. In: LNCS, 6981. Springer; 2011. p. 578‚Äì90 . \\n[73] Touraille L , Traor√© MK , Hill DR . A model-driven software environment for modeling, simulation and analysis of complex systems. In: Proceedings of \\nsymposium on theory of modeling & simulation. SCSC; 2011. p. 229‚Äì37 . \\n[74] Fertalj K , Kalpic D , Mornar V . Source code generator based on a proprietary speciÔ¨Åcation language. In: Proceedings of Hawaii international conference \\non system sciences, 9. IEEE; 2002. p. 3696‚Äì704 . \\n[75] Yen I-L, Goluguri J, Bastani F, Khan L, Linn J. A component-based approach for embedded software development. In: International symposium on \\nobject-oriented real-time distributed computing. ISORC 2002. IEEE Computer Society; 2002. p. 402‚Äì10. doi: 10.1109/ISORC.2002.1003805 . \\n[76] Luhunu L , Syriani E . Comparison of the expressiveness and performance of template-based code generation tools. In: Software Language Engineering. \\nACM; 2017 . \\n[77] Ma M , Meissner M , Hedrich L . A case study: automatic topology synthesis for analog circuit from an ASDEX speciÔ¨Åcation. In: Synthesis, modeling, \\nanalysis and simulation methods and applications to circuit design. IEEE; 2012. p. 9‚Äì12 .')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00251369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split text into chunks\n",
    "#sliding window chunking\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\",\" \",\"\"]\n",
    "    )\n",
    "    split_docs=text_splitter.split_documents(documents)\n",
    "    print(f\"split {len(documents)} document pages into {len(split_docs)} chunks\")\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df699adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 120 document pages into 644 chunks\n"
     ]
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc903112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 0, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1\\nA Survey on Large Language Models for Code Generation\\nJUYONG JIANG‚àó, The Hong Kong University of Science and Technology (Guangzhou), China\\nFAN WANG‚àó, The Hong Kong University of Science and Technology (Guangzhou), China\\nJIASI SHEN‚Ä†, The Hong Kong University of Science and Technology, China\\nSUNGJU KIM‚Ä†, NAVER Cloud, South Korea\\nSUNGHUN KIM‚Ä†, The Hong Kong University of Science and Technology (Guangzhou), China\\nLarge Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks,\\nknown as Code LLMs, particularly in code generation that generates source code with LLM from natural\\nlanguage descriptions. This burgeoning field has captured significant interest from both academic researchers\\nand industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite\\nthe active exploration of LLMs for a variety of code tasks, either from the perspective of natural language'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 0, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language\\nprocessing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive\\nand up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge\\nthis gap by providing a systematic literature review that serves as a valuable reference for researchers\\ninvestigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize\\nand discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest\\nadvances, performance evaluation, ethical implications, environmental impact, and real-world applications.\\nIn addition, we present a historical overview of the evolution of LLMs for code generation and offer an\\nempirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 0, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels\\nof difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities\\nfor code generation. We identify critical challenges and promising opportunities regarding the gap between\\nacademia and practical development. Furthermore, we have established a dedicated resource GitHub page\\n(https://github.com/juyongjiang/CodeLLMSurvey) to continuously document and disseminate the most recent\\nadvances in the field.\\nCCS Concepts: ‚Ä¢ General and reference ‚ÜíSurveys and overviews; ‚Ä¢ Software and its engineering ‚Üí\\nSoftware development techniques; ‚Ä¢ Computing methodologies ‚ÜíArtificial intelligence.\\nAdditional Key Words and Phrases: Large Language Models, Code Large Language Models, Code Generation\\nACM Reference Format:\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2018. A Survey on Large Language Models'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 0, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ACM Reference Format:\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2018. A Survey on Large Language Models\\nfor Code Generation. J. ACM 37, 4, Article 1 (August 2018), 70 pages. https://doi.org/XXXXXXX.XXXXXXX\\n‚àóEqually major contributors.\\n‚Ä†Corresponding authors.\\nAuthors‚Äô addresses: Juyong Jiang, jjiang472@connect.hkust-gz.edu.cn, The Hong Kong University of Science and Technology\\n(Guangzhou), Guangzhou, China; Fan Wang, fwang380@connect.hkust-gz.edu.cn, The Hong Kong University of Science\\nand Technology (Guangzhou), Guangzhou, China; Jiasi Shen, sjs@cse.ust.hk, The Hong Kong University of Science and\\nTechnology, Hong Kong, China; Sungju Kim, sungju.kim@navercorp.com, NAVER Cloud, Seoul, South Korea; Sunghun\\nKim, hunkim@cse.ust.hk, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 0, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\\n¬© 2018 Association for Computing Machinery.\\n0004-5411/2018/8-ART1 $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.\\narXiv:2406.00515v2  [cs.CL]  10 Nov 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 1, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:2\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n1\\nINTRODUCTION\\nThe advent of Large Language Models (LLMs) such as ChatGPT1 [196] has profoundly transformed\\nthe landscape of automated code-related tasks [48], including code completion [87, 171, 270, 282],\\ncode translation [52, 135, 245], and code repair [75, 126, 195, 204, 291, 310]. A particularly intriguing\\napplication of LLMs is code generation, a task that involves producing source code from natural\\nlanguage descriptions. Despite varying definitions across studies [51, 221, 238, 269], for the main\\nscope of this survey, we focus on the code generation task and adopt a consistent definition of code\\ngeneration as the natural-language-to-code (NL2Code) task [16, 17, 307]. To enhance clarity, the\\ndifferentiation between code generation and other code-related tasks, along with a more nuanced\\ndefinition, is summarized in Table 1. This area has garnered substantial interest from both academia'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 1, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='differentiation between code generation and other code-related tasks, along with a more nuanced\\ndefinition, is summarized in Table 1. This area has garnered substantial interest from both academia\\nand industry, as evidenced by the development of tools like GitHub Copilot2 [48], CodeGeeX3 [321],\\nand Amazon CodeWhisperer4, which leverage groundbreaking code LLMs to facilitate software\\ndevelopment.\\nInitial investigations into code generation primarily utilized heuristic rules or expert systems,\\nsuch as probabilistic grammar-based frameworks [10, 62, 119, 125, 288] and specialized language\\nmodels [64, 83, 117]. These early techniques were typically rigid and difficult to scale. However,\\nthe introduction of Transformer-based LLMs has shifted the paradigm, establishing them as the\\npreferred method due to their superior proficiency and versatility. One remarkable aspect of LLMs is\\ntheir capability to follow instructions [56, 187, 200, 275, 289], enabling even novice programmers to'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 1, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='their capability to follow instructions [56, 187, 200, 275, 289], enabling even novice programmers to\\nwrite code by simply articulating their requirements. This emergent ability has democratized coding,\\nmaking it accessible to a broader audience [307]. The performance of LLMs on code generation\\ntasks has seen remarkable improvements, as illustrated by the HumanEval leaderboard5, which\\nshowcases the evolution from PaLM 8B [54] of 3.6% to LDB [325] of 95.1% on Pass@1 metrics.\\nAs can be seen, the HumanEval benchmark [48] has been established as a de facto standard for\\nevaluating the coding proficiency of LLMs [48].\\nTo offer a comprehensive chronological evolution, we present an overview of the development\\nof LLMs for code generation, as illustrated in Figure 1. The landscape of LLMs for code generation\\nis characterized by a spectrum of models, with certain models like ChatGPT [200], GPT4 [5],\\nLLaMA [252, 253], and Claude 3 [14] serving general-purpose applications, while others such'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 1, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='is characterized by a spectrum of models, with certain models like ChatGPT [200], GPT4 [5],\\nLLaMA [252, 253], and Claude 3 [14] serving general-purpose applications, while others such\\nas StarCoder [147, 170], Code LLaMA [227], DeepSeek-Coder [88], and Code Gemma [59] are\\ntailored specifically for code-centric tasks. The convergence of code generation with the latest LLM\\nadvancements is pivotal, especially when programming languages can be considered as distinct\\ndialects of multilingual natural language [16, 321]. These models are not only tested against software\\nengineering (SE) requirements but also propel the advancement of LLMs into practical production\\n[317].\\nWhile recent surveys have shed light on code LLMs from the lenses of Natural Language Pro-\\ncessing (NLP), Software Engineering (SE), or a combination of both disciplines [74, 101, 174, 307,\\n317, 324], they have often encompassed a broad range of code-related tasks. There remains a dearth'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 1, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='cessing (NLP), Software Engineering (SE), or a combination of both disciplines [74, 101, 174, 307,\\n317, 324], they have often encompassed a broad range of code-related tasks. There remains a dearth\\nof literature specifically reviewing advanced topics in code generation, such as meticulous data\\ncuration, instruction tuning, alignment with feedback, prompting techniques, the development of\\nautonomous coding agents, retrieval augmented code generation, LLM-as-a-Judge for code genera-\\ntion, among others. A notably pertinent study [16, 307] also concentrates on LLMs for text-to-code\\ngeneration (NL2Code), yet it primarily examines models released from 2020 to 2022. Consequently,\\n1https://chat.openai.com\\n2https://github.com/features/copilot\\n3https://codegeex.cn/en-US\\n4https://aws.amazon.com/codewhisperer\\n5https://paperswithcode.com/sota/code-generation-on-humaneval\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 2, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:3\\nTable 1. The applications of code LLMs in various code-related understanding and generation tasks. The I-O\\ncolumn indicates the type of input and output for each task, where C, NL, and K represent code, natural\\nlanguage, and label, respectively. Note that the detailed definitions of each task aligns with the descriptions\\nin [7, 16, 17, 194, 307]. The main scope of this survey focuses on code generation while it may involve code\\ncompletion in Section 5.7 and 5.8, aiming to illustrate the corresponding advancements.\\nType\\nI-O\\nTask\\nDefinition\\nUnderstanding\\nC-K\\nCode Classification\\nClassify code snippets based on functionality, purpose, or attributes\\nto aid in organization and analysis.\\nBug Detection\\nDetect and diagnose bugs or vulnerabilities in code to ensure\\nfunctionality and security.\\nClone Detection\\nIdentifying duplicate or similar code snippets in software to enhance\\nmaintainability, reduce redundancy, and check plagiarism.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 2, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='functionality and security.\\nClone Detection\\nIdentifying duplicate or similar code snippets in software to enhance\\nmaintainability, reduce redundancy, and check plagiarism.\\nException Type Prediction\\nPredict different exception types in code to manage and handle\\nexceptions effectively.\\nC-C\\nCode-to-Code Retrieval\\nRetrieve relevant code snippets based on a given\\ncode query for reuse or analysis.\\nNL-C\\nCode Search\\nFind relevant code snippets based on natural language\\nqueries to facilitate coding and development tasks.\\nGeneration\\nC-C\\nCode Completion\\nPredict and suggest the next portion of code, given contextual\\ninformation from the prefix (and suffix), while typing to enhance\\ndevelopment speed and accuracy.\\nCode Translation\\nTranslate the code from one programming language to another\\nwhile preserving functionality and logic.\\nCode Repair\\nIdentify and fix bugs in code by generating the correct version to\\nimprove functionality and reliability.\\nMutant Generation'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 2, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='while preserving functionality and logic.\\nCode Repair\\nIdentify and fix bugs in code by generating the correct version to\\nimprove functionality and reliability.\\nMutant Generation\\nGenerate modified versions of code to test and evaluate the\\neffectiveness of testing strategies.\\nTest Generation\\nGenerate test cases to validate code functionality, performance,\\nand robustness.\\nC-NL\\nCode Summarization\\nGenerate concise textual descriptions or explanations of code to\\nenhance understanding and documentation.\\nNL-C\\nCode Generation\\nGenerate source code from natural language descriptions to\\nstreamline development and reduce manual coding efforts.\\nthis noticeable temporal gap has resulted in an absence of up-to-date literature reviews that con-\\ntemplate the latest advancements, including models like CodeQwen [249], WizardCoder [173],\\nCodeFusion [241], and PPOCoder [238], as well as the comprehensive exploration of the advanced\\ntopics previously mentioned.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 2, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='CodeFusion [241], and PPOCoder [238], as well as the comprehensive exploration of the advanced\\ntopics previously mentioned.\\nRecognizing the need for a dedicated and up-to-date literature review, this survey endeavors to fill\\nthat void. We provide a systematic review that will serve as a foundational reference for researchers\\nquickly exploring the latest progress in LLMs for code generation. A taxonomy is introduced to\\ncategorize and examine recent advancements, encompassing data curation [173, 268, 278], advanced\\ntopics [45, 51, 104, 139, 163, 171, 187, 190, 205, 239, 309], evaluation methods [48, 95, 123, 332], and\\npractical applications [48, 321]. This category aligns with the comprehensive lifecycle of an LLM for\\ncode generation. Furthermore, we pinpoint critical challenges and identify promising opportunities\\nto bridge the research-practicality divide. Therefore, this survey allows NLP and SE researchers'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 2, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='code generation. Furthermore, we pinpoint critical challenges and identify promising opportunities\\nto bridge the research-practicality divide. Therefore, this survey allows NLP and SE researchers\\nto seamlessly equip with a thorough understanding of LLM for code generation, highlighting\\ncutting-edge directions and current hurdles and prospects.\\nThe remainder of the survey is organized following the structure outlined in our taxonomy in\\nFigure 6. In Section 2, we introduce the preliminaries of LLM with Transformer architecture and\\nformulate the task of LLM for code generation. Section 3, we detail the systematic methodologies\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 3, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:4\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nemployed in conducting literature reviews. Then, in Section 4, we propose a taxonomy, categorizing\\nthe complete process of LLMs in code generation. Section 5 delves into the specifics of LLMs\\nfor code generation within this taxonomy framework. In Section 6, we underscore the critical\\nchallenges and promising opportunities for bridging the research-practicality gap and conclude\\nthis work in Section 7.\\n2\\nBACKGROUND\\n2.1\\nLarge Language Models\\nThe effectiveness of large language models (LLMs) is fundamentally attributed to their substantial\\nquantity of model parameters, large-scale and diversified datasets, and the immense computational\\npower utilized during training [97, 127]. Generally, scaling up language models consistently results\\nin enhanced performance and sample efficiency across a broad array of downstream tasks [275, 319].'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 3, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='power utilized during training [97, 127]. Generally, scaling up language models consistently results\\nin enhanced performance and sample efficiency across a broad array of downstream tasks [275, 319].\\nHowever, with the expansion of the model size to a certain extent (e.g., GPT-3 [33] with 175B-\\nparameters and PaLM [54] with 540B), LLMs have exhibited an unpredictable phenomenon known\\nas emergent abilities6, including instruction following [200], in-context learning [70], and step-\\nby-step reasoning [105, 276], which are absent in smaller models but apparent in larger ones\\n[275].\\nAdhering to the same architectures of the Transformer [257] in LLMs, code LLMs are specifically\\npre-trained (or continually pre-trained on general LLMs) using large-scale unlabeled code corpora\\nwith a smaller portion of text (and math) data, whereas general-purpose LLMs are pre-trained\\nprimarily on large-scale text data, incorporating a smaller amount of code and math data to'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 3, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='with a smaller portion of text (and math) data, whereas general-purpose LLMs are pre-trained\\nprimarily on large-scale text data, incorporating a smaller amount of code and math data to\\nenhance logical reasoning capabilities. Additionally, some code LLMs, such as Qwen2.5-Coder\\n[109], incorporate synthetic data in their training processes, a practice that is attracting increasing\\nattention from both industry and academia. Analogous to LLMs, Code LLMs can also be classified\\ninto three architectural categories: encoder-only models, decoder-only models, and encoder-decoder\\nmodels. For encoder-only models, such as CodeBERT [76], they are typically suitable for code\\ncomprehension tasks including type prediction, code retrieval, and clone detection. For decoder-\\nonly models, such as StarCoder [33], they predominantly excel in generation tasks, such as code\\ngeneration, code translation, and code summarization. Encoder-decoder models, such as CodeT5'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 3, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='only models, such as StarCoder [33], they predominantly excel in generation tasks, such as code\\ngeneration, code translation, and code summarization. Encoder-decoder models, such as CodeT5\\n[271], can accommodate both code understanding and generation tasks but do not necessarily\\noutperform encoder-only or decoder-only models. The overall architectures of the different Code\\nLLMs for code generation are depicted in Figure 2.\\nIn the following subsection, we will delineate the key modules of the Transformer layers in Code\\nLLMs.\\n2.1.1\\nMulti-Head Self-Attention Modules. Each Transformer layer incorporates a multi-head self-\\nattention (MHSA) mechanism to discern the inherent semantic relationships within a sequence\\nof tokens across ‚Ñédistinct latent representation spaces. Formally, the MHSA employed by the\\nTransformer can be formulated as follows:\\nh(ùëô) = MultiHeadSelfAttn(Q, K, V) = Concat {Headùëñ}‚Ñé\\nùëñ=1 WO,\\n(1)\\nHeadùëñ= Attention(H(ùëô‚àí1)WQ\\nùëñ\\n|      {z      }\\nQ\\n, H(ùëô‚àí1)WK\\nùëñ\\n|      {z      }\\nK'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 3, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Transformer can be formulated as follows:\\nh(ùëô) = MultiHeadSelfAttn(Q, K, V) = Concat {Headùëñ}‚Ñé\\nùëñ=1 WO,\\n(1)\\nHeadùëñ= Attention(H(ùëô‚àí1)WQ\\nùëñ\\n|      {z      }\\nQ\\n, H(ùëô‚àí1)WK\\nùëñ\\n|      {z      }\\nK\\n, H(ùëô‚àí1)WV\\nùëñ\\n|      {z      }\\nV\\n),\\n(2)\\n6It should be noted that an LLM is not necessarily superior to a smaller language model, and emergent abilities may not\\nmanifest in all LLMs [319].\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 4, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:5\\nphi-2\\n2021\\nMay\\nGPT-C\\nCodeGPT\\nFeb.\\nMar.\\nMay\\nGPT-Neo PLBART\\nGPT-J\\nJul.\\nCodex\\nSep.\\nCodeT5\\nNov.\\nCodeParrot\\nPolyCoder\\nAlphaCode\\nJuPyT5\\nCodeGen\\nGPT-NeoX\\nPaLM-Coder InCoder\\nCodeRL\\nPanGu-Coder\\nPyCodeGPT\\nCodeGeeX\\nBLOOM\\nERNIE-Code\\nSantaCoder\\nJan.\\nPPOCoder\\nLLaMA\\nFeb.\\nMay\\nCodeGen2\\nreplit-code\\nStarCoder\\nCodeT5+\\nCodeTF\\nJun.\\nWizardCoder\\nphi-1\\nJul.\\nChainCoder\\nCodeGeeX2\\nPanGu-Coder2\\nAug.\\nOctoPack\\nSep.\\nMFTCoder\\nOct.\\nCodeShell\\nphi-1.5\\nCodeFusion\\nNov.\\nDeepSeek-Coder\\nDec.\\nMagicoder\\nAlphaCode 2\\nWaveCoder\\nJan.\\nFeb.\\nAST-T5\\nToolGen\\nStableCode\\nAlphaCodium\\nStepCoder OpenCodeInterpreter StarCoder2\\nMar.\\nDevin\\nOpenDevin\\nCodeS\\nApr.\\nProCoder\\nCodeQwen1.5\\nCodeGemma\\nCode Llama\\nApr.\\nSelf-Debugging\\nJan.\\nFeb.\\nJun.\\nJul.\\nSep.\\nNov.\\nDec.\\nApr.\\nMar.\\n2020\\n2022\\n2023\\n2024\\nChatGPT\\nMar.\\nGPT4\\nLlama 2\\nLlama 3\\nClaude 3\\nCodeT\\nSelfEvolve\\nLEVER\\nRLTF\\nOct.\\nPyMT5\\nStarCoder2-Instruct\\nOpen Source Closed Source\\n9\\n3\\n6\\n6\\n5\\n3\\n1\\n4\\nMar.\\nCodestral'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 4, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Nov.\\nDec.\\nApr.\\nMar.\\n2020\\n2022\\n2023\\n2024\\nChatGPT\\nMar.\\nGPT4\\nLlama 2\\nLlama 3\\nClaude 3\\nCodeT\\nSelfEvolve\\nLEVER\\nRLTF\\nOct.\\nPyMT5\\nStarCoder2-Instruct\\nOpen Source Closed Source\\n9\\n3\\n6\\n6\\n5\\n3\\n1\\n4\\nMar.\\nCodestral\\nFig. 1. A chronological overview of large language models (LLMs) for code generation in recent years. The\\ntimeline was established mainly according to the release date. The models with publicly available model\\ncheckpoints are highlighted in green color.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 5, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:6\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nAttention(Q, K, V) = softmax\\n \\nQKùëá\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñé\\n!\\nV,\\n(3)\\nwhere H(ùëô‚àí1) ‚ààRùëõ√óùëëùëöùëúùëëùëíùëôdenotes the input to the ùëô-th Transformer layer, while h(ùëô) ‚ààRùëõ√óùëëùëöùëúùëëùëíùëô\\nrepresents the output of MHSA sub-layer. The quantity of distinct attention heads is represented\\nby ‚Ñé, and ùëëùëöùëúùëëùëíùëôrefers to the model dimension. The set of projections\\nn\\nWQ\\nùëñ, WK\\nùëñ, WV\\nùëñ, WO\\nùëñ\\no\\n‚àà\\nRùëëùëöùëúùëëùëíùëô√óùëëùëöùëúùëëùëíùëô/‚Ñéencompasses the affine transformation parameters for each attention head Headùëñ,\\ntransforming the Query Q, Key K, Value V, and the output of the attention sub-layer. The softmax\\nfunction is applied in a row-wise manner. The dot-products of queries and keys are divided by\\na scaling factor\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñéto counteract the potential risk of excessive large inner products and\\ncorrespondingly diminished gradients in the softmax function, thus encouraging a more balanced\\nattention landscape.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 5, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñéto counteract the potential risk of excessive large inner products and\\ncorrespondingly diminished gradients in the softmax function, thus encouraging a more balanced\\nattention landscape.\\nIn addition to multi-head self-attention, there are two other types of attention based on the\\nsource of queries and key-value pairs:\\n‚Ä¢ Masked Multi-Head Self-Attention. Within the decoder layers of the Transformer, the\\nself-attention mechanism is constrained by introducing an attention mask, ensuring that\\nqueries at each position can only attend to all key-value pairs up to and inclusive of that\\nposition. To facilitate parallel training, this is typically executed by assigning a value of 0\\nto the lower triangular part and setting the remaining elements to ‚àí‚àû. Consequently, each\\nitem attends only to its predecessors and itself. Formally, this modification in Equation 3 can\\nbe depicted as follows:\\nAttention(Q, K, V) = softmax\\n \\nQKùëá\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñé\\n+ Mùëöùëéùë†ùëò\\n!\\nV,\\n(4)\\nMùëöùëéùë†ùëò=\\n\\x10\\nùëöùëñùëó\\n\\x11\\nùëõ√óùëõ=\\n\\x10\\nI(ùëñ‚â•ùëó)\\n\\x11'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 5, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='be depicted as follows:\\nAttention(Q, K, V) = softmax\\n \\nQKùëá\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñé\\n+ Mùëöùëéùë†ùëò\\n!\\nV,\\n(4)\\nMùëöùëéùë†ùëò=\\n\\x10\\nùëöùëñùëó\\n\\x11\\nùëõ√óùëõ=\\n\\x10\\nI(ùëñ‚â•ùëó)\\n\\x11\\nùëõ√óùëõ=\\n(\\n0\\nfor ùëñ‚â•ùëó\\n‚àí‚àû\\notherwise ,\\n(5)\\nThis form of self-attention is commonly denoted as autoregressive or causal attention [157].\\n‚Ä¢ Cross-Layer Multi-Head Self-Attention. The queries are derived from the outputs of the\\npreceding (decoder) layer, while the keys and values are projected from the outputs of the\\nencoder.\\n2.1.2\\nPosition-wise Feed-Forward Networks. Within each Transformer layer, a Position-wise Feed-\\nForward Network (PFFN) is leveraged following the MHSA sub-layer to refine the sequence\\nembeddings at each position ùëñin a separate and identical manner, thereby encoding more intricate\\nfeature representations. The PFFN is composed of a pair of linear transformations, interspersed\\nwith a ReLU activation function. Formally,\\nPFFN(‚Ñé(ùëô)) =\\n\\x10\\nConcat\\nn\\nFFN(‚Ñé(ùëô)\\nùëñ)ùëáoùëõ\\nùëñ=1\\n\\x11ùëá\\n,\\n(6)\\nFFN(‚Ñé(ùëô)\\nùëñ) = ReLU(‚Ñé(ùëô)\\nùëñW(1) + ùëè(1))W(2) + ùëè(2),\\n(7)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 5, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='with a ReLU activation function. Formally,\\nPFFN(‚Ñé(ùëô)) =\\n\\x10\\nConcat\\nn\\nFFN(‚Ñé(ùëô)\\nùëñ)ùëáoùëõ\\nùëñ=1\\n\\x11ùëá\\n,\\n(6)\\nFFN(‚Ñé(ùëô)\\nùëñ) = ReLU(‚Ñé(ùëô)\\nùëñW(1) + ùëè(1))W(2) + ùëè(2),\\n(7)\\nwhere ‚Ñé(ùëô) ‚ààRùëõ√óùëëùëöùëúùëëùëíùëôis the outputs of MHSA sub-layer in ùëô-th Transformer layer, and ‚Ñé(ùëô)\\nùëñ\\n‚àà\\nRùëëùëöùëúùëëùëíùëôdenotes the latent representation at each sequence position. The projection matrices\\n\\x08\\nW(1), (W(2))ùëá\\t\\n‚ààRùëëùëöùëúùëëùëíùëô√ó4ùëëùëöùëúùëëùëíùëôand bias vectors {b(1), b(2)} ‚ààRùëëùëöùëúùëëùëíùëôare parameters learned\\nduring training. These parameters remain consistent across all positions while are individually\\ninitialized from layer to layer. In this context, ùëárepresents the transpose operation on a matrix.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 6, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:7\\nMasked\\nMulti-Head\\nSelf-Attention\\nMulti-Head\\nSelf-Attention\\n+\\n+\\n+\\n+\\n+\\nLayer Norm\\nPosition-wise\\nFeed Forward\\nLinear & Softmax\\nToken & Position\\nEmbedding\\nInputs\\nOutputs (Shifted Right)\\nMulti-Head\\nSelf-Attention\\nLayer Norm\\nLayer Norm\\nLayer Norm\\nLayer Norm\\nPosition-wise\\nFeed Forward\\nToken & Position\\nEmbedding\\nOutput Probabilities\\nùëÅ√ó\\nùëÅ√ó\\n(a) Encoder-Decoder Models\\nMasked\\nMulti-Head\\nSelf-Attention\\nPosition-wise\\nFeed Forward\\n+\\n+\\nLinear & Softmax\\nToken & Position\\nEmbedding\\nInputs\\nLayer Norm\\nLayer Norm\\nOutput Probabilities\\nùëÅ√ó\\n(b) Decoder-only Models\\nFig. 2. The overview of large language models (LLMs) with encoder-decoder and decoder-only Transformer\\narchitecture for code generation, adapted from [257].\\n2.1.3\\nResidual Connection and Normalization. To alleviate the issue of vanishing or exploding\\ngradients resulting from network deepening, the Transformer model incorporates a residual con-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 6, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='2.1.3\\nResidual Connection and Normalization. To alleviate the issue of vanishing or exploding\\ngradients resulting from network deepening, the Transformer model incorporates a residual con-\\nnection [94] around each of the aforementioned modules, followed by Layer Normalization [18].\\nFor the placement of Layer Normalization operation, there are two widely used approaches: 1)\\nPost-Norm: Layer normalization is implemented subsequent to the element-wise residual addition,\\nin accordance with the vanilla Transformer [257]. 2) Pre-Norm: Layer normalization is applied to\\nthe input of each sub-layer, as seen in models like GPT-2 [214]. Formally, it can be formulated as:\\nPost-Norm : H(l) = LayerNorm(PFFN(h(l)) + h(l)),\\nh(l) = LayerNorm(MHSA(H(l‚àí1)) + H(l‚àí1))\\n(8)\\nPre-Norm : H(l) = PFFN(LayerNorm(h(l))) + h(l),\\nh(l) = MHSA(LayerNorm(H(l‚àí1))) + H(l‚àí1)\\n(9)\\n2.1.4\\nPositional Encoding. Given that self-attention alone cannot discern the positional information'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 6, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='(8)\\nPre-Norm : H(l) = PFFN(LayerNorm(h(l))) + h(l),\\nh(l) = MHSA(LayerNorm(H(l‚àí1))) + H(l‚àí1)\\n(9)\\n2.1.4\\nPositional Encoding. Given that self-attention alone cannot discern the positional information\\nof each input token, the vanilla Transformer introduces an absolute positional encoding method to\\nsupplement this positional information, known as sinusoidal position embeddings [257]. Specifically,\\nfor a token at position ùëùùëúùë†, the position embedding is defined as:\\npùëùùëúùë†,2ùëñ= sin(\\nùëùùëúùë†\\n100002ùëñ/ùëëùëöùëúùëëùëíùëô),\\n(10)\\npùëùùëúùë†,2ùëñ+1 = cos(\\nùëùùëúùë†\\n100002ùëñ/ùëëùëöùëúùëëùëíùëô),\\n(11)\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 7, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:8\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nwhere 2ùëñ, 2ùëñ+1 represent the dimensions of the position embedding, while ùëëùëöùëúùëëùëíùëôdenotes the model\\ndimension. Subsequently, each position embedding is added to the corresponding token embedding,\\nand the sum is fed into the Transformer. Since the inception of this method, a variety of innovative\\npositional encoding approaches have emerged, such as learnable embeddings [66], relative position\\nembeddings [232], RoPE [243], and ALiBi [211]. For more detailed descriptions of each method,\\nplease consult [157, 318].\\n2.1.5\\nArchitecture. There are two types of Transformer architecture for code generation task,\\nincluding encoder-decoder and decoder-only. For the encoder-decoder architecture, it consists of\\nboth an encoder and a decoder, in which the encoder processes the input data and generates a\\nset of representations, which are then used by the decoder to produce the output. However, for'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 7, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='both an encoder and a decoder, in which the encoder processes the input data and generates a\\nset of representations, which are then used by the decoder to produce the output. However, for\\ndecoder-only architecture, it consists only of the decoder part of the transformer, where it uses\\na single stack of layers to both process input data and generate output. Therefore, the encoder-\\ndecoder architecture is suited for tasks requiring mapping between different input and output\\ndomains, while the decoder-only architecture is designed for tasks focused on sequence generation\\nand continuation. The overview of LLMs with these two architectures are illustrated in Figure 2.\\n2.2\\nCode Generation\\nLarge language models (LLMs) for code generation refer to the use of LLM to generate source\\ncode from natural language descriptions, a process also known as a natural-language-to-code\\ntask. Typically, these natural language descriptions encompass programming problem statements'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 7, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='code from natural language descriptions, a process also known as a natural-language-to-code\\ntask. Typically, these natural language descriptions encompass programming problem statements\\n(or docstrings) and may optionally include some programming context (e.g., function signatures,\\nassertions, etc.). Formally, these natural language (NL) descriptions can be represented as x. Given x,\\nthe use of an LLM with model parameters ùúÉto generate a code solution y can be denoted as ùëÉùúÉ(y | x).\\nThe advent of in-context learning abilities in LLM [275] has led to the appending of exemplars to\\nthe natural language description x as demonstrations to enhance code generation performance or\\nconstrain the generation format [145, 206]. A fixed set of ùëÄexemplars is denoted as {(xi, yi)}ùëÄ\\nùëñ=1.\\nConsequently, following [190], a more general formulation of LLMs for code generation with\\nfew-shot (or zero-shot) exemplars can be revised as:\\nùëÉùúÉ(y | x) ‚áíùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(12)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 7, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='few-shot (or zero-shot) exemplars can be revised as:\\nùëÉùúÉ(y | x) ‚áíùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(12)\\nwhere prompt(x, {(xi, yi)}ùëò\\nùëñ=1)) is a string representation of the overall input, and {(xi, yi)}ùëò\\nùëñ=1\\ndenotes a set of ùëòexemplars randomly selected from {(xi, yi)}ùëÄ\\nùëñ=1. In particular, when ùëò= 0, this\\ndenotes zero-shot code generation, equivalent to vanilla ones without in-context learning. In the\\ndecoding process, a variety of decoding strategies can be performed for code generation, including\\ndeterministic-based strategies (e.g., greedy search and beam search) and sampling-based strategies\\n(e.g., temperature sampling, top-k sampling, and top-p (nucleus) sampling). For more detailed\\ndescriptions of each decoding strategy, please consult [99]. For example, the greedy search and\\nsampling-based decoding strategies can be formulated as follows:\\nGreedy Search : y‚àó= argmax\\ny\\nùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(13)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 7, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='sampling-based decoding strategies can be formulated as follows:\\nGreedy Search : y‚àó= argmax\\ny\\nùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(13)\\nSampling : y ‚àºùëÉùúÉ(y | prompt(x, {(xi, yùëñ)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(14)\\nTo verify the functionality correctness of the generated code solution, y is subsequently executed\\nvia a compiler or interpreter, represented as Exe(¬∑), on a suit of unit tests T. The feedback from\\nthis execution can be denoted as Feedback(Exe(y, T)). If the generated code solution fails to pass\\nall test cases, the error feedback can be iteratively utilized to refine the code by leveraging the\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 8, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:9\\nResearch\\nQuestions\\nLarge Language\\nModel (LLM)\\nCode Generation\\nTop-tier LLMs\\nand SE Venues\\nTotal 235 Papers\\nSnowballing\\nSearch\\nQuality\\nAssessment\\nInclusion and\\nExclusion Criteria\\nAutomatic\\nFiltering\\nSearch Strings\\nAutomated Search\\nManual Search\\n235\\n247\\n351\\n294\\n73\\n36\\n261\\nFig. 3. Overview of the paper search and collection process.\\nprevious attempt (yùëùùëüùëí) and the associated feedback. Formally,\\ny ‚àºùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1, yùëùùëüùëí, Feedback(Exe(y, T)))),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(15)\\nFurther details and relevant studies on using feedback to improve code generation are comprehen-\\nsively discussed in Section 5.5 and 5.6.\\n3\\nMETHODOLOGY\\nIn this section, we detail the systematic methodologies employed in conducting literature reviews.\\nWe follow the systematic literature review methodology outlined by [131], which has been widely\\nadopted in numerous software engineering literature reviews [101, 146, 169, 219, 262]. The overall'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 8, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='We follow the systematic literature review methodology outlined by [131], which has been widely\\nadopted in numerous software engineering literature reviews [101, 146, 169, 219, 262]. The overall\\nprocess is illustrated in Figure 3, and the detailed steps in our methodology are documented below.\\n3.1\\nResearch Questions\\nTo deliver a comprehensive and up-to-date literature review on the latest advancements in large\\nlanguage models (LLMs) for code generation, this systematic literature review addresses the\\nfollowing research questions (RQs):\\nRQ1: How can we categorize and evaluate the latest advances in LLMs for code genera-\\ntion? The recent proliferation of LLMs has resulted in many of these models being adapted for code\\ngeneration task. While the adaptation of LLMs for code generation essentially follows the evolution\\nof LLMs, this evolution encompasses a broad spectrum of research directions and advancements.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 8, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='generation task. While the adaptation of LLMs for code generation essentially follows the evolution\\nof LLMs, this evolution encompasses a broad spectrum of research directions and advancements.\\nFor software engineering (SE) researchers, it can be challenging and time-consuming to fully grasp\\nthe comprehensive research landscape of LLMs and their adaptation to code generation. RQ1 aims\\nto propose a taxonomy that serves as a comprehensive reference for researchers, enabling them to\\nquickly familiarize themselves with the state-of-the-art in this dynamic field and identify specific\\nresearch problems and directions of interest.\\nRQ2: What are the key insights into LLMs for code generation? RQ2 seeks to assist\\nresearchers in establishing a comprehensive, up-to-date, and advanced understanding of LLMs for\\ncode generation. This includes discussing various aspects of this rapidly evolving domain, such as'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 8, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='researchers in establishing a comprehensive, up-to-date, and advanced understanding of LLMs for\\ncode generation. This includes discussing various aspects of this rapidly evolving domain, such as\\ndata curation, latest advancements, performance evaluation, ethical and environmental implications,\\nand real-world applications. A historical overview of the evolution of LLMs for code generation is\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 9, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:10\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 2. Publication venues for conference proceedings and journals articles for manual search.\\nDomain\\nVenue\\nAcronym\\nLLMs\\nInternational Conference on Learning Representations\\nICLR\\nConference on Neural Information Processing Systems\\nNeurIPS\\nInternational Conference on Machine Learning\\nICML\\nAnnual Meeting of the Association for Computational Linguistics\\nACL\\nConference on Empirical Methods in Natural Language Processing\\nEMNLP\\nInternational Joint Conference on Artificial Intelligence\\nNAACL\\nAAAI Conference on Artificial Intelligence\\nAAAI\\nSE\\nInternational Conference on Software Engineering\\nICSE\\nJoint European Software Engineering Conference and Symposium on the Foundations of Software Engineering\\nESEC/FSE\\nInternational Conference on Automated Software Engineering\\nASE\\nTransactions on Software Engineering and Methodology\\nTOSEM\\nTransactions on Software Engineering\\nTSE\\nInternational Symposium on Software Testing and Analysis'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 9, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ASE\\nTransactions on Software Engineering and Methodology\\nTOSEM\\nTransactions on Software Engineering\\nTSE\\nInternational Symposium on Software Testing and Analysis\\nISSTA\\nTable 3. Keywords related to LLMs and code generation task for automated search.\\nKeywords Related to LLMs\\nKeywords Related to Code Generation Task\\nCode Large Language Model‚àó, Code LLMs, Code Language Model,\\nCode LMs, Large Language Model‚àó, LLM, Language Model‚àó, LM,\\nPre-trained Language Model‚àó, PLM, Pre-trained model,\\nNatural Language Processing, NLP, GPT-3, ChatGPT, GPT-4, LLaMA,\\nCodeLlama, PaLM‚àó, CodeT5, Codex, CodeGen, InstructGPT\\nCode Generation, Program Synthesis, Code Intelligence,\\n‚àóCoder‚àó, natural-language-to-code, NL2Code, Programming\\nprovided, along with an empirical comparison using the widely recognized HumanEval and MBPP\\nbenchmarks, as well as the more practical and challenging BigCodeBench benchmark, to highlight\\nthe progressive enhancements in LLM capabilities for code generation. RQ2 offers an in-depth'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 9, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='benchmarks, as well as the more practical and challenging BigCodeBench benchmark, to highlight\\nthe progressive enhancements in LLM capabilities for code generation. RQ2 offers an in-depth\\nanalysis of critical insights related to LLMs for code generation.\\nRQ3: What are the critical challenges and promising research opportunities in LLMs for\\ncode generation? Despite the revolutionary impact of LLMs on the paradigm of code generation\\nand their remarkable performance, numerous challenges remain unaddressed. These challenges\\nprimarily stem from the gap between academic research and practical development. For instance,\\nwhile the HumanEval benchmark is established as a de facto standard for evaluating the coding\\nproficiency of LLMs in academia, it has been shown that this evaluation does not adequately reflect\\npractical development scenarios [68, 72, 123, 162]. RQ3 aims to identify critical challenges and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 9, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='proficiency of LLMs in academia, it has been shown that this evaluation does not adequately reflect\\npractical development scenarios [68, 72, 123, 162]. RQ3 aims to identify critical challenges and\\nhighlight promising opportunities to bridge the gap between research and practical application.\\n3.2\\nSearch Process\\n3.2.1\\nSearch Strings. To address the aforementioned three research questions (RQs), we initiate a\\nmanual review of conference proceedings and journal articles from top-tier venues in the fields of\\nLLMs and SE, as detailed in Table 2. This process allowed us to identify relevant studies and derive\\nsearch strings, which are subsequently utilized for an automated search across various scientific\\ndatabases. The complete set of search keywords is presented in Table 3.\\n3.2.2\\nSearch Databases. Following the development of search strings, we executed an automated\\nsearch using four popular scientific databases: the ACM Digital Library, IEEE Xplore Digital Library,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 9, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='3.2.2\\nSearch Databases. Following the development of search strings, we executed an automated\\nsearch using four popular scientific databases: the ACM Digital Library, IEEE Xplore Digital Library,\\narXiv, and DBLP. Our search focus on identifying papers whose titles contain keywords pertinent\\nto LLMs and code generation. This approach enhances the likelihood of retrieving relevant papers\\nsince both sets of keywords must be present in the title. Although this title-based search strategy\\neffectively retrieves a large volume of papers, it is important to note that in some instances [238],\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 10, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:11\\nthe scope of code generation can be broader, encompassing areas such as code completion, code\\ntranslation, and program synthesis. As outlined in Section 1, this survey adopts a prevalent definition\\nof code generation as the natural-language-to-code (NL2Code) task [16, 17, 307].\\nConsequently, we conduct further automatic filtering based on the content of the papers. Papers\\nfocusing on ‚Äúcode completion‚Äù and ‚Äúcode translation‚Äù are excluded unless they pertain to the\\nspecific topics discussed in Section 5.7 and Section 5.8, where code completion is a primary focus.\\nAfter completing the automated search, the results from each database are merged and deduplicated\\nusing scripts. This process yields 294 papers from arXiv, 73 papers from the ACM Digital Library,\\n36 papers from IEEE Xplore, and 261 papers from DBLP.\\n3.3\\nInclusion and Exclusion Criteria'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 10, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='using scripts. This process yields 294 papers from arXiv, 73 papers from the ACM Digital Library,\\n36 papers from IEEE Xplore, and 261 papers from DBLP.\\n3.3\\nInclusion and Exclusion Criteria\\nThe search process conducted across various databases and venues is intentionally broad to gather\\na comprehensive pool of candidate papers. This approach maximizes the collection of potentially\\nrelevant studies. However, such inclusivity may lead to the inclusion of papers that do not align\\nwith the scope of this survey, as well as duplicate entries from multiple sources. To address this,\\nwe have established a clear set of inclusion and exclusion criteria, based on the guidelines from\\n[101, 260]. These criteria are applied to each paper to ensure alignment with our research scope\\nand questions, and to eliminate irrelevant studies.\\nInclusion Criteria. A paper will be included if it meets any of the following criteria:\\n‚Ä¢ It is available in full text.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 10, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and questions, and to eliminate irrelevant studies.\\nInclusion Criteria. A paper will be included if it meets any of the following criteria:\\n‚Ä¢ It is available in full text.\\n‚Ä¢ It presents a dataset or benchmark specifically designed for code generation with LLMs.\\n‚Ä¢ It explores specific LLM techniques, such as pre-training or instruction tuning, for code\\ngeneration.\\n‚Ä¢ It provides an empirical study or evaluation related to the use of LLMs for code generation.\\n‚Ä¢ It discusses the ethical considerations and environmental impact of deploying LLMs for code\\ngeneration.\\n‚Ä¢ It proposes tools or applications powered by LLMs for code generation.\\nExclusion Criteria. Conversely, papers will be excluded if they meet any of the following\\nconditions:\\n‚Ä¢ They are not written in English.\\n‚Ä¢ They are found in books, theses, monographs, keynotes, panels, or venues (excluding arXiv)\\nthat do not undergo a full peer-review process.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 10, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='conditions:\\n‚Ä¢ They are not written in English.\\n‚Ä¢ They are found in books, theses, monographs, keynotes, panels, or venues (excluding arXiv)\\nthat do not undergo a full peer-review process.\\n‚Ä¢ They are duplicate papers or different versions of similar studies by the same authors.\\n‚Ä¢ They focus on text generation rather than source code generation, such as generating code\\ncomments, questions, test cases, or summarization.\\n‚Ä¢ They do not address the task of code generation, for instance, focusing on code translation\\ninstead.\\n‚Ä¢ They leverage software engineering methods to enhance code generation without emphasiz-\\ning LLMs.\\n‚Ä¢ They do not utilize LLMs, opting for other models like Long Short-Term Memory (LSTM)\\nnetworks.\\n‚Ä¢ They use encoder-only language models, such as BERT, which are not directly applicable to\\ncode generation task.\\n‚Ä¢ LLMs are mentioned only in future work or discussions without being central to the proposed\\napproach.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 10, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='code generation task.\\n‚Ä¢ LLMs are mentioned only in future work or discussions without being central to the proposed\\napproach.\\nPapers identified through both manual and automated searches undergo a detailed manual review\\nto ensure they meet the inclusion criteria and do not fall under the exclusion criteria. Specifically,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 11, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:12\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nthe first two authors independently review each paper to determine its eligibility. In cases of\\ndisagreement, the third author makes the final inclusion decision.\\n3.4\\nQuality Assessment\\nTo ensure the inclusion of high-quality studies, we have developed a comprehensive set of ten\\nQuality Assessment Criteria (QAC) following [101]. These QAC are designed to evaluate the\\nrelevance, clarity, validity, and significance of the papers considered for our review.\\nIn accordance with [101], the first three QAC assess the study‚Äôs alignment with our objectives.\\nThese criteria are rated as ‚Äúirrelevant/unmet‚Äù, ‚Äúpartially relevant/met‚Äù, or ‚Äúrelevant/fully met‚Äù,\\ncorresponding to scores of -1, 0, and 1, respectively. If a study receive a score of -1 across these\\ninitial three criteria, it is deemed ineligible for further consideration and subsequently excluded\\nfrom our review process.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 11, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='initial three criteria, it is deemed ineligible for further consideration and subsequently excluded\\nfrom our review process.\\nThe subsequent seven QAC focus on a more detailed content evaluation, employing a scoring\\nrange of -1 to 2, representing ‚Äúpoor‚Äù, ‚Äúfair‚Äù, ‚Äúgood‚Äù, and ‚Äúexcellent‚Äù. We compute a cumulative score\\nbased on the responses to QAC4 through QAC10 for each paper. For published works, the maximum\\nachievable score is 14 (2 points per question). We retain those with a score of 11.2 (80% of the total\\nscore) or higher. For unpublished papers available on arXiv, QAC4 defaults to a score of 0, making\\nthe maximum possible score for the remaining criteria 12. Accordingly, we retain papers scoring\\n9.6 (80% of the adjusted total score) or above .\\n‚Ä¢ QAC1: Is the research not classified as a secondary study, such as a systematic literature\\nreview or survey? (-1, 0, 1)\\n‚Ä¢ QAC2: Does the study incorporate the use of LLMs? (-1, 0, 1)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 11, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ QAC1: Is the research not classified as a secondary study, such as a systematic literature\\nreview or survey? (-1, 0, 1)\\n‚Ä¢ QAC2: Does the study incorporate the use of LLMs? (-1, 0, 1)\\n‚Ä¢ QAC3: Is the study relevant to the code generation task? (-1, 0, 1)\\n‚Ä¢ QAC4: Is the research published in a prestigious venue? (-1, 0, 1, 2)\\n‚Ä¢ QAC5: Does the study present a clear research motivation? (-1, 0, 1, 2)\\n‚Ä¢ QAC6: Are the key contributions and limitations of the study discussed? (-1, 0, 1, 2)\\n‚Ä¢ QAC7: Does the study contribute to the academic or industrial community? (-1, 0, 1, 2)\\n‚Ä¢ QAC8: Are the LLM techniques employed in the study clearly described? (-1, 0, 1, 2)\\n‚Ä¢ QAC9: Are the experimental setups, including experimental environments and dataset infor-\\nmation, thoroughly detailed? (-1, 0, 1, 2)\\n‚Ä¢ QAC10: Does the study clearly confirm its experimental findings? (-1, 0, 1, 2)\\n3.5\\nSnowballing Search'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 11, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='mation, thoroughly detailed? (-1, 0, 1, 2)\\n‚Ä¢ QAC10: Does the study clearly confirm its experimental findings? (-1, 0, 1, 2)\\n3.5\\nSnowballing Search\\nFollowing the quality assessment, we establish an initial set of papers for our study. To minimize\\nthe risk of excluding pertinent literature, we implement a snowballing search strategy. Snowballing\\nsearch involves utilizing a paper‚Äôs reference list or its citations to discover additional relevant\\nstudies, known as backward and forward snowballing, respectively. In this survey, we exclusively\\nemployed backward snowballing following [260]. Despite this effort, no additional studies are\\nidentified through this method. This could be attributed to the task-specific nature of the code\\ngeneration (natural-language-to-code), where reference studies are typically published earlier.\\nConsequently, our methodology, which encompassed an extensive manual and automated search,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 11, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='generation (natural-language-to-code), where reference studies are typically published earlier.\\nConsequently, our methodology, which encompassed an extensive manual and automated search,\\nlikely covered the relevant literature comprehensively, explaining the lack of additional studies\\nthrough snowballing search.\\n3.6\\nData Collection and Analysis\\nThe data collection process for our study, illustrated in Figure 3, began with a manual search\\nthrough conference proceedings and journal articles from leading venues in LLMs and SE. This\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 12, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:13\\n2018\\n2019\\n2020\\n2021\\n2022\\n2023\\n2024\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n# Number of Papers\\n1\\n1\\n1\\n6\\n11\\n75\\n140\\nVenue\\nTrend\\nAAAI\\nACL\\nCAV\\nCCS\\nCHI\\nCVPR\\nEMNLP\\nFSE\\nICLR\\nICML\\nICSE\\nISSTA\\nKDD\\nNAACL\\nNeurIPS\\nOthers\\nTACL\\nTOSEM\\nTSE\\nUSENIX\\narXiv\\nPre-Training & \\nFoundation Model (21.5%)\\nFine-tuning (7.5%)\\nReinforcement Learning (4.8%)\\nPrompting (11.8%)\\nEvaluation &\\n Benchmark (24.1%)\\nData\\n Synthesis (1.8%)\\nRepository\\n Level (5.7%)\\nRetrieval Augmented (3.1%)\\nOthers (8.3%)\\nCode LLMs\\n Alignment (7.0%)\\nAutonomous Coding \\nAgents (4.4%)\\nTotal Papers:\\n235\\nResearch Topics\\nPre-Training & Foundation Model\\nFine-tuning\\nReinforcement Learning\\nPrompting\\nEvaluation & Benchmark\\nData Synthesis\\nRepository Level\\nRetrieval Augmented\\nOthers\\nCode LLMs Alignment\\nAutonomous Coding Agents\\nFig. 4. Data qualitative analysis. Top: Annual distribution of selected papers across various publication'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 12, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Repository Level\\nRetrieval Augmented\\nOthers\\nCode LLMs Alignment\\nAutonomous Coding Agents\\nFig. 4. Data qualitative analysis. Top: Annual distribution of selected papers across various publication\\nvenues. Bottom: Distribution analysis of research topics covered in the included papers.\\ninitial step yielded 42 papers, from which we extracted relevant search strings. Following this,\\nwe performed an automated search across four academic databases using keyword-based queries,\\nresulting in the retrieval of 664 papers. After performing automatic filtering (351 papers), applying\\ninclusion and exclusion criteria (247 papers), conducting quality assessments (235 papers), and\\nutilizing snowballing search (235 papers), we finalize a collection of 235 papers focusing on LLMs\\nfor code generation.\\nTo provide insights from the selected papers, we begin by presenting an overview of their distribu-\\ntion across publication venues each year, as illustrated at the top of Figure 4. Our analysis indicates'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 12, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='tion across publication venues each year, as illustrated at the top of Figure 4. Our analysis indicates\\nthat 14% of the papers are published in LLM-specific venues and 7% in SE venues. Remarkably, 49%\\nof the papers remain unpublished in peer-reviewed venues and are available on arXiv. This trend is\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 13, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:14\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nunderstandable given the emerging nature of this field, with many works being recent and pending\\nformal submission. Despite the absence of peer review on arXiv, our quality assessment process\\nensures that only high-quality papers are included, thereby maintaining the integrity of this survey.\\nFurthermore, the annual trend in the number of collected papers indicates nearly exponential\\ngrowth in the field. From a single paper in the period 2018 to 2020, the numbers increased to 6 in\\n2021, 11 in 2022, 75 in 2023, and 140 in 2024. This trend reflects growing interest and attention\\nin this research area, with expectations for continued expansion in the future. Additionally, to\\ncapture the breadth of advancements in LLMs for code generation, we conducted a distribution\\nanalysis of the research topics covered in the included papers, as shown at the bottom of Figure 4.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 13, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='capture the breadth of advancements in LLMs for code generation, we conducted a distribution\\nanalysis of the research topics covered in the included papers, as shown at the bottom of Figure 4.\\nWe observe that the development of LLMs for code generation closely aligns with broader trends\\nin general-purpose LLM research. Notably, the most prevalent research topics are Pre-training and\\nFoundation Models (21.5%), Prompting (11.8%), and Evaluation and Benchmarks (24.1%). These\\nareas hold significant promise for enhancing, refining, and evaluating LLM-driven code generation.\\n4\\nTAXONOMY\\nThe recent surge in the development of LLMs has led to a significant number of these models\\nbeing repurposed for code generation task through continual pre-training or fine-tuning. This\\ntrend is particularly observable in the realm of open-source models. For instance, Meta AI initially\\nmade the LLaMA [252] model publicly available, which was followed by the release of Code Llama'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 13, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='trend is particularly observable in the realm of open-source models. For instance, Meta AI initially\\nmade the LLaMA [252] model publicly available, which was followed by the release of Code Llama\\n[227], designed specifically for code generation. Similarly, DeepSeek LLM [26] developed and\\nreleased by DeepSeek has been extended to create DeepSeek Coder [88], a variant tailored for code\\ngeneration. The Qwen team has developed and released Code Qwen [249], building on their original\\nQwen [20] model. Microsoft, on the other hand, has unveiled WizardLM [289] and is exploring its\\ncoding-oriented counterpart, WizardCoder [173]. Google has joined the fray by releasing Gemma\\n[248], subsequently followed by Code Gemma [59]. Beyond simply adapting general-purpose LLMs\\nfor code-related tasks, there has been a proliferation of models specifically engineered for code\\ngeneration. Notable examples include StarCoder [147], OctoCoder [187], and CodeGen [193]. These'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 13, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='for code-related tasks, there has been a proliferation of models specifically engineered for code\\ngeneration. Notable examples include StarCoder [147], OctoCoder [187], and CodeGen [193]. These\\nmodels underscore the trend of LLMs being developed with a focus on code generation.\\nRecognizing the importance of these developments, we conduct a thorough analysis of selected\\npapers on LLMs for code generation, sourced from widely used scientific databases as mentioned\\nin Section 3. Based on this analysis, we propose a taxonomy that categorizes and evaluates the\\nlatest advancements in LLMs for code generation. This taxonomy, depicted in Figure 6, serves as a\\ncomprehensive reference for researchers seeking to quickly familiarize themselves with the state-\\nof-the-art in this dynamic field. It is important to highlight that the category of recent advances\\nemphasizes the core techniques used in the current state-of-the-art code LLMs.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 13, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='of-the-art in this dynamic field. It is important to highlight that the category of recent advances\\nemphasizes the core techniques used in the current state-of-the-art code LLMs.\\nIn the subsequent sections, we will provide an in-depth analysis of each category related to code\\ngeneration. This will encompass a definition of the problem, the challenges to be addressed, and a\\ncomparison of the most prominent models and their performance evaluation.\\n5\\nLARGE LANGAUGE MODELS FOR CODE GENERATION\\nLLMs with Transformer architecture have revolutionized a multitude of fields, and their application\\nin code generation has been particularly impactful. These models follow a comprehensive process\\nthat starts with the curation and synthesis of code data, followed by a structured training approach\\nthat includes pre-training and fine-tuning (instruction tuning), reinforcement learning with various\\nfeedback, and the use of sophisticated prompt engineering techniques. Recent advancements have'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 13, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='that includes pre-training and fine-tuning (instruction tuning), reinforcement learning with various\\nfeedback, and the use of sophisticated prompt engineering techniques. Recent advancements have\\nseen the integration of repository-level and retrieval-augmented code generation, as well as the\\ndevelopment of autonomous coding agents. Furthermore, the evaluation of coding abilities of LLMs\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 14, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:15\\nPretrained\\n(Base) LLM\\nInstruct\\nCode LLM\\nSupervised\\nFine-tuning\\n(SFT)\\nHuman Preference\\nAlignment with RL\\n(e.g., RLHF)\\n(Optional)\\nPre-training Database\\nInstruction Database\\nPreference Database\\nStage ‚ë†\\nStage ‚ë¢\\nStage ‚ë£\\nPre-training Database\\nStage ‚ë°\\nContinual\\nPre-training\\n(Optional)\\nTask\\nDescription\\nGenerated\\nSource Code\\nInference\\nEvaluation\\nBenchmark\\nFig. 5. A diagram illustrating the general training, inference, and evaluation workflow for Code LLMs and\\ntheir associated databases. The training workflow is mainly divided into four distinct stages: Stage 1‚óãand 2‚óã\\nare the pre-training phase, whereas Stages 3‚óãand 4‚óãrepresent the post-training phases. It is important to\\nnote that Stage 2‚óãand 4‚óãare optional. For instance, StarCoder [147] incorporates only Stage 1‚óã. WizardCoder\\n[173], fine-tuned upon StarCoder, includes only Stage 3‚óã, while Code Llama [227], continually pre-trained on'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 14, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[173], fine-tuned upon StarCoder, includes only Stage 3‚óã, while Code Llama [227], continually pre-trained on\\nLlama 2, encompasses Stages 2‚óãand 3‚óã. DeepSeek-Coder-V2 [331], continually pre-trained on DeepSeek-V2,\\ncovers Stages 2‚óã, 3‚óã, and 4‚óã. Note that pre-trained model can be directly used for inference through prompt\\nengineering.\\nhas become a critical component of this research area. Figure 5 illustrates the general training,\\ninference, and evaluation workflow for Code LLMs and their associated databases.\\nIn the forthcoming sections, we will explore these dimensions of LLMs in the context of code\\ngeneration in detail. Section 5.1 will address the data curation and processing strategies employed\\nthroughout the various stages of LLM development. Section 5.2 will discuss data synthesis methods\\ndesigned to mitigate the scarcity of high-quality data. Section 5.3 will outline the prevalent model\\narchitectures used in LLMs for code generation. Moving to Section 5.4, we will examine the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 14, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='designed to mitigate the scarcity of high-quality data. Section 5.3 will outline the prevalent model\\narchitectures used in LLMs for code generation. Moving to Section 5.4, we will examine the\\ntechniques for full parameter fine-tuning and parameter-efficient fine-tuning, which are essential\\nfor tailoring LLMs to code generation task. Section 5.5 will shed light on enhancing code quality\\nthrough reinforcement learning, utilizing the power of feedback. Section 5.6 will delve into the\\nstrategic use of prompts to maximize the coding capabilities of LLMs. The innovative approaches\\nof repository-level and retrieval-augmented code generation will be elaborated in Sections 5.7 and\\n5.8, respectively. Additionally, Section 5.9 will discuss the exciting field of autonomous coding\\nagents. Section 5.10 discusses various evaluation strategies and offer an empirical comparison using\\nthe widely recognized HumanEval, MBPP, and the more practical and challenging BigCodeBench'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 14, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='agents. Section 5.10 discusses various evaluation strategies and offer an empirical comparison using\\nthe widely recognized HumanEval, MBPP, and the more practical and challenging BigCodeBench\\nbenchmarks to highlight the progressive enhancements in LLM capabilities for code generation.\\nFurthermore, the ethical implications and the environmental impact of using LLMs for code\\ngeneration are discussed in Section 5.11, aiming to establish a trustworthiness, responsibility, safety,\\nefficiency, and green of LLM for code generation. Lastly, Section 5.12 will provide insights into some\\nof the practical applications that leverage LLMs for code generation, demonstrating the real-world\\nimpact of these sophisticated models. Through this comprehensive exploration, we aim to highlight\\nthe significance and potential of LLMs within the domain of automated code generation.\\n5.1\\nData Curation & Processing\\nThe exceptional performance of LLMs can be attributed to their training on large-scale and diverse'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 14, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='5.1\\nData Curation & Processing\\nThe exceptional performance of LLMs can be attributed to their training on large-scale and diverse\\ndatasets [307]. Meanwhile, the extensive parameters of these models necessitate substantial data to\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 15, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:16\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nLLMs for Code Generation\\nData\\nCuration\\n(Sec. 5.1)\\nPre-training\\nCodeSearchNet[110], Google BigQuery[96], The Pile[78], CodeParrot[254], GitHub Code[254]\\nROOTS[137], The Stack[132], The Stack v2[170]\\nInstruction\\nTuning\\nCommitPackFT [187], Code Alpaca[43], OA-Leet[63], OSS-Instruct[278], Evol-instruction[225]\\nSelf-OSS-Instruct-SC2-Exec-Filter[304]\\nBenchmarks\\nGeneral\\nHumanEval[48], HumanEval+[162], HumanEvalPack[187], MBPP[17]\\nMBPP+[162], CoNaLa[297], Spider[300], CONCODE[113], ODEX[273]\\nCoderEval[299], ReCode[263], StudentEval[19]\\nCompetitions\\nAPPS[95], CodeContests[151]\\nData Science\\nDSP[41], DS-1000[136], ExeDS[107]\\nMultilingual\\nMBXP[16], Multilingual HumanEval[16], HumanEval-X[321], MultiPL-E[39]\\nxCodeEval[128]\\nReasoning\\nMathQA-X[16], MathQA-Python[17], GSM8K[58], GSM-HARD[79]\\nRepository\\nRepoEval[309], Stack-Repo[239], Repobench[167], EvoCodeBench[144]\\nSWE-bench[123], CrossCodeEval[68], SketchEval[308]\\nRecent\\nAdvances'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 15, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Repository\\nRepoEval[309], Stack-Repo[239], Repobench[167], EvoCodeBench[144]\\nSWE-bench[123], CrossCodeEval[68], SketchEval[308]\\nRecent\\nAdvances\\nData\\nSynthesis\\n(Sec. 5.2)\\nSelf-Instruct [268], Evol-Instruct [289], Phi-1[84], Code Alpaca[43], WizardCoder[173]\\nMagicoder[278], StarCoder2-instruct [304]\\nPre-training\\n(Sec. 5.3)\\nModel\\nArchitectures\\nEncoder-Decoder\\nPyMT5[57], PLBART[7], CodeT5[271], JuPyT5[41]\\nAlphaCode[151], CodeRL[139], ERNIE-Code[40]\\nPPOCoder[238], CodeT5+[269], CodeFusion[241]\\nAST-T5[81]\\nDecoder-Only\\nGPT-C[244], GPT-Neo[30], GPT-J[258], Codex[48]\\nCodeGPT[172], CodeParrot[254], PolyCoder[290]\\nCodeGen[193], GPT-NeoX[29], PaLM-Coder[54]\\nInCoder[77], PanGu-Coder[55], PyCodeGPT[306]\\nCodeGeeX[321], BLOOM[140], ChatGPT[196]\\nSantaCoder[9], LLaMA[252], GPT-4[5]\\nCodeGen2[192], replit-code[223], StarCoder[147]\\nWizardCoder[173], phi-1[84], ChainCoder[323]\\nCodeGeeX2[321], PanGu-Coder2[234], Llama 2[253]\\nOctoPack[187], Code Llama[227], MFTCoder[160]'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 15, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='CodeGen2[192], replit-code[223], StarCoder[147]\\nWizardCoder[173], phi-1[84], ChainCoder[323]\\nCodeGeeX2[321], PanGu-Coder2[234], Llama 2[253]\\nOctoPack[187], Code Llama[227], MFTCoder[160]\\nphi-1.5[150], CodeShell[285], Magicoder[278]\\nAlphaCode 2[11], StableCode[210], WaveCoder[301]\\nphi-2[182], DeepSeek-Coder[88], StepCoder[71]\\nOpenCodeInterpreter[322], StarCoder 2[170]\\nClaude 3[14], ProCoder[27], CodeGemma[59]\\nCodeQwen[249], Llama3[180]\\nStarCoder2-Instruct[304], Codestral[181]\\nPre-training\\nTasks\\nCLM[88, 147, 173, 278], DAE[7, 269, 271], Auxiliary[40, 269, 271]\\nFine-tuning\\nInstruction\\nTuning\\n(Sec. 5.4)\\nFull Parameter\\nFine-tuning\\nCode Alpaca[43], CodeT5+[271], WizardCoder[173]\\nStarCoder[147], Pangu-Coder2[234], OctoPack[187]\\nCodeGeeX2[321], Magicoder[278], CodeGemma[59]\\nStarCoder2-instruct[304]\\nParameter\\nEfficient\\nFine-tuning\\nCodeUp[121], ASTRAIOS[334]\\nReinforcement\\nLearning\\nwith Feedback\\n(Sec. 5.5)\\nCodeRL[139], CompCoder[266], PPOCoder[238], RLTF[163]\\nPanGu-Coder2[234], StepCoder[71]'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 15, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Parameter\\nEfficient\\nFine-tuning\\nCodeUp[121], ASTRAIOS[334]\\nReinforcement\\nLearning\\nwith Feedback\\n(Sec. 5.5)\\nCodeRL[139], CompCoder[266], PPOCoder[238], RLTF[163]\\nPanGu-Coder2[234], StepCoder[71]\\nPrompting\\nEngineering\\n(Sec. 5.6)\\nReflexion[236], LATS[327], Self-Debugging[51], SelfEvolve[122]\\nTheo X. et al.[195], CodeT[45], LEVER[190], AlphaCodium[224]\\nRepository\\nLevel & Long\\nContext\\n(Sec. 5.7)\\nRepoCoder[309], CoCoMIC[69], RepoHyper[209], RLPG[240]\\nRepoformer[282], RepoFusion[239], ToolGen[259], CodePlan[22]\\nCodeS[308]\\nRetrieval\\nAugmented\\n(Sec. 5.8)\\nHGNN[166], REDCODER[205], ReACC[171], DocPrompting[330]\\nRepoCoder[309], Su et al.[242]\\nAutonomous\\nCoding Agents\\n(Sec. 5.9)\\nAgentCoder [104], MetaGPT[100], CodeAct [265], AutoCodeRover [316], Devin[61]\\nOpenDevin[199], SWE-agent[124], L2MAC[98], OpenDevin CodeAct 1.0[287]\\nEvaluation\\n(Sec. 5.10)\\nMetrics\\nExact Match, BLEU[203], ROUGE[156], METEOR[23], CodeBLEU[221], pass@k[48]'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 15, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='OpenDevin[199], SWE-agent[124], L2MAC[98], OpenDevin CodeAct 1.0[287]\\nEvaluation\\n(Sec. 5.10)\\nMetrics\\nExact Match, BLEU[203], ROUGE[156], METEOR[23], CodeBLEU[221], pass@k[48]\\nn@k[151], test case average[95], execution accuracy[218], pass@t[195], perplexity[116]\\nHuman\\nEvaluation\\nCodePlan[22], RepoFusion[239], CodeBLEU[221]\\nLLM-as-a-Judge\\nAlpacaEval[148], MT-bench[320], ICE-Score[332]\\nCode LLMs\\nAlignment\\n(Sec. 5.10.3)\\nGreen[235, 277], Responsibility[168, 292], Efficiency[293], Safety[8, 9, 77, 91, 231, 294, 302], Trustworthiness[120, 202]\\nApplication\\n(Sec. 5.12)\\nGitHub Copilot[48], CodeGeeX[321], CodeWhisperer[12], Codeium[60], CodeArts Snap[234], TabNine[246], Replit[222]\\nFig. 6. Taxonomy of LLMs for code generation.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 16, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:17\\nunlock their full potential, in alignment with established scaling law [97, 127]. For a general-purpose\\nLLM, amassing a large-scale corpus of natural language from a variety of sources is imperative.\\nSuch sources include webpages, conversation data, books and news, scientific data, and code\\n[20, 33, 54, 252, 253, 298], while these data are often crawled from the web and must undergo\\nmeticulous and aggressive pre-processing [217, 317]. Fortunately, multiple platforms and websites\\noffer large-scale, open-source, and permissively licensed code corpora, such as GitHub7 and Stack\\nOverflow8. Notably, the number of stars or forks of GitHub repositories has emerged as a valuable\\nmetric for filtering high-quality code datasets. In a similar vein, the quantity of votes on Stack\\nOverflow can serve to discern the most relevant and superior answers.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 16, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='metric for filtering high-quality code datasets. In a similar vein, the quantity of votes on Stack\\nOverflow can serve to discern the most relevant and superior answers.\\nNonetheless, raw datasets are frequently laden with redundant, noisy data and personal infor-\\nmation, eliciting concerns regarding privacy leakage, which may include the names and email\\naddresses of repository contributors [8, 37, 137]. Consequently, it is essential to undertake rigorous\\ndata-cleaning procedures. Typically, this process encompasses exact match deduplication, code\\ndata filtering based on average line length and a defined threshold for the fraction of alphanumeric\\ncharacters, the removal of auto-generated files through keyword searches, and the expunction of\\npersonal user data [132, 254]. Specifically, the standard data preprocessing workflow is depicted in\\nFigure 7.\\nThe development of a proficient LLM for code generation necessitates the utilization of various'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 16, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Figure 7.\\nThe development of a proficient LLM for code generation necessitates the utilization of various\\ntypes of code data at different developmental stages. Therefore, we categorize code data into three\\ndistinct classes: pre-training datasets, instruction-tuning datasets, and benchmarks for performance\\nevaluation. The subsequent subsections will provide a detailed illustration of code data within each\\nclassification.\\n5.1.1\\nPre-training. The remarkable success of bidirectional pre-trained language models (PLMs)\\nsuch as BERT [66] and unidirectional PLMs like GPT [213] has firmly established the practice of\\npre-training on large-scale unlabeled datasets to endow models with a broad spectrum of general\\nknowledge. Extending this principle to the realm of code generation enables LLMs to assimilate\\nfundamental coding principles, including the understanding of code structure dependencies, the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 16, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='knowledge. Extending this principle to the realm of code generation enables LLMs to assimilate\\nfundamental coding principles, including the understanding of code structure dependencies, the\\nsemantics of code identifiers, and the intrinsic logic of code sequences [48, 85, 269, 271]. In light of\\nthis advancement, there has been a proliferation of large-scale unlabeled code datasets proposed\\nto serve as the foundational training ground for LLMs to develop coding proficiency. A brief\\nintroduction of these datasets is as follows, with the statistics available in Table 4.\\n‚Ä¢ CodeSearchNet [110]: CodeSearchNet corpus is a comprehensive dataset, consisting of 2\\nmillion (comment, code) pairs from open-source repositories on GitHub. It includes code\\nand documentation in several programming languages including Go, Java, PHP, Python,\\nJavaScript, and Ruby. The dataset was primarily compiled to promote research into the\\nproblem of code retrieval using natural language.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 16, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='JavaScript, and Ruby. The dataset was primarily compiled to promote research into the\\nproblem of code retrieval using natural language.\\n‚Ä¢ Google BigQuery [96]: the Google BigQuery Public Datasets program offers a full snapshot\\nof the content of more than 2.8 million open source GitHub repositories in BigQuery.\\n‚Ä¢ The Pile [78]: the Pile is an 825 GiB diverse and open source language modeling dataset\\naggregating 22 smaller, high-quality datasets including GitHub, Books3, and Wikipedia (en).\\nIt aims to encompass text from as many modalities as possible, thereby facilitating the\\ndevelopment of models with broader generalization capabilities. For code generation, the\\nGitHub composite is specifically utilized.\\n‚Ä¢ CodeParrot [254]: the CodeParrot dataset contains Python files used to train the code genera-\\ntion model in Chapter 10: Training Transformers from Scratch in the ‚ÄúNLP with Transformers\\n7https://github.com\\n8https://stackoverflow.com'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 16, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='tion model in Chapter 10: Training Transformers from Scratch in the ‚ÄúNLP with Transformers\\n7https://github.com\\n8https://stackoverflow.com\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 17, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:18\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nQuality Filtering\\n‚Ä¢\\nProgramming Language \\n‚Ä¢\\nStatistic Number\\n‚Ä¢\\nMetric Threshold\\n‚Ä¢\\nKeyword Search\\nDe-duplication\\n‚Ä¢\\nExact Match\\n‚Ä¢\\nSimilarity Metrics\\n‚Ä¢\\nFunction Level\\nPrivacy Reduction\\n‚Ä¢\\nDetect Personally Identifiable \\nInformation (PII)\\n‚Ä¢\\nDelete PII\\nRaw Corpus\\nTokenization\\n‚Ä¢\\nOpen Source Tokenizer\\n‚Ä¢\\nSentencePiece\\n‚Ä¢\\nByte-level BPE\\nPre-training Database\\n# Sum numbers from 1 to 10 \\nand print the result\\ntotal = sum(range(1, 11))\\nprint(total)\\ntotal = 0\\nfor i in range(1, 11):\\ntotal += i\\ntotal = sum(range(1, 11))\\n# Copyright 2024 @ John\\n# Email: csjohn@gmail.com\\n# Institution: HKUST\\ninputs = tokenizer.encode([\"def \\nprint_hello_world():\",...], \\nreturn_tensors=\"pt\").to(\"cuda\")\\n[\\n[755, 1194, 97824, \\n32892, 4658],\\n[755, 4062, 18942, \\n11179, 997, 262, 4304, \\n10442, 264, 1160, 315, \\n5219, 304, 36488, 2015, \\n1701, 279, 17697, 6354, \\n371, 12384,...],...\\n]\\n. . .'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 17, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[\\n[755, 1194, 97824, \\n32892, 4658],\\n[755, 4062, 18942, \\n11179, 997, 262, 4304, \\n10442, 264, 1160, 315, \\n5219, 304, 36488, 2015, \\n1701, 279, 17697, 6354, \\n371, 12384,...],...\\n]\\n. . .\\nFig. 7. A diagram depicting the standard data preprocessing workflow utilized in the pre-training phase of\\nLLMs for code generation.\\nbook‚Äù [254]. Created with the GitHub dataset available via Google‚Äôs BigQuery, the CodeParrot\\ndataset includes approximately 22 million Python files and is 180 GB (50 GB compressed) big.\\n‚Ä¢ GitHub Code [254]: the GitHub Code dataset comprises 115M code files derived from GitHub,\\nspanning 32 programming languages and 60 extensions totaling 1TB of data. The dataset\\nwas created from the public GitHub dataset on Google BiqQuery.\\n‚Ä¢ ROOTS [137]: the BigScience ROOTS Corpus is a 1.6TB dataset spanning 59 languages that\\nwas used to train the 176B BigScience Large Open-science Open-access Multilingual (BLOOM)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 17, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ ROOTS [137]: the BigScience ROOTS Corpus is a 1.6TB dataset spanning 59 languages that\\nwas used to train the 176B BigScience Large Open-science Open-access Multilingual (BLOOM)\\nlanguage model. For the code generation task, the code subset of the ROOTS Corpus will be\\nspecifically utilized.\\n‚Ä¢ The Stack [132]: the Stack contains over 6TB of permissively licensed source code files that\\ncover 358 programming languages. The dataset was compiled as part of the BigCode Project,\\nan open scientific collaboration working on the responsible development of Large Language\\nModels for Code (Code LLMs).\\n‚Ä¢ The Stack v2 [170]: The Stack v2, a dataset created as part of the BigCode Project, contains\\nover 3B files across more than 600 programming and markup languages. The dataset is\\nderived from the Software Heritage archive9, the largest public archive of software source\\ncode and accompanying development history.\\n5.1.2'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 17, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='derived from the Software Heritage archive9, the largest public archive of software source\\ncode and accompanying development history.\\n5.1.2\\nInstruction Tuning. Instruction tuning refers to the process of supervised fine-tuning LLMs\\nusing a collection of datasets structured as various instructions, with the purpose of following a\\nwide range of task instructions [56, 200, 229, 274]. This method has demonstrated a considerable\\nimprovement in model performance and an enhanced ability to generalize to unseen tasks that the\\n9https://archive.softwareheritage.org\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 18, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:19\\nTable 4. The statistics of some commonly-used pre-training datasets for LLMs aimed at code generation. The\\ncolumn labeled ‚Äò#PL‚Äô indicates the number of programming languages included in each dataset. It should\\nbe noted that in the CodeSearchNet [110] dataset, each file represents a function, and for the Pile [78] and\\nROOTS [137] datasets, only the code components are considered.\\nDataset\\nSize (GB)\\nFiles (M)\\n#PL\\nDate\\nLink\\nCodeSearchNet [110]\\n20\\n6.5\\n6\\n2022-01\\nhttps://huggingface.co/datasets/code_search_net\\nGoogle BigQuery[96]\\n-\\n-\\n-\\n2016-06\\ngithub-on-bigquery-analyze-all-the-open-source-code\\nThe Pile [78]\\n95\\n19\\n-\\n2022-01\\nhttps://huggingface.co/datasets/EleutherAI/pile\\nCodeParrot [254]\\n180\\n22\\n1\\n2021-08\\nhttps://huggingface.co/datasets/transformersbook/codeparrot\\nGitHub Code[254]\\n1,024\\n115\\n32\\n2022-02\\nhttps://huggingface.co/datasets/codeparrot/github-code\\nROOTS [137]\\n163\\n15\\n13\\n2023-03\\nhttps://huggingface.co/bigscience-data'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 18, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='GitHub Code[254]\\n1,024\\n115\\n32\\n2022-02\\nhttps://huggingface.co/datasets/codeparrot/github-code\\nROOTS [137]\\n163\\n15\\n13\\n2023-03\\nhttps://huggingface.co/bigscience-data\\nThe Stack [132]\\n3,136\\n317\\n30\\n2022-10\\nhttps://huggingface.co/datasets/bigcode/the-stack\\nThe Stack v2 [170]\\n32K\\n3K\\n619\\n2024-04\\nhttps://huggingface.co/datasets/bigcode/the-stack-v2\\nTable 5. The statistics of several representative datasets used in instruction-tuning LLMs for code generation.\\nThe column labeled ‚Äò#PL‚Äô indicates the number of programming languages encompassed by each dataset.\\nDataset\\nSize\\n#PL\\nDate\\nLink\\nCodeAlpaca-20K [43]\\n20k\\n-\\n2023-03\\nhttps://huggingface.co/datasets/sahil2801/CodeAlpaca-20k\\nCommitPackFT [187]\\n2GB\\n277\\n2023-08\\nhttps://huggingface.co/datasets/bigcode/commitpackft\\nEvol-Instruct-Code-80k [225]\\n80k\\n-\\n2023-07\\nhttps://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1\\nevol-codealpaca-v1 [251]\\n110K\\n-\\n2023-07\\nhttps://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 18, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='80k\\n-\\n2023-07\\nhttps://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1\\nevol-codealpaca-v1 [251]\\n110K\\n-\\n2023-07\\nhttps://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1\\nMagicoder-OSS-Instruct-75k [278]\\n75k\\nPython, Shell,\\nTypeScript, C++,\\nRust, PHP, Java,\\nSwift, C#\\n2023-12\\nhttps://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K\\nSelf-OSS-Instruct-SC2-Exec-Filter-50k [304]\\n50k\\nPython\\n2024-04\\nhttps://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k\\nmodel has not previously encountered, as evidenced by recent studies [56, 200]. Leveraging the\\nbenefits of instruction tuning, instruction tuning has been expanded into coding domains, especially\\nfor code generation, which involves the automatic generation of the intended code from a natural\\nlanguage description. The promise of instruction tuning in this area has led numerous researchers\\nto develop large-scale instruction-tuning datasets tailored for code generation. Below, we provide an'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 18, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='language description. The promise of instruction tuning in this area has led numerous researchers\\nto develop large-scale instruction-tuning datasets tailored for code generation. Below, we provide an\\noverview of several notable datasets tailored for instruction tuning, with their respective statistics\\ndetailed in Table 5.\\n‚Ä¢ CodeAlpaca-20k [43]: CodeAlpaca-20k is a collection of 20K instruction-following data\\ngenerated using the data synthesis techniques termed Self-Instruct outlined in [268], with\\nmodifications for code generation, editing, and optimization tasks instead of general tasks.\\n‚Ä¢ CommitPackFT [187]: CommitPackFT is a 2GB refined version of CommitPack. It is filtered\\nto only include high-quality commit messages that resemble natural language instructions.\\n‚Ä¢ Evol-Instruct-Code-80k [225]: Evol-Instruct-Code-80k is an open-source implementation of\\nEvol-Instruct-Code described in the WizardCoder paper [173], which enhances the fine-tuning'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 18, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Evol-Instruct-Code-80k [225]: Evol-Instruct-Code-80k is an open-source implementation of\\nEvol-Instruct-Code described in the WizardCoder paper [173], which enhances the fine-tuning\\neffect of pre-trained code large models by adding complex code instructions.\\n‚Ä¢ Magicoder-OSS-Instruct-75k [278]: is a 75k synthetic data generated through OSS-Instruct\\nwith gpt-3.5-turbo-1106 and used to train both Magicoder and Magicoder-S series models.\\n‚Ä¢ Self-OSS-Instruct-SC2-Exec-Filter-50k [304]: Self-OSS-Instruct-SC2-Exec-Filter-50k is gen-\\nerated by StarCoder2-15B using the OSS-Instruct [278] data synthesis approach. It was\\nsubsequently used to fine-tune StarCoder-15B without any human annotations or distilled\\ndata from huge and proprietary LLMs.\\n5.1.3\\nBenchmarks. To rigorously assess the efficacy of LLMs for code generation, the research\\ncommunity has introduced a variety of high-quality benchmarks in recent years. Building on\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 19, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:20\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nthe foundational work by [48], numerous variations of the HumanEval dataset and additional\\nbenchmarks have emerged, aiming to evaluate a broader spectrum of code generation capabilities\\nin LLMs. We roughly divide these benchmarks into six distinct categories based on their application\\ncontexts, including general-purpose, competitive programming, data science, multilingual, logical\\nreasoning, and repository-level. It is important to highlight that logical reasoning encompasses math-\\nrelated benchmarks, as it aims to create ‚Äúcode-based solutions‚Äù for solving complex mathematical\\nproblems [50, 79, 326]. This strategy can therefore mitigate the limitations of LLMs in performing\\nintricate mathematical computations. The statistics for these benchmarks are presented in Table 6.\\nGeneral\\n‚Ä¢ HumanEval [48]: HumanEval comprises 164 manually scripted Python programming prob-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 19, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='intricate mathematical computations. The statistics for these benchmarks are presented in Table 6.\\nGeneral\\n‚Ä¢ HumanEval [48]: HumanEval comprises 164 manually scripted Python programming prob-\\nlems, each featuring a function signature, docstring, body, and multiple unit tests.\\n‚Ä¢ HumanEval+ [162]: HumanEval+ extends the original HumanEval [48] benchmark by in-\\ncreasing the scale of the test cases by 80 times. As the test cases increase, HumanEval+ can\\ncatch significant amounts of previously undetected incorrect code synthesized by LLMs.\\n‚Ä¢ HumanEvalPack [187]: expands HumanEval [48] by extending it to encompass three coding\\ntasks across six programming languages, namely code synthesis, code repair, and code\\nexplanation.\\n‚Ä¢ MBPP [17]: MBPP is a collection of approximately 974 Python programming problems, crowd-\\nsourced and designed for entry-level programmers. Each problem comes with an English\\ntask description, a code solution, and three automated test cases.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 19, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='sourced and designed for entry-level programmers. Each problem comes with an English\\ntask description, a code solution, and three automated test cases.\\n‚Ä¢ MBPP+ [162]: MBPP+ enhances MBPP [17] by eliminating ill-formed problems and rectifying\\nproblems with incorrect implementations. The test scale of MBPP+ is also expanded by 35\\ntimes for test augmentation.\\n‚Ä¢ CoNaLa [297]: CoNaLa contains almost 597K data samples for evaluating Python code\\ngeneration. The curated part of CoNaLa is crawled from Stack Overflow, automatically\\nfiltered, and then curated by annotators. The mined part of CoNaLais automatically mined,\\nwith almost 600k examples.\\n‚Ä¢ Spider [300]: Spider is large-scale complex text-to-SQL dataset covering 138 different domains.\\nIt has over 10K questions and 5.6K complex SQL queries on 200 databases. This dataset aims\\nto test a model‚Äôs ability to generalize to SQL queries, database schemas, and new domains.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 19, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='It has over 10K questions and 5.6K complex SQL queries on 200 databases. This dataset aims\\nto test a model‚Äôs ability to generalize to SQL queries, database schemas, and new domains.\\n‚Ä¢ CONCODE [113]: CONCODE is a dataset with over 100K samples consisting of Java classes\\nfrom public GitHub repositories. It provides near zero-shot conditions that can test the\\nmodel‚Äôs ability to generalize to unseen natural language tokens with unseen environments.\\n‚Ä¢ ODEX [273]: ODEX is an open-domain dataset focused on the execution-based generation\\nof Python code from natural language. It features 945 pairs of natural language queries and\\ntheir corresponding Python code, all extracted from StackOverflow forums.\\n‚Ä¢ CoderEval [299]: CoderEval is a pragmatic code generation benchmark that includes 230\\nPython and 230 Java code generation problems. It can be used to evaluate the model perfor-\\nmance in generating pragmatic code beyond just generating standalone functions.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 19, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Python and 230 Java code generation problems. It can be used to evaluate the model perfor-\\nmance in generating pragmatic code beyond just generating standalone functions.\\n‚Ä¢ ReCode [263]: Recode serves as a comprehensive robustness evaluation benchmark. ReCode\\napplies perturbations to docstrings, function and variable names, code syntax, and code\\nformat, thereby providing multifaceted assessments of a model‚Äôs robustness performance.\\n‚Ä¢ StudentEval [19]: StudentEval is a dataset of 1,749 prompts for 48 problems, authored by 80\\nstudents who have only completed a one-semester Python programming class. Unlike many\\nother benchmarks, it has multiple prompts per problem and multiple attempts by the same\\nparticipant, each problem is also accompanied by a set of instructor-written test cases.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 20, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:21\\n‚Ä¢ BigCodeBench [333]: BigCodeBench has 1,140 complex Python programming tasks, covering\\n723 function calls from 139 popular libraries across 7 domains. This benchmark is specifically\\ndesigned to assess LLMs‚Äô ability to call multiple functions from cross-domain libraries and\\nfollow complex instructions to solve programming tasks, helping to bridge the evaluation\\ngap between isolated coding exercises and the real-world programming scenario.\\n‚Ä¢ ClassEval [72]: ClassEval is a manually-crafted benchmark consisting of 100 classes and\\n412 methods for evaluating LLMs in the class-level code generation scenario. Particularly,\\nthe task samples of ClassEval present higher complexities, involving long code generation\\nand sophisticated docstring information, thereby benefiting the evaluation of the LLMs‚Äô\\ncapabilities in generating complicated code.\\n‚Ä¢ NaturalCodeBench [314]: NaturalCodeBench is a comprehensive code benchmark featuring'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 20, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='capabilities in generating complicated code.\\n‚Ä¢ NaturalCodeBench [314]: NaturalCodeBench is a comprehensive code benchmark featuring\\n402 high-quality problems in Python and Java. These problems are selected from natural\\nuser queries from online coding services and span 6 distinct domains, shaping an evaluation\\nenvironment aligned with real-world applications.\\nCompetitions\\n‚Ä¢ APPS [95]: The APPS benchmark is composed of 10K Python problems, spanning three levels\\nof difficulty: introductory, interview, and competition. Each entry in the dataset includes a\\nprogramming problem described in English, corresponding ground truth Python solutions,\\ntest cases defined by their inputs and outputs or function names if provided.\\n‚Ä¢ CodeContests [151]: CodeContests is a competitive programming dataset consisting of sam-\\nples from various sources including Aizu, AtCoder, CodeChef, Codeforces, and HackerEarth.\\nThe dataset encompasses programming problems accompanied by test cases in the form of'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 20, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ples from various sources including Aizu, AtCoder, CodeChef, Codeforces, and HackerEarth.\\nThe dataset encompasses programming problems accompanied by test cases in the form of\\npaired inputs and outputs, along with both correct and incorrect human solutions in multiple\\nprogramming languages.\\n‚Ä¢ LiveCodeBench [188]: LiveCodeBench is a comprehensive and contamination-free benchmark\\nfor evaluating a wide array of code-related capabilities of LLMs, including code generation,\\nself-repair, code execution, and test output prediction. It continuously gathers new coding\\nproblems from contests across three reputable competition platforms: LeetCode, AtCoder,\\nand CodeForces. The latest release of the dataset includes 713 problems that were released\\nbetween May 2023 and September 2024.\\nData Science\\n‚Ä¢ DSP [41]: DSP allows for model evaluation based on real data science pedagogical notebooks.\\nIt includes well-structured problems, along with unit tests to verify the correctness of solutions'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 20, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ DSP [41]: DSP allows for model evaluation based on real data science pedagogical notebooks.\\nIt includes well-structured problems, along with unit tests to verify the correctness of solutions\\nand a Docker environment for reproducible execution.\\n‚Ä¢ DS-1000 [136]: DS-1000 has 1K science questions from seven Python libraries, namely NumPy,\\nPandas, TensorFlow, PyTorch, SciPy, Scikit-learn, and Matplotlib. The DS-1000 benchmark\\nfeatures: (1) realistic problems with diverse contexts (2) implementation of multi-criteria\\nevaluation metrics, and (3) defense against memorization.\\n‚Ä¢ ExeDS [107]: ExeDS is a data science code generation dataset specifically designed for execu-\\ntion evaluation. It contains 534 problems with execution outputs from Jupyter Notebooks, as\\nwell as 123K examples for training and validation.\\nMultilingual\\n‚Ä¢ MBXP [16]: MBXP is a multilingual adaptation of the original MBPP [17] dataset. It is created'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 20, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='well as 123K examples for training and validation.\\nMultilingual\\n‚Ä¢ MBXP [16]: MBXP is a multilingual adaptation of the original MBPP [17] dataset. It is created\\nusing a framework that translates prompts and test cases from the original Python datasets\\ninto the corresponding data in the targeted programming language.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 21, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:22\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n‚Ä¢ Multilingual HumanEval [16]: Multilingual HumanEval is a dataset derived from HumanEval\\n[48]. It is designed to assess the performance of models in a multilingual context. It helps\\nuncover the generalization ability of the given model on languages that are out-of-domain.\\n‚Ä¢ HumanEval-X [321]: HumanEval-X is developed for evaluating the multilingual ability of\\ncode generation models with 820 hand-writing data samples in C++, Java, JavaScript, and Go.\\n‚Ä¢ MultiPL-E [39]: MultiPL-E is a dataset for evaluating LLMs for code generation across 18 pro-\\ngramming languages. It adopts the HumanEval [48] and the MBPP [17] Python benchmarks\\nand uses little compilers to translate them to other languages.\\n‚Ä¢ xCodeEval [128]: xCodeEval is an executable multilingual multitask benchmark consisting of\\n25M examples covering 17 programming languages. Its tasks include code understanding,\\ngeneration, translation, and retrieval.\\nReasoning'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 21, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='25M examples covering 17 programming languages. Its tasks include code understanding,\\ngeneration, translation, and retrieval.\\nReasoning\\n‚Ä¢ MathQA-X [16] MathQA-X is the multilingual version of MathQA [13]. It is generated by\\nutilizing a conversion framework that converts samples from Python datasets into the target\\nlanguage.\\n‚Ä¢ MathQA-Python [17] MathQA-Python is a Python version of the MathQA benchmark[13].\\nThe benchmark, containing more than 23K problems, is designed to assess the capability of\\nmodels to synthesize code from complex textual descriptions.\\n‚Ä¢ GSM8K [58]: GSM8K is a dataset of 8.5K linguistically diverse grade school math problems.\\nThe dataset is crafted to facilitate the task of question answering on basic mathematical\\nproblems that requires multi-step reasoning.\\n‚Ä¢ GSM-HARD [79]: GSM-HARD is a more challenging version of the GSM8K [58] dataset. It\\nreplaces the numbers in the GSM8K questions with larger, less common numbers, thereby'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 21, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ GSM-HARD [79]: GSM-HARD is a more challenging version of the GSM8K [58] dataset. It\\nreplaces the numbers in the GSM8K questions with larger, less common numbers, thereby\\nincreasing the complexity and difficulty level of the problems.\\n‚Ä¢ CRUXEval [82]: CRUXEval contains 800 Python functions, each paired with an input-output\\nexample. This benchmark supports two tasks: input prediction and output prediction, designed\\nto evaluate the code reasoning, understanding, and execution capabilities of code LLMs.\\nRepository\\n‚Ä¢ RepoEval [309]: RepoEval enables the evaluation of repository-level code completion. It can\\noffer different levels of granularity and improved evaluation accuracy through the use of unit\\ntests.\\n‚Ä¢ Stack-Repo [239]: Stack-Repo is a dataset of 200 Java repositories from GitHub with near-\\ndeduplicated files. These files are augmented with three types of repository contexts: prompt\\nproposal contexts, BM25 Contexts (based on BM25 similarity scores), and RandomNN Con-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 21, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='deduplicated files. These files are augmented with three types of repository contexts: prompt\\nproposal contexts, BM25 Contexts (based on BM25 similarity scores), and RandomNN Con-\\ntexts (obtained using the nearest neighbors in the representation space of an embedding\\nmodel).\\n‚Ä¢ Repobench [167]: Repobench is a benchmark specifically used for evaluating repository-\\nlevel code auto-completion systems. Supporting both Python and Java, it consists of three\\ninterconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion),\\nand RepoBench-P (Pipeline).\\n‚Ä¢ EvoCodeBench [144]: EvoCodeBench is an evolutionary code generation benchmark, con-\\nstructed through a rigorous pipeline and aligned with real-world repositories. This benchmark\\nalso provides comprehensive annotations and robust evaluation metrics.\\n‚Ä¢ SWE-bench [123]: SWE-bench is a dataset that tests a model‚Äôs ability to automatically solve'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 21, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='also provides comprehensive annotations and robust evaluation metrics.\\n‚Ä¢ SWE-bench [123]: SWE-bench is a dataset that tests a model‚Äôs ability to automatically solve\\nGitHub issues. The dataset has 2,294 Issue-Pull Request pairs from 12 popular Python reposi-\\ntories.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 22, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:23\\nTable 6. The detailed statistics of commonly-used benchmarks used in evaluating LLMs for code generation.\\nThe column labeled ‚Äò#PL‚Äô indicates the number of programming languages included in each dataset. For the\\nsake of brevity, we list the programming languages (PLs) for benchmarks that support fewer than or include\\nfive PLs. For benchmarks with six or more PLs, we provide only a numerical count of the PLs supported.\\nScenario\\nBenchmark\\nSize\\n#PL\\nDate\\nLink\\nGeneral\\nHumanEval [48]\\n164\\nPython\\n2021-07\\nhttps://huggingface.co/datasets/openai_humaneval\\nHumanEval+ [162]\\n164\\nPython\\n2023-05\\nhttps://huggingface.co/datasets/evalplus/humanevalplus\\nHumanEvalPack [187]\\n164\\n6\\n2023-08\\nhttps://huggingface.co/datasets/bigcode/humanevalpack\\nMBPP [17]\\n974\\nPython\\n2021-08\\nhttps://huggingface.co/datasets/mbpp\\nMBPP+ [162]\\n378\\nPython\\n2023-05\\nhttps://huggingface.co/datasets/evalplus/mbppplus\\nCoNaLa [297]\\n596.88K\\nPython\\n2018-05'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 22, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='MBPP [17]\\n974\\nPython\\n2021-08\\nhttps://huggingface.co/datasets/mbpp\\nMBPP+ [162]\\n378\\nPython\\n2023-05\\nhttps://huggingface.co/datasets/evalplus/mbppplus\\nCoNaLa [297]\\n596.88K\\nPython\\n2018-05\\nhttps://huggingface.co/datasets/neulab/conala\\nSpider [300]\\n8,034\\nSQL\\n2018-09\\nhttps://huggingface.co/datasets/xlangai/spider\\nCONCODE [113]\\n104K\\nJava\\n2018-08\\nhttps://huggingface.co/datasets/AhmedSSoliman/CONCOD\\nODEX [273]\\n945\\nPython\\n2022-12\\nhttps://huggingface.co/datasets/neulab/odex\\nCoderEval [299]\\n460\\nPython, Java\\n2023-02\\nhttps://github.com/CoderEval/CoderEval\\nReCode [263]\\n1,138\\nPython\\n2022-12\\nhttps://github.com/amazon-science/recode\\nStudentEval [19]\\n1,749\\nPython\\n2023-06\\nhttps://huggingface.co/datasets/wellesley-easel/StudentEval\\nBigCodeBench [333]\\n1,140\\nPython\\n2024-06\\nhttps://huggingface.co/datasets/bigcode/bigcodebench\\nClassEval [72]\\n100\\nPython\\n2023-08\\nhttps://huggingface.co/datasets/FudanSELab/ClassEval\\nNaturalCodeBench [314]\\n402\\nPython, Java\\n2024-05\\nhttps://github.com/THUDM/NaturalCodeBench'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 22, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ClassEval [72]\\n100\\nPython\\n2023-08\\nhttps://huggingface.co/datasets/FudanSELab/ClassEval\\nNaturalCodeBench [314]\\n402\\nPython, Java\\n2024-05\\nhttps://github.com/THUDM/NaturalCodeBench\\nCompetitions\\nAPPS [95]\\n10,000\\nPython\\n2021-05\\nhttps://huggingface.co/datasets/codeparrot/apps\\nCodeContests [151]\\n13,610\\nC++, Python,\\nJava\\n2022-02\\nhttps://huggingface.co/datasets/deepmind/code_contests\\nLiveCodeBench [188]\\n713\\nUpdating\\nPython\\n2024-03\\nhttps://github.com/LiveCodeBench/LiveCodeBench\\nData Science\\nDSP [41]\\n1,119\\nPython\\n2022-01\\nhttps://github.com/microsoft/DataScienceProblems\\nDS-1000 [136]\\n1,000\\nPython\\n2022-11\\nhttps://huggingface.co/datasets/xlangai/DS-1000\\nExeDS [107]\\n534\\nPython\\n2022-11\\nhttps://github.com/Jun-jie-Huang/ExeDS\\nMultilingual\\nMBXP [16]\\n12.4K\\n13\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/mbxp\\nMultilingual HumanEval [16]\\n1.9K\\n12\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/multi-humaneval\\nHumanEval-X [321]\\n820\\nPython, C++,\\nJava, JavaScript,\\nGo\\n2023-03'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 22, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Multilingual HumanEval [16]\\n1.9K\\n12\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/multi-humaneval\\nHumanEval-X [321]\\n820\\nPython, C++,\\nJava, JavaScript,\\nGo\\n2023-03\\nhttps://huggingface.co/datasets/THUDM/humaneval-x\\nMultiPL-E [39]\\n161\\n18\\n2022-08\\nhttps://huggingface.co/datasets/nuprl/MultiPL-E\\nxCodeEval [128]\\n5.5M\\n11\\n2023-03\\nhttps://github.com/ntunlp/xCodeEval\\nReasoning\\nMathQA-X [16]\\n5.6K\\nPython, Java,\\nJavaScript\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/mathqa-x\\nMathQA-Python [17]\\n23,914\\nPython\\n2021-08\\nhttps://github.com/google-research/google-research\\nGSM8K [58]\\n8.5K\\nPython\\n2021-10\\nhttps://huggingface.co/datasets/gsm8k\\nGSM-HARD [79]\\n1.32K\\nPython\\n2022-11\\nhttps://huggingface.co/datasets/reasoning-machines/gsm-hard\\nCRUXEval [82]\\n800\\nPython\\n2024-01\\nhttps://huggingface.co/datasets/cruxeval-org/cruxeval\\nRepository\\nRepoEval [309]\\n3,573\\nPython, Java\\n2023-03\\nhttps://paperswithcode.com/dataset/repoeval\\nStack-Repo [239]\\n200\\nJava\\n2023-06\\nhttps://huggingface.co/datasets/RepoFusion/Stack-Repo'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 22, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Repository\\nRepoEval [309]\\n3,573\\nPython, Java\\n2023-03\\nhttps://paperswithcode.com/dataset/repoeval\\nStack-Repo [239]\\n200\\nJava\\n2023-06\\nhttps://huggingface.co/datasets/RepoFusion/Stack-Repo\\nRepobench [167]\\n27k\\nPython, Java\\n2023-01\\nhttps://github.com/Leolty/repobench\\nEvoCodeBench [144]\\n275\\nPython\\n2024-03\\nhttps://huggingface.co/datasets/LJ0815/EvoCodeBench\\nSWE-bench [123]\\n2,294\\nPython\\n2023-10\\nhttps://huggingface.co/datasets/princeton-nlp/SWE-bench\\nCrossCodeEval [68]\\n10K\\nPython, Java,\\nTypeScript, C#\\n2023-10\\nhttps://github.com/amazon-science/cceval\\nSketchEval [308]\\n20,355\\nPython\\n2024-03\\nhttps://github.com/nl2code/codes\\n‚Ä¢ CrossCodeEval [68]: CrossCodeEval is a diverse and multilingual scope completion dataset\\ncovering four languages: Python, Java, TypeScript, and C#. This benchmark tests the model‚Äôs\\nability to understand in-depth cross-file information and accurately complete the code.\\n‚Ä¢ SketchEval [308]: SketchEval is a repository-oriented benchmark that encompasses data from'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 22, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ability to understand in-depth cross-file information and accurately complete the code.\\n‚Ä¢ SketchEval [308]: SketchEval is a repository-oriented benchmark that encompasses data from\\n19 repositories, each varying in complexity. In addition to the dataset, SketchEval introduces\\na metric, known as SketchBLEU, to measure the similarity between two repositories based\\non their structures and semantics.\\n5.2\\nData Synthesis\\nNumerous studies have demonstrated that high-quality datasets are integral to enhancing the\\nperformance of LLMs in various downstream tasks [33, 133, 179, 280, 286, 328]. For instance, the\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 23, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:24\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nSeed Problem\\n<Problem, Solution>\\nEvolved Problem\\nSelf-Instruct\\nEvol-Instruct\\nSolution\\nSeed Problem\\nEvolution Type\\n<Debiased Problem, Solution>\\nOSS-Instruct\\nSeed Code Snippet\\nùêøùêøùëÄ\\nùêøùêøùëÄ\\nùêøùêøùëÄ\\nFig. 8. The comparison among three representative data synthesis methods used for generating instruction\\ndata with LLMs. The Code Alpaca [43] employs the self-instruct method, whereas WizardCoder [173] and\\nMagicoder [278] utilize the Evol-Instruct and OSS-Instruct methods, respectively.\\nLIMA model, a 65B parameter LLaMa language model fine-tuned with a standard supervised loss\\non a mere 1,000 meticulously curated prompts and responses, achieved performance on par with, or\\neven superior to, GPT-4 in 43% of evaluated cases. This figure rose to 58% when compared to Bard\\nand 65% against DaVinci003, all without the use of reinforcement learning or human preference'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 23, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='even superior to, GPT-4 in 43% of evaluated cases. This figure rose to 58% when compared to Bard\\nand 65% against DaVinci003, all without the use of reinforcement learning or human preference\\nmodeling [328]. The QuRating initiative strategically selects pre-training data embodying four key\\ntextual qualities ‚Äî writing style, facts & trivia, required expertise, and educational value ‚Äî that\\nresonate with human intuition. Training a 1.3B parameter model on such data resulted in reduced\\nperplexity and stronger in-context learning compared to baseline models [280].\\nDespite these advancements, acquiring quality data remains a significant challenge due to issues\\nsuch as data scarcity, privacy concerns, and prohibitive costs [165, 268]. Human-generated data is\\noften labor-intensive and expensive to produce, and it may lack the necessary scope and detail to\\nnavigate complex, rare, or ambiguous scenarios. As a resolution to these challenges, synthetic data'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 23, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='often labor-intensive and expensive to produce, and it may lack the necessary scope and detail to\\nnavigate complex, rare, or ambiguous scenarios. As a resolution to these challenges, synthetic data\\nhas emerged as a viable alternative. By generating artificial datasets that replicate the intricacies\\nof real-world information, models such as GPT-3.5-turbo [196] and GPT-4 [5] have enabled the\\ncreation of rich datasets without the need for human annotation [92, 138, 165, 268]. This approach\\nis particularly beneficial in enhancing the instruction-following capabilities of LLMs, with a focus\\non generating synthetic instruction-based data.\\nA notable example of this approach is the Self-Instruct [268] framework, which employs an off-the-\\nshelf language model to generate a suite of instructions, inputs, and outputs. This data is then refined\\nby removing invalid or redundant entries before being used to fine-tune the model. The empirical'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 23, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='by removing invalid or redundant entries before being used to fine-tune the model. The empirical\\nevidence supports the efficacy of this synthetic data generation methodology. Building upon this\\nconcept, the Alpaca [247] model, fine-tuned on 52k pieces of instruction-following data from a 7B\\nparameter LLaMa [252] model, exhibits performance comparable to the text-davinci-003 model.\\nWizardLM [289] introduced the Evol-Instruct technique, which incrementally transforms simple\\ninstructions into more complex variants. The fine-tuned LLaMa model using this technique has\\nshown promising results in comparison to established proprietary LLMs such as ChatGPT [196] and\\nGPT-4 [5], to some extent. Moreover, Microsoft has contributed to this field with their Phi series of\\nmodels, predominantly trained on synthetic high-quality data, which includes Phi-1 (1.3B) [84]\\nfor Python coding, Phi-1.5 (1.3B) [150] for common sense reasoning and language understanding,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 23, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='models, predominantly trained on synthetic high-quality data, which includes Phi-1 (1.3B) [84]\\nfor Python coding, Phi-1.5 (1.3B) [150] for common sense reasoning and language understanding,\\nPhi-2 (2.7B) [182] for advanced reasoning and language understanding, and Phi-3 (3.8B) [4] for\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 24, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:25\\ngeneral purposes. These models have consistently outperformed larger counterparts across various\\nbenchmarks, demonstrating the efficacy of synthetic data in model training.\\nDrawing on the successes of data synthesis for general-purpose LLMs, researchers have expanded\\nthe application of synthetic data to the realm of code generation. The Code Alpaca model, as de-\\nscribed in [43], has been fine-tuned on a 7B and 13B LLaMA model using a dataset of 20k instruction-\\nfollowing examples for code generation. This dataset was created by text-davinci-00310 and\\nemployed the Self-Instruct technique [268]. Building on this, the WizardCoder 15B [173] utilizes\\nthe Evol-Instruct technique to create an enhanced dataset of 78k evolved code instruction examples.\\nThis dataset originates from the initial 20k instruction-following dataset used by Code Alpaca\\n[43], which was also generated by text-davinci-003. The WizardCoder model, fine-tuned on'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 24, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='This dataset originates from the initial 20k instruction-following dataset used by Code Alpaca\\n[43], which was also generated by text-davinci-003. The WizardCoder model, fine-tuned on\\nthe StarCoder [147] base model, achieved a 57.3% pass@1 on the HumanEval benchmarks. This\\nperformance not only surpasses all other open-source Code LLMs by a significant margin but also\\noutperforms leading closed LLMs such as Anthropic‚Äôs Claude and Google‚Äôs Bard. In a similar vein,\\nMagicoder [278] introduces a novel data synthesis approach termed OSS-INSTRUCT which enlight-\\nens LLMs with open-source code snippets to generate high-quality instruction data for coding tasks.\\nIt aims to address the inherent biases often present in synthetic data produced by LLMs. Building\\nupon CodeLlama [227], the MagicoderS-CL-7B model ‚Äî fine-tuned with 75k synthetic instruction\\ndata using the OSS-INSTRUCT technique and with gpt-3.5-turbo-1106 as the data generator ‚Äî'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 24, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='upon CodeLlama [227], the MagicoderS-CL-7B model ‚Äî fine-tuned with 75k synthetic instruction\\ndata using the OSS-INSTRUCT technique and with gpt-3.5-turbo-1106 as the data generator ‚Äî\\nhas outperformed the prominent ChatGPT on the HumanEval Plus benchmark, achieving pass@1\\nof 66.5% versus 65.9%. In a noteworthy development, Microsoft has introduced the phi-1 model [84],\\na more compact LLM of only 1.3B parameters. Despite its smaller size, phi-1 has been trained on\\nhigh-quality textbook data sourced from the web (comprising 6 billion tokens) and supplemented\\nwith synthetic textbooks and exercises generated with GPT-3.5 (1 billion tokens). It has achieved\\npass@1 of 50.6% on HumanEval and 55.5% on MBPP, setting a new state-of-the-art for Python\\ncoding performance among existing small language models (SLMs). The latest contribution to\\nthis field is from the BigCode team, which has presented StarCoder2-15B-instruct [304], the first'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 24, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='coding performance among existing small language models (SLMs). The latest contribution to\\nthis field is from the BigCode team, which has presented StarCoder2-15B-instruct [304], the first\\nentirely self-aligned code LLM trained with a transparent and permissive pipeline. This model\\naligns closely with the OSS-INSTRUCT principles established by Magicoder, generating instructions\\nbased on seed functions filtered from the Stack v1 dataset [132] and producing responses through\\nself-validation. Unlike Magicoder, StarCoder2-15B-instruct employs its base model, StarCoder2-15B,\\nas the data generator, thus avoiding reliance on large and proprietary LLMs like GPT-3.5-turbo\\n[196]. Figure 8 illustrates the comparison between Self-Instruct, Evol-Instruct, and OSS-Instruct\\ndata synthesis methods.\\nWhile synthetic data has demonstrated its potential across both small- and large-scale LMs for a\\nvariety of general and specialized tasks, including code generation, it also poses several challenges'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 24, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='While synthetic data has demonstrated its potential across both small- and large-scale LMs for a\\nvariety of general and specialized tasks, including code generation, it also poses several challenges\\nthat must be addressed. These challenges include a lack of data diversity [280], the need to ensure\\nthe factuality and fidelity of the information [256, 281], and the potential to amplify existing biases\\nor introduce new ones [24, 89].\\n5.3\\nPre-Training\\n5.3.1\\nModel Architectures. Since the inception of the Transformer architecture for machine transla-\\ntion [257], it has become the de facto backbone for a multitude of LLMs that address a wide range of\\ndownstream tasks. The Transformer and its derivatives owe their prominence to their exceptional\\nability to parallelize computation and their powerful representational capacities [298, 319]. Through\\ninnovative scaling techniques, such as Mixture-of-Experts (MoE) [35, 233] and Depth-Up-Scaling'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 24, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ability to parallelize computation and their powerful representational capacities [298, 319]. Through\\ninnovative scaling techniques, such as Mixture-of-Experts (MoE) [35, 233] and Depth-Up-Scaling\\n(DUS) [130], the capacity of Transformer-based LLMs has expanded to encompass hundreds of\\n10https://platform.openai.com\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 25, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:26\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 7. The overview of LLMs with encoder-decoder architectures for code generation.\\nModel\\nInstitution\\nSize\\nVocabulary\\nContext\\nWindow\\nDate\\nOpen Source\\nPyMT5[57]\\nMicrosoft\\n374M\\n50K\\n1024+1024\\n2020-10\\nPLBART[7]\\nUCLA\\n140M\\n50K\\n1024+1024\\n2021-03\\n\"\\nCodeT5 [271]\\nSalesforce\\n60M, 220M, 770M\\n32K\\n512+256\\n2021-09\\n\"\\nJuPyT5[41]\\nMicrosoft\\n350M\\n50K\\n1024+1024\\n2022-01\\nAlphaCode[151]\\nDeepMind\\n284M, 1.1B, 2.8B,\\n8.7B, 41.1B\\n8K\\n1536+768\\n2022-02\\nCodeRL[139]\\nSalesforce\\n770M\\n32K\\n512+256\\n2022-06\\n\"\\nERNIE-Code[40]\\nBaidu\\n560M\\n250K\\n1024+1024\\n2022-12\\n\"\\nPPOCoder[238]\\nVirginia Tech\\n770M\\n32K\\n512+256\\n2023-01\\nCodeT5+[269]\\nSalesforce\\n220M, 770M, 2B,\\n6B, 16B\\n50K\\n2048+2048\\n2023-05\\n\"\\nCodeFusion[241]\\nMicrosoft\\n75M\\n32k\\n128+128\\n2023-10\\n\"\\nAST-T5[81]\\nUC Berkeley\\n226M\\n32k\\n512+200/300\\n2024-01\\n\"\\nbillions or even trillions of parameters. These scaled-up models have exhibited a range of emergent'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 25, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Microsoft\\n75M\\n32k\\n128+128\\n2023-10\\n\"\\nAST-T5[81]\\nUC Berkeley\\n226M\\n32k\\n512+200/300\\n2024-01\\n\"\\nbillions or even trillions of parameters. These scaled-up models have exhibited a range of emergent\\nabilities [97, 127, 275], such as instruction following [200], in-context learning [70], and step-by-step\\nreasoning [105, 276] that were previously unforeseen.\\nIn the domain of code generation using LLMs, the architecture of contemporary models generally\\nfalls into one of two categories: encoder-decoder models, such as CodeT5 [271], CodeT5+ [269],\\nand CodeRL [139]; or decoder-only models, such as Codex [48], StarCoder [147], Code Llama [227],\\nand CodeGemma [59]. These architectures are depicted in Figure 2(b) and (c), respectively. For a\\ncomprehensive overview, Table 7 details the encoder-decoder architectures, while Table 8 focuses\\non the decoder-only models utilized in code generation.\\n5.3.2\\nPre-training Tasks. In the initial phase, language models for code generation are typically'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 25, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='on the decoder-only models utilized in code generation.\\n5.3.2\\nPre-training Tasks. In the initial phase, language models for code generation are typically\\ntrained from scratch using datasets consisting of manually annotated pairs of natural language\\ndescriptions and corresponding code snippets, within a supervised learning framework. However,\\nmanual annotation is not only laborious and time-consuming, but the efficacy of the resulting\\nmodels is also constrained by both the volume and the quality of the available annotated data. This\\nlimitation is especially pronounced in the context of low-resource programming languages, such\\nas Swahili and Yoruba, where annotated examples are scarce [38, 46]. In light of these challenges,\\nthere has been a shift towards an alternative training strategy that involves pre-training models on\\nextensive and unlabelled code corpora. This method is aimed at imbuing the models with a broad'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 25, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='there has been a shift towards an alternative training strategy that involves pre-training models on\\nextensive and unlabelled code corpora. This method is aimed at imbuing the models with a broad\\nunderstanding of programming knowledge, encompassing elements like identifiers, code structure,\\nand underlying semantics [48]. In this regard, two pre-training tasks have gained prominence\\nfor their effectiveness, namely Causal Language Modeling (CLM), also known as unidirectional\\nlanguage modeling or next-token prediction, and Denoising Autoencoding (DAE). The CLM task\\ncan be applied to both decoder-only and encoder-decoder model architectures, while DAE tasks are\\nspecifically designed for encoder-decoder frameworks. It should also be noted that there is a variety\\nof additional auxiliary pre-training tasks that can further enhance model performance. These\\ninclude Masked Identifier Prediction, Identifier Tagging, Bimodal Dual Generation [271], Text-Code'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 25, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='of additional auxiliary pre-training tasks that can further enhance model performance. These\\ninclude Masked Identifier Prediction, Identifier Tagging, Bimodal Dual Generation [271], Text-Code\\nMatching, and Text-Code Contrastive Learning [269]. These tasks contribute to a more nuanced\\nand comprehensive pre-training process, equipping the models with the capabilities necessary to\\nhandle a wide range of code generation scenarios.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 26, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:27\\nTable 8. The overview of LLMs with decoder-only architectures for code generation.\\nModel\\nInstitution\\nSize\\nVocabulary\\nContext\\nWindow\\nDate\\nOpen Source\\nGPT-C [244]\\nMicrosoft\\n366M\\n60K\\n1024\\n2020-05\\nCodeGPT [172]\\nMicrosoft\\n124M\\n50K\\n1024\\n2021-02\\n\"\\nGPT-Neo[30]\\nEleutherAI\\n125M, 1.3B, 2.7B\\n50k\\n2048\\n2021-03\\n\"\\nGPT-J [258]\\nEleutherAI\\n6B\\n50k\\n2048\\n2021-05\\n\"\\nCodex [48]\\nOpenAI\\n12M, 25M, 42M,\\n85M, 300M, 679M,\\n2.5B, 12B\\n-\\n4096\\n2021-07\\nCodeParrot [254]\\nHugging Face\\n110M, 1.5B\\n33k\\n1024\\n2021-11\\n\"\\nPolyCoder [290]\\nCMU\\n160M, 400M, 2.7B\\n50k\\n2048\\n2022-02\\n\"\\nCodeGen [193]\\nSalesforce\\n350M, 2.7B, 6.1B,\\n16.1B\\n51k\\n2048\\n2022-03\\n\"\\nGPT-NeoX [29]\\nEleutherAI\\n20B\\n50k\\n2048\\n2022-04\\n\"\\nPaLM-Coder [54]\\nGoogle\\n8B, 62B, 540B\\n256k\\n2048\\n2022-04\\nInCoder [77]\\nMeta\\n1.3B, 6.7B\\n50k\\n2049\\n2022-04\\n\"\\nPanGu-Coder [55]\\nHuawei\\n317M, 2.6B\\n42k\\n1024\\n2022-07\\nPyCodeGPT [306]\\nMicrosoft\\n110M\\n32k\\n1024\\n2022-06\\n\"\\nCodeGeeX [321]\\nTsinghua\\n13B\\n52k\\n2048\\n2022-09\\n\"\\nBLOOM [140]\\nBigScience\\n176B\\n251k\\n-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 26, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='2049\\n2022-04\\n\"\\nPanGu-Coder [55]\\nHuawei\\n317M, 2.6B\\n42k\\n1024\\n2022-07\\nPyCodeGPT [306]\\nMicrosoft\\n110M\\n32k\\n1024\\n2022-06\\n\"\\nCodeGeeX [321]\\nTsinghua\\n13B\\n52k\\n2048\\n2022-09\\n\"\\nBLOOM [140]\\nBigScience\\n176B\\n251k\\n-\\n2022-11\\n\"\\nChatGPT [196]\\nOpenAI\\n-\\n-\\n16k\\n2022-11\\n\"\\nSantaCoder [9]\\nHugging Face\\n1.1B\\n49k\\n2048\\n2022-12\\n\"\\nLLaMA [252]\\nMeta\\n6.7B, 13.0B, 32.5B,\\n65.2B\\n32K\\n2048\\n2023-02\\n\"\\nGPT-4 [5]\\nOpenAI\\n-\\n-\\n32K\\n2023-03\\nCodeGen2 [192]\\nSalesforce\\n1B, 3.7B, 7B, 16B\\n51k\\n2048\\n2023-05\\n\"\\nreplit-code [223]\\nreplit\\n3B\\n33k\\n2048\\n2023-05\\n\"\\nStarCoder [147]\\nHugging Face\\n15.5B\\n49k\\n8192\\n2023-05\\n\"\\nWizardCoder [173]\\nMicrosoft\\n15B, 34B\\n49k\\n8192\\n2023-06\\n\"\\nphi-1 [84]\\nMicrosoft\\n1.3B\\n51k\\n2048\\n2023-06\\n\"\\nCodeGeeX2 [321]\\nTsinghua\\n6B\\n65k\\n8192\\n2023-07\\n\"\\nPanGu-Coder2 [234]\\nHuawei\\n15B\\n42k\\n1024\\n2023-07\\nLlama 2 [253]\\nMeta\\n7B, 13B, 70B\\n32K\\n4096\\n2023-07\\n\"\\nOctoCoder [187]\\nHugging Face\\n15.5B\\n49k\\n8192\\n2023-08\\n\"\\nCode Llama [227]\\nMeta\\n7B, 13B, 34B\\n32k\\n16384\\n2023-08\\n\"\\nCodeFuse [160]\\nAnt Group\\n350M, 13B, 34B\\n101k\\n4096\\n2023-09\\n\"\\nphi-1.5 [150]\\nMicrosoft'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 26, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='\"\\nOctoCoder [187]\\nHugging Face\\n15.5B\\n49k\\n8192\\n2023-08\\n\"\\nCode Llama [227]\\nMeta\\n7B, 13B, 34B\\n32k\\n16384\\n2023-08\\n\"\\nCodeFuse [160]\\nAnt Group\\n350M, 13B, 34B\\n101k\\n4096\\n2023-09\\n\"\\nphi-1.5 [150]\\nMicrosoft\\n1.3B\\n51k\\n2048\\n2023-09\\n\"\\nCodeShell [285]\\nPeking University\\n7B\\n70k\\n8192\\n2023-10\\n\"\\nMagicoder [278]\\nUIUC\\n7B\\n32k\\n16384\\n2023-12\\n\"\\nAlphaCode 2 [11]\\nGoogle DeepMind\\n-\\n-\\n-\\n2023-12\\nStableCode [210]\\nStabilityAI\\n3B\\n50k\\n16384\\n2024-01\\n\"\\nWaveCoder [301]\\nMicrosoft\\n6.7B\\n32k\\n16384\\n2023-12\\n\"\\nphi-2 [182]\\nMicrosoft\\n2.7B\\n51k\\n2048\\n2023-12\\n\"\\nDeepSeek-Coder [88]\\nDeepSeek\\n1.3B, 6.7B, 33B\\n32k\\n16384\\n2023-11\\n\"\\nStarCoder 2 [170]\\nHugging Face\\n15B\\n49k\\n16384\\n2024-02\\n\"\\nClaude 3 [14]\\nAnthropic\\n-\\n-\\n200K\\n2024-03\\nCodeGemma [59]\\nGoogle\\n2B, 7B\\n25.6k\\n8192\\n2024-04\\n\"\\nCode-Qwen [249]\\nQwen Group\\n7B\\n92K\\n65536\\n2024-04\\n\"\\nLlama3 [180]\\nMeta\\n8B, 70B\\n128K\\n8192\\n2024-04\\n\"\\nStarCoder2-Instruct [304]\\nHugging Face\\n15.5B\\n49K\\n16384\\n2024-04\\n\"\\nCodestral [181]\\nMistral AI\\n22B\\n33k\\n32k\\n2024-05\\n\"'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 26, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Qwen Group\\n7B\\n92K\\n65536\\n2024-04\\n\"\\nLlama3 [180]\\nMeta\\n8B, 70B\\n128K\\n8192\\n2024-04\\n\"\\nStarCoder2-Instruct [304]\\nHugging Face\\n15.5B\\n49K\\n16384\\n2024-04\\n\"\\nCodestral [181]\\nMistral AI\\n22B\\n33k\\n32k\\n2024-05\\n\"\\nCausal Language Modeling. In decoder-only LLMs, given a sequence of tokens x = {ùë•1, . . . ,ùë•ùëõ},\\nthe CLM task refers to autoregressively predict the target tokens ùë•ùëñbased on the preceding tokens\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 27, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:28\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nùë•<ùëñin a sequence. The causal language modeling objective for training decoder LLMs is to minimize\\nthe following likelihood:\\nLùê∑ùëíùëêùëúùëëùëíùëü‚àíùëúùëõùëôùë¶\\nùê∂ùêøùëÄ\\n(x) = ‚àílog(\\nùëõ\\n√ñ\\nùëñ=1\\nùëÉùúÉ(ùë•ùëñ| x<ùëñ)) =\\nùëõ\\n‚àëÔ∏Å\\nùëñ=1\\n‚àílog ùëÉùúÉ(ùë•ùëñ| x<ùëñ)\\n(16)\\nwhere x<ùëñrepresents the sequence of preceding tokens {ùë•1, . . . ,ùë•ùëñ‚àí1} before xùëñin the input, ùúÉ\\ndenotes the model parameters. The conditional probability ùëÉùúÉ(ùë•ùëñ|x<ùëñ)) is modeled by adding a\\ncausal attention mask to the multi-head self-attention matrix of each Transformer block. To be\\nspecific, causal attention masking is implemented by setting the lower triangular part of the\\nmatrix to 0 and the remaining elements to ‚àí‚àû, ensuring that each token ùë•ùëñattends only to its\\npredecessors and itself. On the contrary, in encoder-decoder LLMs, a pivot token ùë•ùëòis randomly\\nselected in a sequence of tokens and then regarding the context before it as the source sequence'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 27, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='predecessors and itself. On the contrary, in encoder-decoder LLMs, a pivot token ùë•ùëòis randomly\\nselected in a sequence of tokens and then regarding the context before it as the source sequence\\nxùëñùëõ= {ùë•1, . . . ,ùë•ùëò} of the encoder and the sequence after it as the target output xùëúùë¢ùë°= {ùë•ùëò+1, . . . ,ùë•ùëõ}\\nof decoder. Formally, the causal language modeling objective for training encoder-decoder LLMs is\\nto minimize loss function as follows:\\nLùê∏ùëõùëêùëúùëëùëíùëü‚àíùê∑ùëíùëêùëúùëëùëíùëü\\nùê∂ùêøùëÄ\\n(x) = ‚àílog(\\nùëõ\\n√ñ\\nùëñ=ùëò+1\\nùëÉùúÉ(ùë•ùëñ| x‚â§ùëò, x<ùëñ)) =\\nùëõ\\n‚àëÔ∏Å\\nùëñ=ùëò+1\\n‚àílog ùëÉùúÉ(ùë•ùëñ| x‚â§ùëò, x<ùëñ)\\n(17)\\nwhere x‚â§ùëòis the source sequence input and x<ùëñdenotes the target sequence autoregressively\\ngenerated so far. During the inference phase, pre-trained LLMs that have been trained on large-\\nscale code corpus can generate code in a zero-shot manner without the need for fine-tuning. This\\nis achieved through the technique of prompt engineering, which guides the model to produce the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 27, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='scale code corpus can generate code in a zero-shot manner without the need for fine-tuning. This\\nis achieved through the technique of prompt engineering, which guides the model to produce the\\ndesired output11 [33, 214]. Additionally, recent studies have explored the use of few-shot learning,\\nalso referred to as in-context learning, to enhance model performance further [145, 206].\\nDenoising Autoencoding. In addition to causal language modeling (CLM), the denoising\\nautoencoding (DAE) task has been extensively applied in pre-training encoder-decoder architectures\\nfor code generation, such as PLBART [7], CodeT5 [271], and its enhanced successor, CodeT5+ [269].\\nFollowing T5 [217] and CodeT5 [271], the DAE refers to initially perturbing the source sequence\\nby introducing randomly masked spans of varying lengths. This corrupted sequence serves as the\\ninput for the encoder. Subsequently, the decoder employs an autoregressive strategy to reconstruct'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 27, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='by introducing randomly masked spans of varying lengths. This corrupted sequence serves as the\\ninput for the encoder. Subsequently, the decoder employs an autoregressive strategy to reconstruct\\nthe masked spans, integrating sentinel tokens to facilitate the generation process. This method\\nhas proven effective in improving the model‚Äôs ability to generate semantically and syntactically\\naccurate code by learning robust contextual representations [269, 271]. Formally, the denoising\\nautoencoding objective for training encoder-decoder LLMs is to minimize the following likelihood:\\nLùê∏ùëõùëêùëúùëëùëíùëü‚àíùê∑ùëíùëêùëúùëëùëíùëü\\nùê∑ùê¥ùê∏\\n(x) =\\nùëò\\n‚àëÔ∏Å\\nùëñ=1\\n‚àílog ùëÉùúÉ(xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\nùëñ\\n| x\\\\ùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†, xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\n<ùëñ\\n)\\n(18)\\nwhere ùúÉdenotes the model parameters, x\\\\ùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†is the noisy input with masked spans,\\nxùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†is the masked spans to predict from the decoder with ùëòdenoting the number of\\ntokens in xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†, and xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\n<ùëñ\\nis the span sequence autoregressively generated so far.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 27, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†is the masked spans to predict from the decoder with ùëòdenoting the number of\\ntokens in xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†, and xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\n<ùëñ\\nis the span sequence autoregressively generated so far.\\nCompared with CLM, the DAE task presents a more challenging scenario, as it necessitates a deeper\\nunderstanding and capture of the intrinsic semantic relationships among token sequences by LLMs\\n[217].\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 28, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:29\\nModel learns to perform\\nmany code tasks via natural\\nlanguage instructions\\nInference on\\nunseen code\\ntask\\n(A) Train from Scratch (Transformer)\\n‚Ä¢ Randomly initialized\\nmodel parameters\\n‚Ä¢ Typically requires many\\ncode task-specific examples\\n‚Ä¢ One specialized code model\\nfor each code task\\nImprove performance via\\nfew-shot prompting or\\nprompt engineering\\nUse RL to align\\nwith human\\npreferences\\nInference on\\nunseen code\\ntask\\n(B) Prompting (StarCoder)\\n(C) Pretrain-Finetune (CodeBERT, CodeT5)\\n(E) RLHF (InstructGPT)\\n‚Ä¢ Typically requires many\\ncode task-specific examples\\n‚Ä¢ One specialized code model for\\neach code task\\n(D) Instruction Tuning (WizardCoder)\\nPretrained\\nLM\\nPretrained\\nLM\\nPretrained\\nLM\\nPretrained\\nLM\\nInference\\non task A\\nInference\\non task A\\nInference\\non task A\\nInference\\non task A\\nInference\\non task A\\nTrained LM\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nFinetune\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nRLHF'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 28, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Inference\\non task A\\nInference\\non task A\\nInference\\non task A\\nTrained LM\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nFinetune\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nRLHF\\nTask-specific Knowledge\\nWorld/General Knowledge\\nInstruction-following with\\nMulti-task Learning\\nHuman Preference Alignment\\nModel learns to perform\\nmany code tasks via natural\\nlanguage instructions\\nFig. 9. Comparison of instruction tuning with various fine-tuning strategies and prompting for code tasks,\\nadapted from [274]. For (A), which involves training a Transformer from scratch, please refer to [6] for its use\\nin source code summarization task. In the case of (E), we utilize a representative RLHF [200] as an example.\\nAdditional reinforcement learning methods, such as DPO [216], are also applicable at this stage.\\n5.4\\nInstruction Tuning\\nAfter pre-training LLMs on large-scale datasets, the next phase typically involves augmenting the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 28, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='5.4\\nInstruction Tuning\\nAfter pre-training LLMs on large-scale datasets, the next phase typically involves augmenting the\\nmodel‚Äôs ability to process and follow various instructions, known as instruction tuning. Instruction\\ntuning generally refers to the supervised fine-tuning of pre-trained LLMs using datasets comprised\\nof structured examples framed as various natural language instructions [114, 200, 274, 313]. The\\ncomparison of instruction tuning with various fine-tuning strategies and prompting for code tasks\\nis depicted in Figure 9. Two exemplars of instruction data sampled from Code Alpaca [43] are\\ndemonstrated in Figure 10. It capitalizes on the heterogeneity of instruction types, positioning\\ninstruction tuning as a form of multi-task prompted training that significantly enhances the model‚Äôs\\ngeneralization to unseen tasks [56, 200, 229, 274].\\nIn the realm of code generation, natural language descriptions serve as the instructions guiding'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 28, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='generalization to unseen tasks [56, 200, 229, 274].\\nIn the realm of code generation, natural language descriptions serve as the instructions guiding\\nthe model to generate corresponding code snippets. Consequently, a line of research on instruction\\ntuning LLMs for code generation has garnered substantial interest across academia and industry.\\nTo perform instruction tuning, instruction data are typically compiled from source code with\\npermissive licenses [110, 132, 170] (refer to Section 5.1.2) or are constructed from synthetic code\\ndata [173, 278, 304] (refer to Section 5.2). These datasets are then utilized to fine-tune LLMs through\\na supervised learning paradigm. However, the substantial computational resources required for\\nfull parameter fine-tuning (FFT) LLM pose a notable challenge, particularly in scenarios with\\nconstrained resources [67, 153]. To mitigate this issue, parameter-efficient fine-tuning (PEFT) has'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 28, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='full parameter fine-tuning (FFT) LLM pose a notable challenge, particularly in scenarios with\\nconstrained resources [67, 153]. To mitigate this issue, parameter-efficient fine-tuning (PEFT) has\\nemerged as a compelling alternative strategy, gaining increasing attention for its potential to reduce\\nresource consumption [67]. In the following subsection, we categorize existing works based on\\ntheir instruction-tuning strategies to provide a comprehensive and systematic review.\\n11For more information on prompt engineering, visit https://www.promptingguide.ai\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 29, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:30\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\ndef find_primes(n): \\n    prime_list = [2] \\n    for number in range(2, n + 1): \\n        is_prime = True\\n        for k in range(2, number): \\n            if number % k == 0: \\n                is_prime = False \\n        if is_prime: \\n            prime_list.append(number) \\n    return prime_list\\nOutput:\\nInput:\\nInstruction:\\nWrite code to create a list of all \\nprime numbers between 2 and 100.\\nimport re\\nstring = \"This string contains some \\nurls such as https://www.google.com and \\nhttps://www.facebook.com.\"\\nurls = re.findall(\\'http[s]?://(?:[a-zA-\\nZ]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-\\nfA-F][0-9a-fA-F]))+\\', string) \\nprint(urls)\\nN/A\\nThis string contains some urls such as \\nhttps://www.google.com and \\nhttps://www.facebook.com.\\nGenerate a snippet of code to extract \\nall the URLs from the given string.\\nOutput:\\nInput:\\nInstruction:\\nFig. 10. Two exemplars of instruction data sampled from Code Alpaca [43] used to instruction-tune pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 29, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='all the URLs from the given string.\\nOutput:\\nInput:\\nInstruction:\\nFig. 10. Two exemplars of instruction data sampled from Code Alpaca [43] used to instruction-tune pre-\\ntrained code LLM to enhance their alignment with natural language instructions. The instruction corpus\\nencompasses a variety of tasks, each accompanied by distinct instructions, such as prime numbers generation\\nand URLs extraction.\\n5.4.1\\nFull Parameter Fine-tuning. Full parameter fine-tuning (FFT) involves updating all parameters\\nwithin a pre-trained model, as shown in Figure 11(a). This approach is often preferred when ample\\ncomputational resources and substantial training data are available, as it typically leads to better\\nperformance. [271] introduces an encoder-decoder pre-trained language model for code generation,\\nnamed CodeT5+. They instruction-tune this model on a dataset comprising 20k instruction samples\\nfrom Code Alpaca [43], resulting in an instruction-following model called InstructCodeT5+, which'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 29, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='named CodeT5+. They instruction-tune this model on a dataset comprising 20k instruction samples\\nfrom Code Alpaca [43], resulting in an instruction-following model called InstructCodeT5+, which\\nexhibited improved capabilities in code generation. [173] leverages the Evol-Instruct data synthesis\\ntechnique from WizardLM [289] to evolve 20K code Alpaca [43] instruction samples into a 78K\\ncode instruction dataset. This enriched dataset is then used to fine-tune the StarCoder base model,\\nresulting in WizardCoder, which showcases notable advancements in code generation. In a similar\\nvein, inspired by the successes of WizardCoder [173] and RRHF [303], Pangu-Coder 2 [234] applies\\nthe Evol-Instruct method to generate 68k high-quality instruction samples from the initial 20k Code\\nAlpaca [43] instruction samples. Additionally, they introduces a novel reinforcement learning via\\nRank Responses to align Test & Teacher Feedback (RRTF), which further enhances the performance'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 29, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Alpaca [43] instruction samples. Additionally, they introduces a novel reinforcement learning via\\nRank Responses to align Test & Teacher Feedback (RRTF), which further enhances the performance\\nof Pangu-Coder 2 in code generation. Diverging from synthetic instruction data generation methods,\\nOctoPack [187] utilizes real-world data by curating CommitPack from the natural structure of\\nGit commits, which inherently pair code changes with human-written instructions. This dataset,\\nconsisting of 4 terabytes of Git commits across 350 programming languages, is employed to fine-\\ntune StarCoder [147] and CodeGeeX2 [321], leading to the instruction-following code models of\\nOctoCoder and OctoGeeX for code generation, respectively. The most recent innovation comes\\nfrom Magicoder [278], who proposes OSS-INSTRUCT, a novel data synthesis method that leverages\\nopen-source code snippets to generate high-quality instruction data for code generation. This'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 29, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='from Magicoder [278], who proposes OSS-INSTRUCT, a novel data synthesis method that leverages\\nopen-source code snippets to generate high-quality instruction data for code generation. This\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 30, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:31\\napproach seeks to reduce the bias often present in synthetic data generated by LLM. In line with\\nOSS-INSTRUCT, the BigCode team introduces StarCoder2-15B-instruct [304], which they claim\\nto be the first entirely self-aligned LLM for code generation, trained with a fully permissive and\\ntransparent pipeline. Moreover, [59] harnesses open-source mathematics datasets, such as MATH\\n[95] and GSM8k [58], along with synthetically generated code following the OSS-INSTRUCT\\n[278] paradigm, to instruction-tune CodeGemma 7B, yielding exceptional results in mathematical\\nreasoning and code generation tasks.\\n5.4.2\\nParameter-Efficient Fine-tuning. To mitigate the extensive computational and resource de-\\nmands inherent in fine-tuning LLMs, the concept of parameter-efficient fine-tuning (PEFT) has\\nemerged to focus on updating a minimal subset of parameters, which may either be a selection of'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 30, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='mands inherent in fine-tuning LLMs, the concept of parameter-efficient fine-tuning (PEFT) has\\nemerged to focus on updating a minimal subset of parameters, which may either be a selection of\\nthe model‚Äôs parameters or an array of additional parameters specifically introduced for the tuning\\nprocess [67, 153]. The categorization of these methods is depicted in Figure 11(b), (c), and (d). A\\nplethora of innovative PEFT approaches have been developed, among which BitFit [305], Adapter\\n[102], Prompt tuning [142], Prefix-tuning [149], LoRA [103], IA3 [161], QLoRA [65], and AdaLoRA\\n[312] are particularly noteworthy. A seminal study in this field, LoRA [103], proposes a parameter\\nupdate mechanism for a pre-trained weight matrix ‚Äî such as those found in the key or value\\nprojection matrices of a Transformer block‚Äôs multi-head self-attention layer ‚Äî by factorizing the\\nupdate into two low-rank matrices. Crucially, all original model parameters remain frozen, with'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 30, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='projection matrices of a Transformer block‚Äôs multi-head self-attention layer ‚Äî by factorizing the\\nupdate into two low-rank matrices. Crucially, all original model parameters remain frozen, with\\nonly the pair of low-rank matrices being trainable. After fine-tuning, the product of these low-rank\\nmatrices can be seamlessly incorporated into the existing weight matrix through an element-wise\\naddition. This process can be formally described as:\\n(W0 + ŒîW)ùë•= W0ùë•+ ŒîWùë•= Wùëìùëüùëúùëßùëíùëõ\\n0\\nùë•+ ùõº\\nùëüBùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùë¢ùëù\\nAùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùëëùëúùë§ùëõ\\n|                     {z                     }\\nŒîW\\nùë•\\n(19)\\nwhere W0 ‚ààRùëë√óùëòdenotes a pre-trained weight matrix, Bùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùë¢ùëù\\n‚ààRùëë√óùëüand Aùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùëëùëúùë§ùëõ\\n‚ààRùëü√óùëòare\\ntwo trainable low-rank matrixes and initialized by a zero matrix and a random Gaussian distribution\\nN (0, ùúé2) respectively, to ensure ŒîW = 0 at the beginning of training. The rank ùëü‚â™min(ùëë,ùëò), the\\nùõº\\nùëüis a scaling coefficient to balance the importance of the LoRA module, like a learning rate.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 30, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='N (0, ùúé2) respectively, to ensure ŒîW = 0 at the beginning of training. The rank ùëü‚â™min(ùëë,ùëò), the\\nùõº\\nùëüis a scaling coefficient to balance the importance of the LoRA module, like a learning rate.\\nDespite the advancements in PEFT methods, their application in code generation remains limited.\\nFor instance, [121] pioneered the use of parameter-efficient instruction-tuning on a Llama 2 [253]\\nmodel with a single RTX 3090 GPU, leading to the development of a multilingual code generation\\nmodel called CodeUp. More recently, ASTRAIOS [334] conducted a thorough empirical examination\\nof parameter-efficient instruction tuning for code comprehension and generation tasks. This study\\nyielded several perceptive observations and conclusions, contributing valuable insights to the\\ndomain.\\n5.5\\nReinforcement Learning with Feedback\\nLLMs have exhibited remarkable instruction-following capabilities through instruction tuning.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 30, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='domain.\\n5.5\\nReinforcement Learning with Feedback\\nLLMs have exhibited remarkable instruction-following capabilities through instruction tuning.\\nHowever, they often produce outputs that are unexpected, toxic, biased, or hallucinated outputs that\\ndo not align with users‚Äô intentions or preferences [118, 200, 272]. Consequently, aligning LLMs with\\nhuman preference has emerged as a pivotal area of research. A notable work is InstructGPT [200],\\nwhich further fine-tunes an instruction-tuned model utilizing reinforcement learning with human\\nfeedback (RLHF) on a dataset where labelers have ranked model outputs in order of quality, from\\nbest to worst. This method has been instrumental in the development of advanced conversational\\nlanguage models, such as ChatGPT [196] and Bard [177]. Despite its success, acquiring high-quality\\nhuman preference ranking data is a resource-intensive process [141]. To address this, Reinforcement\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 31, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:32\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTrainable\\nInput\\nLayer N+1\\nüî•\\nLayer N\\nLayer 1\\nLayer 2\\n...\\nInput\\nLayer 2\\nüî•\\nLayer N\\nLayer 1\\n...\\nüî•\\nFrozen\\nInput\\n...\\nLayer 2\\nüî•\\nLayer 1\\nüî•\\nLayer N\\nüî•\\nInput\\nLayer N\\nLayer 1\\nLayer 2\\n‚ñ≥WN\\nüî•\\n‚ñ≥W2\\nüî•\\n‚ñ≥W1\\nüî•\\n...\\n...\\n(a) Full Fine-tuning\\n(b) Specification\\n(c) Addition\\n(d) Reparameterization\\nFig. 11. An illustration of full parameter fine-tuning (FFT) and parameter-efficient fine-tuning (PEFT) methods.\\n(a) refers to the Full Fine-tuning method, which updates all parameters of the base model during fine-tuning.\\n(b) stands for the Specification-based PEFT method that conditionally fine-tunes a small subset of the model\\nparameters while freezing the rest of the model, e.g. BitFit [305]. (c) represents the Addition-based PEFT\\nmethod that fine-tunes the incremental parameters introduced into the base model or input, e.g. Adapter\\n[102], Prefix-tuning [149], and Prompt-tuning [142]. (d) symbolizes the Reparameterization-based method'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 31, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[102], Prefix-tuning [149], and Prompt-tuning [142]. (d) symbolizes the Reparameterization-based method\\nwhich reparameterizes existing model parameters by low-rank transformation, e.g. LoRA [103], QLoRA [65],\\nand AdaLoRA [312].\\nLearning from AI Feedback (RLAIF) [21, 141] has been proposed to leverage powerful off-the-shelf\\nLLMs (e.g., ChatGPT [196] and GPT-4 [5]) to simulate human annotators by generating preference\\ndata.\\nBuilding on RLHF‚Äôs success, researchers have explored reinforcement learning with feedback to\\nenhance code generation in LLMs. Unlike RLHF, which relies on human feedback, this approach\\nemploys compilers or interpreters to automatically provide feedback on code samples through code\\nexecution on unit test cases, catalyzing the advancement of this research domain. CodeRL [139]\\nintroduced an actor-critic reinforcement learning framework for code generation. In this setup, the\\nlanguage model serves as the actor-network, while a token-level functional correctness reward'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 31, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='introduced an actor-critic reinforcement learning framework for code generation. In this setup, the\\nlanguage model serves as the actor-network, while a token-level functional correctness reward\\npredictor acts as the critic. Generated code is assessed through unit test signals from a compiler,\\nwhich can indicate compiler errors, runtime errors, unit test failures, or passes. CompCoder [266]\\nenhances code compilability by employing compiler feedback, including language model fine-\\ntuning, compilability reinforcement, and compilability discrimination strategies. Subsequently,\\nPPOCoder [238] integrates pre-trained code model CodeT5 [271] with Proximal Policy Optimization\\n(PPO) [230]. This integration not only utilizes execution (i.e., compilers or interpreters) feedback to\\nassess syntactic and functional correctness but also incorporates a reward function that evaluates\\nthe syntactic and semantic congruence between abstract syntax tree (AST) sub-trees and data flow'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 31, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='assess syntactic and functional correctness but also incorporates a reward function that evaluates\\nthe syntactic and semantic congruence between abstract syntax tree (AST) sub-trees and data flow\\ngraph (DFG) edges in the generated code against the ground truth. Additionally, the framework\\napplies a KL-divergence penalty to maintain fidelity between the actively learned policy and the\\nreferenced pre-trained model, enhancing the optimization process. More recently, RLTF [163] has\\nproposed an online reinforcement learning framework that provides fine-grained feedback based\\non compiler error information and location, along with adaptive feedback that considers the ratio\\nof passed test cases.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 32, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:33\\nDespite these successes, reinforcement learning algorithms face inherent limitations such as\\ninefficiency, instability, extensive resource requirements, and complex hyperparameter tuning,\\nwhich can impede the performance and scalability of LLMs. To overcome these challenges, recent\\nstudies have introduced various variants of RL methods that do not rely on PPO, including DPO\\n[216], RRHF [303], and sDPO [129]. In essence, these methods aim to maximize the likelihood\\nbetween the logarithm of conditional probabilities of preferred and rejected responses, which may\\nbe produced by LLMs with varying capabilities. Inspired by RRHF [303], PanGu-Coder 2 [234]\\nleverages a novel framework, Reinforcement Learning via Rank Responses to align Test & Teacher\\nFeedback (RRTF), significantly enhancing code generation capabilities, as evidenced by pass@1 of\\n62.20% on the HumanEval benchmark.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 32, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Feedback (RRTF), significantly enhancing code generation capabilities, as evidenced by pass@1 of\\n62.20% on the HumanEval benchmark.\\nTaking a step forward, the integration of more non-differentiable code features, such as coding\\nstyle [44, 178] and readability [34], into the reinforcement learning feedback for LLM-based code\\ngeneration, presents an exciting avenue for future research.\\n5.6\\nPrompting Engineering\\nLarge-scale language models (LLMs) such as GPT-3 and its successors have been trained on large-\\nscale data corpora, endowing them with substantial world knowledge [33, 200, 274]. Despite this,\\ncrafting an effective prompting as a means of communicating with LLMs to harness their full\\npotential remains a long-standing challenge [164]. Recent advancements in prompting engineering\\nhave expanded the capabilities of LLMs, enabling more sophisticated task completion and enhancing\\nboth reliability and performance. Notable techniques include Chain-of-Thought (CoT) [276], Self-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 32, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='have expanded the capabilities of LLMs, enabling more sophisticated task completion and enhancing\\nboth reliability and performance. Notable techniques include Chain-of-Thought (CoT) [276], Self-\\nConsistency [267], Tree-of-Thought (ToT) [295], Program of Thoughts (PoT) [50], Reasoning via\\nPlanning (RAP) [93], ReAct [296], Self-Refine [176], Reflexion [236], and LATS [327]. For instance,\\nCoT significantly improves the LLMs‚Äô ability to perform complex reasoning by providing a few\\nchain-of-thought demonstrations as exemplars in prompting.\\nPrompting engineering is particularly advantageous as it bypasses the need for additional training\\nand can significantly elevate performance. Consequently, numerous studies have leveraged this\\ntechnique for iterative and self-improving (refining) code generation within proprietary LLMs\\nsuch as ChatGPT and GPT-4. Figure 12 illustrates the general pipeline for self-improving code'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 32, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='technique for iterative and self-improving (refining) code generation within proprietary LLMs\\nsuch as ChatGPT and GPT-4. Figure 12 illustrates the general pipeline for self-improving code\\ngeneration with LLMs. For instance, Self-Debugging [51] involves prompting an LLM to iteratively\\nrefine a predicted program by utilizing feedback composed of code explanations combined with\\nexecution results, which assists in identifying and rectifying errors. When unit tests are unavailable,\\nthis feedback can rely solely on code explanations. Similarly, LDB [325] prompts LLMs to refine\\ngenerated code by incorporating debugging feedback, which consists of the evaluation of the\\ncorrectness of variable values throughout runtime execution, as assessed by the LLMs. In parallel,\\nSelfEvolve [122] employs a two-stage process where LLMs first generate domain-specific knowledge\\nfor a problem, followed by a trial code. This code is then iteratively refined through interactive'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 32, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='SelfEvolve [122] employs a two-stage process where LLMs first generate domain-specific knowledge\\nfor a problem, followed by a trial code. This code is then iteratively refined through interactive\\nprompting and execution feedback. An empirical investigation by [195] provides a comprehensive\\nanalysis of the self-repairing capabilities for code generation in models like Code Llama, GPT-\\n3.5, and GPT-4, using problem sets from HumanEval and APPS. This study yields a series of\\ninsightful observations and findings, shedding light on the self-refinement effectiveness of these\\nLLMs. Moreover, Reflexion [236] introduces a general approach for code generation wherein LLM-\\npowered agents engage in verbal self-reflection on task feedback signals, storing these reflections\\nin an episodic memory buffer to inform and improve decision-making in subsequent interactions.\\nLATS [327] adopts a novel strategy, utilizing LLMs as agents, value functions, and optimizers. It'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 32, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='in an episodic memory buffer to inform and improve decision-making in subsequent interactions.\\nLATS [327] adopts a novel strategy, utilizing LLMs as agents, value functions, and optimizers. It\\nenhances decision-making by meticulously constructing trajectories through Monte Carlo Tree\\nSearch (MCTS) algorithms, integrating external feedback, and learning from experience. This\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 33, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content=\"1:34\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nStep 2: Trajectory\\nStep 3: Evaluation\\nStep 4: Self-Reflection\\nStep 1: Code Task\\nFeedback\\nCode LLM\\nExecutor\\nWrite a Python script to \\nprint all unique elements \\nin a list.\\ndef unique_elements(lst):\\nresult = set(lst)\\n    return list(result)\\nassert unique_elements\\n(['a', 'b', 'c', 'a', 'd']) \\n== ['a', 'b', 'c', 'd'] \\n[‚Ä¶] does not work as \\nexpected because it uses \\nthe built-in `set()` \\nfunction in Python, which \\ndoes not maintain the order \\nof elements.[‚Ä¶]\\n(Optional)\\nassert unique_elements([1, \\n2, 3, 4, 4]) == [1, 2, 3, 4]\\n(Code) LLM\\nFig. 12. An illustration of the self-improving code generation pipeline using prompts for LLMs. This process\\nincorporates iterative self-refinement by integrating execution outcomes and includes an optional self-\\nreflection mechanism to enhance generation quality.\\napproach has demonstrated remarkable results in code generation, achieving a pass@1 of 94.4% on\"),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 33, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='reflection mechanism to enhance generation quality.\\napproach has demonstrated remarkable results in code generation, achieving a pass@1 of 94.4% on\\nthe HumanEval benchmark with GPT-4.\\nDistinct from the aforementioned methods, CodeT [45] and LEVER [190] prompt LLMs to\\ngenerate numerous code samples, which are then re-ranked based on execution outcomes to select\\nthe optimal solution. Notably, these approaches do not incorporate a self-refinement step to further\\nimprove code generation.\\n5.7\\nRepository Level & Long Context\\nIn contemporary software engineering practices, modifications to a code repository are widespread\\nand encompass a range of activities, including package migration, temporary code edits, and the\\nresolution of GitHub issues. While LLMs showcase impressive prowess in function-level code\\ngeneration, they often falter when grappling with the broader context inherent to a repository,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 33, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='resolution of GitHub issues. While LLMs showcase impressive prowess in function-level code\\ngeneration, they often falter when grappling with the broader context inherent to a repository,\\nsuch as import dependencies, parent classes, and files bearing similar names. These deficiencies\\nresult in suboptimal performance in repository-level code generation, as identified in recent studies\\n[239, 240]. The challenges faced by LLMs in this domain are primarily due to the following factors:\\n‚Ä¢ Code repositories typically contain intricate interdependencies scattered across various\\nfiles, including shared utilities, configurations, and cross-API invocations, which arise from\\nmodular design principles [22, 309].\\n‚Ä¢ Repositories are characterized by their unique structures, naming conventions, and coding\\nstyles, which are essential for maintaining clarity and facilitating ongoing maintenance [44].\\n‚Ä¢ The vast context of an entire repository often exceeds the context length limitations of LLMs,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 33, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='styles, which are essential for maintaining clarity and facilitating ongoing maintenance [44].\\n‚Ä¢ The vast context of an entire repository often exceeds the context length limitations of LLMs,\\nthus hindering their ability to integrate comprehensive contextual information [22].\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 34, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:35\\n‚Ä¢ LLMs may not have been adequately trained on extensive sets of repository data, such as\\nproprietary software or projects that are still in development [239].\\nGiven that the scope of a typical software repository encompasses hundreds of thousands of\\ntokens, it is imperative to enhance the capacity of LLMs to handle extensive contexts when they\\nare employed for repository-level code generation. Fortunately, recent advancements in positional\\nencoding techniques, such as ALiBi [211] and RoPE [243], have shown promise in improving the\\nTransformer‚Äôs ability to generalize from shorter training sequences to longer inference sequences\\n[318]. This progress addresses the third challenge mentioned above to a certain degree, thereby\\nenabling better contextualization of coding activities within full repositories.\\nTo further refine LLMs for repository-level code completion, several innovative approaches have'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 34, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='enabling better contextualization of coding activities within full repositories.\\nTo further refine LLMs for repository-level code completion, several innovative approaches have\\nbeen introduced. RepoCoder [309] leverages a similarity-based retrieval system within an iterative\\nretrieval-generation paradigm to enrich the context and enhance code completion quality. In a\\nsimilar vein, CoCoMIC [69] employs a cross-file context finder named CCFINDER to pinpoint and\\nretrieve the most relevant cross-file contexts within a repository. RepoHyper [209] introduces a\\nsemantic graph structure, termed RSG, to encapsulate the expansive context of code repositories\\nand uses an ‚ÄúExpand and Refine‚Äù retrieval method to obtain relevant code snippets. Moreover, a\\nframework known as RLPG [240] has been proposed to generate repository-level prompts that\\nintegrate the repository‚Äôs structure with the relevant context across all files. However, the constant'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 34, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='framework known as RLPG [240] has been proposed to generate repository-level prompts that\\nintegrate the repository‚Äôs structure with the relevant context across all files. However, the constant\\nreliance on retrieval mechanisms has raised concerns regarding efficiency and robustness, as some\\nretrieved contexts may prove unhelpful or harmful. In response, Repoformer [282] introduces a\\nselective Retrieval-Augmented Generation (RAG) framework that judiciously bypasses retrieval\\nwhen it is deemed redundant. This approach incorporates a self-supervised learning strategy that\\nequips a code LLM with the ability to perform a self-assessment on the utility of retrieval for\\nenhancing the quality of its output, thereby effectively utilizing potentially noisy retrieved contexts.\\nAdditionally, RepoFusion [239] has been developed to train models to combine multiple relevant\\ncontexts from a repository, aiming to produce more precise and context-aware code completions.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 34, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Additionally, RepoFusion [239] has been developed to train models to combine multiple relevant\\ncontexts from a repository, aiming to produce more precise and context-aware code completions.\\nIn a novel approach, Microsoft‚Äôs CodePlan [22] frames repository-level coding tasks as a planning\\nproblem, generating a multi-step chain of edits (plan) where each step involves invoking an LLM on a\\nspecific code location, considering context from the entire repository, preceding code modifications,\\nand task-specific instructions.\\nAdvancing the state-of-the-art, [308] tackles the formidable challenge of NL2Repo, an endeavor\\nthat seeks to create a complete code repository from natural language requirements. To address\\nthis complex task, they introduce the CodeS framework, which strategically breaks down NL2Repo\\ninto a series of manageable sub-tasks using a multi-layer sketch approach. The CodeS framework'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 34, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='this complex task, they introduce the CodeS framework, which strategically breaks down NL2Repo\\ninto a series of manageable sub-tasks using a multi-layer sketch approach. The CodeS framework\\ncomprises three distinct modules: 1) RepoSketcher, for creating a directory structure of the reposi-\\ntory based on given requirements; 2) FileSketcher, for sketching out each file within that structure;\\nand 3) SketchFiller, for fleshing out the specifics of each function within the file sketches [308].\\nAccordingly, a surge of benchmarks tailored for repository-level code generation has emerged,\\nsuch as RepoEval [309], Stack-Repo [239], Repobench [167], EvoCodeBench [144], SWE-bench\\n[123], CrossCodeEval [68], and SketchEval [308]. The detailed statistics and comparisons of these\\nbenchmarks are presented in Table 6.\\nDespite the progress made by these methods in repository-level code generation, significant chal-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 34, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='benchmarks are presented in Table 6.\\nDespite the progress made by these methods in repository-level code generation, significant chal-\\nlenges remain to be addressed. Programming developers are often required to invest considerable\\ntime in editing and debugging [25, 28, 186, 239, 255]. However, the advent of LLM-powered coding\\nagents, such as AutoCodeRover [316], SWE-Agent [124], and OpenDevin [199], has demonstrated\\ntheir potential to tackle complex problems, paving the way for future exploration in this field (for\\nmore details, see Section 5.9).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 35, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:36\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nEmbedding \\nModel\\nQuery\\nRetrieved Context\\nCreate a quick-\\nsort algorithm \\nin Python.\\nCombine Prompts and Context\\nCreate a quick-sort algorithm \\nin Python.\\nPlease solve the above problem \\nbased on the following context:\\n{context}\\ndef quick_sort(arr):\\n\"\"\"Sort a list of numbers in ascending \\norder using the Quick-Sort algorithm\"\"\"\\nif len(arr) == 0:\\nreturn []\\npivot = arr[0]\\nleft_arr = [x for x in arr if x < pivot]\\nright_arr = [x for x in arr if x > pivot]\\nreturn quick_sort(left_arr) + [pivot] + \\nquick_sort(right_arr)\\nCode LLM\\nEmbedding \\nModel\\nVector \\nDatabase\\nCode Solution\\nCode Data \\nChunks\\nOpen \\nSource\\nStage 1: Retrieval\\nStage 2: Generation\\nAlgorithm:\\n1. If the input \\narray...already \\nsorted....\\n5. Recursively \\ncall quicksort...\\nFig. 13. A workflow illustration of the Retrieval-Augmented Code Generation (RACG). Upon receiving a query'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 35, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Algorithm:\\n1. If the input \\narray...already \\nsorted....\\n5. Recursively \\ncall quicksort...\\nFig. 13. A workflow illustration of the Retrieval-Augmented Code Generation (RACG). Upon receiving a query\\n(instruction), the retriever selects the relevant contexts from a large-scale vector database. Subsequently, the\\nretrieved contexts are merged with the query, and this combined input is fed into the generator (LLM) to\\nproduce the target code solution.\\n5.8\\nRetrieval Augmented\\nLLMs have exhibited impressive capabilities but are hindered by several critical issues such as\\nhallucination [154, 315], obsolescence of knowledge [115], and non-transparent [32], untraceable\\nreasoning processes [80, 106, 276, 329]. While techniques like instruction-tuning (see Section 5.4)\\nand reinforcement learning with feedback (see Section 5.5) mitigate these issues, they also introduce\\nnew challenges, such as catastrophic forgetting and the requirement for substantial computational'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 35, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and reinforcement learning with feedback (see Section 5.5) mitigate these issues, they also introduce\\nnew challenges, such as catastrophic forgetting and the requirement for substantial computational\\nresources during training [90, 201].\\nRecently, Retrieval-Augmented Generation (RAG) has emerged as an innovative approach to\\novercoming these limitations by integrating knowledge from external databases. Formally defined,\\nRAG denotes a model that, in response to queries, initially sources relevant information from\\nan extensive corpus of documents, and then leverages this retrieved information in conjunction\\nwith the original query to enhance the response‚Äôs quality and accuracy, especially for knowledge-\\nintensive tasks. The RAG framework typically consists of a vector database, a retriever, a re-ranker,\\nand a generator. It is commonly implemented using tools such as LangChain12 and LLamaIndex13.\\nBy performing continuous knowledge updates of the database and the incorporation of domain-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 35, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and a generator. It is commonly implemented using tools such as LangChain12 and LLamaIndex13.\\nBy performing continuous knowledge updates of the database and the incorporation of domain-\\nspecific data, RAG circumvents the need for re-training LLMs from scratch [80]. Consequently,\\nRAG has substantially advanced LLM performance across a variety of tasks [47, 143].\\nDue to the nature of code, code LLMs are also susceptible to the aforementioned issues that\\naffect general-purpose LLMs. For instance, they may exhibit a hallucination phenomenon when\\ninstructions fall outside the scope of their training data or necessitate the latest programming\\npackages. Given the dynamic nature of publicly available source-code libraries like PyTorch, which\\nundergo frequent expansion and updates, deprecated calling methods can become a significant\\nchallenge. If Code LLMs are not updated in tandem with the latest functions and APIs, this can'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 35, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='undergo frequent expansion and updates, deprecated calling methods can become a significant\\nchallenge. If Code LLMs are not updated in tandem with the latest functions and APIs, this can\\nintroduce potential errors and safety risks. Retrieval-Augmented Code Generation (RACG) stands\\n12LangChain facilitates the development of LLM-powered applications. https://www.langchain.com\\n13LLamaIndex is a leading data framework for building LLM applications. https://www.llamaindex.ai\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 36, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:37\\nas a promising solution to these concerns. A workflow illustration of the RACG is depicted in\\nFigure 13.\\nDespite its potential, the adoption of RAG for code generation remains limited. Drawing in-\\nspiration from the common practice among programmers of referencing related code snippets,\\n[166] introduced a novel retrieval-augmented mechanism with graph neural networks (GNNs),\\ntermed HGNN, which unites the advantages of similar examples retrieval with the generalization\\ncapabilities of generative models for code summarization, which is the reverse process of code\\ngeneration. [205] pioneered a retrieval augmented framework named REDCODER for code gener-\\nation by retrieving and integrating relevant code snippets from a source-code database, thereby\\nproviding supplementary context for the generation process. Subsequently, a retrieval-augmented'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 36, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ation by retrieving and integrating relevant code snippets from a source-code database, thereby\\nproviding supplementary context for the generation process. Subsequently, a retrieval-augmented\\ncode completion framework termed ReACC [171] is proposed to leverage both lexical copying and\\nsemantic referencing of related code, achieving state-of-the-art performance on the CodeXGLUE\\nbenchmark [172]. In the spirit of how programmers often consult textual resources such as code\\nmanuals and documentation to comprehend functionalities, DocPrompting [330] explicitly utilizes\\ncode documentation by retrieving the relevant documentation pieces based on a natural language\\nquery and then generating the target code by blending the query with the retrieved information.\\nMore recently, RepoCoder [309], an iterative retrieval-generation framework, is proposed for\\nenhancing repository-level code completion by effectively utilizing code analogies across different'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 36, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='More recently, RepoCoder [309], an iterative retrieval-generation framework, is proposed for\\nenhancing repository-level code completion by effectively utilizing code analogies across different\\nfiles within a repository to inform and improve code suggestions. Furthermore, breaking away\\nfrom reliance on a singular source of retrieval, [242] developed a multi-faceted ‚Äúknowledge soup‚Äù\\nthat integrates web searches, documentation, execution feedback, and evolved code snippets. Then,\\nit incorporates an active retrieval strategy that iteratively refines the query and enriches the\\nknowledge soup, expanding the scope of information available for code generation.\\nDespite these advancements, several limitations in retrieval-augmented code generation warrant\\nfurther exploration: 1) the quality of the retrieved information significantly impacts overall perfor-\\nmance; 2) the effective integration of retrieved code information with the query needs optimization;'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 36, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='mance; 2) the effective integration of retrieved code information with the query needs optimization;\\n3) an over-reliance on retrieved information may lead to inadequate responses that fail to address\\nthe query‚Äôs intent; 4) additional retrieved information necessitates larger context windows for the\\nLLM, resulting in increased computational demands.\\n5.9\\nAutonomous Coding Agents\\nThe advent of LLMs has marked the beginning of a new era of potential pathways toward artificial\\ngeneral intelligence (AGI), capturing significant attention in both academia and industry [108, 261,\\n279, 284]. A rapidly expanding array of applications for LLM-based autonomous agents, including\\nAutoGPT [2], AgentGPT [1], BabyAGI [3], and AutoGen [283], underlines the promise of this\\ntechnology.\\nLLM-powered autonomous agents are systems endowed with sophisticated reasoning abilities,\\nleveraging an LLM as a central computational engine or controller. This allows them to formulate'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 36, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='technology.\\nLLM-powered autonomous agents are systems endowed with sophisticated reasoning abilities,\\nleveraging an LLM as a central computational engine or controller. This allows them to formulate\\nand execute problem-solving plans through a series of tool-enabled functions or API calls. Moreover,\\nthese agents are designed to function within a shared environment where they can communicate\\nand engage in cooperative, competitive, or negotiating interactions [104, 261, 283]. The typical\\narchitecture of such an agent encompasses an LLM-based Agent, a memory module, a planning\\ncomponent, and a tool utilization module, as depicted in Figure 14.\\nIn the realm of automated code generation, LLM-powered autonomous agents have demon-\\nstrated remarkable proficiency. For instance, AgentCoder [104] achieved a groundbreaking pass@1\\nof 96.3% on the HumanEval benchmark, forwarding a step closer to the future of automated soft-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 36, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='strated remarkable proficiency. For instance, AgentCoder [104] achieved a groundbreaking pass@1\\nof 96.3% on the HumanEval benchmark, forwarding a step closer to the future of automated soft-\\nware development [111]. The innovative meta-programming framework termed MetaGPT [100]\\nintegrates human workflow efficiencies into LLM-based multi-agent collaboration, as shown in\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 37, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:38\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nAgent\\nMemory\\nAction\\nTools\\nPlanning\\nShort-term Memory\\nLong-term Memory\\nCalendar ( )\\nCalculator ( )\\nCode Interpreter ( )\\nSearch ( )\\n...more\\nReflection\\nSelf-critics\\nChain of Thoughts\\nSubgoal Decomposition\\nFig. 14. The general architecture of an LLM-powered autonomous agent system, adapted from [279]. Plan-\\nning: The agent decomposes large tasks into smaller, manageable sub-goals or engages in self-criticism\\nand self-reflection on past actions to learn from mistakes and improve future performance. Memory: This\\ncomponent enables the agent to store and retrieve past information. Tools: The agent is trained to invoke\\nexternal functions or APIs. Action: The agent executes actions, with or without the use of tools, to interact\\nwith the environment. The gray dashed lines represent the data flow within the system.\\nFig. 15. MetaGPT integrates human workflow efficiencies into LLM-based multi-agent collaboration to break'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 37, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='with the environment. The gray dashed lines represent the data flow within the system.\\nFig. 15. MetaGPT integrates human workflow efficiencies into LLM-based multi-agent collaboration to break\\ndown complex code-related tasks into specific, actionable procedures. These procedures are then assigned to\\nvarious roles, such as Product Manager, Architect, and Engineer played by LLM. The image is sourced from\\nthe original paper [100].\\nFigure 15. Furthermore, [104] introduces AgentCoder, a multi-agent framework composed of three\\nspecialized agents, each with distinct roles and capabilities. These roles include a programmer agent\\nresponsible for code generation, a test designer agent tasked with generating unit test cases, and\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 38, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:39\\na test executor agent that executes the code and provides feedback. This division of labor within\\nAgentCoder promotes more efficient and effective code generation. CodeAct [265] distinguishes\\nitself by utilizing executable Python code to consolidate LLM agent actions within a unified action\\nspace, in contrast to the generation of JSON or textual formats. Additionally, AutoCodeRover [316]\\nis proposed to autonomously resolve GitHub issues for program enhancement.\\nTo address the complexity of tasks within software engineering, two innovative autonomous AI\\nsoftware engineers Devin14[61] and OpenDevin15[199], have been released and rapidly garnered\\nconsiderable interest within the software engineering (SE) and artificial general intelligence (AGI)\\ncommunity. Subsequently, an autonomous system, SWE-agent [124], leverages a language model'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 38, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='considerable interest within the software engineering (SE) and artificial general intelligence (AGI)\\ncommunity. Subsequently, an autonomous system, SWE-agent [124], leverages a language model\\nto interact with a computer to address software engineering tasks, successfully resolving 12.5% of\\nissues on the SWE-bench benchmark [123]. L2MAC [98] has been introduced as the first practical,\\nLLM-based, multi-agent, general-purpose stored-program automatic computer that utilizes a von\\nNeumann architecture, designed specifically for the generation of long and consistent outputs.\\nAt the time of writing this survey, OpenDevin has enhanced CodeAct with bash command-based\\ntools, leading to the release of OpenDevin CodeAct 1.0 [287], which sets a new state-of-the-art\\nperformance on the SWE-Bench Lite benchmark [123].\\nDespite these remarkable advancements, the journey toward fully realized AI software engineers\\nemploying LLM-powered autonomous agents is far from complete [261, 284]. Critical aspects'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 38, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Despite these remarkable advancements, the journey toward fully realized AI software engineers\\nemploying LLM-powered autonomous agents is far from complete [261, 284]. Critical aspects\\nsuch as prompt design, context length, agent count, and toolsets call for further refinement and\\noptimization, especially as problem complexities escalate [111].\\n5.10\\nEvaluation\\nDespite the impressive capabilities of LLMs, they exhibit a range of behaviors that are both beneficial\\nand potentially risky. These behaviors can enhance performance across various downstream tasks\\nbut may also introduce reliability and trustworthiness concerns in LLM deployment [42, 48, 290].\\nConsequently, it is imperative to develop precise evaluation approaches to discern the qualitative\\nand quantitive differences between models, thereby encouraging further advancements in LLM\\ncapabilities.\\nEvaluation strategies for LLMs in code generation mirror those for general-purpose LLMs and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 38, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and quantitive differences between models, thereby encouraging further advancements in LLM\\ncapabilities.\\nEvaluation strategies for LLMs in code generation mirror those for general-purpose LLMs and\\ncan be divided into three principal categories: metrics-based, human-centered, and LLM-based\\napproaches. Detailed benchmarks for these evaluation strategies are presented in Section 5.1.3 and\\nsummarized in Table 6. Subsequent subsections will provide a thorough analysis of each approach.\\n5.10.1\\nMetrics. The pursuit of effective and reliable automatic evaluation metrics for generated\\ncontent is a long-standing challenge within the field of natural language processing (NLP) [49, 156,\\n203]. At the early stage, most works directly leverage token-matching-based metrics, such as Exact\\nMatch, BLEU [203], ROUGE [156], and METEOR [23], which are prevalent in text generation of\\nNLP, to assess the quality of code generation.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 38, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Match, BLEU [203], ROUGE [156], and METEOR [23], which are prevalent in text generation of\\nNLP, to assess the quality of code generation.\\nWhile these metrics offer a rapid and cost-effective approach for assessing the quality of gener-\\nated code, they often fall short of capturing the syntactical and functional correctness, as well as\\nthe semantic features of the code. To eliminate this limitation, CodeBLEU [221] was introduced, en-\\nhancing the traditional BLEU metric [203] by incorporating syntactic information through abstract\\nsyntax trees (AST) and semantic understanding via data-flow graph (DFG). Despite these improve-\\nments, the metric does not fully resolve issues pertaining to execution errors or discrepancies in\\nthe execution results of the generated code. In light of these challenges, execution-based metrics\\nhave gained prominence for evaluating code generation, including pass@k [48], n@k [151], test\\n14https://www.cognition.ai/introducing-devin'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 38, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='have gained prominence for evaluating code generation, including pass@k [48], n@k [151], test\\n14https://www.cognition.ai/introducing-devin\\n15https://github.com/OpenDevin/OpenDevin\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 39, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:40\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 9. The performance comparison of LLMs for code generation on the HumanEval [48] benchmark,\\nmeasured by Pass@1. For models with various sizes, we report only the largest size version of each model\\nwith a magnitude of B parameters. ‚Ä° denotes instruction-tuned models.\\nModel\\nSize\\npass@1 (%)\\nAvailability\\nClosed Source\\nGPT-4o-0513 [197]\\n-\\n91.0\\n[API Access]\\nGPT-4-Turbo-0409 [198]\\n-\\n88.2\\n[API Access]\\nGPT-4-1106 [5]\\n-\\n87.8\\n[API Access]\\nGPT-3.5-Turbo-0125 [196]\\n-\\n76.2\\n[API Access]\\nClaude-3.5-Sonnet [14]\\n-\\n92.0\\n[API Access]\\nClaude-3-Opus [14]\\n-\\n84.9\\n[API Access]\\nClaude-3-Sonnet [14]\\n-\\n73.0\\n[API Access]\\nClaude-3-Haiku [14]\\n-\\n75.9\\n[API Access]\\nGemini-1.5-Pro [220]\\n-\\n84.1\\n[API Access]\\nGemini-1.5-Flash [220]\\n-\\n74.3\\n[API Access]\\nGemini-1.0-Ultra [220]\\n-\\n74.4\\n[API Access]\\nGemini-1.0-Pro [220]\\n-\\n67.7\\n[API Access]\\n‚Ä°PanGu-Coder2 [234]\\n15B\\n61.64\\n-\\nPanGu-Coder [55]\\n2.6B\\n23.78\\n-\\nCodex [48]\\n12B\\n28.81\\nDeprecated\\nPaLM-Coder [54]\\n540B\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 39, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='-\\n74.4\\n[API Access]\\nGemini-1.0-Pro [220]\\n-\\n67.7\\n[API Access]\\n‚Ä°PanGu-Coder2 [234]\\n15B\\n61.64\\n-\\nPanGu-Coder [55]\\n2.6B\\n23.78\\n-\\nCodex [48]\\n12B\\n28.81\\nDeprecated\\nPaLM-Coder [54]\\n540B\\n36\\n-\\nAlphaCode [151]\\n1.1B\\n17.1\\n-\\nOpen Source\\n‚Ä°Codestral [181]\\n22B\\n81.1\\n[Checkpoint Download]\\n‚Ä°DeepSeek-Coder-V2-Instruct [331]\\n21B (236B)\\n90.2\\n[Checkpoint Download]\\n‚Ä°Qwen2.5-Coder-Instruct [109]\\n7B\\n88.4\\n[Checkpoint Download]\\nQwen2.5-Coder [109]\\n7B\\n61.6\\n[Checkpoint Download]\\n‚Ä°StarCoder2-Instruct [304]\\n15.5B\\n72.6\\n[Checkpoint Download]\\n‚Ä°CodeGemma-Instruct [59]\\n7B\\n56.1\\n[Checkpoint Download]\\nCodeGemma [59]\\n7B\\n44.5\\n[Checkpoint Download]\\nStarCoder 2 [170]\\n15B\\n46.3\\n[Checkpoint Download]\\n‚Ä°WaveCoder-Ultra [301]\\n6.7B\\n79.9\\n[Checkpoint Download]\\n‚Ä°WaveCoder-Pro [301]\\n6.7B\\n74.4\\n[Checkpoint Download]\\n‚Ä°WaveCoder-DS [301]\\n6.7B\\n65.8\\n[Checkpoint Download]\\nStableCode [210]\\n3B\\n29.3\\n[Checkpoint Download]\\nCodeShell [285]\\n7B\\n34.32\\n[Checkpoint Download]\\n‚Ä°CodeQwen1.5-Chat [249]\\n7B\\n83.5\\n[Checkpoint Download]\\nCodeQwen1.5 [249]\\n7B\\n51.8'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 39, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[Checkpoint Download]\\nStableCode [210]\\n3B\\n29.3\\n[Checkpoint Download]\\nCodeShell [285]\\n7B\\n34.32\\n[Checkpoint Download]\\n‚Ä°CodeQwen1.5-Chat [249]\\n7B\\n83.5\\n[Checkpoint Download]\\nCodeQwen1.5 [249]\\n7B\\n51.8\\n[Checkpoint Download]\\n‚Ä°DeepSeek-Coder-Instruct [88]\\n33B\\n79.3\\n[Checkpoint Download]\\nDeepSeek-Coder [88]\\n33B\\n56.1\\n[Checkpoint Download]\\nreplit-code [223]\\n3B\\n20.12\\n[Checkpoint Download]\\n‚Ä°MagicoderùëÜ-CL [278]\\n7B\\n70.7\\n[Checkpoint Download]\\n‚Ä°Magicoder-CL [278]\\n7B\\n60.4\\n[Checkpoint Download]\\n‚Ä°WizardCoder [173]\\n33B\\n79.9\\n[Checkpoint Download]\\nCodeFuse [160]\\n34B\\n74.4\\n[Checkpoint Download]\\nPhi-1 [84]\\n1.3B\\n50.6\\n[Checkpoint Download]\\n‚Ä°Code Llama-Instruct [227]\\n70B\\n67.8\\n[Checkpoint Download]\\nCode Llama [227]\\n70B\\n53.0\\n[Checkpoint Download]\\n‚Ä°OctoCoder [187]\\n15.5B\\n46.2\\n[Checkpoint Download]\\nCodeGeeX2 [321]\\n6B\\n35.9\\n[Checkpoint Download]\\n‚Ä°InstructCodeT5+ [269]\\n16B\\n35.0\\n[Checkpoint Download]\\nCodeGen-NL [193]\\n16.1B\\n14.24\\n[Checkpoint Download]\\nCodeGen-Multi [193]\\n16.1B\\n18.32\\n[Checkpoint Download]\\nCodeGen-Mono [193]'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 39, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[Checkpoint Download]\\n‚Ä°InstructCodeT5+ [269]\\n16B\\n35.0\\n[Checkpoint Download]\\nCodeGen-NL [193]\\n16.1B\\n14.24\\n[Checkpoint Download]\\nCodeGen-Multi [193]\\n16.1B\\n18.32\\n[Checkpoint Download]\\nCodeGen-Mono [193]\\n16.1B\\n29.28\\n[Checkpoint Download]\\nStarCoder [147]\\n15B\\n33.60\\n[Checkpoint Download]\\nCodeT5+ [271]\\n16B\\n30.9\\n[Checkpoint Download]\\nCodeGen2 [192]\\n16B\\n20.46\\n[Checkpoint Download]\\nSantaCoder [9]\\n1.1B\\n14.0\\n[Checkpoint Download]\\nInCoder [77]\\n6.7B\\n15.2\\n[Checkpoint Download]\\nPolyCoder [290]\\n2.7B\\n5.59\\n[Checkpoint Download]\\nCodeParrot [254]\\n1.5B\\n3.99\\n[Checkpoint Download]\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 40, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:41\\ncase average [95], execution accuracy [218], and pass@t [195]. In particular, the pass@k, serving\\nas a principal evaluation metric, assesses the probability that at least one out of ùëòcode samples\\ngenerated by a model will pass all unit tests. An unbiased estimator for pass@k introduced by [48]\\nis defined as:\\npass@k B Etask\\n\"\\n1 ‚àí\\n\\x00ùëõ‚àíùëê\\nùëò\\n\\x01\\n\\x00ùëõ\\nùëò\\n\\x01\\n#\\n(20)\\nwhere ùëõis the total number of sampled candidate code solutions, ùëòis the number of randomly\\nselected code solutions from these candidates for each programming problem, with ùëõ‚â•ùëò, and ùëêis\\nthe count of correct samples within the ùëòselected.\\nNevertheless, these execution-based methods are heavily dependent on the quality of unit tests\\nand are limited to evaluating executable code [307]. Consequently, when unit tests are unavailable,\\ntoken-matching-based metrics are often employed as an alternative for evaluation. Furthermore, in'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 40, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and are limited to evaluating executable code [307]. Consequently, when unit tests are unavailable,\\ntoken-matching-based metrics are often employed as an alternative for evaluation. Furthermore, in\\nscenarios lacking a ground truth label, unsupervised metrics such as perplexity (PPL) [116] can\\nserve as evaluative tools. Perplexity quantifies an LLM‚Äôs uncertainty in predicting new content,\\nthus providing an indirect measure of the model‚Äôs generalization capabilities and the quality of the\\ngenerated code.\\nTaken together, while the aforementioned methods primarily focus on the functional correctness\\nof code, they do not provide a holistic evaluation that encompasses other critical dimensions such\\nas code vulnerability [189], maintainability [15], readability [34], complexity and efficiency [208],\\nstylistic consistency [178], and execution stability [215]. A comprehensive evaluation framework'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 40, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='as code vulnerability [189], maintainability [15], readability [34], complexity and efficiency [208],\\nstylistic consistency [178], and execution stability [215]. A comprehensive evaluation framework\\nthat integrates these aspects remains an open area for future research and development in the field\\nof code generation assessment.\\n5.10.2\\nHuman Evaluation. Given the intrinsic characteristics of code, the aforementioned automatic\\nevaluation metrics are inherently limited in their capacity to fully assess code quality. For instance,\\nmetrics specifically designed to measure code style consistency are challenging to develop and\\noften fail to capture this aspect adequately [44]. When it comes to repository-level code generation,\\nthe evaluation of overall code quality is substantially complicated due to the larger scale of the\\ntask, which involves cross-file designs and intricate internal as well as external dependencies, as\\ndiscussed by [22, 239].'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 40, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='task, which involves cross-file designs and intricate internal as well as external dependencies, as\\ndiscussed by [22, 239].\\nTo overcome these challenges, conducting human evaluations becomes necessary, as it yields\\nrelatively robust and reliable results. Human assessments also offer greater adaptability across\\nvarious tasks, enabling the simplification of complex and multi-step evaluations. Moreover, human\\nevaluations are essential for demonstrating the effectiveness of certain token-matching-based\\nmetrics, such as CodeBLEU [221]. These studies typically conduct experiments to evaluate the\\ncorrelation coefficient between proposed metrics and quality scores assigned by actual users,\\ndemonstrating their superiority over existing metrics.\\nMoreover, in an effort to better align LLMs with human preferences and intentions, InstructGPT\\n[200] employs human-written prompts and demonstrations, and model output ranking in the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 40, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Moreover, in an effort to better align LLMs with human preferences and intentions, InstructGPT\\n[200] employs human-written prompts and demonstrations, and model output ranking in the\\nfine-tuning of LLMs using reinforcement learning from human feedback (RLHF). Although similar\\nalignment learning techniques have been applied to code generation, the feedback in this domain\\ntypically comes from a compiler or interpreter, which offers execution feedback, rather than from\\nhuman evaluators. Notable examples include CodeRL [139], PPOCoder [238], RLTF [163], and\\nPanGu-Coder2 [234]. Further information on this topic is available in Section 5.5.\\nNonetheless, human evaluations are not without drawbacks, as they can be prone to certain\\nissues that may compromise their accuracy and consistency. For instance, 1) personalized tastes\\nand varying levels of expertise among human evaluators can introduce biases and inconsistencies'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 40, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='issues that may compromise their accuracy and consistency. For instance, 1) personalized tastes\\nand varying levels of expertise among human evaluators can introduce biases and inconsistencies\\ninto the evaluation process; 2) conducting comprehensive and reliable human evaluations often\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 41, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:42\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nInstruction\\nCode LLM\\n(Generator)\\n(Code) LLM\\n(Judge)\\nWhich response is better?\\nInstruction: {instruction}\\nResponse 1: {response 1}\\nResponse 2: {response 2}\\nPairwise Comparison\\nRate the response on a scale\\nof 1 to 10.\\nInstruction: {instruction}\\nResponse: {response 1/2}\\nSingle Answer Grading\\nResponse 2 is better\\nThe score is 7\\nResponse 1\\nResponse 2\\nFig. 16. The pipeline of (Code) LLM-as-a-judge for evaluating generated code by Code LLMs. There are\\nprimarily two types of approaches: pairwise comparison and single answer grading.\\nnecessitates a substantial number of evaluators, leading to significant expenses and time-consuming;\\n3) the reproducibility of human evaluations is often limited, which presents challenges in extending\\nprevious evaluation outcomes or monitoring the progress of LLMs, as highlighted by [319].\\n5.10.3\\nLLM-as-a-Judge. The powerful instruction-following capabilities of LLMs have stimulated'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 41, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='previous evaluation outcomes or monitoring the progress of LLMs, as highlighted by [319].\\n5.10.3\\nLLM-as-a-Judge. The powerful instruction-following capabilities of LLMs have stimulated\\nresearchers to innovatively investigate the potential of LLM-based evaluations. The LLM-as-a-Judge\\n[320] refers to the application of advanced proprietary LLMs (e.g., GPT4, Gemini, and Claud 3)\\nas proxies for human evaluators. This involves designing prompts with specific requirements\\nto guide LLMs in conducting evaluations, as demonstrated by AlpacaEval [148] and MT-bench\\n[320]. This method reduces reliance on human participation, thereby facilitating more efficient\\nand scalable evaluations. Moreover, LLMs can offer insightful explanations for the assigned rating\\nscores, thereby augmenting the interpretability of evaluations [319].\\nNevertheless, the use of LLM-based evaluation for code generation remains relatively underex-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 41, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='scores, thereby augmenting the interpretability of evaluations [319].\\nNevertheless, the use of LLM-based evaluation for code generation remains relatively underex-\\nplored compared with general-purpose LLM. The pipeline of (Code) LLM-as-a-judge for evaluating\\ngenerated code by Code LLMs is depicted in Figure 16. A recent work [332] introduces the ICE-Score\\nevaluation metric, which instructs LLM for code assessments. This approach attains superior corre-\\nlations with functional correctness and human preferences, thereby eliminating the requirement\\nfor test oracles or references. As the capabilities of LLM continue to improve, we anticipate seeing\\nmore research in this direction.\\nDespite their scalability and explainability, the effectiveness of LLM-based evaluation is con-\\nstrained by the inherent limitations of the chosen LLM. Several studies have shown that most LLMs,\\nincluding GPT-4, suffer from several issues, including position, verbosity, and self-enhancement'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 41, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='strained by the inherent limitations of the chosen LLM. Several studies have shown that most LLMs,\\nincluding GPT-4, suffer from several issues, including position, verbosity, and self-enhancement\\nbiases, as well as restricted reasoning ability [320]. Specifically, position bias refers to the tendency\\nof LLMs to disproportionately favor responses that are presented in certain positions, which can\\nskew the perceived quality of answers based on their order of presentation. Meanwhile, verbosity\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 42, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:43\\n1.1\\n5.5\\n13 15\\n33\\n70\\n# Parameters (B)\\n0\\n20\\n40\\n60\\n80\\nPass@1 (%)\\nGPT-3.5-Turbo\\nClaude-3-Sonnet\\nClaude-3-Haiku\\nClaude-3-Opus\\nQwen2.5-Coder\\nCodeGemma\\nStarCoder 2\\nWaveCoder\\nCodeFuse\\nCodeShell\\nCodeQwen1.5\\nDeepSeek-Coder\\nphi-1\\nCode Llama\\nCodeGeeX2\\nCodeGeeX\\nPanGu-Coder\\nCodeGen-NL\\nCodeGen-Multi\\nCodeGen-Mono\\nStarCoder\\nCodeT5+\\nSantaCoder\\nInCoder\\nPolyCoder\\nCodeParrot\\nCodestral\\nQwen2.5-Coder-Instruct\\nStarCoder2-Instruct\\nCodeGemma-Instruct\\nCodeQwen1.5-Chat\\nDeepSeek-Coder-Instruct\\nMagicoderS-CL\\nMagicoder-CL\\nWizardCoder\\nCode Llama-Instruct\\nBase\\nInstruct\\nFig. 17. The performance comparison of LLMs for code generation on the MBPP [17] benchmark, measured by\\nPass@1. For models with various sizes, we report only the largest size version of each model with a magnitude\\nof B parameters.\\nbias describes the inclination of LLMs to prefer lengthier responses, even when these are not'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 42, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='of B parameters.\\nbias describes the inclination of LLMs to prefer lengthier responses, even when these are not\\nnecessarily of higher quality compared to more concise ones. Self-enhancement bias, on the other\\nhand, is observed when LLMs consistently overvalue the quality of the text they generate [319, 320].\\nMoreover, due to their inherent limitations in tackling complex reasoning challenges, LLMs may not\\nbe entirely reliable as evaluators for tasks that require intensive reasoning, such as those involving\\nmathematical problem-solving. However, these shortcomings can be partially addressed through\\nthe application of deliberate prompt engineering and fine-tuning techniques, as suggested by [320].\\n5.10.4\\nEmpirical Comparison. In this section, we present a performance comparison of LLMs for\\ncode generation using the well-regarded HumanEval, MBPP, and the more practical and chal-\\nlenging BigCodeBench benchmarks. This empirical comparison aims to highlight the progressive'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 42, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='code generation using the well-regarded HumanEval, MBPP, and the more practical and chal-\\nlenging BigCodeBench benchmarks. This empirical comparison aims to highlight the progressive\\nenhancements in LLM capabilities for code generation. These benchmarks assess an LLM‚Äôs ability to\\ngenerate source code across various levels of difficulty and types of programming tasks. Specifically,\\nHumanEval focuses on complex code generation, MBPP targets basic programming tasks, and\\nBigCodeBench emphasizes practical and challenging programming tasks.\\nDue to the limitations in computational resources we faced, we have cited experimental results\\nfrom original papers or widely recognized open-source leaderboards within the research community,\\nsuch as the HumanEval Leaderboard 16, EvalPlus Leaderboard 17, Big Code Models Leaderboard\\n18, and BigCodeBench Leaderboard 19. We report performance on HumanEval using the pass@1'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 42, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='such as the HumanEval Leaderboard 16, EvalPlus Leaderboard 17, Big Code Models Leaderboard\\n18, and BigCodeBench Leaderboard 19. We report performance on HumanEval using the pass@1\\nmetric, as shown in Table 9, while MBPP and BigCodeBench results are presented with pass@1 in\\nFigures 17 and 18, respectively.\\nWe offer the following insights:\\n16https://paperswithcode.com/sota/code-generation-on-humaneval\\n17https://evalplus.github.io/leaderboard.html\\n18https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard\\n19https://bigcode-bench.github.io/\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 43, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:44\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nCodeGemma-7B\\nStarCoder 2-15B\\nCodeGemma-Instruct-7B\\nCodeQwen1.5-Chat-7B\\nWaveCoder-Ultra-6.7B\\nCode Llama-70B\\nStarCoder2-Instruct-15B\\nCodeQwen1.5-7B\\nDeepSeek-Coder-33B\\nMagicoder-S-DS-7B\\nPhi-3-Medium-128K-Instruct-14B\\nQwen2.5-Coder-Instruct-7B\\nCodeGeeX4-9B\\nCode Llama-Instruct-70B\\nClaude-3-Haiku\\nGPT-3.5-Turbo-0125\\nDeepSeek-Coder-Instruct-33B\\nCodestral-22B\\nClaude-3-Sonnet\\nGemini-1.5-Flash\\nGPT-4-0613\\nClaude-3-Opus\\nGemini-1.5-Pro\\nGPT-4-Turbo-0409\\nClaude-3.5-Sonnet\\nDeepSeek-Coder-V2-Instruct-21B (236B)\\nGPT-4o-0513\\n40\\n45\\n50\\n55\\n60\\nPass@1 (%)\\n38.3\\n38.4\\n39.3\\n43.6\\n43.7\\n44\\n45.1\\n45.6\\n46.6\\n47.6\\n48.7\\n48.8\\n49\\n49.6\\n50.1\\n50.6\\n51.1\\n52.5\\n53.8\\n55.1\\n57.2\\n57.4\\n57.5\\n58.2\\n58.6\\n59.7\\n61.1\\nGPT-3.5-Turbo-0125\\nClosed Source\\nOpen Source\\nFig. 18. The performance comparison of LLMs for code generation on the BigCodeBench [333] benchmark,\\nmeasured by Pass@1. For models with various sizes, we report only the largest size version of each model'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 43, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='measured by Pass@1. For models with various sizes, we report only the largest size version of each model\\nwith a magnitude of B parameters.\\n‚Ä¢ The performance gap between open-source and closed-source models across the three bench-\\nmarks is gradually narrowing. For instance, on the HumanEval benchmark, DeepSeek-Coder-\\nV2-Instruct with 21B activation parameters and Qwen2.5-Coder-Instruct 7B achieve 90% and\\n88.4% pass@1, respectively. These results are comparable to the much larger closed-source\\nLLMs, such as Claude-3.5-Sonnet, which achieves 92.0% pass@1. On the MBPP benchmark,\\nQwen2.5-Coder-Instruct 7B with 83.5% pass@1 significantly outperforms GPT-3.5-Turbo\\nwith 52.2% pass@1 and closely rivals the closed-source Claude-3-Opus with 86.4% pass@1.\\nOn the BigCodeBench, DeepSeek-Coder-V2-Instruct achieves 59.7%, surpassing all compared\\nclosed-source and open-source LLMs except for slightly falling behind GPT-4o-0513, which\\nachieves 61.1%.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 43, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='On the BigCodeBench, DeepSeek-Coder-V2-Instruct achieves 59.7%, surpassing all compared\\nclosed-source and open-source LLMs except for slightly falling behind GPT-4o-0513, which\\nachieves 61.1%.\\n‚Ä¢ Generally, as the number of model parameters increases, the performance of code LLMs\\nimproves. However, Qwen2.5-Coder-Instruct 7B achieves 88.4% pass@1, outperforming larger\\nmodels like StarCoder2-Instruct 15.5B with 72.6% pass@1, DeepSeek-Coder-Instruct 33B\\nwith 79.3% pass@1, and Code Llama-Instruct 70B with 67.8% pass@1 on the HumanEval\\nbenchmark. Similar trends are observed across the other two benchmarks, suggesting that\\ncode LLMs with 7B parameters may be sufficiently capable for code generation task.\\n‚Ä¢ Instruction-tuned models consistently outperform their base (pretrained) counterparts across\\nthe HumanEval and MBPP benchmarks. For instance, Qwen2.5-Coder-Instruct surpasses\\nQwen2.5-Coder by an average of 26.04%, StarCoder2-Instruct improves upon StarCoder 2'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 43, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='the HumanEval and MBPP benchmarks. For instance, Qwen2.5-Coder-Instruct surpasses\\nQwen2.5-Coder by an average of 26.04%, StarCoder2-Instruct improves upon StarCoder 2\\nby an average of 35.20%, and CodeGemma-Instruct enhances CodeGemma by an average\\nof 11.26%. Additionally, DeepSeek-Coder-Instruct outperforms DeepSeek-Coder by an aver-\\nage of 23.71%, while Code Llama-Instruct shows a 13.80% improvement over Code Llama.\\nDetailed results can be found in Table 10. These findings underscore the effectiveness of\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 44, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:45\\nTable 10. The performance improvement of instruction-tuned models over their pretrained counterparts\\non the HumanEval, MBPP, and BigCodeBench benchmarks. The last two rows demonstrate the average\\nimprovement on the first two benchmarks and the three benchmarks, respectively.\\nQwen2.5-Coder-\\nInstruct 7B\\nStarCoder2-\\nInstruct 15.5B\\nCodeGemma-\\nInstruct 7B\\nDeepSeek-Coder-\\nInstruct 33B\\nCode Llama-\\nInstruct 70B\\nHumanEval\\n43.51%\\n56.80%\\n26.07%\\n41.35%\\n27.92%\\nMBPP\\n8.58%\\n13.60%\\n-3.56%\\n6.06%\\n-0.32%\\nBigCodeBench\\n-\\n17.45%\\n2.61%\\n9.66%\\n12.73%\\n# Avg. Imp. H. M.\\n26.04%\\n35.20%\\n11.26%\\n23.71%\\n13.80%\\n# Avg. Imp. H. M. B.\\n-\\n29.28%\\n8.37%\\n19.02%\\n13.44%\\ninstruction tuning, although the quality of the instruction tuning dataset plays a critical role\\nin determining model performance [173, 328].\\n‚Ä¢ Performance on the HumanEval benchmark is nearly saturated. However, MBPP, which'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 44, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='in determining model performance [173, 328].\\n‚Ä¢ Performance on the HumanEval benchmark is nearly saturated. However, MBPP, which\\ninvolves basic programming tasks, and BigCodeBench, which involves more practical and\\nchallenging programming tasks, demand more capable code LLMs. Additionally, while these\\nbenchmarks primarily evaluate the functional correctness of code, they do not provide\\na comprehensive assessment across other critical dimensions. Developing a more holistic\\nevaluation framework that integrates various aspects remains an open area for future research\\nand development in LLMs for code generation evaluation.\\nDiscussion: We discuss certain code LLMs in Table 9 for clarity: (1) General LLMs accessed via\\nAPI are not specifically trained on large code corpora but achieve state-of-the-art performance\\nin code generation, such as Claude-3.5-Sonnet with 92.0% pass@1 on HumanEval benchmark.\\n(2) AlphaCode targets code generation for more complex and unseen problems that require a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 44, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='in code generation, such as Claude-3.5-Sonnet with 92.0% pass@1 on HumanEval benchmark.\\n(2) AlphaCode targets code generation for more complex and unseen problems that require a\\ndeep understanding of algorithms and intricate natural language, such as those encountered in\\ncompetitive programming. The authors of AlphaCode found that large-scale model sampling to\\nnavigate the search space, such as 1M samples per problem for CodeContests, followed by filtering\\nbased on program behavior to produce a smaller set of submissions, is crucial for achieving good and\\nreliable performance on problems that necessitate advanced reasoning. (3) Phi-1 1.3B is a specialized\\nLLM for code, trained on ‚Äútextbook quality‚Äù data from the web (6B tokens) and synthetically\\ngenerated textbooks and exercises using GPT-3.5 (1B tokens). (4) Code Llama 70B is initialized with\\nLlama 2 model weights and continually pre-trained on 1T tokens from a code-heavy dataset and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 44, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='generated textbooks and exercises using GPT-3.5 (1B tokens). (4) Code Llama 70B is initialized with\\nLlama 2 model weights and continually pre-trained on 1T tokens from a code-heavy dataset and\\nlong-context fine-tuned with approximately 20B tokens. However, Code Llama-Instruct 70B is fine-\\ntuned from Code Llama-Python 70B without long-context fine-tuning, using an additional 260M\\ntokens to better follow human instructions. Surprisingly, these models underperform compared to\\nsmaller parameter Code LLMs like Qwen2.5-Coder-Instruct 7B, DeepSeek-Coder-V2-Instruct 21B,\\nand Codestral 22B across all three benchmarks. The underlying reasons for this discrepancy remain\\nunclear and warrant further exploration. (5) Unlike other open-source Code LLMs, DeepSeek-Coder-\\nV2-Instruct is further pre-trained on DeepSeek-V2 [159], which employs a Mixture-of-Experts (MoE)\\narchitecture with only 21B activation parameters out of 236B parameters, using an additional 6'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 44, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='V2-Instruct is further pre-trained on DeepSeek-V2 [159], which employs a Mixture-of-Experts (MoE)\\narchitecture with only 21B activation parameters out of 236B parameters, using an additional 6\\ntrillion tokens composed of 60% source code, 10% math corpus, and 30% natural language corpus.\\nFor a comprehensive understanding of MoE in LLMs, please refer to [36].\\n5.11\\nCode LLMs Alignment\\nThe pre-training of LLMs for next-token prediction, aimed at maximizing conditional generation\\nlikelihood across vast textual corpora, equips these models with extensive world knowledge and\\nemergent capabilities [33]. This training approach enables the generation of coherent and fluent\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 45, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:46\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\ntext in response to diverse instructions. Nonetheless, LLMs can sometimes misinterpret human\\ninstructions, produce biased content, or generate factually incorrect information (commonly referred\\nto as hallucinations), which may limit their practical utility [118, 272, 319].\\nAligning LLMs with human intentions and values, known as LLM alignment, has consequently\\nbecome a critical research focus [118, 272]. Key objectives frequently discussed in the context of LLM\\nalignment include robustness, interpretability, controllability, ethicality, trustworthiness, security,\\nprivacy, fairness, and safety. In recent years, significant efforts have been made by researchers\\nto achieve this alignment, employing techniques such as Reinforcement Learning with Human\\nFeedback (RLHF) [200].\\nHowever, the alignment of Code LLMs has not been extensively explored. Compared to text'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 45, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Feedback (RLHF) [200].\\nHowever, the alignment of Code LLMs has not been extensively explored. Compared to text\\ngeneration, aligning code generation with human intentions and values is even more crucial. For\\ninstance, users without programming expertise might prompt Code LLM to generate source code\\nand subsequently execute it on their computers, potentially causing catastrophic damage. Some\\npotential risks include:\\n‚Ä¢ Malware Infection: The code could contain viruses, worms, or trojans that compromise our\\nsystem‚Äôs security.\\n‚Ä¢ Data Loss: It might delete or corrupt important files and data.\\n‚Ä¢ Unauthorized Access: It can create backdoors, allowing attackers to access our system\\nremotely.\\n‚Ä¢ Performance Issues: The code might consume excessive resources, slowing down our\\nsystem.\\n‚Ä¢ Privacy Breaches: Sensitive information, such as passwords or personal data, might be\\nstolen.\\n‚Ä¢ System Damage: It may alter system settings or damage hardware components.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 45, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='system.\\n‚Ä¢ Privacy Breaches: Sensitive information, such as passwords or personal data, might be\\nstolen.\\n‚Ä¢ System Damage: It may alter system settings or damage hardware components.\\n‚Ä¢ Network Spread: It could propagate across networks, affecting other devices.\\n‚Ä¢ Financial Loss: If the code is ransomware, it might encrypt data and demand payment for\\ndecryption.\\n‚Ä¢ Legal Consequences: Running certain types of malicious code can lead to legal repercus-\\nsions.\\nAs illustrated, aligning Code LLMs to produce source code consistent with human preferences and\\nvalues is of paramount importance in software development. A recent study [293] provides the first\\nsystematic literature review identifying seven critical non-functional properties of LLMs for code,\\nbeyond accuracy, including robustness, security, privacy, explainability, efficiency, and usability.\\nThis study is highly pertinent to the alignment of Code LLMs. We recommend readers refer to this\\nsurvey for more detailed insights.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 45, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='This study is highly pertinent to the alignment of Code LLMs. We recommend readers refer to this\\nsurvey for more detailed insights.\\nIn this survey, we identify five core principles that serve as the key objectives for aligning Code\\nLLMs: Green, Responsibility, Efficiency, Safety, and Trustworthiness (collectively referred to as\\nGREST). These principles are examined from a broader perspective. Each category encompasses\\nvarious concepts and properties, which are summarized in Table 11. In the following, we define\\neach principle and briefly introduce a few notable works to enhance understanding.\\nGreen: The Green principle underscores the importance of environmental sustainability in\\nthe development and deployment of LLMs for code generation. This involves optimizing energy\\nconsumption and reducing both the carbon footprint and financial costs associated with training\\nand inference processes. Currently, training, inference, and deployment of Code LLMs are notably'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 45, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='consumption and reducing both the carbon footprint and financial costs associated with training\\nand inference processes. Currently, training, inference, and deployment of Code LLMs are notably\\nresource-intensive. For example, training GPT-3, with its 175 billion parameters, required the\\nequivalent of 355 years of single-processor computing time and consumed 284,000 kWh of energy,\\nresulting in an estimated 552.1 tons of CO2 emissions [228]. Furthermore, a ChatGPT-like application,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:47\\nTable 11. Five core principles serve as the key objectives for Code LLMs alignment: Green, Responsibility,\\nEfficiency, Safety, and Trustworthiness (collectively referred to as GREST).\\nPrinciples\\nInvolved Concepts and Properties\\nGreen\\nEnergy Efficiency: Minimizing computational energy use and reduce environmental impact and financial costs.\\nSustainable Materials: Leveraging eco-friendly infrastructure and servers for code generation, lowering long-term expenses.\\nCarbon Footprint: Reducing emissions associated with model training and inference to enhance efficiency and save costs.\\nResource Optimization: Efficiently utilizing computational resources to minimize waste and reduce expenses in code generation.\\nRecycling Management: Responsibly dispose of hardware used in model development to reduce waste management costs.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Recycling Management: Responsibly dispose of hardware used in model development to reduce waste management costs.\\nRenewable Energy: Utilizing renewable energy sources for powering training and inference processes to decrease energy costs.\\nLifecycle Assessment: Evaluating the environmental and financial impacts of models from creation to deployment and disposal.\\nResponsibility\\nEthical Considerations: Adhering to ethical guidelines to ensure responsible use and deployment of generated code.\\nAccountability: Establishing clear lines of responsibility for code generation outcomes and potential impacts.\\nUser Education: Providing resources and guidance to help users understand and responsibly use generated code.\\nImpact Assessment: Evaluating the social and technical implications of code generation to minimize negative effects.\\nRegulatory Compliance: Ensuring that generated code adheres to relevant laws (e.g., copyright) and industry regulations.\\nEfficiency'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Regulatory Compliance: Ensuring that generated code adheres to relevant laws (e.g., copyright) and industry regulations.\\nEfficiency\\nModel Optimization: Streamlining models to reduce computational load and improve speed.\\nPrompt Engineering: Designing effective prompts to generate accurate code efficiently.\\nResource Management: Allocating computational resources wisely to balance speed and cost.\\nInference Optimization: Enhancing the inference process to quickly generate code with minimal latency.\\nParallel Processing: Utilizing parallelism to speed up code generation tasks.\\nCaching Mechanisms: Implementing caching to reuse previous results and reduce redundant computations.\\nEvaluation Metrics: Using precise metrics to assess and improve the efficiency of code outputs.\\nSafety\\nInput Validation: Ensuring inputs (prompts) are safe and sanitized to prevent malicious exploitation.\\nSecurity Audits: Regularly reviewing generated code for vulnerabilities and potential exploits.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Input Validation: Ensuring inputs (prompts) are safe and sanitized to prevent malicious exploitation.\\nSecurity Audits: Regularly reviewing generated code for vulnerabilities and potential exploits.\\nMonitoring and Logging: Keeping track of generation outputs to quickly identify and address safety issues.\\nUser Access Control: Limiting access to generation capabilities to trusted users to minimize risk.\\nContinuous Updates: Regularly updating models with the latest safety protocols and security patches.\\nEthical Guidelines: Implementing ethical standards to guide safe and responsible code generation.\\nTrustworthiness\\nReliability: Ensuring that generated code consistently meets functional requirements and performs as expected.\\nTransparency: Providing clear explanations of how code is generated to build user confidence.\\nVerification and Testing: Using rigorous testing frameworks to ensure the generated code accuracy and reliability.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Verification and Testing: Using rigorous testing frameworks to ensure the generated code accuracy and reliability.\\nBias Mitigation: Actively working to identify and reduce biases in code generation to ensure fairness and impartiality.\\nUser Feedback Integration: Continuously incorporating user feedback to refine and improve code generation processes.\\nDocumentation: Providing comprehensive documentation for generated code to enhance understanding and trust.\\nwith an estimated usage of 11 million requests per hour, can produce emissions of 12.8k metric\\ntons of CO2 per year, which is 25 times the carbon emissions associated with training GPT-3\\n[53]. To mitigate these costs, several techniques are often employed, such as the development of\\nspecialized hardware (e.g., Tensor Processing Units (TPUs) and Neural Processing Units (NPUs)),\\nmodel compression methods (e.g., quantization and knowledge distillation), parameter-efficient'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='specialized hardware (e.g., Tensor Processing Units (TPUs) and Neural Processing Units (NPUs)),\\nmodel compression methods (e.g., quantization and knowledge distillation), parameter-efficient\\nfine-tuning (PEFT), and the use of renewable energy sources. For instance, Shi et al. [235] applied\\nknowledge distillation to reduce the size of CodeBERT [76] and GraphCodeBERT [86], resulting in\\noptimized models of just 3MB. These models are 160 times smaller than the original large models\\nand significantly reduce energy consumption by up to 184 times and carbon footprint by up to 157\\ntimes. Similarly, Wei et al. [277] utilized quantization techniques for Code LLMs such as CodeGen\\n[193] and Incoder [77] by employing lower-bit integers (e.g., int8). This approach reduced storage\\nrequirements by 67.3% to 70.8%, carbon footprint by 28.8% to 55.0%, and pricing costs by 28.9% to\\n55.0%.\\nResponsibility: The Responsibility principle in the context of Code LLMs underscores the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='requirements by 67.3% to 70.8%, carbon footprint by 28.8% to 55.0%, and pricing costs by 28.9% to\\n55.0%.\\nResponsibility: The Responsibility principle in the context of Code LLMs underscores the\\nimportance of ethical considerations, fairness, and accountability throughout their lifecycle. This\\ninvolves addressing biases in training data, ensuring fairness and transparency in model decision-\\nmaking, maintaining accountability for outputs, adhering to applicable laws (e.g., copyright),\\nimplementing safeguards against misuse, and providing clear communication about the model‚Äôs\\ncapabilities and limitations. Specifically,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 47, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:48\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n‚Ä¢ Bias Mitigation. Biases in code generation can lead to flawed software and reinforce stereo-\\ntypes, potentially causing significant societal impacts. For example, an Code LLM that in-\\nherits biases from its training data may produce source code/software that inadvertently\\ndiscriminates against certain user groups. This can result in applications that fail to meet the\\ndiverse needs of users, promoting exclusionary practices and reinforcing existing stereotypes\\n[168, 185].\\n‚Ä¢ Fairness and Transparency. A lack of fairness and transparency in Code LLM decision-making\\ncan result in biased or suboptimal code solutions. If the model‚Äôs decision-making process is\\nopaque, developers might unknowingly introduce code that favors specific frameworks or\\nlibraries, thereby limiting innovation and diversity in software development. This opacity'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 47, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='opaque, developers might unknowingly introduce code that favors specific frameworks or\\nlibraries, thereby limiting innovation and diversity in software development. This opacity\\ncan create unfair advantages and hinder collaborative efforts within tech communities [31].\\n‚Ä¢ Legal Compliance. Compliance with relevant laws, such as licensing and copyright, is crucial\\nwhen using Code LLMs for code generation to avoid legal complications. If an Code LLM\\ngenerates code snippets that inadvertently infringe on existing copyrights, it can lead to\\nlegal disputes and financial liabilities for developers and organizations [292]. Such risks may\\ndiscourage the use of advanced AI tools, thus stifling innovation and affecting growth and\\ncollaboration within the tech community.\\n‚Ä¢ Accountability. Without accountability for code generated by Code LLMs, addressing bugs\\nor security vulnerabilities becomes challenging. If a model generates faulty code leading to'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 47, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Accountability. Without accountability for code generated by Code LLMs, addressing bugs\\nor security vulnerabilities becomes challenging. If a model generates faulty code leading to\\na security breach, the absence of clear accountability can result in significant financial and\\nreputational damage for companies. This uncertainty can delay critical issue resolution and\\nimpede trust in AI-assisted development [155].\\n‚Ä¢ Misuse Prevention. Failing to implement mechanisms to prevent the misuse of Code LLMs can\\nenable the creation of harmful software. For example, models could be exploited to generate\\nmalware or unauthorized scripts, posing cybersecurity risks. Without proper safeguards,\\nthese models can facilitate malicious activities, threatening both individual and organizational\\nsecurity [184].\\n‚Ä¢ Clear Communication. Without clear communication about a model‚Äôs capabilities and limita-\\ntions, developers may misuse the model or overestimate its abilities. Relying on the model'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 47, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='security [184].\\n‚Ä¢ Clear Communication. Without clear communication about a model‚Äôs capabilities and limita-\\ntions, developers may misuse the model or overestimate its abilities. Relying on the model\\nto generate complex, mission-critical code without human oversight can lead to significant\\nsoftware failures. Misunderstanding its limitations can result in faulty implementations and\\nlost productivity [226].\\nTo adhere to this principle, potential mitigation methods include bias detection and mitigation,\\nquantification and evaluation, and adherence to ethical guidelines. Liu et al. [168] propose a new\\nparadigm for constructing code prompts, successfully uncovering social biases in code generation\\nmodels, and developing a dataset along with three metrics to evaluate overall social bias. Recently,\\nXu et al. [292] introduced LiCoEval, an evaluation benchmark for assessing the license compliance\\ncapabilities of LLMs. Additionally, incorporating diverse perspectives in development teams and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 47, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Xu et al. [292] introduced LiCoEval, an evaluation benchmark for assessing the license compliance\\ncapabilities of LLMs. Additionally, incorporating diverse perspectives in development teams and\\nengaging with stakeholders from various communities can further align Code LLM outputs with\\nethical standards and societal values.\\nEfficiency: The Efficiency principle emphasizes optimizing the performance and speed of Code\\nLLMs for code generation while minimizing the computational resources required for training\\nand inference. For instance, training the GPT-3 model, which consists of 175 billion parameters,\\ndemands substantial resources. It requires approximately 1,024 NVIDIA V100 GPUs, costing around\\n4.6 million and taking approximately 34 days to complete the training process. To address these\\nchallenges, various techniques are employed, including model compression methods such as\\npruning, quantization, and knowledge distillation. Additionally, optimized algorithms like AdamW,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 47, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='challenges, various techniques are employed, including model compression methods such as\\npruning, quantization, and knowledge distillation. Additionally, optimized algorithms like AdamW,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 48, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:49\\nparallel strategies such as tensor, pipeline, and data parallelism, and parameter-efficient fine-tuning\\n(PEFT) (see Section 5.4.2) are often utilized. For a comprehensive and detailed discussion on methods\\nto enhance the efficiency of Code LLMs for code generation, please refer to Section 4.5.2, ‚ÄúEfficiency\\nEnhancement‚Äù, in [293].\\nSafety: The Safety principle of Code LLMs is of utmost importance due to their potential\\nto introduce vulnerabilities, errors, or privacy breaches into software systems. Ensuring safety\\ninvolves comprehensive testing and validation processes to detect and mitigate these risks. For\\ninstance, attackers might compromise the training process of LLMs by injecting malicious examples\\ninto the training data, a method known as data poisoning attacks [231]. Even when attackers\\nlack access to the training process, they may employ techniques like the black-box inversion'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 48, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='into the training data, a method known as data poisoning attacks [231]. Even when attackers\\nlack access to the training process, they may employ techniques like the black-box inversion\\napproach introduced by Hajipour et al. [91]. This method uses few-shot prompting to identify\\nprompts that coax black-box code generation models into producing vulnerable code. Furthermore,\\nYang et al. [294] and Al-Kaswan et al. [8] reveals that Code LLMs, such as CodeParrot [73], can\\nmemorize training data, potentially outputting personally identifiable information like emails,\\nnames, and IP addresses, thereby posing significant privacy risks. Additionally, Yuan et al. [302]\\ndemonstrate that engaging with ChatGPT and GPT-4 in non-natural languages can circumvent\\nsafety alignment measures, leading to unsafe outcomes, such as ‚ÄúThe steps involved in stealing\\nmoney from a bank.‚Äù. To bolster the safety of LLMs in code generation, it is crucial to detect and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 48, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='safety alignment measures, leading to unsafe outcomes, such as ‚ÄúThe steps involved in stealing\\nmoney from a bank.‚Äù. To bolster the safety of LLMs in code generation, it is crucial to detect and\\neliminate privacy-related information from training datasets. For example, approaches outlined in\\n[77] and [9] utilize carefully crafted regular expressions to identify and remove private information\\nfrom training data. To counteract black-box inversion, implementing prompt filtering mechanisms\\nis recommended to identify and block prompts that might result in insecure code generation.\\nMoreover, adversarial training can enhance the model‚Äôs resilience to malicious prompts. Employing\\nreinforcement learning methods can further align Code LLMs with human preferences, thereby\\nreducing the likelihood of producing harmful outputs.\\nTrustworthiness: The Trustworthiness principle focuses on developing Code LLMs that users'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 48, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='reducing the likelihood of producing harmful outputs.\\nTrustworthiness: The Trustworthiness principle focuses on developing Code LLMs that users\\ncan depend on for accurate and reliable code generation, which is crucial for their acceptance and\\nwidespread adoption. Achieving this requires ensuring model transparency, providing explanations\\nfor decisions, and maintaining consistent performance across various scenarios. For instance,\\nJi et al. [120] propose a causal graph-based representation of prompts and generated code to\\nidentify the causal relationships between them. This approach offers insights into the effectiveness\\nof Code LLMs and assists end-users in understanding the generation. Similarly, Palacio et al.\\n[202] introduce ASTxplainer, a tool that extracts and aggregates normalized model logits within\\nAbstract Syntax Tree (AST) structures. This alignment of token predictions with AST nodes'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 48, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[202] introduce ASTxplainer, a tool that extracts and aggregates normalized model logits within\\nAbstract Syntax Tree (AST) structures. This alignment of token predictions with AST nodes\\nprovides visualizations that enhance end-user understanding of Code LLM predictions. Therefore,\\nby prioritizing trustworthiness, we can bolster user confidence and facilitate the integration of\\nCode LLMs into diverse coding environments. By adhering to the aforementioned principles as key\\nobjectives for aligning Code LLMs, researchers and developers can create LLMs for code generation\\nthat are not only capable but also ethical, sustainable, and user-centric.\\n5.12\\nApplications\\nCode LLMs have been integrated with development tools and platforms, such as integrated de-\\nvelopment environments (IDEs) and version control systems, improving programming efficiency\\nsubstantially. In this section, we will briefly introduce several widely used applications as coding'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 48, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='velopment environments (IDEs) and version control systems, improving programming efficiency\\nsubstantially. In this section, we will briefly introduce several widely used applications as coding\\nassistants. The statistics of these applications are provided in Table 12.\\nGitHub Copilot. GitHub Copilot, powered by OpenAI‚Äôs Codex, is an AI pair programmer that\\nhelps you write better code faster. Copilot suggests whole lines or blocks of code as you type, based\\non the context provided by your existing code and comments. It‚Äôs trained on a dataset that includes\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 49, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:50\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 12. The overview of code assistant applications powered by LLMs. The column labeled ‚ÄòPLs‚Äô and ‚ÄòIDEs‚Äô\\nindicate programming languages and integrated development environments, respectively [307].\\nInstitution\\nProducts\\nModel\\nSupported Features\\nSupported PLs\\nSupported IDEs\\nGitHub & OpenAI\\nGitHub Copilot [48]\\nCodex\\nCode Completions, Code Generation,\\nCoding Questions Answering,\\nCode Refactoring, Code Issues Fix,\\nUnit Test Cases Generation,\\nCode Documentation Generation\\nJava, Python, JavaScript, TypeScript,\\nPerl, R, PowerShell, Rust, SQL, CSS,\\nRuby, Julia, C#, PHP, Swift, C++,Go,\\nHTML, JSON, SCSS, .NET, Less,\\nT-SQL, Markdown\\nVisual Studio, VS Code, Neovim,\\nJetBrains IDE\\nZhipu AI\\nCodeGeeX [321]\\nCodeGeeX\\nCode Generation, Code Translation,\\nCode Completion, Code Interpretation,\\nCode Bugs Fix, Comment Generation,\\nAI Chatbot\\nPHP, Go, C, C#, C++, Rust, Perl, CSS,\\nJava, Python, JavaScript, TypeScript,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 49, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Code Generation, Code Translation,\\nCode Completion, Code Interpretation,\\nCode Bugs Fix, Comment Generation,\\nAI Chatbot\\nPHP, Go, C, C#, C++, Rust, Perl, CSS,\\nJava, Python, JavaScript, TypeScript,\\nObjective C++, Objective C, Pascal,\\nHTML, SQL, Kotlin, R, Shell, Cuda,\\nFortran, Tex, Lean, Scala\\nClion, RubyMine, AppCode, Aqua,\\nIntelliJ IDEA, VS Code, PyCharm,\\nAndroid Studio, WebStorm, Rider,\\nGoLand, DataGrip, DataSpell\\nAmazon\\nCodeWhisperer [12]\\n‚àí\\nCode Completion, Code Explanation,\\nCode Translation,\\nCode Security Identification,\\nCode Suggestion\\nJava, Python, TypeScript, JavaScript,\\nC#\\nJetBrains IDE, VS Code, AWS Cloud9,\\nAWS Lambda\\nCodeium\\nCodeium [60]\\n‚àí\\nCode Completion, Bug Detection,\\nCode Suggestions, AI Chatbot,\\nTest Type Generation,\\nTest Plan Creation,\\nCodebase Search\\nMore than 70 languages in total,\\nincluding but not limited to:\\nC, C#, C++, Dart, CSS, Go, Elixir,\\nHTML, Haskell, Julia, Java, JavaScript,\\nLisp, Kotlin, Lua, Objective-C,\\nPerl, Pascal, PHP, Protobuf,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 49, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='More than 70 languages in total,\\nincluding but not limited to:\\nC, C#, C++, Dart, CSS, Go, Elixir,\\nHTML, Haskell, Julia, Java, JavaScript,\\nLisp, Kotlin, Lua, Objective-C,\\nPerl, Pascal, PHP, Protobuf,\\nR, Python, Ruby, Scala, Rust,\\nSwift, SQL, TS, Vue\\nJetBrains, VSCode, Visual Studio,\\nColab, Jupyter, Deepnote,\\nNotebooks, Databricks, Chrome,\\nVim, Neovim, Eclipse, Emacs,\\nVSCode Web IDEs, Sublime Text\\nHuawei\\nCodeArts Snap [234]\\nPanGu-Coder\\nCode Generation, Code Explanation\\nResearch and Development Knowledge\\nQuestion and Answer\\nCode Comment, Code Debug\\nUnit Test Case Generation\\nJava, Python\\nPyCharm, VS Code, IntelliJ\\nTabnine\\nTabNine [246]\\n‚àí\\nCode Generation, Code Completion,\\nCode Explanation, Bug Fix,\\nCode Recommendation, Code Refactoring,\\nCode Test Generation,\\nDocstring Generation\\nPython, Javascript, Java, TypeScript,\\nHTML, Haskell, Matlab, Kotlin, Sass,\\nGo, PHP, Ruby, C, C#, C++, Swift,\\nRust, CSS, Perl, Angular, Dart, React,\\nObjective C, NodeJS, Scala,\\nSublime, PyCharm, Neovim, Rider,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 49, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='HTML, Haskell, Matlab, Kotlin, Sass,\\nGo, PHP, Ruby, C, C#, C++, Swift,\\nRust, CSS, Perl, Angular, Dart, React,\\nObjective C, NodeJS, Scala,\\nSublime, PyCharm, Neovim, Rider,\\nVS Code, IntelliJ IDE, Visual Studio,\\nPhpStorm, Vim, RubyMine, DataGrip,\\nAndroid Studio, WebStorm, Emacs,\\nClion, Jupyter Notebook, JupyterLab,\\nEclipse, GoLand, AppCode\\nReplit\\nReplit[222]\\nreplit-code\\nCode Completion, Code Editing,\\nCode Generation, Code Explanation,\\nCode Suggestion, Code Test Generation\\nC#, Bash, C, CSS, C++, Java, Go,\\nHTML, JavaScript, Perl, PHP,\\nRuby, Python, R, SQL, Rust\\n‚àí\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 50, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:51\\nFig. 19. An exemplar of GitHub Copilot to demonstrate how to use development tools powered by LLMs,\\nincluding powerful GPT 4o, o1-preview (Preview), and o1-mini (Preview). To illustrate its capabilities, we input\\nthe description of the ‚Äú5. Longest Palindromic Substring‚Äù problem from LeetCode into Copilot‚Äôs chat box. The\\ncode generated by Copilot is then submitted to the online judge platform, where it is successfully accepted.\\na significant portion of the public code available on GitHub, which enables it to understand a wide\\nrange of programming languages and coding styles. Copilot not only improves productivity but\\nalso serves as a learning tool by providing programmers with examples of how certain functions\\ncan be implemented or how specific problems can be solved.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 51, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:52\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nCodeGeeX. CodeGeeX stands out as a multifaceted programming assistant, proficient in code\\ncompletion, comment generation, code translation, and developer interactions. Its underlying code\\ngeneration LLM has been refined with extensive training on vast amounts of code data, exhibiting\\nsuperior performance on benchmarks like HumanEval, HumanEval-X, and DS1000. Renowned for\\nsupporting multilingual code generation, CodeGeeX plays a pivotal role in enhancing the efficiency\\nof code development.\\nCodeWhisperer. Amazon‚Äôs CodeWhisperer is a versatile, machine learning-driven code genera-\\ntor that offers on-the-fly code recommendations. Tailored to your coding patterns and comments,\\nCodeWhisperer provides personalized suggestions that range from succinct comments to complex\\nfunctions, all aimed at streamlining your coding workflow.\\nCodeium. Codeium is an AI-accelerated coding toolkit that offers a suite of functions, including'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 51, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='functions, all aimed at streamlining your coding workflow.\\nCodeium. Codeium is an AI-accelerated coding toolkit that offers a suite of functions, including\\ncode completion, explanation, translation, search, and user chatting. Compatible with over 70\\nprogramming languages, Codeium delivers fast and cutting-edge solutions to coding challenges,\\nsimplifying the development process for its users.\\nCodeArts Snap. Huawei‚Äôs CodeArts Snap is capable of generating comprehensive function-level\\ncode from both Chinese and English descriptions. This tool not only reduces the monotony of\\nmanual coding but also efficiently generates test code, in addition to providing automatic code\\nanalysis and repair services.\\nTabnine. Tabnine is an AI coding assistant that empowers development teams to leverage\\nAI for streamlining the software development lifecycle while maintaining strict standards for\\nprivacy, security, and compliance. With a focus on enhancing coding efficiency, code quality, and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 51, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='AI for streamlining the software development lifecycle while maintaining strict standards for\\nprivacy, security, and compliance. With a focus on enhancing coding efficiency, code quality, and\\ndeveloper satisfaction, Tabnine offers AI-driven automation that is tailored to the needs of your\\nteam. Supporting over one million developers worldwide, Tabnine is applicable across various\\nindustries.\\nReplit. Replit is a multifunctional platform that caters to a diverse array of software development\\nneeds. As a complimentary online IDE, it facilitates code collaboration, and cloud services, and\\nfosters a thriving developer community. Replit also enables users to compile and execute code in\\nmore than 50 programming languages directly within a web browser, eliminating the need for local\\nsoftware installations.\\nTo illustrate the use of development tools powered by LLMs, we employ GitHub Copilot within\\nVisual Studio Code (VS Code) as our example. Note that'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 51, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='software installations.\\nTo illustrate the use of development tools powered by LLMs, we employ GitHub Copilot within\\nVisual Studio Code (VS Code) as our example. Note that\\n1‚óãFor details on using the GitHub Copilot extension in VS Code, please refer to the useful\\ndocument at https://code.visualstudio.com/docs/copilot/overview.\\n2‚óãIf you would like to get free access to Copilot as a student, teacher, or open-source maintainer,\\nplease refer to this tutorial at https://docs.github.com/en/copilot/managing-copilot/managing-\\ncopilot-as-an-individual-subscriber/managing-your-copilot-subscription/getting-free-access-\\nto-copilot-as-a-student-teacher-or-maintainer and GitHub education application portal at\\nhttps://education.github.com/discount_requests/application.\\nAs depicted in the upper section of Figure 19, users can interact with Copilot through the chat box in\\nthe lower left corner, where they can inquire about various coding-related tasks. This feature is now'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 51, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='the lower left corner, where they can inquire about various coding-related tasks. This feature is now\\nsupported by the advanced capabilities of GPT-4o, o1-preview (Preview), and o1-mini (Preview).\\nFrom the generated content, Copilot demonstrates the ability to plan solutions to coding problems.\\nIt can write code and subsequently explain the generated code to enhance user comprehension.\\nWithin the right-side workspace, users can engage in inline chat conversations to generate or\\nrefactor source code, conduct code explanations, fix coding errors, resolve issues encountered\\nduring terminal command executions, produce documentation comments, and generate unit tests.\\nTo illustrate its capabilities, we input the description of the ‚Äú5. Longest Palindromic Substring‚Äù\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 52, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:53\\nproblem from LeetCode into Copilot‚Äôs chat box. The code generated by Copilot is then submitted\\nto the online judge platform, where it is successfully accepted, as shown at the lower section of\\nFigure 19.\\n6\\nCHALLENGES & OPPORTUNITIES\\nAccording to our investigations, the LLMs have revolutionized the paradigm of code generation\\nand achieved remarkable performance. Despite this promising progress, there are still numerous\\nchallenges that need to be addressed. These challenges are mainly caused by the gap between\\nacademia and practical development. For example, in academia, the HumanEval benchmark has\\nbeen established as a de facto standard for evaluating the coding proficiency of LLMs. However,\\nmany works have illustrated the evaluation of HumanEval can‚Äôt reflect the scenario of practical\\ndevelopment [68, 72, 123, 162]. In contrast, these serious challenges offer substantial opportunities'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 52, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='many works have illustrated the evaluation of HumanEval can‚Äôt reflect the scenario of practical\\ndevelopment [68, 72, 123, 162]. In contrast, these serious challenges offer substantial opportunities\\nfor further research and applications. In this section, we pinpoint critical challenges and identify\\npromising opportunities, aiming to bridge the research-practicality divide.\\nEnhancing complex code generation at repository and software scale. In practical de-\\nvelopment scenarios, it often involves a large number of complex programming problems of\\nvarying difficulty levels [151, 311]. While LLMs have shown proficiency in generating function-\\nlevel code snippets, these models often struggle with more complex, unseen programming problems,\\nrepository- and software-level problems that are commonplace in real-world software develop-\\nment. To this end, it requires strong problem-solving skills in LLM beyond simply functional-level'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 52, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='repository- and software-level problems that are commonplace in real-world software develop-\\nment. To this end, it requires strong problem-solving skills in LLM beyond simply functional-level\\ncode generation. For example, AlphaCode [151] achieved an average ranking in the top 54.3% in\\nprogramming competitions where an understanding of algorithms and complex natural language is\\nrequired to solve competitive programming problems. [123] argues that existing LLMs can‚Äôt resolve\\nreal-world GitHub issues well since the best-performing model, Claude 2, is able to solve a mere\\n1.96% of the issues. The reason for poor performance is mainly attributed to the weak reasoning\\ncapabilities [105], complex internal- and external- dependencies [22], and context length limitation\\nof LLMs [22]. Therefore, the pursuit of models that can handle more complex, repository- and\\nsoftware-level code generation opens up new avenues for automation in software development'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 52, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='of LLMs [22]. Therefore, the pursuit of models that can handle more complex, repository- and\\nsoftware-level code generation opens up new avenues for automation in software development\\nand makes programming more productive and accessible.\\nInnovating model architectures tuned to code structures. Due to their scalability and effec-\\ntiveness, Transformer-based LLM architectures have become dominant in solving code generation\\ntask. Nevertheless, they might not be optimally designed to capture the inherent structure and\\nsyntax of programming languages (PLs) [85, 86, 134, 175]. Code has a highly structured nature,\\nwith a syntax that is more rigid than natural language. This presents a unique challenge for LLMs,\\nwhich are often derived from models that were originally designed for natural language processing\\n(NLP). The development of novel model architectures that inherently understand and integrate the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 52, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='which are often derived from models that were originally designed for natural language processing\\n(NLP). The development of novel model architectures that inherently understand and integrate the\\nstructural properties of code represents a significant opportunity to improve code generation and\\ncomprehension. Innovations such as tree-based neural networks [183], which mirror the abstract\\nsyntax tree (AST) representation of code, can offer a more natural way for models to learn and\\ngenerate programming languages. Additionally, leveraging techniques from the compiler theory,\\nsuch as intermediate representations (IR) [152], could enable models to operate on a more abstract\\nand generalizable level, making them effective across multiple programming languages [207]. By\\nexploring architectures beyond the traditional sequential models, researchers can unlock new\\npotentials in code generation.\\nCurating high-quality code data for pre-training and fine-tuning of LLMs. The efficacy'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 52, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='potentials in code generation.\\nCurating high-quality code data for pre-training and fine-tuning of LLMs. The efficacy\\nof LLMs largely depends on the quality and diversity of code datasets used during pre-training and\\nfine-tuning phases [133, 280, 328]. Currently, there is a scarcity of large, high-quality datasets that\\nencompass a wide range of programming tasks, styles, and languages. This limitation constrains the\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 53, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:54\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nability of LLMs to generalize across unseen programming tasks, different coding environments, and\\nreal-world software development scenarios. The development of more sophisticated data acquisition\\ntechniques, such as automated code repositories mining [158], advanced filtering algorithms, and\\ncode data synthesis [165] (see Section 5.2), can lead to the creation of richer datasets. Collaborations\\nwith industry partners (e.g., GitHub) could also facilitate access to proprietary codebases, thereby\\nenhancing the practical relevance of the training material. Furthermore, the adoption of open-source\\nmodels for dataset sharing can accelerate the collective effort to improve the breadth and depth of\\ncode data available for LLM research.\\nDeveloping comprehensive benchmarks and metrics for coding proficiency evaluation\\nin LLMs. Current benchmarks like HumanEval may not capture the full spectrum of coding'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 53, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Developing comprehensive benchmarks and metrics for coding proficiency evaluation\\nin LLMs. Current benchmarks like HumanEval may not capture the full spectrum of coding\\nskills required for practical software development [191]. Additionally, metrics often focus on\\nsyntactic correctness or functional accuracy, neglecting aspects such as code efficiency [208],\\nstyle [44], readability [34], or maintainability [15]. The design of comprehensive benchmarks that\\nsimulate real-world software development challenges could provide a more accurate assessment\\nof LLMs‚Äô coding capabilities. These benchmarks should include diverse programming tasks of\\nvarying difficulty levels, such as debugging [325], refactoring [237], and optimization [112], and\\nshould be complemented by metrics that evaluate qualitative aspects of code. The establishment of\\ncommunity-driven benchmarking platforms could facilitate continuous evaluation and comparison\\nof LLMs for code generation across the industry and academia.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 53, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='community-driven benchmarking platforms could facilitate continuous evaluation and comparison\\nof LLMs for code generation across the industry and academia.\\nSupport for low-resource, low-level, and domain-specific programming languages. LLMs\\nare predominantly trained in popular high-level programming languages, leaving low-resource, low-\\nlevel, and domain-specific languages underrepresented. This lack of focus restricts the applicability\\nof LLMs in certain specialized fields and systems programming [250]. Intensifying research on\\ntransfer learning and meta-learning approaches may enable LLMs to leverage knowledge from\\nhigh-resource languages to enhance their performance on less common ones [38, 46]. Additionally,\\npartnerships with domain experts can guide the creation of targeted datasets and fine-tuning\\nstrategies to better serve niche markets. The development of LLMs with a capacity for multilingual'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 53, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='partnerships with domain experts can guide the creation of targeted datasets and fine-tuning\\nstrategies to better serve niche markets. The development of LLMs with a capacity for multilingual\\ncode generation also presents a significant opportunity for broadening the scope of applications.\\nContinuous learning for LLMs to keep pace with evolving coding knowledge. The\\nsoftware development landscape is continuously evolving, with new languages, frameworks, and\\nbest practices emerging regularly. LLMs risk becoming outdated if they cannot adapt to these\\nchanges and incorporate the latest programming knowledge [115, 264]. While retrieval augmented\\ncode generation mitigates these issues, the performance is limited by the quality of the retrieval\\ncontext While retrieval-augmented code generation offers a partial solution to these issues, its\\neffectiveness is inherently constrained by the quality of retrieved context. [171, 309, 330]. Therefore,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 53, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='effectiveness is inherently constrained by the quality of retrieved context. [171, 309, 330]. Therefore,\\nestablishing mechanisms for continuous learning and updating of LLMs can help maintain their\\nrelevance over time. This could involve real-time monitoring of code repositories to identify trends\\nand innovations, as well as the creation of incremental learning systems that can assimilate new\\ninformation without forgetting previously acquired knowledge. Engaging the LLMs in active\\nlearning scenarios where they interact with human developers may also foster ongoing knowledge\\nacquisition.\\nEnsuring code safety and aligning LLM outputs with human coding preferences. Ensuring\\nthe safety and security of code generated by LLMs is a paramount concern, as is their ability to\\nalign with human preferences and ethical standards. Current models may inadvertently introduce\\nvulnerabilities or generate code that does not adhere to desired norms [48, 293]. Research into'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 53, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='align with human preferences and ethical standards. Current models may inadvertently introduce\\nvulnerabilities or generate code that does not adhere to desired norms [48, 293]. Research into\\nthe integration of formal verification tools within the LLM pipeline can enhance the safety of the\\nproduced code. Additionally, developing frameworks for alignment learning that capture and reflect\\nhuman ethical preferences can ensure that the code generation process aligns with societal values\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 54, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:55\\n[200, 212]. Transparent and explainable AI methodologies can also contribute to building trust in\\nthe LLM-generated code by making the decision-making process more accessible to developers.\\n7\\nCONCLUSION\\nIn this survey, we provide a systematic literature review, serving as a valuable reference for\\nresearchers investigating the cutting-edge progress in LLMs for code generation. A thorough\\nintroduction and analysis for data curation, the latest advances, performance evaluation, ethical\\nimplications, environmental impact, and real-world applications are illustrated. In addition, we\\npresent a historical overview of the evolution of LLMs for code generation in recent years and\\noffer an empirical comparison using the widely recognized HumanEval, MBPP, and the more\\npractical and challenging BigCodeBench benchmarks to highlight the progressive enhancements'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 54, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='offer an empirical comparison using the widely recognized HumanEval, MBPP, and the more\\npractical and challenging BigCodeBench benchmarks to highlight the progressive enhancements\\nin LLM capabilities for code generation. Critical challenges and promising opportunities regarding\\nthe gap between academia and practical development are also identified for future investigation.\\nFurthermore, we have established a dedicated resource website to continuously document and\\ndisseminate the most recent advances in the field. We hope this survey can contribute to a compre-\\nhensive and systematic overview of LLM for code generation and promote its thriving evolution.\\nWe optimistically believe that LLM will ultimately change all aspects of coding and automatically\\nwrite safe, helpful, accurate, trustworthy, and controllable code, like professional programmers,\\nand even solve coding problems that currently cannot be solved by humans.\\nREFERENCES'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 54, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='write safe, helpful, accurate, trustworthy, and controllable code, like professional programmers,\\nand even solve coding problems that currently cannot be solved by humans.\\nREFERENCES\\n[1] 2023. AgentGPT: Assemble, configure, and deploy autonomous AI Agents in your browser. https://github.com/\\nreworkd/AgentGPT.\\n[2] 2023. AutoGPT is the vision of accessible AI for everyone, to use and to build on. https://github.com/Significant-\\nGravitas/AutoGPT.\\n[3] 2023. BabyAGI. https://github.com/yoheinakajima/babyagi.\\n[4] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach,\\nAmit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: A highly capable language model\\nlocally on your phone. arXiv preprint arXiv:2404.14219 (2024).\\n[5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 54, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='locally on your phone. arXiv preprint arXiv:2404.14219 (2024).\\n[5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\\n(2023).\\n[6] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A Transformer-based Approach for\\nSource Code Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\\n4998‚Äì5007.\\n[7] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified pre-training for program\\nunderstanding and generation. arXiv preprint arXiv:2103.06333 (2021).\\n[8] Ali Al-Kaswan, Maliheh Izadi, and Arie Van Deursen. 2024. Traces of memorisation in large language models for\\ncode. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. 1‚Äì12.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 54, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='code. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. 1‚Äì12.\\n[9] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas\\nMuennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. SantaCoder: don‚Äôt reach for the stars! arXiv preprint\\narXiv:2301.03988 (2023).\\n[10] Miltiadis Allamanis and Charles Sutton. 2014. Mining idioms from source code. In Proceedings of the 22nd acm sigsoft\\ninternational symposium on foundations of software engineering. 472‚Äì483.\\n[11] Google DeepMind AlphaCode Team. 2023. AlphaCode 2 Technical Report. https://storage.googleapis.com/deepmind-\\nmedia/AlphaCode2/AlphaCode2_Tech_Report.pdf.\\n[12] Amazon. 2022. What is CodeWhisperer? https://docs.aws.amazon.com/codewhisperer/latest/userguide/what-is-\\ncwspr.html.\\n[13] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 54, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='cwspr.html.\\n[13] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019.\\nMathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. In Proceedings of the\\n2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers). 2357‚Äì2367.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:56\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[14] Anthropic. 2024.\\nThe Claude 3 Model Family: Opus, Sonnet, Haiku.\\nhttps://www-cdn.anthropic.com/\\nde8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.\\n[15] Luca Ardito, Riccardo Coppola, Luca Barbato, and Diego Verga. 2020. A tool-based perspective on software code\\nmaintainability metrics: a systematic literature review. Scientific Programming 2020 (2020), 1‚Äì26.\\n[16] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad,\\nShiqi Wang, Qing Sun, Mingyue Shang, et al. 2022. Multi-lingual evaluation of code generation models. arXiv preprint\\narXiv:2210.14868 (2022).\\n[17] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie\\nCai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732\\n(2021).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732\\n(2021).\\n[18] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450\\n(2016).\\n[19] Hannah McLean Babe, Sydney Nguyen, Yangtian Zi, Arjun Guha, Molly Q Feldman, and Carolyn Jane Anderson. 2023.\\nStudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code. arXiv:2306.04556 [cs.LG]\\n[20] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang,\\net al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023).\\n[21] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna\\nGoldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv\\npreprint arXiv:2212.08073 (2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv\\npreprint arXiv:2212.08073 (2022).\\n[22] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok,\\nShashank Shet, et al. 2023. Codeplan: Repository-level coding using llms and planning. arXiv preprint arXiv:2309.12499\\n(2023).\\n[23] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation\\nwith human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine\\ntranslation and/or summarization. 65‚Äì72.\\n[24] Enrico Barbierato, Marco L Della Vedova, Daniele Tessera, Daniele Toti, and Nicola Vanoli. 2022. A methodology for\\ncontrolling bias and fairness in synthetic data generation. Applied Sciences 12, 9 (2022), 4619.\\n[25] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[25] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with\\ncode-generating models. Proceedings of the ACM on Programming Languages 7, OOPSLA1 (2023), 85‚Äì111.\\n[26] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi\\nDu, Zhe Fu, et al. 2024. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint\\narXiv:2401.02954 (2024).\\n[27] Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan, Fangxin Lu, Zili Zhang, Yulei Sui, Xuanhua Shi,\\nand Hai Jin. 2024. Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler\\nFeedback. arXiv preprint arXiv:2403.16792 (2024).\\n[28] Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini Kalliamvakou, Travis Lowdermilk, and\\nIdan Gazit. 2022. Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools.\\nQueue 20, 6 (2022), 35‚Äì57.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Idan Gazit. 2022. Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools.\\nQueue 20, 6 (2022), 35‚Äì57.\\n[29] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy,\\nKyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. arXiv\\npreprint arXiv:2204.06745 (2022).\\n[30] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive\\nLanguage Modeling with Mesh-Tensorflow. https://doi.org/10.5281/zenodo.5297715 If you use this software, please\\ncite it using these metadata..\\n[31] Veronika Bogina, Alan Hartman, Tsvi Kuflik, and Avital Shulner-Tal. 2022. Educating software and AI stakeholders\\nabout algorithmic fairness, accountability, transparency and ethics. International Journal of Artificial Intelligence in\\nEducation (2022), 1‚Äì26.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='about algorithmic fairness, accountability, transparency and ethics. International Journal of Artificial Intelligence in\\nEducation (2022), 1‚Äì26.\\n[32] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,\\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models.\\narXiv preprint arXiv:2108.07258 (2021).\\n[33] Tom B Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).\\n[34] Raymond PL Buse and Westley R Weimer. 2009. Learning a metric for code readability. IEEE Transactions on software\\nengineering 36, 4 (2009), 546‚Äì558.\\n[35] Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, and Jiayi Huang. 2024. Shortcut-connected Expert\\nParallelism for Accelerating Mixture-of-Experts. arXiv preprint arXiv:2404.05019 (2024).\\n[36] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. A survey on mixture of experts.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[36] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. A survey on mixture of experts.\\narXiv preprint arXiv:2407.06204 (2024).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:57\\n[37] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts,\\nTom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th\\nUSENIX Security Symposium (USENIX Security 21). 2633‚Äì2650.\\n[38] Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg,\\nAbhinav Jangda, and Arjun Guha. 2023. Knowledge Transfer from High-Resource to Low-Resource Programming\\nLanguages for Code LLMs. arXiv preprint arXiv:2308.09895 (2023).\\n[39] Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho\\nYee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. 2022. A scalable and extensible approach to\\nbenchmarking nl2code for 18 programming languages. arXiv preprint arXiv:2208.08227 (2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. 2022. A scalable and extensible approach to\\nbenchmarking nl2code for 18 programming languages. arXiv preprint arXiv:2208.08227 (2022).\\n[40] Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, and Hua Wu. 2022. ERNIE-Code: Beyond english-centric\\ncross-lingual pretraining for programming languages. arXiv preprint arXiv:2212.06742 (2022).\\n[41] Shubham Chandel, Colin B Clement, Guillermo Serrato, and Neel Sundaresan. 2022. Training and evaluating a jupyter\\nnotebook data science assistant. arXiv preprint arXiv:2201.12901 (2022).\\n[42] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang,\\nYidong Wang, et al. 2024. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems\\nand Technology 15, 3 (2024), 1‚Äì45.\\n[43] Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA model for code generation. https://github.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and Technology 15, 3 (2024), 1‚Äì45.\\n[43] Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA model for code generation. https://github.\\ncom/sahil280114/codealpaca.\\n[44] Binger Chen and Ziawasch Abedjan. 2023. DUETCS: Code Style Transfer through Generation and Retrieval. In 2023\\nIEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2362‚Äì2373.\\n[45] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. Codet:\\nCode generation with generated tests. arXiv preprint arXiv:2207.10397 (2022).\\n[46] Fuxiang Chen, Fatemeh H Fard, David Lo, and Timofey Bryksin. 2022. On the transferability of pre-trained language\\nmodels for low-resource programming languages. In Proceedings of the 30th IEEE/ACM International Conference on\\nProgram Comprehension. 401‚Äì412.\\n[47] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Program Comprehension. 401‚Äì412.\\n[47] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented\\ngeneration. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 17754‚Äì17762.\\n[48] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\\nYuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv\\npreprint arXiv:2107.03374 (2021).\\n[49] Stanley F Chen, Douglas Beeferman, and Roni Rosenfeld. 1998. Evaluation metrics for language models. (1998).\\n[50] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling\\ncomputation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588 (2022).\\n[51] Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, and Denny Zhou. 2023. Teaching large language models to self-debug.\\narXiv preprint arXiv:2304.05128 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[51] Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, and Denny Zhou. 2023. Teaching large language models to self-debug.\\narXiv preprint arXiv:2304.05128 (2023).\\n[52] Xinyun Chen, Chang Liu, and Dawn Song. 2018. Tree-to-tree neural networks for program translation. Advances in\\nneural information processing systems 31 (2018).\\n[53] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. 2023. Reducing\\nthe Carbon Impact of Generative AI Inference (today and in 2035). In Proceedings of the 2nd workshop on sustainable\\ncomputer systems. 1‚Äì7.\\n[54] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.\\nJournal of Machine Learning Research 24, 240 (2023), 1‚Äì113.\\n[55] Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Journal of Machine Learning Research 24, 240 (2023), 1‚Äì113.\\n[55] Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng\\nXiao, Bo Shen, Lin Li, et al. 2022. Pangu-coder: Program synthesis with function-level language modeling. arXiv\\npreprint arXiv:2207.11280 (2022).\\n[56] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa\\nDehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine\\nLearning Research 25, 70 (2024), 1‚Äì53.\\n[57] Colin B Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan. 2020. PyMT5:\\nmulti-mode translation of natural language and Python code with transformers. arXiv preprint arXiv:2010.03150\\n(2020).\\n[58] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='(2020).\\n[58] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\\nTworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint\\narXiv:2110.14168 (2021).\\n[59] CodeGemma Team, Ale Jakse Hartman, Andrea Hu, Christopher A. Choquette-Choo, Heri Zhao, Jane Fine, Jeffrey\\nHui, Jingyue Shen, Joe Kelley, Joshua Howland, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Nam Nguyen, Paul Michel,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:58\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nPeter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Siqi Zuo, Tris Warkentin, and Zhitao\\net al. Gong. 2024. CodeGemma: Open Code Models Based on Gemma. (2024). https://goo.gle/codegemma\\n[60] Codeium. 2023. Free, ultrafast Copilot alternative for Vim and Neovim. https://github.com/Exafunction/codeium.vim.\\n[61] Cognition. 2024. Introducing Devin, the first AI software engineer. https://www.cognition.ai/introducing-devin.\\n[62] Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. The Journal of\\nMachine Learning Research 11 (2010), 3053‚Äì3096.\\n[63] Cognitive Computations. 2023. oa_leet10k. https://huggingface.co/datasets/cognitivecomputations/oa_leet10k.\\n[64] Leonardo De Moura and Nikolaj Bj√∏rner. 2008. Z3: An efficient SMT solver. In International conference on Tools and\\nAlgorithms for the Construction and Analysis of Systems. Springer, 337‚Äì340.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[64] Leonardo De Moura and Nikolaj Bj√∏rner. 2008. Z3: An efficient SMT solver. In International conference on Tools and\\nAlgorithms for the Construction and Analysis of Systems. Springer, 337‚Äì340.\\n[65] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized\\nllms. Advances in Neural Information Processing Systems 36 (2024).\\n[66] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\\n[67] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min\\nChan, Weize Chen, et al. 2022. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained\\nlanguage models. arXiv preprint arXiv:2203.06904 (2022).\\n[68] Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='language models. arXiv preprint arXiv:2203.06904 (2022).\\n[68] Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh\\nNallapati, Parminder Bhatia, Dan Roth, et al. 2024. Crosscodeeval: A diverse and multilingual benchmark for cross-file\\ncode completion. Advances in Neural Information Processing Systems 36 (2024).\\n[69] Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia,\\nDan Roth, and Bing Xiang. 2022. Cocomic: Code completion by jointly modeling in-file and cross-file context. arXiv\\npreprint arXiv:2212.10007 (2022).\\n[70] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022.\\nA survey on in-context learning. arXiv preprint arXiv:2301.00234 (2022).\\n[71] Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie Shan, Caishuang Huang, Wei Shen, Xiaoran Fan,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A survey on in-context learning. arXiv preprint arXiv:2301.00234 (2022).\\n[71] Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie Shan, Caishuang Huang, Wei Shen, Xiaoran Fan,\\nZhiheng Xi, et al. 2024. StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback.\\narXiv preprint arXiv:2402.01391 (2024).\\n[72] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng,\\nand Yiling Lou. 2024. Evaluating large language models in class-level code generation. In Proceedings of the IEEE/ACM\\n46th International Conference on Software Engineering. 1‚Äì13.\\n[73] Hugging Face. 2023. Training CodeParrot from Scratch. https://github.com/huggingface/blog/blob/main/codeparrot.\\nmd.\\n[74] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M Zhang. 2023. Large\\nlanguage models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='language models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on\\nSoftware Engineering: Future of Software Engineering (ICSE-FoSE). IEEE, 31‚Äì53.\\n[75] Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei Tan. 2023. Automated repair of programs\\nfrom large language models. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE,\\n1469‚Äì1481.\\n[76] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu,\\nDaxin Jiang, et al. 2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint\\narXiv:2002.08155 (2020).\\n[77] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke\\nZettlemoyer, and Mike Lewis. 2022. Incoder: A generative model for code infilling and synthesis. arXiv preprint\\narXiv:2204.05999 (2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Zettlemoyer, and Mike Lewis. 2022. Incoder: A generative model for code infilling and synthesis. arXiv preprint\\narXiv:2204.05999 (2022).\\n[78] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish\\nThite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint\\narXiv:2101.00027 (2020).\\n[79] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.\\nPal: Program-aided language models. In International Conference on Machine Learning. PMLR, 10764‚Äì10799.\\n[80] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023.\\nRetrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023).\\n[81] Linyuan Gong, Mostafa Elhoushi, and Alvin Cheung. 2024. AST-T5: Structure-Aware Pretraining for Code Generation'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[81] Linyuan Gong, Mostafa Elhoushi, and Alvin Cheung. 2024. AST-T5: Structure-Aware Pretraining for Code Generation\\nand Understanding. arXiv preprint arXiv:2401.03003 (2024).\\n[82] Alex Gu, Baptiste Rozi√®re, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I Wang. 2024. Cruxeval:\\nA benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065 (2024).\\n[83] Sumit Gulwani. 2010. Dimensions in program synthesis. In Proceedings of the 12th international ACM SIGPLAN\\nsymposium on Principles and practice of declarative programming. 13‚Äì24.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:59\\n[84] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan\\nJavaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks are all you need. arXiv preprint\\narXiv:2306.11644 (2023).\\n[85] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022. UniXcoder: Unified Cross-Modal\\nPre-training for Code Representation. In Proceedings of the 60th Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers). 7212‚Äì7225.\\n[86] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svy-\\natkovskiy, Shengyu Fu, et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint\\narXiv:2009.08366 (2020).\\n[87] Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023. Longcoder: A long-range pre-trained language'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='arXiv:2009.08366 (2020).\\n[87] Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023. Longcoder: A long-range pre-trained language\\nmodel for code completion. In International Conference on Machine Learning. PMLR, 12098‚Äì12107.\\n[88] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li,\\net al. 2024. DeepSeek-Coder: When the Large Language Model Meets Programming‚ÄìThe Rise of Code Intelligence.\\narXiv preprint arXiv:2401.14196 (2024).\\n[89] Aman Gupta, Deepak Bhatt, and Anubha Pandey. 2021. Transitioning from Real to Synthetic data: Quantifying the\\nbias in model. arXiv preprint arXiv:2105.04144 (2021).\\n[90] Aman Gupta, Anup Shirgaonkar, Angels de Luis Balaguer, Bruno Silva, Daniel Holstein, Dawei Li, Jennifer Marsman,\\nLeonardo O Nunes, Mahsa Rouzbahman, Morris Sharp, et al. 2024. RAG vs Fine-tuning: Pipelines, Tradeoffs, and a\\nCase Study on Agriculture. arXiv preprint arXiv:2401.08406 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Leonardo O Nunes, Mahsa Rouzbahman, Morris Sharp, et al. 2024. RAG vs Fine-tuning: Pipelines, Tradeoffs, and a\\nCase Study on Agriculture. arXiv preprint arXiv:2401.08406 (2024).\\n[91] Hossein Hajipour, Keno Hassler, Thorsten Holz, Lea Sch√∂nherr, and Mario Fritz. 2024. CodeLMSec Benchmark:\\nSystematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models. In 2024 IEEE\\nConference on Secure and Trustworthy Machine Learning (SaTML). IEEE, 684‚Äì709.\\n[92] Perttu H√§m√§l√§inen, Mikke Tavast, and Anton Kunnari. 2023. Evaluating large language models in generating synthetic\\nhci research data: a case study. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems.\\n1‚Äì19.\\n[93] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning\\nwith language model is planning with world model. arXiv preprint arXiv:2305.14992 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='with language model is planning with world model. arXiv preprint arXiv:2305.14992 (2023).\\n[94] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In\\nProceedings of the IEEE conference on computer vision and pattern recognition. 770‚Äì778.\\n[95] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\\nSteinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874\\n(2021).\\n[96] Felipe Hoffa. 2016. GitHub on BigQuery: Analyze all the open source code. URL: https://cloud.google.com/blog/topics/\\npublic-datasets/github-on-bigquery-analyze-all-the-open-source-code (2016).\\n[97] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las\\nCasas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language\\nmodels. arXiv preprint arXiv:2203.15556 (2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language\\nmodels. arXiv preprint arXiv:2203.15556 (2022).\\n[98] Samuel Holt, Max Ruiz Luyten, and Mihaela van der Schaar. 2023. L2MAC: Large Language Model Automatic\\nComputer for Unbounded Code Generation. In The Twelfth International Conference on Learning Representations.\\n[99] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration.\\narXiv preprint arXiv:1904.09751 (2019).\\n[100] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing\\nYau, Zijuan Lin, Liyang Zhou, et al. 2023. Metagpt: Meta programming for multi-agent collaborative framework.\\narXiv preprint arXiv:2308.00352 (2023).\\n[101] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='arXiv preprint arXiv:2308.00352 (2023).\\n[101] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang.\\n2024. Large Language Models for Software Engineering: A Systematic Literature Review. arXiv:2308.10620 [cs.SE]\\n[102] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,\\nMona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International conference on\\nmachine learning. PMLR, 2790‚Äì2799.\\n[103] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\n2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).\\n[104] Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and Heming Cui. 2023. AgentCoder: Multi-Agent-based Code\\nGeneration with Iterative Testing and Optimisation. arXiv preprint arXiv:2312.13010 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Generation with Iterative Testing and Optimisation. arXiv preprint arXiv:2312.13010 (2023).\\n[105] Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. arXiv preprint\\narXiv:2212.10403 (2022).\\n[106] Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards Reasoning in Large Language Models: A Survey. In 61st\\nAnnual Meeting of the Association for Computational Linguistics, ACL 2023. Association for Computational Linguistics\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:60\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n(ACL), 1049‚Äì1065.\\n[107] Junjie Huang, Chenglong Wang, Jipeng Zhang, Cong Yan, Haotian Cui, Jeevana Priya Inala, Colin Clement, Nan\\nDuan, and Jianfeng Gao. 2022. Execution-based evaluation for data science code generation models. arXiv preprint\\narXiv:2211.09374 (2022).\\n[108] Qiuyuan Huang, Naoki Wake, Bidipta Sarkar, Zane Durante, Ran Gong, Rohan Taori, Yusuke Noda, Demetri Ter-\\nzopoulos, Noboru Kuno, Ade Famoti, et al. 2024. Position Paper: Agent AI Towards a Holistic Intelligence. arXiv\\npreprint arXiv:2403.00833 (2024).\\n[109] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai\\nDang, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186 (2024).\\n[110] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[110] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet\\nchallenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).\\n[111] Yoichi Ishibashi and Yoshimasa Nishimura. 2024. Self-Organized Agents: A LLM Multi-Agent Framework toward\\nUltra Large-Scale Code Generation and Optimization. arXiv preprint arXiv:2404.02183 (2024).\\n[112] Shu Ishida, Gianluca Corrado, George Fedoseev, Hudson Yeo, Lloyd Russell, Jamie Shotton, Jo√£o F Henriques, and\\nAnthony Hu. 2024. LangProp: A code optimization framework using Language Models applied to driving. arXiv\\npreprint arXiv:2401.10314 (2024).\\n[113] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Mapping Language to Code in Pro-\\ngrammatic Context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.\\n1643‚Äì1652.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='grammatic Context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.\\n1643‚Äì1652.\\n[114] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu\\nWang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: Scaling language model instruction meta learning through\\nthe lens of generalization. arXiv preprint arXiv:2212.12017 (2022).\\n[115] Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Jungkyu Choi, and Minjoon\\nSeo. 2022. Towards Continual Knowledge Learning of Language Models. In 10th International Conference on Learning\\nRepresentations, ICLR 2022. International Conference on Learning Representations.\\n[116] Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. 1977. Perplexity‚Äîa measure of the difficulty of speech\\nrecognition tasks. The Journal of the Acoustical Society of America 62, S1 (1977), S63‚ÄìS63.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='recognition tasks. The Journal of the Acoustical Society of America 62, S1 (1977), S63‚ÄìS63.\\n[117] Susmit Jha, Sumit Gulwani, Sanjit A Seshia, and Ashish Tiwari. 2010. Oracle-guided component-based program\\nsynthesis. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 1. 215‚Äì224.\\n[118] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi\\nZhou, Zhaowei Zhang, et al. 2023. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852 (2023).\\n[119] Ruyi Ji, Jingjing Liang, Yingfei Xiong, Lu Zhang, and Zhenjiang Hu. 2020. Question selection for interactive program\\nsynthesis. In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation.\\n1143‚Äì1158.\\n[120] Zhenlan Ji, Pingchuan Ma, Zongjie Li, and Shuai Wang. 2023. Benchmarking and explaining large language model-\\nbased code generation: A causality-centric approach. arXiv preprint arXiv:2310.06680 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='based code generation: A causality-centric approach. arXiv preprint arXiv:2310.06680 (2023).\\n[121] Juyong Jiang and Sunghun Kim. 2023. CodeUp: A Multilingual Code Generation Llama2 Model with Parameter-\\nEfficient Instruction-Tuning. https://github.com/juyongjiang/CodeUp.\\n[122] Shuyang Jiang, Yuhao Wang, and Yu Wang. 2023. Selfevolve: A code evolution framework via large language models.\\narXiv preprint arXiv:2306.02907 (2023).\\n[123] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. 2023.\\nSWE-bench: Can Language Models Resolve Real-world Github Issues?. In The Twelfth International Conference on\\nLearning Representations.\\n[124] Alexander Wettig Kilian Lieret Shunyu Yao Karthik Narasimhan Ofir Press John Yang, Carlos E. Jimenez. 2024.\\nSWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING. (2024).\\nhttps://swe-agent.com/'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='SWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING. (2024).\\nhttps://swe-agent.com/\\n[125] Aravind Joshi and Owen Rambow. 2003. A formalism for dependency grammar based on tree adjoining grammar. In\\nProceedings of the Conference on Meaning-text Theory. MTT Paris, France, 207‚Äì216.\\n[126] Harshit Joshi, Jos√© Cambronero Sanchez, Sumit Gulwani, Vu Le, Gust Verbruggen, and Ivan Radiƒçek. 2023. Repair\\nis nearly generation: Multilingual program repair with llms. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, Vol. 37. 5131‚Äì5140.\\n[127] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford,\\nJeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\\n[128] Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[128] Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty.\\n2023. xcodeeval: A large scale multilingual multitask benchmark for code understanding, generation, translation and\\nretrieval. arXiv preprint arXiv:2303.03004 (2023).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:61\\n[129] Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, and Chanjun Park. 2024. sDPO:\\nDon‚Äôt Use Your Data All at Once. arXiv preprint arXiv:2403.19270 (2024).\\n[130] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim,\\nHyeonju Lee, Jihoo Kim, et al. 2023. Solar 10.7 b: Scaling large language models with simple yet effective depth\\nup-scaling. arXiv preprint arXiv:2312.15166 (2023).\\n[131] Barbara Kitchenham, O Pearl Brereton, David Budgen, Mark Turner, John Bailey, and Stephen Linkman. 2009.\\nSystematic literature reviews in software engineering‚Äìa systematic literature review. Information and software\\ntechnology 51, 1 (2009), 7‚Äì15.\\n[132] Denis Kocetkov, Raymond Li, LI Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Mu√±oz Ferrandis,\\nSean Hughes, Thomas Wolf, Dzmitry Bahdanau, et al. 2022. The Stack: 3 TB of permissively licensed source code.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, et al. 2022. The Stack: 3 TB of permissively licensed source code.\\nTransactions on Machine Learning Research (2022).\\n[133] Andreas K√∂pf, Yannic Kilcher, Dimitri von R√ºtte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum,\\nDuc Nguyen, Oliver Stanley, Rich√°rd Nagyfi, et al. 2024. Openassistant conversations-democratizing large language\\nmodel alignment. Advances in Neural Information Processing Systems 36 (2024).\\n[134] Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang. 2023. Is model attention aligned with human\\nattention? an empirical study on large language models for code generation. arXiv preprint arXiv:2306.01220 (2023).\\n[135] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised translation of\\nprogramming languages. arXiv preprint arXiv:2006.03511 (2020).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[135] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised translation of\\nprogramming languages. arXiv preprint arXiv:2006.03511 (2020).\\n[136] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida\\nWang, and Tao Yu. 2023. DS-1000: A natural and reliable benchmark for data science code generation. In International\\nConference on Machine Learning. PMLR, 18319‚Äì18345.\\n[137] Hugo Lauren√ßon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao,\\nLeandro Von Werra, Chenghao Mou, Eduardo Gonz√°lez Ponferrada, Huu Nguyen, et al. 2022. The bigscience\\nroots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems 35 (2022),\\n31809‚Äì31826.\\n[138] Moritz Laurer. 2024. Synthetic data: save money, time and carbon with open source. https://huggingface.co/blog/\\nsynthetic-data-save-costs.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='31809‚Äì31826.\\n[138] Moritz Laurer. 2024. Synthetic data: save money, time and carbon with open source. https://huggingface.co/blog/\\nsynthetic-data-save-costs.\\n[139] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering\\ncode generation through pretrained models and deep reinforcement learning. Advances in Neural Information\\nProcessing Systems 35 (2022), 21314‚Äì21328.\\n[140] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Iliƒá, Daniel Hesslow, Roman Castagn√©, Alexan-\\ndra Sasha Luccioni, Fran√ßois Yvon, Matthias Gall√©, et al. 2023. Bloom: A 176b-parameter open-access multilingual\\nlanguage model. (2023).\\n[141] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and\\nAbhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint\\narXiv:2309.00267 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Abhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint\\narXiv:2309.00267 (2023).\\n[142] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning.\\narXiv preprint arXiv:2104.08691 (2021).\\n[143] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler,\\nMike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp\\ntasks. Advances in Neural Information Processing Systems 33 (2020), 9459‚Äì9474.\\n[144] Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. 2024. EvoCodeBench: An Evolving Code Generation\\nBenchmark Aligned with Real-World Code Repositories. arXiv preprint arXiv:2404.00599 (2024).\\n[145] Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. 2023. Towards enhancing in-context learning for code generation.\\narXiv preprint arXiv:2303.17780 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[145] Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. 2023. Towards enhancing in-context learning for code generation.\\narXiv preprint arXiv:2303.17780 (2023).\\n[146] Li Li, Tegawend√© F Bissyand√©, Mike Papadakis, Siegfried Rasthofer, Alexandre Bartel, Damien Octeau, Jacques Klein,\\nand Le Traon. 2017. Static analysis of android apps: A systematic literature review. Information and Software\\nTechnology 88 (2017), 67‚Äì95.\\n[147] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\\nChristopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161\\n(2023).\\n[148] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B.\\nHashimoto. 2023. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-\\nlab/alpaca_eval.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Hashimoto. 2023. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-\\nlab/alpaca_eval.\\n[149] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint\\narXiv:2101.00190 (2021).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 61, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:62\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[150] Yuanzhi Li, S√©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks are\\nall you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 (2023).\\n[151] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R√©mi Leblond, Tom Eccles, James Keeling,\\nFelix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science 378, 6624\\n(2022), 1092‚Äì1097.\\n[152] Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang, Qiyi Tang, Sen Nie, and Shi Wu. 2022. Unleashing the power of\\ncompiler intermediate representation to enhance neural program embeddings. In Proceedings of the 44th International\\nConference on Software Engineering. 2253‚Äì2265.\\n[153] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameter-efficient\\nfine-tuning. arXiv preprint arXiv:2303.15647 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 61, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[153] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameter-efficient\\nfine-tuning. arXiv preprint arXiv:2303.15647 (2023).\\n[154] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak\\nNarayanan, Yuhuai Wu, Ananya Kumar, et al. 2023. Holistic Evaluation of Language Models. Transactions on Machine\\nLearning Research (2023).\\n[155] Andreas Liesenfeld, Alianda Lopez, and Mark Dingemanse. 2023. Opening up ChatGPT: Tracking openness, trans-\\nparency, and accountability in instruction-tuned text generators. In Proceedings of the 5th international conference on\\nconversational user interfaces. 1‚Äì6.\\n[156] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.\\n74‚Äì81.\\n[157] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022. A survey of transformers. AI open 3 (2022),\\n111‚Äì132.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 61, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='74‚Äì81.\\n[157] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022. A survey of transformers. AI open 3 (2022),\\n111‚Äì132.\\n[158] Erik Linstead, Paul Rigor, Sushil Bajracharya, Cristina Lopes, and Pierre Baldi. 2007. Mining internet-scale software\\nrepositories. Advances in neural information processing systems 20 (2007).\\n[159] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai,\\nDaya Guo, et al. 2024. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv\\npreprint arXiv:2405.04434 (2024).\\n[160] Bingchang Liu, Chaoyu Chen, Cong Liao, Zi Gong, Huan Wang, Zhichao Lei, Ming Liang, Dajun Chen, Min Shen,\\nHailian Zhou, et al. 2023. Mftcoder: Boosting code llms with multitask fine-tuning. arXiv preprint arXiv:2311.02303\\n(2023).\\n[161] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 61, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='(2023).\\n[161] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel.\\n2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural\\nInformation Processing Systems 35 (2022), 1950‚Äì1965.\\n[162] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is your code generated by chatgpt really\\ncorrect? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing\\nSystems 36 (2024).\\n[163] Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. 2023. Rltf: Reinforcement learning\\nfrom unit test feedback. arXiv preprint arXiv:2307.04349 (2023).\\n[164] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt,\\nand predict: A systematic survey of prompting methods in natural language processing. Comput. Surveys 55, 9 (2023),\\n1‚Äì35.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 61, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and predict: A systematic survey of prompting methods in natural language processing. Comput. Surveys 55, 9 (2023),\\n1‚Äì35.\\n[165] Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang,\\nDenny Zhou, et al. 2024. Best Practices and Lessons Learned on Synthetic Data for Language Models. arXiv preprint\\narXiv:2404.07503 (2024).\\n[166] Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, and Yang Liu. 2020. Retrieval-Augmented Generation for Code\\nSummarization via Hybrid GNN. In International Conference on Learning Representations.\\n[167] Tianyang Liu, Canwen Xu, and Julian McAuley. 2023.\\nRepobench: Benchmarking repository-level code auto-\\ncompletion systems. arXiv preprint arXiv:2306.03091 (2023).\\n[168] Yan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji Zhang, Daoguang Zan, Jian-Guang Lou, Pin-Yu Chen, and Tsung-Yi\\nHo. 2023. Uncovering and quantifying social biases in code generation. Advances in Neural Information Processing'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 61, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Ho. 2023. Uncovering and quantifying social biases in code generation. Advances in Neural Information Processing\\nSystems 36 (2023), 2368‚Äì2380.\\n[169] Yue Liu, Chakkrit Tantithamthavorn, Li Li, and Yepang Liu. 2022. Deep learning for android malware defenses: a\\nsystematic literature review. Comput. Surveys 55, 8 (2022), 1‚Äì36.\\n[170] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang,\\nDmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. 2024. StarCoder 2 and The Stack v2: The Next Generation. arXiv\\npreprint arXiv:2402.19173 (2024).\\n[171] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy. 2022. ReACC: A Retrieval-\\nAugmented Code Completion Framework. In Proceedings of the 60th Annual Meeting of the Association for Computa-\\ntional Linguistics (Volume 1: Long Papers). 6227‚Äì6240.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:63\\n[172] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain,\\nDaxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and\\ngeneration. arXiv preprint arXiv:2102.04664 (2021).\\n[173] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin,\\nand Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. In The Twelfth\\nInternational Conference on Learning Representations.\\n[174] Michael R Lyu, Baishakhi Ray, Abhik Roychoudhury, Shin Hwei Tan, and Patanamon Thongtanunam. 2024. Automatic\\nProgramming: Large Language Models and Beyond. arXiv preprint arXiv:2405.02213 (2024).\\n[175] Wei Ma, Mengjie Zhao, Xiaofei Xie, Qiang Hu, Shangqing Liu, Jie Zhang, Wenhan Wang, and Yang Liu. 2022. Are'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[175] Wei Ma, Mengjie Zhao, Xiaofei Xie, Qiang Hu, Shangqing Liu, Jie Zhang, Wenhan Wang, and Yang Liu. 2022. Are\\nCode Pre-trained Models Powerful to Learn Code Syntax and Semantics? arXiv preprint arXiv:2212.10017 (2022).\\n[176] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\\nShrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. Advances in\\nNeural Information Processing Systems 36 (2024).\\n[177] James Manyika and Sissie Hsiao. 2023. An overview of Bard: an early experiment with generative AI. AI. Google\\nStatic Documents 2 (2023).\\n[178] Vadim Markovtsev, Waren Long, Hugo Mougard, Konstantin Slavnov, and Egor Bulychev. 2019. STYLE-ANALYZER:\\nfixing code style inconsistencies with interpretable unsupervised algorithms. In 2019 IEEE/ACM 16th International\\nConference on Mining Software Repositories (MSR). IEEE, 468‚Äì478.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='fixing code style inconsistencies with interpretable unsupervised algorithms. In 2019 IEEE/ACM 16th International\\nConference on Mining Software Repositories (MSR). IEEE, 468‚Äì478.\\n[179] Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. 2022. Generating training data with language models: Towards\\nzero-shot language understanding. Advances in Neural Information Processing Systems 35 (2022), 462‚Äì477.\\n[180] Meta. 2024. Introducing Meta Llama 3: The most capable openly available LLM to date. https://ai.meta.com/blog/meta-\\nllama-3/.\\n[181] MistralAI. 2024. Codestral. https://mistral.ai/news/codestral/.\\n[182] S√©bastien Bubeck Mojan Javaheripi. 2023. Phi-2: The surprising power of small language models. https://www.\\nmicrosoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models.\\n[183] Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang. 2014. TBCNN: A tree-based convolutional neural network for\\nprogramming language processing. arXiv preprint arXiv:1409.5718 (2014).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[183] Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang. 2014. TBCNN: A tree-based convolutional neural network for\\nprogramming language processing. arXiv preprint arXiv:1409.5718 (2014).\\n[184] Zahra Mousavi, Chadni Islam, Kristen Moore, Alsharif Abuadbba, and M Ali Babar. 2024. An investigation into\\nmisuse of java security apis by large language models. In Proceedings of the 19th ACM Asia Conference on Computer\\nand Communications Security. 1299‚Äì1315.\\n[185] Spyridon Mouselinos, Mateusz Malinowski, and Henryk Michalewski. 2022. A simple, yet effective approach to\\nfinding biases in code generation. arXiv preprint arXiv:2211.00609 (2022).\\n[186] Hussein Mozannar, Gagan Bansal, Adam Fourney, and Eric Horvitz. 2022. Reading between the lines: Modeling user\\nbehavior and costs in AI-assisted programming. arXiv preprint arXiv:2210.14306 (2022).\\n[187] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[187] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru\\nTang, Leandro Von Werra, and Shayne Longpre. 2023. Octopack: Instruction tuning code large language models.\\narXiv preprint arXiv:2308.07124 (2023).\\n[188] King Han Naman Jain, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik\\nSen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for\\ncode. arXiv preprint arXiv:2403.07974 (2024).\\n[189] Antonio Nappa, Richard Johnson, Leyla Bilge, Juan Caballero, and Tudor Dumitras. 2015. The attack of the clones: A\\nstudy of the impact of shared code on vulnerability patching. In 2015 IEEE symposium on security and privacy. IEEE,\\n692‚Äì708.\\n[190] Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. 2023. Lever:'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='692‚Äì708.\\n[190] Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. 2023. Lever:\\nLearning to verify language-to-code generation with execution. In International Conference on Machine Learning.\\nPMLR, 26106‚Äì26128.\\n[191] Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz,\\nCaiming Xiong, et al. 2023. L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language\\nModels. arXiv preprint arXiv:2309.17446 (2023).\\n[192] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. 2023. Codegen2: Lessons for\\ntraining llms on programming and natural languages. arXiv preprint arXiv:2305.02309 (2023).\\n[193] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022.\\nCodegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474\\n(2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474\\n(2022).\\n[194] Changan Niu, Chuanyi Li, Bin Luo, and Vincent Ng. 2022. Deep learning meets software engineering: A survey on\\npre-trained models of source code. arXiv preprint arXiv:2205.11739 (2022).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:64\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[195] Theo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. 2023. Is Self-Repair\\na Silver Bullet for Code Generation?. In The Twelfth International Conference on Learning Representations.\\n[196] OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/chatgpt.\\n[197] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/.\\n[198] OpenAI. 2024. New models and developer products announced at DevDay. https://openai.com/index/new-models-\\nand-developer-products-announced-at-devday/.\\n[199] OpenDevin. 2024. OpenDevin: Code Less, Make More. https://github.com/OpenDevin/OpenDevin.\\n[200] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.\\nAdvances in neural information processing systems 35 (2022), 27730‚Äì27744.\\n[201] Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023. Fine-tuning or retrieval? comparing\\nknowledge injection in llms. arXiv preprint arXiv:2312.05934 (2023).\\n[202] David N Palacio, Alejandro Velasco, Daniel Rodriguez-Cardenas, Kevin Moran, and Denys Poshyvanyk. 2023. Eval-\\nuating and explaining large language models for code using syntactic structures. arXiv preprint arXiv:2308.03873\\n(2023).\\n[203] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation\\nof machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics.\\n311‚Äì318.\\n[204] Nikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr, and Sergey Mechtaev.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='311‚Äì318.\\n[204] Nikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr, and Sergey Mechtaev.\\n2024. The Fact Selection Problem in LLM-Based Program Repair. arXiv preprint arXiv:2404.05520 (2024).\\n[205] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval Augmented\\nCode Generation and Summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021.\\n2719‚Äì2734.\\n[206] Arkil Patel, Siva Reddy, Dzmitry Bahdanau, and Pradeep Dasigi. 2023. Evaluating In-Context Learning of Libraries\\nfor Code Generation. arXiv preprint arXiv:2311.09635 (2023).\\n[207] Indraneil Paul, Jun Luo, Goran Glava≈°, and Iryna Gurevych. 2024. IRCoder: Intermediate Representations Make\\nLanguage Models Robust Multilingual Code Generators. arXiv preprint arXiv:2403.03894 (2024).\\n[208] Norman Peitek, Sven Apel, Chris Parnin, Andr√© Brechmann, and Janet Siegmund. 2021. Program comprehension and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[208] Norman Peitek, Sven Apel, Chris Parnin, Andr√© Brechmann, and Janet Siegmund. 2021. Program comprehension and\\ncode complexity metrics: An fmri study. In 2021 IEEE/ACM 43rd International Conference on Software Engineering\\n(ICSE). IEEE, 524‚Äì536.\\n[209] Huy N Phan, Hoang N Phan, Tien N Nguyen, and Nghi DQ Bui. 2024. RepoHyper: Better Context Retrieval Is All You\\nNeed for Repository-Level Code Completion. arXiv preprint arXiv:2403.06095 (2024).\\n[210] Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, Ashish Datta, Maksym Zhu-\\nravinskyi, Dakota Mahan, Marco Bellagente, Carlos Riquelme, et al. 2024. Stable Code Technical Report. arXiv\\npreprint arXiv:2404.01226 (2024).\\n[211] Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input\\nlength extrapolation. arXiv preprint arXiv:2108.12409 (2021).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[211] Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input\\nlength extrapolation. arXiv preprint arXiv:2108.12409 (2021).\\n[212] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023. Fine-tuning\\naligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693\\n(2023).\\n[213] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by\\ngenerative pre-training. (2018).\\n[214] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are\\nunsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.\\n[215] Steven Raemaekers, Arie Van Deursen, and Joost Visser. 2012. Measuring software library stability through historical\\nversion analysis. In 2012 28th IEEE international conference on software maintenance (ICSM). IEEE, 378‚Äì387.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='version analysis. In 2012 28th IEEE international conference on software maintenance (ICSM). IEEE, 378‚Äì387.\\n[216] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct\\npreference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing\\nSystems 36 (2024).\\n[217] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\nPeter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine\\nlearning research 21, 140 (2020), 1‚Äì67.\\n[218] Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. 2022. Evaluating the text-to-sql capabilities of large\\nlanguage models. arXiv preprint arXiv:2204.00498 (2022).\\n[219] Aurora Ramirez, Jose Raul Romero, and Christopher L Simons. 2018. A systematic review of interaction in search-based'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='language models. arXiv preprint arXiv:2204.00498 (2022).\\n[219] Aurora Ramirez, Jose Raul Romero, and Christopher L Simons. 2018. A systematic review of interaction in search-based\\nsoftware engineering. IEEE Transactions on Software Engineering 45, 8 (2018), 760‚Äì781.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:65\\n[220] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Sori-\\ncut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding\\nacross millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024).\\n[221] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco,\\nand Shuai Ma. 2020. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297\\n(2020).\\n[222] Replit. 2016. Idea to software, fast. https://replit.com.\\n[223] Replit. 2023. replit-code-v1-3b. https://huggingface.co/replit/replit-code-v1-3b.\\n[224] Tal Ridnik, Dedy Kredo, and Itamar Friedman. 2024. Code Generation with AlphaCodium: From Prompt Engineering\\nto Flow Engineering. arXiv preprint arXiv:2401.08500 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[224] Tal Ridnik, Dedy Kredo, and Itamar Friedman. 2024. Code Generation with AlphaCodium: From Prompt Engineering\\nto Flow Engineering. arXiv preprint arXiv:2401.08500 (2024).\\n[225] Nick Roshdieh. 2023. Evol-Instruct-Code-80k. https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1.\\n[226] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D Weisz. 2023. The programmer‚Äôs\\nassistant: Conversational interaction with a large language model for software development. In Proceedings of the\\n28th International Conference on Intelligent User Interfaces. 491‚Äì514.\\n[227] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal\\nRemez, J√©r√©my Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950\\n(2023).\\n[228] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='(2023).\\n[228] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy\\nKepner, Devesh Tiwari, and Vijay Gadepally. 2023. From words to watts: Benchmarking the energy costs of large\\nlanguage model inference. In 2023 IEEE High Performance Extreme Computing Conference (HPEC). IEEE, 1‚Äì9.\\n[229] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud\\nStiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask Prompted Training Enables Zero-Shot Task Generalization.\\nIn ICLR 2022-Tenth International Conference on Learning Representations.\\n[230] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization\\nalgorithms. arXiv preprint arXiv:1707.06347 (2017).\\n[231] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. 2021. You autocomplete me: Poisoning vulnera-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='algorithms. arXiv preprint arXiv:1707.06347 (2017).\\n[231] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. 2021. You autocomplete me: Poisoning vulnera-\\nbilities in neural code completion. In 30th USENIX Security Symposium (USENIX Security 21). 1559‚Äì1575.\\n[232] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv\\npreprint arXiv:1803.02155 (2018).\\n[233] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017.\\nOutrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538\\n(2017).\\n[234] Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang\\nZhao, et al. 2023. Pangu-coder2: Boosting large language models for code with ranking feedback. arXiv preprint\\narXiv:2307.14936 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Zhao, et al. 2023. Pangu-coder2: Boosting large language models for code with ranking feedback. arXiv preprint\\narXiv:2307.14936 (2023).\\n[235] Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, and David Lo. 2024. Greening large language models\\nof code. In Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society.\\n142‚Äì153.\\n[236] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language\\nagents with verbal reinforcement learning. Advances in Neural Information Processing Systems 36 (2024).\\n[237] Atsushi Shirafuji, Yusuke Oda, Jun Suzuki, Makoto Morishita, and Yutaka Watanobe. 2023. Refactoring Programs\\nUsing Large Language Models with Few-Shot Examples. arXiv preprint arXiv:2311.11690 (2023).\\n[238] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based code generation using\\ndeep reinforcement learning. arXiv preprint arXiv:2301.13816 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[238] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based code generation using\\ndeep reinforcement learning. arXiv preprint arXiv:2301.13816 (2023).\\n[239] Disha Shrivastava, Denis Kocetkov, Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. 2023. RepoFusion:\\nTraining Code Models to Understand Your Repository. arXiv preprint arXiv:2306.10998 (2023).\\n[240] Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. 2023. Repository-level prompt generation for large language\\nmodels of code. In International Conference on Machine Learning. PMLR, 31693‚Äì31715.\\n[241] Mukul Singh, Jos√© Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, and Gust Verbruggen. 2023. Codefusion: A\\npre-trained diffusion model for code generation. arXiv preprint arXiv:2310.17680 (2023).\\n[242] Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, and Tao Yu. 2024. ARKS: Active'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[242] Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, and Tao Yu. 2024. ARKS: Active\\nRetrieval in Knowledge Soup for Code Generation. arXiv preprint arXiv:2402.12317 (2024).\\n[243] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer\\nwith rotary position embedding. Neurocomputing 568 (2024), 127063.\\n[244] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: Code generation\\nusing transformer. In Proceedings of the 28th ACM joint meeting on European software engineering conference and\\nsymposium on the foundations of software engineering. 1433‚Äì1443.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:66\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[245] Marc Szafraniec, Baptiste Roziere, Hugh Leather, Francois Charton, Patrick Labatut, and Gabriel Synnaeve. 2022.\\nCode translation with compiler representations. In Proceedings of the Eleventh International Conference on Learning\\nRepresentations: ICLR.\\n[246] TabNine. 2018. AI Code Completions. https://github.com/codota/TabNine.\\n[247] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.\\nHashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_\\nalpaca.\\n[248] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\\nMorgane Rivi√®re, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and\\ntechnology. arXiv preprint arXiv:2403.08295 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Morgane Rivi√®re, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and\\ntechnology. arXiv preprint arXiv:2403.08295 (2024).\\n[249] Qwen Team. 2024. Code with CodeQwen1.5. https://qwenlm.github.io/blog/codeqwen1.5.\\n[250] Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt,\\nand Siddharth Garg. 2023. Benchmarking large language models for automated verilog rtl code generation. In 2023\\nDesign, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 1‚Äì6.\\n[251] theblackcat102. 2023.\\nThe evolved code alpaca dataset.\\nhttps://huggingface.co/datasets/theblackcat102/evol-\\ncodealpaca-v1.\\n[252] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste\\nRozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.\\narXiv preprint arXiv:2302.13971 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.\\narXiv preprint arXiv:2302.13971 (2023).\\n[253] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.\\narXiv preprint arXiv:2307.09288 (2023).\\n[254] Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. 2022. Natural language processing with transformers. \" O‚ÄôReilly\\nMedia, Inc.\".\\n[255] Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022. Expectation vs. experience: Evaluating the usability\\nof code generation tools powered by large language models. In Chi conference on human factors in computing systems\\nextended abstracts. 1‚Äì7.\\n[256] Boris Van Breugel, Zhaozhi Qian, and Mihaela Van Der Schaar. 2023. Synthetic data, real errors: how (not) to publish'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='extended abstracts. 1‚Äì7.\\n[256] Boris Van Breugel, Zhaozhi Qian, and Mihaela Van Der Schaar. 2023. Synthetic data, real errors: how (not) to publish\\nand use synthetic data. In International Conference on Machine Learning. PMLR, 34793‚Äì34808.\\n[257] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\\nPolosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).\\n[258] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https:\\n//github.com/kingoflolz/mesh-transformer-jax.\\n[259] Chong Wang, Jian Zhang, Yebo Feng, Tianlin Li, Weisong Sun, Yang Liu, and Xin Peng. 2024. Teaching Code LLMs to\\nUse Autocompletion Tools in Repository-Level Code Generation. arXiv preprint arXiv:2401.06391 (2024).\\n[260] Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. 2024. Software testing with'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[260] Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. 2024. Software testing with\\nlarge language models: Survey, landscape, and vision. IEEE Transactions on Software Engineering (2024).\\n[261] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen,\\nYankai Lin, et al. 2024. A survey on large language model based autonomous agents. Frontiers of Computer Science 18,\\n6 (2024), 1‚Äì26.\\n[262] Simin Wang, Liguo Huang, Amiao Gao, Jidong Ge, Tengfei Zhang, Haitao Feng, Ishna Satyarth, Ming Li, He Zhang, and\\nVincent Ng. 2022. Machine/deep learning for software engineering: A systematic literature review. IEEE Transactions\\non Software Engineering 49, 3 (2022), 1188‚Äì1231.\\n[263] Shiqi Wang, Li Zheng, Haifeng Qian, Chenghao Yang, Zijian Wang, Varun Kumar, Mingyue Shang, Samson Tan,\\nBaishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, and Bing Xiang. 2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, and Bing Xiang. 2022.\\nReCode: Robustness Evaluation of Code Generation Models. (2022). https://doi.org/10.48550/arXiv.2212.10264\\n[264] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. 2023. Knowledge editing for large language\\nmodels: A survey. arXiv preprint arXiv:2310.16218 (2023).\\n[265] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024. Executable code\\nactions elicit better llm agents. arXiv preprint arXiv:2402.01030 (2024).\\n[266] Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, and Qun Liu.\\n2022. Compilable Neural Code Generation with Compiler Feedback. In Findings of the Association for Computational\\nLinguistics: ACL 2022. 9‚Äì19.\\n[267] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Linguistics: ACL 2022. 9‚Äì19.\\n[267] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny\\nZhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171\\n(2022).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:67\\n[268] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.\\n2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. In The 61st Annual Meeting Of The\\nAssociation For Computational Linguistics.\\n[269] Yue Wang, Hung Le, Akhilesh Gotmare, Nghi Bui, Junnan Li, and Steven Hoi. 2023. CodeT5+: Open Code Large\\nLanguage Models for Code Understanding and Generation. In Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing. 1069‚Äì1088.\\n[270] Yanlin Wang and Hui Li. 2021. Code completion by modeling flattened abstract syntax trees as graphs. In Proceedings\\nof the AAAI conference on artificial intelligence, Vol. 35. 14015‚Äì14023.\\n[271] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='of the AAAI conference on artificial intelligence, Vol. 35. 14015‚Äì14023.\\n[271] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-\\nDecoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing. 8696‚Äì8708.\\n[272] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and\\nQun Liu. 2023. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966 (2023).\\n[273] Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. 2022. Execution-based evaluation for open-domain\\ncode generation. arXiv preprint arXiv:2212.10481 (2022).\\n[274] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\\nQuoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).\\n[275] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\\nDenny Zhou, Donald Metzler, et al. 2022. Emergent Abilities of Large Language Models. Transactions on Machine\\nLearning Research (2022).\\n[276] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.\\nChain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing\\nsystems 35 (2022), 24824‚Äì24837.\\n[277] Xiaokai Wei, Sujan Kumar Gonugondla, Shiqi Wang, Wasi Ahmad, Baishakhi Ray, Haifeng Qian, Xiaopeng Li, Varun\\nKumar, Zijian Wang, Yuchen Tian, et al. 2023. Towards greener yet powerful code generation via quantization: An\\nempirical study. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='empirical study. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the\\nFoundations of Software Engineering. 224‚Äì236.\\n[278] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need.\\narXiv preprint arXiv:2312.02120 (2023).\\n[279] Lilian Weng. 2023. LLM-powered Autonomous Agents. lilianweng.github.io (Jun 2023). https://lilianweng.github.io/\\nposts/2023-06-23-agent/\\n[280] Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024. QuRating: Selecting High-Quality Data for\\nTraining Language Models. arXiv preprint arXiv:2402.09739 (2024).\\n[281] Erroll Wood, Tadas Baltru≈°aitis, Charlie Hewitt, Sebastian Dziadzio, Thomas J Cashman, and Jamie Shotton. 2021.\\nFake it till you make it: face analysis in the wild using synthetic data alone. In Proceedings of the IEEE/CVF international\\nconference on computer vision. 3681‚Äì3691.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Fake it till you make it: face analysis in the wild using synthetic data alone. In Proceedings of the IEEE/CVF international\\nconference on computer vision. 3681‚Äì3691.\\n[282] Di Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Krishna Ramanathan, and Xiaofei Ma. 2024. Repoformer: Selective\\nRetrieval for Repository-Level Code Completion. arXiv preprint arXiv:2403.10059 (2024).\\n[283] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang,\\nand Chi Wang. 2023. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv\\npreprint arXiv:2308.08155 (2023).\\n[284] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin,\\nEnyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint\\narXiv:2309.07864 (2023).\\n[285] Rui Xie, Zhengran Zeng, Zhuohao Yu, Chang Gao, Shikun Zhang, and Wei Ye. 2024. CodeShell Technical Report.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='arXiv:2309.07864 (2023).\\n[285] Rui Xie, Zhengran Zeng, Zhuohao Yu, Chang Gao, Shikun Zhang, and Wei Ye. 2024. CodeShell Technical Report.\\narXiv preprint arXiv:2403.15747 (2024).\\n[286] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S Liang. 2023. Data selection for language models via\\nimportance resampling. Advances in Neural Information Processing Systems 36 (2023), 34201‚Äì34227.\\n[287] Bowen Li Xingyao Wang and Graham Neubig. 2024. Introducing OpenDevin CodeAct 1.0, a new State-of-the-art in\\nCoding Agents. https://www.cognition.ai/introducing-devin.\\n[288] Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang Huang, and Lu Zhang. 2017. Precise condition\\nsynthesis for program repair. In 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE,\\n416‚Äì426.\\n[289] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='416‚Äì426.\\n[289] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.\\nWizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 (2023).\\n[290] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large\\nlanguage models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:68\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n1‚Äì10.\\n[291] Junjielong Xu, Ying Fu, Shin Hwei Tan, and Pinjia He. 2024. Aligning LLMs for FL-free Program Repair. arXiv preprint\\narXiv:2404.08877 (2024).\\n[292] Weiwei Xu, Kai Gao, Hao He, and Minghui Zhou. 2024. A First Look at License Compliance Capability of LLMs in\\nCode Generation. arXiv preprint arXiv:2408.02487 (2024).\\n[293] Zhou Yang, Zhensu Sun, Terry Zhuo Yue, Premkumar Devanbu, and David Lo. 2024. Robustness, security, privacy,\\nexplainability, efficiency, and usability of large language models for code. arXiv preprint arXiv:2403.07506 (2024).\\n[294] Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsun Kim, Donggyun Han, and David Lo. 2024. Unveiling\\nmemorization in code models. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering.\\n1‚Äì13.\\n[295] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1‚Äì13.\\n[295] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of\\nthoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems\\n36 (2024).\\n[296] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct:\\nSynergizing Reasoning and Acting in Language Models. In International Conference on Learning Representations\\n(ICLR).\\n[297] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned\\ncode and natural language pairs from stack overflow. In Proceedings of the 15th international conference on mining\\nsoftware repositories. 476‚Äì486.\\n[298] Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim,\\nMunhyong Kim, Sungju Kim, et al. 2024. HyperCLOVA X Technical Report. arXiv preprint arXiv:2404.01954 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Munhyong Kim, Sungju Kim, et al. 2024. HyperCLOVA X Technical Report. arXiv preprint arXiv:2404.01954 (2024).\\n[299] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao\\nXie. 2024. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings\\nof the 46th IEEE/ACM International Conference on Software Engineering. 1‚Äì12.\\n[300] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle\\nRoman, et al. 2018. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing\\nand Text-to-SQL Task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.\\n3911‚Äì3921.\\n[301] Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng Yin. 2023.\\nWavecoder: Widespread and versatile enhanced instruction tuning with refined data generation. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation. arXiv preprint\\narXiv:2312.14187 (2023).\\n[302] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2023.\\nGpt-4 is too smart to be safe: Stealthy chat with llms via cipher. arXiv preprint arXiv:2308.06463 (2023).\\n[303] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. Rrhf: Rank responses to\\nalign language models with human feedback without tears. arXiv preprint arXiv:2304.05302 (2023).\\n[304] Jiawei Liu Yifeng Ding Naman Jain Harm de Vries Leandro von Werra Arjun Guha Lingming Zhang Yuxiang Wei,\\nFederico Cassano. 2024. StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation.\\nhttps://github.com/bigcode-project/starcoder2-self-align.\\n[305] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021.\\nBitfit: Simple parameter-efficient fine-tuning for'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='https://github.com/bigcode-project/starcoder2-self-align.\\n[305] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021.\\nBitfit: Simple parameter-efficient fine-tuning for\\ntransformer-based masked language-models. arXiv preprint arXiv:2106.10199 (2021).\\n[306] Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-\\nGuang Lou. 2022. CERT: continual pre-training on sketches for library-oriented code generation. arXiv preprint\\narXiv:2206.06888 (2022).\\n[307] Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, and Jian-Guang Lou. 2023.\\nLarge Language Models Meet NL2Code: A Survey. In Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers). 7443‚Äì7464.\\n[308] Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Computational Linguistics (Volume 1: Long Papers). 7443‚Äì7464.\\n[308] Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan,\\net al. 2024. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. arXiv preprint arXiv:2403.16443\\n(2024).\\n[309] Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\\n2023. RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. In Proceedings of\\nthe 2023 Conference on Empirical Methods in Natural Language Processing. 2471‚Äì2484.\\n[310] Jialu Zhang, Jos√© Pablo Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, and Gust Verbruggen.\\n2024. Pydex: Repairing bugs in introductory python assignments using llms. Proceedings of the ACM on Programming\\nLanguages 8, OOPSLA1 (2024), 1100‚Äì1124.\\n[311] Jialu Zhang, De Li, John Charles Kolesar, Hanyuan Shi, and Ruzica Piskac. 2022. Automated feedback generation'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Languages 8, OOPSLA1 (2024), 1100‚Äì1124.\\n[311] Jialu Zhang, De Li, John Charles Kolesar, Hanyuan Shi, and Ruzica Piskac. 2022. Automated feedback generation\\nfor competition-level code. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:69\\nEngineering. 1‚Äì13.\\n[312] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. 2023.\\nAdaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning\\nRepresentations.\\n[313] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang,\\nFei Wu, et al. 2023. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792 (2023).\\n[314] Shudan Zhang, Hanlin Zhao, Xiao Liu, Qinkai Zheng, Zehan Qi, Xiaotao Gu, Xiaohan Zhang, Yuxiao Dong, and Jie\\nTang. 2024. NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts.\\narXiv preprint arXiv:2405.04520 (2024).\\n[315] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='arXiv preprint arXiv:2405.04520 (2024).\\n[315] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong\\nChen, et al. 2023. Siren‚Äôs song in the AI ocean: a survey on hallucination in large language models. arXiv preprint\\narXiv:2309.01219 (2023).\\n[316] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. 2024. AutoCodeRover: Autonomous Program\\nImprovement. arXiv preprint arXiv:2404.05427 (2024).\\n[317] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023. Unifying the\\nperspectives of nlp and software engineering: A survey on language models for code. arXiv preprint arXiv:2311.07989\\n(2023).\\n[318] Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bin Qin, and Ting Liu. 2023. Length Extrapolation of Transformers: A\\nSurvey from the Perspective of Position Encoding. arXiv preprint arXiv:2312.17044 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Survey from the Perspective of Position Encoding. arXiv preprint arXiv:2312.17044 (2023).\\n[319] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\\nZhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).\\n[320] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,\\nDacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\\nInformation Processing Systems 36 (2024).\\n[321] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li,\\net al. 2023. Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x. In\\nProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 5673‚Äì5684.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 5673‚Äì5684.\\n[322] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. 2024.\\nOpenCodeInterpreter: Integrating Code Generation with Execution and Refinement. arXiv preprint arXiv:2402.14658\\n(2024).\\n[323] Wenqing Zheng, SP Sharan, Ajay Kumar Jaiswal, Kevin Wang, Yihan Xi, Dejia Xu, and Zhangyang Wang. 2023.\\nOutline, then details: Syntactically guided coarse-to-fine code generation. In International Conference on Machine\\nLearning. PMLR, 42403‚Äì42419.\\n[324] Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen. 2023. A survey of\\nlarge language models for code: Evolution, benchmarking, and future trends. arXiv preprint arXiv:2311.10372 (2023).\\n[325] Li Zhong, Zilong Wang, and Jingbo Shang. 2024. LDB: A Large Language Model Debugger via Verifying Runtime\\nExecution Step-by-step. arXiv preprint arXiv:2402.16906 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[325] Li Zhong, Zilong Wang, and Jingbo Shang. 2024. LDB: A Large Language Model Debugger via Verifying Runtime\\nExecution Step-by-step. arXiv preprint arXiv:2402.16906 (2024).\\n[326] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie\\nZhan, et al. 2023. Solving challenging math word problems using gpt-4 code interpreter with code-based self-\\nverification. arXiv preprint arXiv:2308.07921 (2023).\\n[327] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023. Language agent tree\\nsearch unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406 (2023).\\n[328] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili\\nYu, et al. 2024. Lima: Less is more for alignment. Advances in Neural Information Processing Systems 36 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Yu, et al. 2024. Lima: Less is more for alignment. Advances in Neural Information Processing Systems 36 (2024).\\n[329] Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui,\\nOlivier Bousquet, Quoc V Le, et al. 2022. Least-to-Most Prompting Enables Complex Reasoning in Large Language\\nModels. In The Eleventh International Conference on Learning Representations.\\n[330] Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang, and Graham Neubig. 2022. DocPrompting: Generating Code by\\nRetrieving the Docs. In The Eleventh International Conference on Learning Representations.\\n[331] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong\\nMa, et al. 2024. DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence. arXiv\\npreprint arXiv:2406.11931 (2024).\\n[332] Terry Yue Zhuo. 2024. ICE-Score: Instructing Large Language Models to Evaluate Code. In Findings of the Association'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2406.11931 (2024).\\n[332] Terry Yue Zhuo. 2024. ICE-Score: Instructing Large Language Models to Evaluate Code. In Findings of the Association\\nfor Computational Linguistics: EACL 2024. 2232‚Äì2242.\\n[333] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf,\\nHaolan Zhan, Junda He, Indraneil Paul, et al. 2024. Bigcodebench: Benchmarking code generation with diverse\\nfunction calls and complex instructions. arXiv preprint arXiv:2406.15877 (2024).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 69, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:70\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[334] Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian Liu, and Niklas\\nMuennighoff. 2024. Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models. arXiv preprint\\narXiv:2401.00788 (2024).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 0, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33\\nFlashFill++: Scaling Programming by Example by Cutting to\\nthe Chase\\nJOS√â CAMBRONERO‚àó, Microsoft, USA\\nSUMIT GULWANI‚àó, Microsoft, USA\\nVU LE‚àó, Microsoft, USA\\nDANIEL PERELMAN‚àó, Microsoft, USA\\nARJUN RADHAKRISHNA‚àó, Microsoft, USA\\nCLINT SIMON‚àó, Microsoft, USA\\nASHISH TIWARI‚àó, Microsoft, USA\\nProgramming-by-Examples (PBE) involves synthesizing an intended program from a small set of user-provided\\ninput-output examples. A key PBE strategy has been to restrict the search to a carefully designed small\\ndomain-specific language (DSL) with effectively-invertible (EI) operators at the top and effectively-enumerable\\n(EE) operators at the bottom. This facilitates an effective combination of top-down synthesis strategy (which\\nbackpropagates outputs over various paths in the DSL using inverse functions) with a bottom-up synthesis\\nstrategy (which propagates inputs over various paths in the DSL). We address the problem of scaling synthesis'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 0, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='strategy (which propagates inputs over various paths in the DSL). We address the problem of scaling synthesis\\nto large DSLs with several non-EI/EE operators. This is motivated by the need to support a richer class of\\ntransformations and the need for readable code generation. We propose a novel solution strategy that relies\\non propagating fewer values and over fewer paths.\\nOur first key idea is that of cut functions that prune the set of values being propagated by using knowledge\\nof the sub-DSL on the other side. Cuts can be designed to preserve completeness of synthesis; however, DSL\\ndesigners may use incomplete cuts to have finer control over the kind of programs synthesized. In either case,\\ncuts make search feasible for non-EI/EE operators and efficient for deep DSLs. Our second key idea is that of\\nguarded DSLs that allow a precedence on DSL operators, which dynamically controls exploration of various'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 0, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='guarded DSLs that allow a precedence on DSL operators, which dynamically controls exploration of various\\npaths in the DSL. This makes search efficient over grammars with large fanouts without losing recall. It also\\nmakes ranking simpler yet more effective in learning an intended program from very few examples. Both\\ncuts and precedence provide a mechanism to the DSL designer to restrict search to a reasonable, and possibly\\nincomplete, space of programs.\\nUsing cuts and gDSLs, we have built FlashFill++, an industrial-strength PBE engine for performing rich\\nstring transformations, including datetime and number manipulations. The FlashFill++ gDSL is designed to\\nenable readable code generation in different target languages including Excel‚Äôs formula language, PowerFx,\\nand Python. We show FlashFill++ is more expressive, more performant, and generates better quality code than\\ncomparable existing PBE systems. FlashFill++ is being deployed in several mass-market products ranging'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 0, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='comparable existing PBE systems. FlashFill++ is being deployed in several mass-market products ranging\\nfrom spreadsheet software to notebooks and business intelligence applications, each with millions of users.\\nCCS Concepts: ‚Ä¢ Software and its engineering ‚ÜíProgramming by example; Domain specific languages.\\n‚àóAuthors in alphabetic order\\nAuthors‚Äô addresses: Jos√© Cambronero, Microsoft, USA, jcambronero@microsoft.com; Sumit Gulwani, Microsoft, USA,\\nsumitg@microsoft.com; Vu Le, Microsoft, USA, levu@microsoft.com; Daniel Perelman, Microsoft, USA, danpere@microsoft.\\ncom; Arjun Radhakrishna, Microsoft, USA, arradha@microsoft.com; Clint Simon, Microsoft, USA, clint.simon@microsoft.\\ncom; Ashish Tiwari, Microsoft, USA, astiwar@microsoft.com.\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 0, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,\\ncontact the owner/author(s).\\n¬© 2023 Copyright held by the owner/author(s).\\n2475-1421/2023/1-ART33\\nhttps://doi.org/10.1145/3571226\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 1, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:2\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nAdditional Key Words and Phrases: programming by example, domain-specific languages, string transforma-\\ntions\\nACM Reference Format:\\nJos√© Cambronero, Sumit Gulwani, Vu Le, Daniel Perelman, Arjun Radhakrishna, Clint Simon, and Ashish\\nTiwari. 2023. FlashFill++: Scaling Programming by Example by Cutting to the Chase. Proc. ACM Program.\\nLang. 7, POPL, Article 33 (January 2023), 30 pages. https://doi.org/10.1145/3571226\\n1\\nINTRODUCTION\\nProgramming-by-examples (PBE) has seen tremendous interest and progress in the last decade [Gul-\\nwani et al. 2017]. A variety of approaches have been proposed targeting various applications. Starting\\nfrom purely symbolic techniques, the field has explored neural [Devlin et al. 2017] and neurosym-\\nbolic approaches [Chaudhuri et al. 2021; Kalyan et al. 2018; Rahmani et al. 2021; Verbruggen et al.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 1, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='from purely symbolic techniques, the field has explored neural [Devlin et al. 2017] and neurosym-\\nbolic approaches [Chaudhuri et al. 2021; Kalyan et al. 2018; Rahmani et al. 2021; Verbruggen et al.\\n2021]. In this paper, we present novel symbolic techniques to improve the scalability of PBE systems.\\nPBE applications range from enabling non-experts to author programs for spreadsheet data\\nmanipulation [Gulwani et al. 2012] or application creation in a low-code/no-code setting [Lukes\\net al. 2021], to improving productivity of data scientists for data wrangling tasks [Le and Gulwani\\n2014; Miltner et al. 2018] and even automating professional developers‚Äô repeated edits [Pan et al.\\n2021; Rolim et al. 2017]. A flagship application for PBE is that of string transformations, for\\ninstance, converting ‚ÄòAlan Turing‚Äô to ‚Äòturing, alan‚Äô‚Äîsuch tasks are very common and are\\nwell described by examples [Gulwani 2011]. We focus on such string transformations, though our'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 1, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='instance, converting ‚ÄòAlan Turing‚Äô to ‚Äòturing, alan‚Äô‚Äîsuch tasks are very common and are\\nwell described by examples [Gulwani 2011]. We focus on such string transformations, though our\\ntechnical contributions are more generally applicable to any grammar-based synthesis setting.\\nMost PBE engines work by defining a program search space, and then employing some strategy\\nto search over it. The program space is often defined by a domain-specific language (DSL), which\\nfixes a finite set of operators and all the different ways they can be composed to create programs. A\\nkey challenge in PBE is scaling the search to very large program spaces. DSL designers have to\\nbuild DSLs that are expressive enough to be useful, yet small enough to keep the program search\\nspace small. This tension in DSL design has hindered broader applications of program synthesis.\\nUsers implicitly advocate for larger DSLs as they want synthesizers to produce programs that are'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 1, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='space small. This tension in DSL design has hindered broader applications of program synthesis.\\nUsers implicitly advocate for larger DSLs as they want synthesizers to produce programs that are\\ncloser to the ones they would manually write, i.e., ones that use a large variety of functions that\\nare available in general purpose programming languages. On the other hand, these larger DSLs\\n(a) make the search space large and the synthesis slow, and (b) more importantly, allow the large\\nnumber of functions to be combined in unintuitive ways to produce undesirable programs. Program\\nsynthesis research has mainly focused on completeness, i.e., ensuring that we find a program when\\none exists, and insisting on completeness for large DSLs exacerbates these problems. We introduce\\ntwo new mechanisms, cuts and precedence, by which DSL designers can control the program search\\nspace even as the DSL itself grows in size. This not only eases the job of the DSL designer, but also'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 1, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='two new mechanisms, cuts and precedence, by which DSL designers can control the program search\\nspace even as the DSL itself grows in size. This not only eases the job of the DSL designer, but also\\nenables them to build synthesizers based on very expressive DSLs.\\nChallenges. Let us say we are given an input-output example: a tuple of input values and one\\noutput value. There are two commonly used search strategies to find programs that would generate\\nthe output value using the input values: bottom-up and top-down.\\nBottom-up (BU) search starts with the inputs and generates all possible values that can be computed\\nfrom the inputs using all possible (partial) programs in the search space. It does so by applying the\\nexecutable semantics functions of the DSL operators. Thus, information flows from the inputs, and all\\ncomputed intermediate results are completely oblivious to the output. The BU strategy is effective'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 1, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='computed intermediate results are completely oblivious to the output. The BU strategy is effective\\nonly when the sets of values generated in each intermediate stage remains small. This happens\\nwhen there are only a small number of leaf constants and each operator is effectively-enumerable\\n(EE) ‚Äì namely, it has a small arity and many-to-one semantics, thus having a small dynamic fan-out.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 2, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:3\\nTop-down (TD) search starts with the output and applies the inverse semantics of the operators,\\nso-called witness functions, to generate intermediate values that can generate the output value. Thus,\\ninformation flows from the output value, and at every stage, the intermediate values are computed\\nsolely based on the output and are completely oblivious to the input values. The TD strategy is\\neffective only when these intermediate sets are small. This happens only when every operator is\\neffectively invertible (EI) ‚Äì namely, it allows for effective (inverse) computation of various inputs\\nthat can yield a given output.\\nIf the DSL has both non-EE and non-EI operators, then neither bottom-up nor top-down strategies\\nare effective. Recently, it was observed that one could scale synthesis to larger DSLs by combining\\nthe two strategies [Lee 2021]. If there is a partition of the DSL such that the sub-DSL closer to'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 2, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='are effective. Recently, it was observed that one could scale synthesis to larger DSLs by combining\\nthe two strategies [Lee 2021]. If there is a partition of the DSL such that the sub-DSL closer to\\nthe start symbol has EI operators, and the rest contains EE operators, then the two strategies\\ncan be combined to yield a meet-in-the-middle strategy at that cut [Lee 2021]. However, this new\\nstrategy still has only a limited form of information flow between the inputs and the output. In\\nfact, the DSLs used in Duet [Lee 2021] are relatively small, albeit larger than those in FlashFill and\\nFlashMeta [Gulwani 2011; Polozov and Gulwani 2015]. These latter systems are based on a TD\\nstrategy over very small DSLs.\\nAn alternate way to scale PBE synthesis is based on abstraction and refinement types [Feng et al.\\n2017; Guo et al. 2020; Polikarpova et al. 2016; Wang et al. 2017]. The idea behind abstraction is that'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 2, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='2017; Guo et al. 2020; Polikarpova et al. 2016; Wang et al. 2017]. The idea behind abstraction is that\\ninstead of computing the exact set of values that can be generated (in either top-down or bottom-up\\nstrategy), we compute overapproximations of the sets of values. This approach works when high\\nquality abstractions can be quickly generated. Typically, it is still ‚Äúone-sided‚Äù ‚Äì either the inputs flow\\nto intermediate values or the output value flows backwards to intermediate values. Furthermore,\\nabstractions of compositions of operators are computed by composing the abstractions of operators,\\nwhich loses accuracy as the composition depth grows. We overcome some of these shortcomings\\nin our work; however, abstraction-based approaches are inherently complementary.1\\nOur Contribution. In this paper, we present two novel techniques - cuts and precedence - to\\neffectively address the scalability challenges of PBE synthesis. The executable semantics functions'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 2, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Our Contribution. In this paper, we present two novel techniques - cuts and precedence - to\\neffectively address the scalability challenges of PBE synthesis. The executable semantics functions\\n(that are the basis of BU) and the witness functions (that are basis of TD) are defined to allow\\ninformation to flow in one direction. We introduce cuts that prune values generated by witness\\nfunctions guided by the values that sub-DSLs could possibly compute on the inputs. This concept\\ninherently builds in bi-directional information flow in its definition, and in fact, generalizes the\\nsemantics functions and witness functions. A DSL designer can author a cut function based on their\\nintuition of the form of values that can be computed at a nonterminal using the inputs, and then\\nrestricting them to those that would be relevant for the output. Unlike abstractions, cuts are not\\ncomputed compositionally. They are provided for whole sub-DSLs; thus, they avoid information loss'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 2, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='restricting them to those that would be relevant for the output. Unlike abstractions, cuts are not\\ncomputed compositionally. They are provided for whole sub-DSLs; thus, they avoid information loss\\naccumulated by composing lossy abstractions. This is similar to how accelerations avoid information\\nloss in program analysis by capturing the effect (composition or transitive closure) of multiple state\\ntransitions by a single ‚Äúmeta transition‚Äù [Finkel 1987; Karp and Miller 1969].\\nA top-down strategy would get stuck at a non-EI operator. However, a cut function for an\\nargument of that non-EI operator can help unblock TD synthesis. As a special case, a cut for that\\nargument can be generated using bottom-up enumeration, in which case we get the meet-in-the-\\nmiddle strategy [Lee 2021]. However, cuts may be generated by other means based on the DSL\\ndesigner‚Äôs insight. In general, we get a novel search strategy, middle-out synthesis, which uses cuts'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 2, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='middle strategy [Lee 2021]. However, cuts may be generated by other means based on the DSL\\ndesigner‚Äôs insight. In general, we get a novel search strategy, middle-out synthesis, which uses cuts\\n1In this paper, we focus exclusively on synthesis approaches based on concrete values: the specification is a concrete IO\\nexample, and the semantics functions (and the inverse semantics) are given on concrete values (and not abstract values or\\nrefinement types). More specifically, we are in the context of version-space algebra (VSA) driven synthesis, and hence the\\nterms top-down and bottom-up are always used in that context.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 3, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:4\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nto reduce the original PBE problem over a large DSL (with potentially non-EI and non-EE operators)\\ninto simpler PBE problems over (smaller depth) sub-DSLs with only EI or only EE operators.\\nOur second key idea is to introduce precedence over operators in the grammar of domain-specific\\nlanguages. Precedence is a natural concept in grammars and arises naturally when the DSL designer\\nwants to prefer certain operators over others. We show that if the precedence is a series-parallel\\npartial order, then it can be encoded as an ordering on grammar rules to create a guarded DSL (gDSL),\\nand program search can be performed directly on the gDSL without compromising soundness or\\ncompleteness, while gaining efficiency. The ordering on rules in a gDSL is interpreted as a mandate\\nto explore a certain branch only when higher-ordered branches have failed to return a result. This'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 3, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='completeness, while gaining efficiency. The ordering on rules in a gDSL is interpreted as a mandate\\nto explore a certain branch only when higher-ordered branches have failed to return a result. This\\nhas two major advantages. First, it makes the search more scalable by dynamically using different\\nunderapproximations of the DSL to be explored. Second, it makes ranking simpler to write for DSL\\ndesigners because the precedence already builds in a default ranking over programs.\\nCuts and precedence provide DSL designers two new mechanisms to control the program search\\nspace, beyond what they get through designing DSLs. Our contributions include:\\n‚Ä¢ A new algorithmic approach for PBE (middle-out synthesis) that leverages a novel cut rule to\\nspeed up synthesis over large DSLs and to handle non-EI and non-EE operators.\\n‚Ä¢ A new formalism of guarded DSLs that supports operator precedence, and an extension of'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 3, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='speed up synthesis over large DSLs and to handle non-EI and non-EE operators.\\n‚Ä¢ A new formalism of guarded DSLs that supports operator precedence, and an extension of\\nour synthesis approach to gDSLs that scales synthesis to large DSLs and gives ranking based\\non path orderings [Dershowitz and Jouannaud 1990] for free.\\n‚Ä¢ A new and expressive system FlashFill++ for string transformations that supports datetime &\\nnumber manipulations and is designed for readable code generation.\\n‚Ä¢ An extensive comparison of FlashFill++ with existing state-of-the-art PBE systems for string\\ntransformations (FlashFill [Gulwani 2011], SmartFill [Chen et al. 2021a], and Duet [Lee 2021])\\nthat shows improvements in expressiveness, learning performance, and code readability.\\n2\\nOVERVIEW\\n2.1\\nNew Challenges in PBE for String Transformations\\nWe first discuss some challenges faced by the current generation of PBE tools for string trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 3, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='2\\nOVERVIEW\\n2.1\\nNew Challenges in PBE for String Transformations\\nWe first discuss some challenges faced by the current generation of PBE tools for string trans-\\nformations. These challenges were compiled in collaboration with two key industrial deployers\\nof PBE: the Microsoft Excel and the Microsoft PowerBI teams. To compile these challenges, we\\ninterviewed several product managers in these teams, interacted with both expert and novice users\\nof the FlashFill feature, and analyzed online help posts.\\nGenerating Readable Code. Consider the task of transforming the input pair (\"David Walker\",\\n\"623179\") to the output string \"D-6231#walker\". Any string processing library would contain\\nmany redundant methods for extracting \"Walker\" from \"David Walker\". For example, in Python,\\nwe could use the split method to accomplish the task. Alternatively, we could use the find method\\nalong with string slicing, or use regular expressions.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 3, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='we could use the split method to accomplish the task. Alternatively, we could use the find method\\nalong with string slicing, or use regular expressions.\\nIn contrast to the design of string processing libraries, the prevailing wisdom in DSL design for\\nsynthesis has been to work with a minimal number of operators [Gulwani 2016]. For example,\\nFlashFill [Gulwani 2011] and the more recent Duet [Lee 2021] DSLs contain only 3 and 5 functions\\nthat directly operate on strings, respectively. Smaller DSLs lead to smaller program search spaces,\\nyielding better synthesis performance and effective ranking [Polozov and Gulwani 2015]. Following\\nthis minimalism to an extreme can lead to a DSL whose programs translate to very unnatural and\\nunreadable programs in target languages like Python (see Figure 1) or PowerFx (see Figure 2).\\nOne straightforward approach to readability is writing a good translator from the synthesis DSL'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 3, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='unreadable programs in target languages like Python (see Figure 1) or PowerFx (see Figure 2).\\nOne straightforward approach to readability is writing a good translator from the synthesis DSL\\nto the target language. However, if the semantic gap between the DSL and the target languages‚Äô\\noperators is large, then ‚Äúreadable translation‚Äù itself becomes a new and nontrivial synthesis problem.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 4, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:5\\n# Python program generated by FlashFill\\nimport regex\\ndef transformation_text_program(input1 , input2 ):\\ncomputed_value_83 = regex.search(r\"\\\\p{Lu}+\", input1 ). group (0)\\nindex1_83 = regex.search(r\"[-.\\\\p{Lu}\\\\p{Ll}0 -9]+\", input2 ). start ()\\ncomputed_value_0_83 = input2[index1_83 :( len(input2) + -2)]\\nkth_match_83 =\\nl i s t (regex.finditer(r\"[-.\\\\p{Lu}\\\\p{Ll}0 -9]+\", input1 ))[ -1]\\ncomputed_value_1_83 = kth_match_83.group (0). lower ()\\nreturn computed_value_83+\"-\"+computed_value_0_83+\"#\"+computed_value_1_83\\n# Python program generated by FlashFill ++\\ndef formula(i1 , i2):\\ns1 = i1[:1]\\ns2 = i2[:4]\\ns3 = i1.split(\" \")[1]. lower()\\nreturn s1 + \"-\" + s2 + \"#\" + s3\\n# Python program generated by FlashFill ++ and renamed by Codex\\ndef formula(name , number ):\\nfirst_initial = name [:1]\\nnumber_prefix = number [:4]\\nlast_name = name.split(\" \")[1]. lower()\\nreturn first_initial + \"-\" + number_prefix + \"#\" + last_name'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 4, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='def formula(name , number ):\\nfirst_initial = name [:1]\\nnumber_prefix = number [:4]\\nlast_name = name.split(\" \")[1]. lower()\\nreturn first_initial + \"-\" + number_prefix + \"#\" + last_name\\nFig. 1. The python programs generated by FlashFill and FlashFill++ for the task of transforming (\"David\\nWalker\", \"623179\") to \"D-6231#walker\". FlashFill++‚Äôs program is much more readable and its readability\\nis further improved by renaming variables using a pretrained large language model, as shown.\\nOur insight is that to effectively generate readable code the DSLs should not be designed with the\\nsingle-minded goal of efficient learning, but also pay heed to the target languages.\\nWhile generating readable code is challenging, the need is sorely felt in industrial PBE tools‚Äî\\nusers are more likely to trust and use PBE tools if they produce idiomatic, readable code. Quoting\\none study participant in [Drosos et al. 2020]: ‚Äúdon‚Äôt know what is going on there, so I don‚Äôt know if I'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 4, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='one study participant in [Drosos et al. 2020]: ‚Äúdon‚Äôt know what is going on there, so I don‚Äôt know if I\\ncan trust it if I want to extend it to other tasks. I saw my examples were correctly transformed, but\\nbecause the code is hard to read, I would not be able to trust what it is doing‚Äù. The lack of readable\\ncode is one of the primary challenges preventing a broader adoption of PBE technologies.\\nMultiple Target Languages. The need for readable code generation is compounded by the\\nproliferation of different target languages, each with their own set of operations; see Figure 3.\\nThese target languages range across standard programming languages (e.g., Python, R), individual\\nlibraries (e.g., Pandas, PySpark), data query languages (e.g., SQL), and custom application-specific\\nlanguages (e.g., Google Sheets & Excel formula languages, PowerBI‚Äôs M language). Apart from the\\nobvious benefit, multiple target support can also help with learning: seeing the same program in'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 4, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='languages (e.g., Google Sheets & Excel formula languages, PowerBI‚Äôs M language). Apart from the\\nobvious benefit, multiple target support can also help with learning: seeing the same program in\\nmultiple languages helps with cross-language knowledge transfer [Shrestha et al. 2018].\\nDate-Time and Numeric Transformations. Most string PBE technologies cannot natively\\nhandle date-time and numeric operations efficiently, leading to situations like transforming ‚Äòjan‚Äô\\nto ‚ÄòJanember‚Äô given the input-output example ‚Äònov‚Äô ‚Ü¶‚Üí‚ÄòNovember‚Äô. Duet [Lee 2021] does allow\\nfor limited numeric operators, but still lacks support for important data-processing operations\\nsuch as rounding and bucketing. According to the Microsoft Excel team, date-time and numeric\\noperations (of the kind shown in Figure 3) are among the most requested FlashFill features. However,\\nas illustrated in Section 2.2, these operations are not amenable to standard synthesis techniques.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 4, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='operations (of the kind shown in Figure 3) are among the most requested FlashFill features. However,\\nas illustrated in Section 2.2, these operations are not amenable to standard synthesis techniques.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 5, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:6\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\n# PowerFx formula generated by FlashFill\\nConcatenate(\\nMid(Left(input1 , Match(input1 , \"\\\\p{Lu}+\"). StartMatch\\n+ Len(Match(input1 , \"\\\\p{Lu}+\"). FullMatch) - 1),\\nMatch(input1 , \"\\\\p{Lu}+\"). StartMatch),\\nConcatenate(\"-\",\\nConcatenate(Mid(Left(input2 ,Len(input2)-2), Match(input2 ,\"[0 -9]+\"). StartMatch),\\nConcatenate(\"#\",\\nLower(Mid(\\nLeft(input1 ,\\nFirst(LastN(MatchAll(input1 , \"[\\\\p{Lu}\\\\p{Ll}]+\"), 1)). StartMatch\\n+ Len(First(LastN(MatchAll(input1 , \"[\\\\p{Lu}\\\\p{Ll}]+\"), 1)). FullMatch )-1),\\nLast(MatchAll(input1 , \"[\\\\p{Lu}\\\\p{Ll}]+\")). StartMatch ))))))\\n# PowerFx formula generated by FlashFill ++\\nLeft(input1 , 1) & \"-\" & Left(input2 , 4) & \"#\"\\n& Lower(Last(FirstN(Split(input1 , \" \"), 2)). Result)\\nFig. 2. PowerFx formulas generated by FlashFill and FlashFill++ to transform (\"David Walker\", \"623179\") into\\n\"D-6231#walker\". The latter is much more readable than the former.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 5, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Fig. 2. PowerFx formulas generated by FlashFill and FlashFill++ to transform (\"David Walker\", \"623179\") into\\n\"D-6231#walker\". The latter is much more readable than the former.\\nPerforming operations over datetimes and numbers allows our system to handle use cases like\\nthe one detailed in Fig. 4(a), which presents 911 call records that need to be transformed. Each call\\nlog, shown in the Input column, contains an (optional) address, the township, the call date and time,\\nfollowed by possible annotations indicating the specific 911 station that addressed the call. Let us\\nsuppose that a data scientist wants to extract the date (2015-12-11) and time (13:34:52) from each log,\\nand map it to the corresponding weekday (Fri) and the 3-hour window (12PM - 3PM), as shown in\\nthe Output column. Performing this transformation requires string processing to extract candidate\\ndates and times, parsing these substrings into appropriate datatypes, performing type-specific'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 5, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='the Output column. Performing this transformation requires string processing to extract candidate\\ndates and times, parsing these substrings into appropriate datatypes, performing type-specific\\ntransformations on the extracted values, and then formatting them into an appropriate output\\nstring value. This is beyond the capabilities of current synthesizers. Our system can synthesize the\\nintended program (shown in Fig. 4(b)) from just the first example. This program is readable and\\nalso serves educational value (e.g. teaching the API of the popular datetime Python library).\\n2.2\\nOverview of FlashFill++\\nWe now show how our novel techniques address the various challenges from subsection 2.1.\\nExtended Domain-Specific Language. The main strength of FlashFill++ compared to previous\\nsystems is its expanded DSL containing over 40 operators, including 25 for just strings and the rest\\nfor datetime and numbers, such as for rounding and bucketing; see Figure 7. Contrast this with the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 5, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='systems is its expanded DSL containing over 40 operators, including 25 for just strings and the rest\\nfor datetime and numbers, such as for rounding and bucketing; see Figure 7. Contrast this with the\\nnumbers 3 and 5 mentioned previously for FlashFill and Duet.\\nThis extended DSL supports more expressive and more readable programs. Contrast the code\\ngenerated by FlashFill++ in Fig. 1 and 2 to that generated by FlashFill to see the clear difference an\\nextended DSL makes. However, expanding the DSL comes with its own set of challenges: (a) Given\\nthe larger search space, standard synthesis techniques fall short on efficiency. (b) The larger search\\nspace also complicates ranking‚Äîthe problem of picking the best (or intended) program among all\\nthe ones consistent with the examples. (c) Handling numeric and date-time operators requires new\\nsynthesis techniques. Next, we discuss some novel strategies to address these challenges.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 5, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='the ones consistent with the examples. (c) Handling numeric and date-time operators requires new\\nsynthesis techniques. Next, we discuss some novel strategies to address these challenges.\\nCuts and Middle-Out Synthesis. Our new DSL contains several non-EI operators (required for\\nnumber and datetime operations) that inhibit use of a top-down synthesis strategy across those\\noperators. Furthermore, bottom-up synthesis is not feasible for the sub-DSLs below those operators\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 6, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:7\\nRound to last day of month: 2/5/2020 =‚áí2/29/2020\\nfrom datetime import datetime\\nfrom dateutil.relativedelta import *\\ndef formula(i1):\\nmonth_start = datetime(i1.year ,i1.month ,1)\\nmonth_end = month_start\\n+ relativedelta(months =1)\\nreturn month_end - relativedelta(days =1)\\nEOMONTH(A1)\\nWith({ monthStart:\\nDate(Year(i1), Month(i1), 1)\\n},\\nDateAdd(\\nDateAdd(\\nmonthStart , 1, \"\" Months \"\"),\\n-1, \"\"Days\"\"\\n))\\nRound to start of quarter: 2/5/2020 =‚áí1/1/2020\\nfrom datetime import datetime\\ndef formula(i1):\\nquarter = (i1.month - 1) // 3 + 1\\nreturn datetime(i1.year ,3* quarter -2,1)\\nEOMONTH(\\nDATE(YEAR(A1),\\nROUNDUP(MONTH(A1)/3,\\n0)*3,1),\\n0)\\nWith({ quarter:\\nRoundUp(Month(i1) / 3, 0)\\n},\\nDate(Year(i1),quarter *3-2,1)\\n+ Time(0, 0, 0))\\nDays since start of year: 2/5/2029 =‚áí36\\ndef formula(i1):\\nreturn i1.timetuple (). tm_yday\\nA1 - DATE(YEAR(A1), 1, 1) + 1\\nDateDiff(\\nDate(Year(i1),1,1),i1) + 1\\nCreate year-quarter string: 4/5/1983 =‚áí‚Äò1983-Q2‚Äô'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 6, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='def formula(i1):\\nreturn i1.timetuple (). tm_yday\\nA1 - DATE(YEAR(A1), 1, 1) + 1\\nDateDiff(\\nDate(Year(i1),1,1),i1) + 1\\nCreate year-quarter string: 4/5/1983 =‚áí‚Äò1983-Q2‚Äô\\nfrom datetime import datetime\\ndef formula(i1):\\nquarter = (i1.month -1)//3 + 1\\nreturn i1.strftime(\"%Y\") +\\n\"-Q\"+f\"{quarter :01.0f}\"\\nYEAR(A1) & \"-Q\" &\\n& ROUNDUP(MONTH(A1)/3, 0)\\nText(i1 , \"yyyy\", \"en -US\")\\n& \"-Q\" &\\nText(\\nRoundUp(Month(i1) /3, 0),\\n\"0\",\\n\"en -US\")\\nExtract number, convert and round: ‚ÄòYour Total: $1,2564.45‚Äô =‚áí12564.5ùëë\\nfrom decimal import *\\ndef formula(i1):\\nsource = Decimal( str ( float (\\ni1.split(\"$\")[-1]. replace(\",\", \"\"))))\\ndelta = Decimal(\"0.5\")\\nreturn\\nfloat (( source / delta)\\n.quantize(0, ROUND_CEILING) * delta)\\nROUNDUP(\\nNUMBERVALUE(\\nRIGHT(A1 ,\\nLEN(A1)-FIND(\"$\", A1))\\n) / 0.5,\\n0\\n) * 0.5\\nRoundUp(Value(\\nLast(\\nSplit(i1 , \"$\")\\n).Result ,\\n\"en -US\"\\n) * 2, 0) / 2\\nFig. 3. Code produced by FlashFill++ for various date-time and rounding scenarios in Python (left), Excel\\n(center), and PowerFx (right) respectively.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 6, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Split(i1 , \"$\")\\n).Result ,\\n\"en -US\"\\n) * 2, 0) / 2\\nFig. 3. Code produced by FlashFill++ for various date-time and rounding scenarios in Python (left), Excel\\n(center), and PowerFx (right) respectively.\\ndue to enumeration blowup, rendering a meet-in-the-middle strategy infeasible. We propose a\\nnovel middle-out synthesis strategy that uses cuts to deal with such non-EI operators.\\nConsider the number parsing, rounding, and formatting subset of the FlashFill++ DSL below\\ndecimal roundNumber := RoundNumber(parseNumber, roundNumDesc)\\ndecimal parseNumber := ParseNumber(substr, locale) | ...\\nstring substr\\n:= ...\\nFix the input-output example ‚ü®‚ÄúThe price is $24.58 and 46 units are available.‚Äù ‚Ü¶‚Üí\\n24.00‚ü©for the non-terminal roundNumber. We first discuss the short-comings of both bottom-\\nup and top-down synthesis in this case.\\nTop-Down Synthesis using Witness Functions. In FlashMeta-style programming-by-example, the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 6, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='up and top-down synthesis in this case.\\nTop-Down Synthesis using Witness Functions. In FlashMeta-style programming-by-example, the\\nprimary deductive tools are witness functions. Given a specification in the form of an input-output\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 7, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:8\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\n(a)\\nInput\\nOutput\\nCEDAR AVE & COTTAGE AVE; HORSHAM; 2015-12-11 @ 13:34:52;\\nFri, 12PM - 3PM\\nRT202 PKWY; MONTGOMERY; 2016-01-13 @ 09:05:41-Station:STA18;\\nWed, 9AM - 12PM\\n; UPPER GWYNEDD; 2015-12-11 @ 21:11:18;\\nFri, 9PM - 12AM\\n(b)\\ndef derive_value(_input ):\\ntext = _input.split(\";\")[2]\\npart_0 = text.split(\" \")[0]; part_1 = text.split(\" \")[2][:8]\\ndate = datetime.datetime.strptime(part_0 , \"%Y-%m-%d\")\\ntime = datetime.datetime.strptime(part_1 , \"%H:%M:%S\")\\nbase_value = datetime.timedelta(hours=time.hour , minutes=time.minute ,\\nseconds=time.second , microseconds=time.microsecond)\\ndelta_value = datetime.timedelta(hours =3)\\ntime_str = (time - base_value % delta_value ). strftime(\"%#I%p\")\\nrounded_up_next = (time - base_value % delta_value) + delta_value\\ncomputed_value = time_str + \"-\" + rounded_up_next.strftime(\"%#I%p\")\\nreturn date.strftime(\"%a\") + \", \" + computed_value'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 7, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='rounded_up_next = (time - base_value % delta_value) + delta_value\\ncomputed_value = time_str + \"-\" + rounded_up_next.strftime(\"%#I%p\")\\nreturn date.strftime(\"%a\") + \", \" + computed_value\\nFig. 4. (a) A task to map the 911-call logs in the Input column to weekday/time buckets in the Output column.\\n(b) Python function synthesized by our approach for this task from just one example.\\nexample ùëñ‚Ü¶‚Üíùëúand a top-level operator ùêπ, a witness function for position ùëògenerates a sub-\\nspecification for the ùëòùë°‚Ñéparameter for ùêπ. For example, if the top-level operator is concat(ùëÅ1, ùëÅ2)\\nand the input-output example is ùëñ‚Ü¶‚Üí‚Äúabc‚Äù, the witness function for the 1ùë†ùë°position will return\\n{‚Äúa‚Äù, ‚Äúab‚Äù} (assuming we do not consider the trivial case of appending an empty string). In the\\nexample we are considering, writing a witness functions for the RoundNumber operator is not as\\nstraight-forward‚Äîthere are an infinite set of numbers that can round to 24.00. A standard top-down'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 7, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='example we are considering, writing a witness functions for the RoundNumber operator is not as\\nstraight-forward‚Äîthere are an infinite set of numbers that can round to 24.00. A standard top-down\\nprocedure cannot handle this infinite-width witness function.\\nBottom-Up Synthesis. The other major paradigm for programming-by-example is bottom-up syn-\\nthesis: here, the synthesizer starts enumerating programs ‚Äì starting from constants and iteratively\\napplying operators from the grammar on the previously generated programs ‚Äì and checks if any\\nof the enumerated programs satisfies the given input-output example. An efficient bottom-up\\nsynthesizer will avoid enumerating all programs using observational equivalence‚Äîthat is, it only\\ngenerate programs that produce different outputs for the given input. In our running example, the\\nsynthesizer will begin by generating substr sub-programs and concrete values for locale and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 7, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='generate programs that produce different outputs for the given input. In our running example, the\\nsynthesizer will begin by generating substr sub-programs and concrete values for locale and\\nroundNumberDesc. However, enumerating such sub-programs is expensive‚Äîthe fragment of the\\nDSL reachable from the non-terminal substr is large. In fact, string operations like substr are\\nbest handled using witness functions.\\nMiddle-Out Synthesis using Cuts. Examining our input-output example by hand, it is easy to see that\\nin any valid program the output of ParseNumber should be derived from a numerical substring\\nin the input. Intuitively, it does not matter what or how complex the substr sub-program is; we\\ncan be confident that the output of the ParseNumber will be either 24.58 or 46. Cuts capture this\\nsimple intuition‚Äîfor a given input-output example ùëñ‚Ü¶‚Üíùëúand a non-terminal ùëÅthat expands to\\nùëì(ùëÅ1, ùëÅ2), the cut for ùëÅ1 (say) in the context of ùëÅwill be a set of values {ùëú1,ùëú2, . . . ,ùëúùëõ} such that'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 7, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='simple intuition‚Äîfor a given input-output example ùëñ‚Ü¶‚Üíùëúand a non-terminal ùëÅthat expands to\\nùëì(ùëÅ1, ùëÅ2), the cut for ùëÅ1 (say) in the context of ùëÅwill be a set of values {ùëú1,ùëú2, . . . ,ùëúùëõ} such that\\nin any desired program ùëÉgenerated by ùëÅ, the output of the sub-program corresponding to ùëÅ1 will\\nbe one of the ùëúùëòfor ùëò‚àà{1, 2, . . . ,ùëõ}.\\nGiven that the output of ParseNumber will be either 24.58 or 46, the synthesizer has two sub-\\ntasks for the 24.58 case (the 46 case will be similar): (a) synthesizing a program for ùëùùëéùëüùë†ùëíùëÅùë¢ùëöùëèùëíùëü\\nfor the example ùëñ‚Ü¶‚Üí24.58, and (b) synthesizing a program for ùëüùëúùë¢ùëõùëëùëÅùë¢ùëöùëèùëíùëüfor the example\\nùëñ‚Ü¶‚Üí24.00 using a modified DSL, which is generated dynamically in middle-out synthesis, where\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 8, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:9\\nparseNumber ‚Üí24.58 is the only rule whose head is parseNumber. Both sub-tasks can now be\\nrecursively solved, possibly using either the top-down strategy or the bottom-up strategy.\\nGuarded Context-Free Grammars. The larger program space resulting from an extended DSL\\nwith a wide range of operators poses both efficiency and ranking challenges. However, human\\nprogrammers often encounter the same challenge when writing their own implementations and\\ndecide between these operators and programs using simple rules of thumb, which can be leveraged\\nboth for improving search efficiency and ranking. For example, ‚Äúif a task can be done using a date-\\ntime function, do not use string transformation functions‚Äù or ‚Äúif a task can be done using string\\nindexing, do not use regular expressions‚Äù. To mimic this kind of coarse reasoning, we introduce\\nthe notion of gDSLs. In a gDSL, the production rules for each non-terminal are ordered (with a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 8, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='indexing, do not use regular expressions‚Äù. To mimic this kind of coarse reasoning, we introduce\\nthe notion of gDSLs. In a gDSL, the production rules for each non-terminal are ordered (with a\\npartial order |‚ä≤), with production rules earlier in the order preferred to ones later in the order. For\\nexample, the rule concat := segment |‚ä≤Concat(segment, concat) expresses that we always\\nprefer programs that do not use the Concat operation to ones that do. During synthesis for a gDSL\\nrule ùëÅ‚Üíùõº|‚ä≤ùõΩthe branch ùõΩis explored only if the branch ùõºfails to produce a program. This\\ngreatly improves the performance of synthesis and the FlashFill++ synthesis times are competitive\\nwith other synthesis techniques that work with significantly smaller DSLs.\\nApart from improving the efficiency of search, gDSLs also simplify the task of writing ranking\\nfunctions. Intuitively, the precedence in the guarded rules induce a ranking on programs, and any'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 8, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Apart from improving the efficiency of search, gDSLs also simplify the task of writing ranking\\nfunctions. Intuitively, the precedence in the guarded rules induce a ranking on programs, and any\\nadditional ranking function only needs to order the remaining incomparable programs. Precedences\\nrank programs by a lexicographic path ordering (LPO) [Dershowitz and Jouannaud 1990], and our\\nfinal ranker will be a lexicographic combination of LPO and base arithmetic ranker ‚Äì such program\\nrankers have not been used in program synthesis before.\\n3\\nBACKGROUND: PROGRAMMING BY EXAMPLE\\nWe now define the problem of programming-by-example and discuss common solutions.\\n3.1\\nDomain-Specific Languages\\nWe use domain-specific languages (DSLs) to specify the set of target programs for a synthesizer.\\nFormally, a DSL D is given by ‚ü®N, T, F, R, Vin, ùë£out‚ü©where:\\n‚Ä¢ N is a finite set of non-terminal symbols (or non-terminals). The symbol ùë£out is a special start'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 8, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Formally, a DSL D is given by ‚ü®N, T, F, R, Vin, ùë£out‚ü©where:\\n‚Ä¢ N is a finite set of non-terminal symbols (or non-terminals). The symbol ùë£out is a special start\\nnon-terminal in N that represents the output of a program in the DSL.\\n‚Ä¢ T is a set of terminal symbols (or terminals) that is partitioned as Vin ‚à™O into inputs Vin and\\nvalues O. The set Vin contains special terminals, in1, in2, . . ., that represent the input symbols\\nin a program of the DSL. The set O contains constant values.\\n‚Ä¢ F is a finite set of function symbols (or operations). Each operation f ‚ààF has a fixed arity\\nArity(f). The semantics of f, denoted by JfK, is a mapping from OArity(f) to O.\\n‚Ä¢ R is a set of rules of the form ùëÅ‚Üíf(ùë£1, . . . , ùë£ùëò) or ùëÅ‚Üíùë£0 where ùëÅ‚ààN, f ‚ààF , ùë£0 ‚ààT,\\nand ùë£1, . . . , ùë£ùëò‚ààN ‚à™T.\\nThe formalism above is untyped for ease of reading. In practice, the FlashFill++ DSL is typed‚Äî values\\ncan be integers, floats, strings, Booleans, and date-time objects, and each non-terminal, terminal,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 8, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='can be integers, floats, strings, Booleans, and date-time objects, and each non-terminal, terminal,\\nand operator have specific type signatures.\\nEvery terminal or nonterminal ùë£generates a set L(ùë£) of programs defined recursively as follows:\\n(a) L(inùëò) = {inùëò} for all input symbols inùëò‚ààVin, (b) L(ùëú) = {ùëú} for all values ùëú‚ààO, and\\n(c) L(ùëÅ) =\\n\\x08f(ùëÉ1, . . . , ùëÉùëõ) | ùëÅ‚Üíf(ùë£1, . . . , ùë£ùëõ) ‚ààR, ‚àÄùëñ.ùëÉùëñ‚ààL(ùë£ùëñ)\\n\\t\\n‚à™\\n\\x08\\nùëÉ| ùëÅ‚Üíùë£‚ààR, ùë£‚ààT, ùëÉ‚àà\\nL(ùë£)\\n\\t\\n. The set of programs defined by the whole DSL L(D) is L(ùë£out).\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 9, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:10\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nExample 3.1. The following is a simple DSL Dùê¥for affine arithmetic expressions over naturals\\nN: (a) Here, the terminals T are {input1, input2, 0, 1, 2, . . .}, with input1, input2 being the spe-\\ncial input terminals in Vin. (b) The non-terminals N are {output, addend, const}, with output\\nbeing the output symbol ùë£out. (c) The operators F are Plus and Times. (d) The rules R are given\\nby {output ‚ÜíPlus(addend, output), output ‚Üíconst, addend ‚ÜíTimes(const, input1),\\naddend ‚ÜíTimes(const, input2), const ‚Üí0 | 1 | 2 | . . .}. Note that the declaration uint\\nconst; in the listing is shorthand for a set of rules for the form const ‚Üíùëòfor each ùëò‚ààN.\\nPlus(Times(5, input1), 3) is a sample program in L(Dùê¥).\\n‚ñ°\\n@input uint input1, input2;\\n@start uint output := Plus(addend, output) | const;\\nuint addend := Times(const, input1) | Times(const, input2);\\nuint const;\\n3.2\\nSynthesis Tasks and Solutions'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 9, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='‚ñ°\\n@input uint input1, input2;\\n@start uint output := Plus(addend, output) | const;\\nuint addend := Times(const, input1) | Times(const, input2);\\nuint const;\\n3.2\\nSynthesis Tasks and Solutions\\nA state ùëÜis a valuation of all input symbols in a DSL, i.e., ùëÜ= {in1 ‚Ü¶‚Üíùëú1, . . . , inùëò‚Ü¶‚Üíùëúùëò} where inùëñ\\nare input symbols and ùëúùëñare values. An example Ex is a pair ùëÜ‚Ü¶‚Üíùëúof a state ùëÜand value ùëú.\\nA synthesis task ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©is given by: (a) a term ùõº, which is either a nonterminal, terminal, or\\nright-hand side of a rule, (b) a set R of rules, and (c) an example ùëÜ‚Ü¶‚Üíùëú. A solution of the synthesis\\ntask ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©is a program ùëÉsuch that: (a) JùëÉK(ùëÜ) = ùëú, and (b) ùëÉ‚ààL(ùõº). Here, JùëÉK : OVin ‚Ü¶‚ÜíO\\nrepresents the standard semantics of a program. Formally, JùëÉK(ùëÜ) is recursively defined as: (1)\\nJinùëñK(ùëÜ) = ùëÜ(inùëñ), (2) JùëúK(ùëÜ) = ùëúfor every value ùëú, (3) Jf(ùë£1, ùë£2)K(ùëÜ) = JfK(Jùë£1K(ùëÜ), Jùë£2K(ùëÜ)) for every\\noperator f. To keep the presentation simple, the synthesis task is defined to contain one input-output'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 9, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='operator f. To keep the presentation simple, the synthesis task is defined to contain one input-output\\nexample. In practice, a synthesis task often involves multiple examples. It is straight-forward to\\nextend our technique for this, as done in our implementation.\\nExample 3.2. A sample synthesis task for the affine expression DSL Dùê¥is ‚ü®output, R,ùëÜ‚Ü¶‚Üí7‚ü©\\nwhere ùëÜ= {input1 ‚Ü¶‚Üí2, input2 ‚Ü¶‚Üí0}. The program Plus(Times(3, input1), 1) is a solution of\\nthis task. Here Times and Plus have their usual arithmetic semantics.\\n‚ñ°\\nGiven a synthesis task ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, a synthesizer generates a program set PS such that every\\nprogram in the set PS is a solution of the synthesis task, which we denote by the assertion PS |=\\n‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. Note that it is vacuously true that ‚àÖ|= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, and so practical synthesizers\\nstrive to establish the above assertion for nonempty sets PS. The notation Ã∏|= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©denotes\\nthat there is no nonempty set PS that is a solution for the synthesis task.\\n3.3'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 9, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='strive to establish the above assertion for nonempty sets PS. The notation Ã∏|= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©denotes\\nthat there is no nonempty set PS that is a solution for the synthesis task.\\n3.3\\nBottom-Up and Top-Down Synthesis\\nThere are two main approaches for solving the synthesis task: bottom-up and top-down.\\nThe bottom-up (BU) approach enumerates programs generated by different nonterminals of the\\ngrammar and collects the values that those programs compute on the input state ùëÜ. More precisely,\\nfor each nonterminal ùëÅ‚Ä≤, the BU approach computes the bottom-up value set buùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) given by\\n{JùëÉ‚Ä≤K(ùëÜ) | ùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤)}. These sets are computed starting from the leaf (terminals) of the grammar\\nand moving up to the root (start symbol ùë£out). Success is declared if the output value ùëúis found to\\nbe in the set buùë£out (ùëÜ‚Ü¶‚Üíùëú). Note that the BU procedure is not guided by the output value ùëú.\\nThe top-down (TD) approach starts with the output value ùëúthat needs to be generated at start'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 9, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='be in the set buùë£out (ùëÜ‚Ü¶‚Üíùëú). Note that the BU procedure is not guided by the output value ùëú.\\nThe top-down (TD) approach starts with the output value ùëúthat needs to be generated at start\\nsymbol ùë£out, and for every nonterminal ùëÅ‚Ä≤, it computes the set of values that flow to ùëúat ùë£out.\\nMore formally, we say a value ùëú‚Ä≤ at ùëÅ‚Ä≤ flows to value ùëúat ùëÅif either (a) ùëÅ‚Üíùëì(. . . , ùëÅ‚Ä≤, . . .) is a\\ngrammar rule and JùëìK(ùëú1, . . . ,ùëú‚Ä≤, . . . ,ùëúùëò) = ùëúfor some values ùëú1, . . . ,ùëúùëò, or (b) there exist ùëú‚Ä≤‚Ä≤ and\\nùëÅ‚Ä≤‚Ä≤ s.t. ùëú‚Ä≤ at ùëÅ‚Ä≤ flows to ùëú‚Ä≤‚Ä≤ at ùëÅ‚Ä≤‚Ä≤ and ùëú‚Ä≤‚Ä≤ at ùëÅ‚Ä≤‚Ä≤ flows to ùëúat ùëÅ. For each nonterminal ùëÅ‚Ä≤, the\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 10, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:11\\nTD approach computes a top-down value set tdùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) that contains the values ùëú‚Ä≤ at ùëÅ‚Ä≤ that\\nflow into the value ùëúat ùë£out. The top-down value sets are computed using witness functions. An\\noperator f with arity ùëõis associated with ùëõwitness functions - one for each argument position. The\\nùëò-th witness function, WFf,ùëò: Oùëò‚Ü¶‚Üí2O, for operator f maps the desired output and ùëò‚àí1 values\\nfor previous arguments to possible values for the ùëò-th argument. Such a parameterized collection\\nof witness functions is sound (complete) if ùëúùëò‚ààWFf,ùëò(ùëú,ùëú1, . . . ,ùëúùëò‚àí1) for ùëò= 1, . . . ,ùëõimplies (is\\nimplied by) JfK(ùëú1, . . . ,ùëúùëõ) = ùëú. For example, if the top-level operator is Plus and the output is 7, the\\npossible arguments for Plus would be (0, 7), (1, 6), (2, 5), . . ., and hence, WFPlus,1(7) = {0, 1, 2, . . .}\\nand WFPlus,2(7,ùë•) = {7 ‚àíùë•}.\\nTop-down and bottom-up synthesis are both efficient and practical in different scenarios. TD'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 10, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='and WFPlus,2(7,ùë•) = {7 ‚àíùë•}.\\nTop-down and bottom-up synthesis are both efficient and practical in different scenarios. TD\\nsynthesis performs well when operators are effectively invertible (EI), i.e., the witness functions\\nWFf,ùëòfor each operator f produces sets that are finite and manageable in size. BU synthesis performs\\nwell when the grammar is effectively enumerable (EE), i.e., both the number of constants in the\\ngrammar is bounded and the number of different intermediate values produced is manageable.\\nNote that the focus in this paper is exclusively on synthesis approaches based on concrete values:\\nthe semantics and witness functions are given on concrete values, and not abstract values or types.\\nAbstractions and types can make top-down strategies work on grammars with non-EI operators,\\nfor example [Feng et al. 2017; Polikarpova et al. 2016], but require additional machinery, such as\\ntype systems, abstract domains, and constraint solvers.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 10, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='for example [Feng et al. 2017; Polikarpova et al. 2016], but require additional machinery, such as\\ntype systems, abstract domains, and constraint solvers.\\nExample 3.3. The affine expression DSL from Example 3.1 is neither effectively invertible nor\\neffectively enumerable. The operator Plus has a witness function WFPlus,1 which produces a set\\nof size ùëõ+ 1 for an example ùëÜ‚Ü¶‚Üíùëõ. Further, it contains an infinite number of constants (i.e., the\\nnon-negative integers) which make bottom-up approach infeasible. In the FlashFill++ DSL, many\\nstring operators are not effectively invertible. E.g., the LowerCase operator‚Äôs witness function that\\ncan produce a set that is exponential in the input‚Äôs length: WFLowerCase,1(‚Äòabc‚Äô) produces a set of\\nsize 8, i.e., {‚ÄòABC‚Äô, ‚ÄòABc‚Äô, ‚ÄòAbC‚Äô, ‚ÄòaBC‚Äô, ‚ÄòabC‚Äô, ‚ÄòaBc‚Äô, ‚ÄòAbc‚Äô, ‚Äòabc‚Äô}.\\n‚ñ°\\nIn Section 4, we introduce cuts that can be used to decompose the synthesis problem to enable\\nmiddle-out synthesis, which can learn over deep DSLs ‚Äì DSLs that can generate programs with'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 10, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='‚ñ°\\nIn Section 4, we introduce cuts that can be used to decompose the synthesis problem to enable\\nmiddle-out synthesis, which can learn over deep DSLs ‚Äì DSLs that can generate programs with\\nlarge depth ‚Äì where neither bottom-up nor top-down is feasible. In Section 5, we introduce gDSLs,\\nwhich provide a precedence-based mechanism to help learning scale to broad DSLs ‚Äì DSLs with\\nseveral options for a single nonterminal.\\n4\\nCUTS AND MIDDLE-OUT SYNTHESIS\\nTop-down and bottom-up approaches, as well as their combination, struggle to scale to large DSLs.\\nCuts can help scale synthesis. If we want to generate a value ùëúat nonterminal ùëÅ, and another\\nnonterminal ùëÅ‚Ä≤ is on the path from ùëÅto the terminals, then a cut at ùëÅ‚Ä≤ returns values to generate\\nat ùëÅ‚Ä≤ that can help with generating ùëúat ùëÅ.\\nDefinition 4.1 (Cuts). Given a synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and a non-terminal ùëÅ‚Ä≤ ‚ààN, a cut\\nCutùëÅ‚Ä≤,ùëÅfor ùëÅ‚Ä≤ in the context ùëÅ, maps an example, ùëÜ‚Ü¶‚Üíùëú, to a set of values. Such a function is'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 10, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Definition 4.1 (Cuts). Given a synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and a non-terminal ùëÅ‚Ä≤ ‚ààN, a cut\\nCutùëÅ‚Ä≤,ùëÅfor ùëÅ‚Ä≤ in the context ùëÅ, maps an example, ùëÜ‚Ü¶‚Üíùëú, to a set of values. Such a function is\\ncomplete if for every solution ùëÉfor the task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, whenever ùëÉcontains a sub-program\\nùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤), then JùëÉ‚Ä≤K(ùëÜ) ‚ààCutùëÅ‚Ä≤,ùëÅ(ùëÜ‚Ü¶‚Üíùëú).\\nNote that ùëÅneed not be the start symbol of the grammar and ùëúneed not be the original output\\nvalue in the input-output example. Typically, we define cuts CutùëÅ‚Ä≤,ùëÅwhen ùëÅ‚Üíùëì(ùëÅ‚Ä≤, ùëÅ‚Ä≤‚Ä≤) is a\\ngrammar rule. Such a cut can be used in place of the witness function for the first argument of ùëì.\\nLet us illustrate cuts through an example.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 11, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:12\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nMO.Cut\\nPS1 |= ‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©\\nPS2 |= ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©\\n√ò\\nùëú1\\nPS2[ùëú1 ‚Ü¶‚ÜíPS1] |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nif ùëú1 ‚ààCutùëÅ1,ùëÅ(ùëÜ‚Ü¶‚Üíùëú)\\nFig. 5. The cut inference rule that enables middle-out program synthesis.\\nExample 4.2. Consider again the synthesis task from Section 2.2, where the input-output example\\nwas ùëÜ‚Ü¶‚Üí24.00 and the input state ùëÜwas ‚ü®in1 ‚Ü¶‚Üí‚ÄúThe price is $24.58 and 46 units are\\navailable.‚Äù ‚ü©. Suppose we want to synthesize a program from this example starting from the\\nnonterminal roundNumber of the FlashFill++ DSL (Figure 7). One potential cut for parseNumber in\\nthe context roundNumber could work by scanning the input for any maximal substrings that are\\nnumerical constants and returning them (as a number). Here, it would return {24.58, 46}. A more\\nsophisticated cut could additionally look at the output 24.00 and only return the set {24.58}, as it is'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 11, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='numerical constants and returning them (as a number). Here, it would return {24.58, 46}. A more\\nsophisticated cut could additionally look at the output 24.00 and only return the set {24.58}, as it is\\nthe only value in the string that can be rounded down to 24.00. These cuts are not complete as they\\ndo not include 24.5, which can also be extracted from the input and rounded to 24.\\n‚ñ°\\nRecall that we model synthesizers as generating nonempty program sets PS and asserting\\nPS |= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. Figure 5 presents a new inference rule, called the cut rule, that can be used\\nto generate such assertions. This rule can be used in conjunction with any synthesizer (such\\nas those based on top-down or bottom-up approach). The cut rule uses a cut for a nonterminal\\nùëÅ1 in the context of ùëÅto decompose the overall synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©into two subtasks\\n‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©and ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©. The first, or inner, subtask tries to find a program'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 11, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='ùëÅ1 in the context of ùëÅto decompose the overall synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©into two subtasks\\n‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©and ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©. The first, or inner, subtask tries to find a program\\nin L(ùëÅ1) that maps ùëÜto ùëú1, whereas the second, or outer, subtask tries to find a program in L(ùëÅ)\\nthat maps ùëÜto ùëúassuming that we have a program to maps ùëÜto ùëú1. The notation R with ùëÅ1 ‚Üíùëú1\\nsimply means we remove all old rules in R of the form ùëÅ1 ‚Üíùõºand only have one rule ùëÅ1 ‚Üíùëú1.\\nThe cut rule also shows how the solutions to the two subtasks are combined to generate a solution\\nfor the original synthesis task.\\nTheorem 4.3. [Soundness] If program sets PS1 and PS2 are such that PS1 |= ‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©and\\nPS2 |= ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©, then PS2[ùëú1 ‚Ü¶‚ÜíPS1] |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. Furthermore, [complete-\\nness] if a program ùëÉis a solution for the synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and the program ùëÉcontains a\\nsubprogram ùëÉ1 ‚ààL(ùëÅ‚Ä≤) that maps the input ùëÜto a value ùëú1 (i.e., JùëÉ1K(ùëÜ) = ùëú1), then ùëú1 will be in the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 11, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='ness] if a program ùëÉis a solution for the synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and the program ùëÉcontains a\\nsubprogram ùëÉ1 ‚ààL(ùëÅ‚Ä≤) that maps the input ùëÜto a value ùëú1 (i.e., JùëÉ1K(ùëÜ) = ùëú1), then ùëú1 will be in the\\ncut CutùëÅ1,ùëÅ(ùëÜ‚Ü¶‚Üíùëú) assuming that the cut is complete, and moreover, the program ùëÉ[ùëÉ1 ‚Ü¶‚Üíùëú1] is a\\nsolution for the task ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©.\\nWe use the term middle-out synthesis to describe the synthesis approach that uses the Rule MO.Cut\\nto perform synthesis. Note that the subproblems created by Rule MO.Cut can be solved using either\\nthe top-down approach, or the bottom-up approach, or the middle-out approach, or a hybrid\\ncombination of the approaches. One common strategy is: after applying Rule MO.Cut, we solve\\nthe outer subtask using bottom-up or hybrid approach, and for each ùëú1 for which the outer has a\\nsolution, we solve the inner subtask using the top-down or hybrid approach. Note that the cut rule\\ncan be used multiple times to solve a synthesis task.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 11, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='solution, we solve the inner subtask using the top-down or hybrid approach. Note that the cut rule\\ncan be used multiple times to solve a synthesis task.\\nExample 4.4. Consider the synthesis task ‚ü®roundNumber, R,ùëÜ‚Ü¶‚Üí24.00‚ü©from Example 4.2. Us-\\ning the fact that 24.58 was in the cut for parseNumber, we can use MO.Cut rule from Figure 5\\nand get the subtasks ‚ü®parseNumber, R,ùëÜ‚Ü¶‚Üí24.58‚ü©and ‚ü®roundNumber, R‚Ä≤,ùëÜ‚Ü¶‚Üí24.00‚ü©, where R‚Ä≤\\nis R with parseNumber ‚Üí24.58. The second subproblem now has only one rule for parseNumber,\\nwhich directly generates 24.58. The first subproblem now has to generate 24.58 from the input, and\\nthe second subproblem has to round 24.58 to 24.00.\\n‚ñ°\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 12, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:13\\n4.1\\nGeneralizing Top-Down and Bottom-Up Synthesis\\nExamining the top-down and bottom-up synthesis approaches closely, it can be seen that top-\\ndown synthesis is purely output driven and bottom-up synthesis is purely input driven. Cuts neatly\\ngeneralize both these approaches, allowing us the possibility to use a set of values influenced by\\nboth: (a) the set ùêµof values reachable through forward semantics from the input values (bottom-up\\nsearch), and (b) the set ùëáof values reachable through inverse semantics from the output values\\n(top-down search).\\nRecall that the set buùëÅ‚Ä≤ is the set of values that bottom-up enumeration generates corresponding\\nto nonterminal ùëÅ‚Ä≤ and the set tdùëÅ,ùëÅ‚Ä≤ is the set of values that arise at ùëÅ‚Ä≤ by repeatedly applying\\n(precise) witness functions starting from ùëÅ. Let real value set rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) be the set of values\\nJùëÉ‚Ä≤K(ùëÜ) where ùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤) is a sub-program of a program ùëÉ‚ààL(ùëÅ) such that JùëÉK(ùëÜ) = ùëú. Clearly,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 12, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='(precise) witness functions starting from ùëÅ. Let real value set rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) be the set of values\\nJùëÉ‚Ä≤K(ùëÜ) where ùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤) is a sub-program of a program ùëÉ‚ààL(ùëÅ) such that JùëÉK(ùëÜ) = ùëú. Clearly,\\nit can be seen that rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) ‚äÜtdùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) ‚à©buùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú). Restating the definition of\\ncomplete cuts, a cut is complete if and only if the set it returns is a superset of rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú). Both\\ntop-down and bottom-up search for synthesis are special cases of cut-based middle-out synthesis.\\nTheorem 4.5. [Cuts generalize top-down and bottom-up value sets.] Given a synthesis problem\\n‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and a non-terminal ùëÅ‚Ä≤, both the functions buùëÅ‚Ä≤ and tdùëÅ,ùëÅ‚Ä≤ are complete cuts for the\\nnon-terminal ùëÅ‚Ä≤ in the context of ùëÅ.\\nIn the light of this theorem, restricting the cut in the middle-out synthesis rule from Figure 5 to\\nonly being buùëÅ‚Ä≤ produces the state-of-the-art combination of top-down and bottom-up synthesis\\nDuet [Lee 2021]. While the above theorem states that TD and BU analyses produce complete cuts,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 12, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='only being buùëÅ‚Ä≤ produces the state-of-the-art combination of top-down and bottom-up synthesis\\nDuet [Lee 2021]. While the above theorem states that TD and BU analyses produce complete cuts,\\nnot all complete cuts are (overapproximations of) top-down value set or bottom-up value set.\\nExample 4.6. Building on Example 4.2, consider now the example ùëÜ‚Ü¶‚Üí24.58, but we want\\nto synthesize a program from this example starting from the nonterminal parseNumber, which\\nhas a rule parseNumber ‚ÜíParseNumber(substr, locale). One potential cut for substr in the\\ncontext parseNumber could be obtained by scanning the input string for any substrings that are\\nnumerical constants and returning them (as a string). Here it returns a set containing ‚Äú24.58‚Äù and\\n‚Äú46‚Äù and all substrings of these two strings that are valid numbers. This complete cut is not an\\noverapproximation of the bottom-up values that substr can generate since there are many more'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 12, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='‚Äú46‚Äù and all substrings of these two strings that are valid numbers. This complete cut is not an\\noverapproximation of the bottom-up values that substr can generate since there are many more\\nsubstrings in an input. However, it is complete in the context parseNumber because these are the\\nonly strings that can be parsed as numbers.\\n‚ñ°\\n4.2\\nComputing Cuts\\nWe can use top-down value sets or bottom-up value sets as the cuts, as shown in Theorem 4.5;\\nhowever, if we do that, we only replicate top-down, bottom-up, and Duet‚Äôs meet-in-the-middle\\nsynthesis [Lee 2021]. DSL designers can provide more refined cuts that would enable going beyond\\ncurrent methods. How can designers author more refined cuts? For most operators, using witness\\nfunctions to perform top-down synthesis might suffice. Specialized cuts are only required when we\\nhave not-effectively-invertible operators that have either no witness function or very inefficient\\nwitness functions.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 12, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='have not-effectively-invertible operators that have either no witness function or very inefficient\\nwitness functions.\\nIn practice, DSL designers do not necessarily have to use complete cuts. Looking back at Exam-\\nple 4.6, the set {‚Äú24.58‚Äù, ‚Äú46‚Äù} is a reasonable, but incomplete cut, because any program that extracts\\na number from a strict substring, say ‚Äú4.5‚Äù, of these two strings would be a contrived program. Cuts\\nare reminiscent of interpolants or invariants from program analysis: a cut at ùëÅ‚Ä≤ in the context of ùëÅ\\nis the denotation of an invariant that holds true at ùëÅ‚Ä≤ for all programs that compute the desired\\noutput at ùëÅ. We next describe a few cut functions used in FlashFill++ along with the DSL designer‚Äôs\\nintuition about the DSL that helped construct that cut function.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 13, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:14\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nCut for LowerCase. When computing a cut for a nonterminal ùëÅ‚Ä≤ in context of nonterminal ùëÅ, we\\nneed to consider all paths from ùëÅ‚Ä≤ to ùëÅin the grammar. We focus on one path at a time, and take a\\nunion if there are multiple paths. Consider the grammar rule single := LowerCase(concat) in the\\nFlashFill++ DSL (Figure 7), and ignore the other paths between nonterminals single and concat\\nfor now. Clearly, the witness function is given as:\\nWFLowerCase,1(ùë¶) = {ùë•| lowercase(ùë•) = ùë¶}\\nThe returned set contains 2len(ùë¶) strings. In contrast, the cut could be much smaller. Ignoring other\\npaths between the two nonterminals, one possible cut is:\\nCutconcat,single(ùëÜ‚Ü¶‚Üíùë¶) = {ùë•| lowercase(ùë•) = ùë¶and each char of ùë•is in some input in ùëÜ}.\\nHere the DSL designer uses their knowledge that the non-terminal concat can only generate strings'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 13, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Cutconcat,single(ùëÜ‚Ü¶‚Üíùë¶) = {ùë•| lowercase(ùë•) = ùë¶and each char of ùë•is in some input in ùëÜ}.\\nHere the DSL designer uses their knowledge that the non-terminal concat can only generate strings\\nwhose characters are in some input. One exception to this invariance rule are strings that are\\ngenerated as ‚Äúconstant strings\", and hence this cut is not complete, but it may be a reasonable\\ncompromise between completeness and efficiency. An alternative cut could be:\\n{ùë•| lowercase(ùë•) = ùë¶and for each char ùë•[ùëñ] of ùë•, ùë•[ùëñ] is in some input or ùë•[ùëñ] == ùë¶[ùëñ]}.\\nFinally, the designer can further refine the cut to only include those strings that contain large\\nchunks of substrings of some input. Consider input ‚ÄòAmal Ahmed <AMAL@CCS.NEU.EDU>‚Äô and\\noutput ‚Äòamal‚Äô. The witness function would return 16 values, all variations of the string ‚Äòamal‚Äô\\nwith each letter optionally capitalized. However, a cut would only return 2 values ‚ÄòAmal‚Äô and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 13, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='output ‚Äòamal‚Äô. The witness function would return 16 values, all variations of the string ‚Äòamal‚Äô\\nwith each letter optionally capitalized. However, a cut would only return 2 values ‚ÄòAmal‚Äô and\\n‚ÄòAMAL‚Äô, i.e., those variations that occur contiguously in the input. The same kind of reasoning\\ncan be used on all other paths from concat to single that go via the operators UpperCase and\\nProperCase, and thus we can get a cut for concat in the context of single.\\nCut for Concat. Consider the grammar rule concat := segment|Concat(segment, concat) in the\\nFlashFill++ DSL (Figure 7). The witness function for (the first argument of) Concat is given by\\nWFConcat,1(ùë¶) = {ùë•| ùë•is a prefix of ùë¶}\\nThe DSL designer knows the invariant that every string generated by segment is either a substring\\nof an input, or a string representation of date or number, or a constant string. So, a possible cut,\\nCutsegment,concat(ùëÜ‚Ü¶‚Üíùë¶), could be:\\n{ùë•| ùë•is maximal prefix of ùë¶either contained in some input or a number or a date}.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 13, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Cutsegment,concat(ùëÜ‚Ü¶‚Üíùë¶), could be:\\n{ùë•| ùë•is maximal prefix of ùë¶either contained in some input or a number or a date}.\\nThis cut is not complete since segment could generate a constant string, but the DSL designer may\\nprefer to use this cut and fallback on the witness function only if the above choices fail to work.\\nCut for RoundNumber and FormatDateTime. Example 4.2 presented the cut for parseNumber\\nin the context roundNumber. The witness function for roundNumber on the input ùëÜ‚Ü¶‚Üí24.00 will\\nhave to return an infinite set of floating point values that can all round to 24.00. However, the DSL\\ndesigner knows that parseNumber can only generate a number that occurs in the input, i.e., 24.58 or\\n46, and furthermore, numbers such as 4.5 are not reasonable choices. Hence, the designer can pick the\\ncut CutparseNumber,roundNumber(ùëÜ‚Ü¶‚Üíùë¶):\\n{ùë•| ùë•is a maximally long number extracted from a substring of an input}.\\nHere the DSL designer used their knowledge about the form of values that a nonterminal can'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 13, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='{ùë•| ùë•is a maximally long number extracted from a substring of an input}.\\nHere the DSL designer used their knowledge about the form of values that a nonterminal can\\ngenerate to construct a cut. Another such example is the cut, CutasDate,formatDate(ùëÜ‚Ü¶‚Üíùë¶), which\\ncan be computed as:\\n{ùë•| ùë•is a maximally long datetime value extracted from a substring of an input}.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 14, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:15\\nCut for Const in the Affine Grammar. The DSL designer of arithmetic expressions given in\\nExample 3.1 can provide a cut for const in the context output by noting the monotonicity invariant:\\nvalues computed by subexpressions are smaller than values computed by the whole expression. Consider\\nthe input stateùëÜ= ‚ü®in1 ‚Ü¶‚Üí2, in2 ‚Ü¶‚Üí0‚ü©and the input-output exampleùëÜ‚Ü¶‚Üí7. Since the output is 7, we\\ncan restrict the potential values for the coefficient of in1 (which is 2) to at most 3 values, i.e., {0, 1, 2, 3}\\nas any larger value will make the product exceed the value 7. Thus, the cut, Cutconst,output(ùëÜ‚Ü¶‚Üíùë¶),\\nis computed as:\\n{ùë•‚ààN | 0 ‚â§ùë•‚â§‚åä\\nùë¶\\nùëÜ[in1] ‚åã}.\\nUsing this cut we can now perform bottom-up synthesis, whereas it wasn‚Äôt possible before since\\nthere are an infinite set of possible values for the non-terminal const and the grammar is not\\neffectively enumerable.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 14, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='there are an infinite set of possible values for the non-terminal const and the grammar is not\\neffectively enumerable.\\nWhenever there is a nontrivial invariant that holds for all values that can be generated at a\\ncertain nonterminal, we can usually exploit that invariant to design a cut for that nonterminal.\\nWhile we have focused mainly on the FlashFill++ DSL to illustrate this process of designing good\\ncuts, the ideas extend to DSLs for any other domain.\\n5\\nPRECEDENCE IN DOMAIN-SPECIFIC LANGUAGES\\nWe first define the problem of synthesis in presence of precedence over operators. We then introduce\\ngDSLs, which extend the notion of DSL (Section 3.1) with precedence. We then present the gDSL\\nfor FlashFill++ and inference rules that solve the PBE synthesis problem over gDSLs.\\n5.1\\nSynthesis with Preference\\nDSL designers who want to translate programs generated in a DSL into a popular target language,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 14, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='5.1\\nSynthesis with Preference\\nDSL designers who want to translate programs generated in a DSL into a popular target language,\\nsuch as Python, typically want the DSL to contain all operators from the target language libraries.\\nThese operators are often redundant. For example, substrings of a string can be extracted using\\nabsolute index positions, or regular expressions, or finding locations of constant substrings in the\\nstring, or splitting a string by certain delimiters. In such cases, whenever a task is achievable in\\nmany different ways, we get a so-called broad DSL. For broad DSLs, DSL designers often have a\\npreference for which operators to use. For example, they may prefer split over find, which they\\nmay prefer in turn over using regular expressions or absolute indices. As another example, DSL\\ndesigners may prefer transforming a string containing ‚ÄúJan‚Äù to ‚ÄúJanuary‚Äù by treating the substring'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 14, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='may prefer in turn over using regular expressions or absolute indices. As another example, DSL\\ndesigners may prefer transforming a string containing ‚ÄúJan‚Äù to ‚ÄúJanuary‚Äù by treating the substring\\naround ‚ÄúJan‚Äù in the input as a datetime object and working with them, rather than generating\\n‚ÄúJanuary‚Äù by concatenating ‚ÄúJan‚Äù with a constant string ‚Äúuary‚Äù.\\nExample 5.1. Consider the task of extracting the second column from a line in a comma-separated\\nvalues (CSV) file, specified by the input-output example ‚ÄòWA, Olympia, UTC-8‚Äô ‚Ü¶‚Üí‚ÄòOlympia‚Äô.\\nIn the FlashFill++ DSL, this can be done using the program Split(x, ‚Äò,‚Äô, 2), where ùë•is the input,\\nor using the program Slice(x, Find(x, ‚Äò,‚Äô, 1, 0), Find(x, ‚Äò,‚Äô, 2, 0)). A traditional VSA-based syn-\\nthesizer would (possibly implicitly) produce both programs, assign scores to both using a ranking\\nfunction, and return the better ranked one. However, typically we strictly prefer programs that use'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 14, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='thesizer would (possibly implicitly) produce both programs, assign scores to both using a ranking\\nfunction, and return the better ranked one. However, typically we strictly prefer programs that use\\nthe Split operator over the Slice operator. Ideally, a synthesizer should not even examine Slice\\nprograms when an equivalent Split program exists. Similar preference is also seen in Python\\nprogrammers who often prefer to use str.split over regex.find or str.find.\\n‚ñ°\\nThis motivates the need to perform synthesis over a broad DSL where there is preference over\\noperators. Suppose a DSL designer has a DSL and a preference over operators and terminals. Let\\nŒ£ := F ‚à™T be the collection of all operators and terminal symbols in the DSL and let ‚âªŒ£ be a\\nprecedence relation on the symbols f ‚ààŒ£. We make the assumption that (A1) the DSL designer\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 15, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:16\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nonly provides precedence over operators that occur as alternates for the same nonterminal; that is,\\nif the designer sets f1 ‚âªŒ£ f2, then ùëÅ‚Üíf1(. . .) ‚ààR and ùëÅ‚Üíf2(. . .) ‚ààR are two rules with the\\nsame nonterminal ùëÅin the grammar. We also assume that (A2) the relation ‚âªŒ£ is a strict partial\\norder (irreflexive, asymmetric, transitive) to ensure that the DSL designers preference is consistent.\\nWe first need to formalize what it means for a synthesizer to satisfy the operator precedence ‚âªŒ£\\nprovided by the DSL designer. For this, we need to lift ‚âªŒ£ to a precedence on the set of programs ùëÉ\\ngenerated by the DSL. However, this is not easy since it is not clear which of f1(f2(in)) or f‚Ä≤\\n1(f‚Ä≤\\n2(in))\\nto prefer if the DSL designer says f1 ‚âªŒ£ f‚Ä≤\\n1 and f‚Ä≤\\n2 ‚âªŒ£ f2. We resolve this issue by saying that the\\npreference for the operator occurring ‚Äúabove‚Äù in the program is more important than anything'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 15, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='to prefer if the DSL designer says f1 ‚âªŒ£ f‚Ä≤\\n1 and f‚Ä≤\\n2 ‚âªŒ£ f2. We resolve this issue by saying that the\\npreference for the operator occurring ‚Äúabove‚Äù in the program is more important than anything\\nbelow. In the above example, we give more weight to f1 ‚âªŒ£ f‚Ä≤\\n1 and hence we want f1(f2(in)) to be\\npreferred over f‚Ä≤\\n1(f‚Ä≤\\n2(in)). This follows the intuition that operators at the top in, say, the FlashFill++\\nDSL, such as Concat, FormatNumber, or FormatDateTime, are more influential in determining the\\nhigh-level strategy for solving a task than operators at the bottom, such as Split or Slice. Hence,\\nwe extend the user provided ‚âªŒ£ to ‚âªùëí\\nŒ£‚äá‚âªŒ£ so that whenever ùëÅ0 ‚Üíf1(. . . , ùëÅ1, . . .) and ùëÅ1 ‚Üíf2(. . .)\\nare rules in the DSL, then f1 ‚âªùëí\\nŒ£ f2.\\nExample 5.2. Consider the synthesis task from Example 4.2 of generating ‚Äò24.00‚Äô from the input\\nstring. One possible program is Concat(ùëù1, ‚Äò.00‚Äô), where ùëù1 is a subprogram that extracts ‚Äò24‚Äô'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 15, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Œ£ f2.\\nExample 5.2. Consider the synthesis task from Example 4.2 of generating ‚Äò24.00‚Äô from the input\\nstring. One possible program is Concat(ùëù1, ‚Äò.00‚Äô), where ùëù1 is a subprogram that extracts ‚Äò24‚Äô\\nfrom the input string. A second possible program is Segment(FormatNumber(ùëù2, fmt_desc) that\\ngenerates ‚Äò24.00‚Äô by formatting a number computed by program ùëù2. Here, Segment is a dummy\\nidentity operator having higher preference than Concat in the FlashFill++ gDSL (Figure 7). In the\\nFlashFill++ DSL, the second program is preferred since it does not use concatenation, irrespective of\\nhow ùëù1 and ùëù2 work‚Äîat the top-level concatenation is strictly less preferred. Note that here ùëù1 is\\nlikely to be significantly smaller and simpler than ùëù2 as it is just extracting the string ‚Äò24‚Äô, while\\nùëù2 is extracting a number and then rounding it. A traditional arithmetic ranking function (as used\\nin FlashFill and FlashMeta) intuitively computes the score of programs as a weighted sum of the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 15, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='ùëù2 is extracting a number and then rounding it. A traditional arithmetic ranking function (as used\\nin FlashFill and FlashMeta) intuitively computes the score of programs as a weighted sum of the\\nscore of its sub-programs, and hence, will need to be tuned carefully to ensure that the smaller\\nconcat program is scored worse than the larger segment program.\\n‚ñ°\\nWe want the relation ‚âªùëí\\nŒ£ to be a strict partial order. However, in general, it may not be a strict\\npartial order due to cycles in the grammar (where some ùëÅ0 generates a term containing ùëÅ0), which\\nagain makes ‚âªùëí\\nŒ£ violate irreflexivity or transitivity. We make the reasonable assumption that there\\nare no cycles since we often limit the depth of terms being synthesized and then the assumption\\ncan be satisfied by renaming the nonterminals. Thus, without loss of much generality, we can\\nassume that the extended precedence ‚âªùëí\\nŒ£ on Œ£ is a strict partial order.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 15, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='can be satisfied by renaming the nonterminals. Thus, without loss of much generality, we can\\nassume that the extended precedence ‚âªùëí\\nŒ£ on Œ£ is a strict partial order.\\nNow we formalize synthesizing in presence of precedence ‚âªŒ£ by lifting the precedence ‚âªŒ£ on\\nŒ£ to ‚âªùëí\\nŒ£, and then to a preference on programs (trees) over Œ£ using the well-known lexicographic\\npath ordering (LPO), ‚âªùëôùëùùëú, which is defined as follows [Dershowitz and Jouannaud 1990]: Given\\nprograms ùëÉ1 := f1(ùëÉ11, . . . , ùëÉ1ùëò) and ùëÉ2 := f2(ùëÉ21, . . . , ùëÉ2ùëô), we have ùëÉ1 ‚âªùëôùëùùëúùëÉ2 if either (a) f1 ‚âªùëí\\nŒ£ f2\\nand ùëÉ1 ‚âªùëôùëùùëúùëÉ2ùëñfor all ùëñ, or (b) f1 = f2 and there exists a ùëös.t. ùëÉ1ùëñ= ùëÉ2ùëñfor ùëñ< ùëöand ùëÉ1ùëö‚âªùëôùëùùëúùëÉ2ùëö,\\nor (c) ùëÉ1ùëñ‚âªùëôùëùùëúùëÉ2 for some ùëñ.\\nDefinition 5.3. Let ‚™∞base be the base ordering to rank programs that are unordered by ‚âªùëôùëùùëú. Given\\na DSL D with precedence ‚âªŒ£ on the set Œ£ := F ‚à™T of all operators and terminal symbols in D, and\\n‚™∞base, the PBE synthesis with precedence problem is to find the maximally ranked program by the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 15, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='a DSL D with precedence ‚âªŒ£ on the set Œ£ := F ‚à™T of all operators and terminal symbols in D, and\\n‚™∞base, the PBE synthesis with precedence problem is to find the maximally ranked program by the\\nlexicographic combination of ‚âªùëôùëùùëúand ‚™∞base that satisfies a given example, where ‚âªùëôùëùùëúis the LPO\\ninduced by the extended precedence ‚âªùëí\\nŒ£.2\\n2Given orderings ‚âª1 and ‚âª2, the lexicographic combination ‚âª‚âª1,‚âª2 is defined as follows: ùë†‚âª‚âª1,‚âª2 ùë°if either (a) ùë†‚âª1 ùë°, or\\n(b) ùë†‚äÅ1 ùë°and ùë°‚äÅ1 ùë†and ùë†‚âª2 ùë°.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 16, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:17\\nWe solve the PBE synthesis with precedence problem by extending DSLs to gDSLs and modifying\\nthe inference rules to correctly handle the precedence.\\n5.2\\nGuarded Domain-Specific Languages\\nA guarded domain-specific language (gDSL) is a DSL D = ‚ü®N, T, F, R, Vin, ùë£out‚ü©where the set R of\\nrules can additionally contain guarded rules of the form ùëÅ‚Üíùõº1 |‚ä≤ùõº2 |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤ùõºùëòwhere each ùõºùëñis\\neither f(ùë£1, . . . , ùë£ùëõ) or ùë£0, where f ‚ààF , ùë£0 ‚ààT, and ùë£1, . . . , ùë£ùëõ‚ààN ‚à™T. A non-terminal ùëÅcan have\\nany number of guarded rules associated with it, each with possibly different values of ùëò‚â•1.\\nThe rules in a regular DSL can be viewed as a special case of guarded rules where ùëò= 1 (in the\\ndefinition of guarded production rules above.) When ùëò> 1, a guarded rule ùëÅ‚Üíùõº1 |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤ùõºùëò\\nassociated with the nonterminal ùëÅhas ùëòalternates on the right-hand side that are ordered. The ùëñ-th'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 16, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='definition of guarded production rules above.) When ùëò> 1, a guarded rule ùëÅ‚Üíùõº1 |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤ùõºùëò\\nassociated with the nonterminal ùëÅhas ùëòalternates on the right-hand side that are ordered. The ùëñ-th\\nalternate ùõºùëñyields a (regular) rule ùëÅ‚Üíùõºùëñ. We call ùëÅ‚Üíùõºùëñthe ùëñ-th constituent rule of the original\\nguarded rule. Define Rùëêas the collection of all constituent rules of rules in R; that is, Rùëê:= {ùëÅ‚Üí\\nùõºùëñ| ùëÅ‚Üí(. . . |‚ä≤ùõºùëñ|‚ä≤. . .) ‚ààR}. We call the (regular) DSL Dùëê:= ‚ü®N, T, F, Rùëê, Vin, ùë£out‚ü©obtained\\nfrom a gDSL D := ‚ü®N, T, F, R, Vin, ùë£out‚ü©a constituent DSL of the gDSL D.\\nGiven an instance of the PBE synthesis with precedence problem, we can annotate the given DSL\\nwith precedence to get a gDSL. We assume that the precedence relation ‚âªŒ£ satisfies Assumptions (A1)\\nand (A2). Furthermore, we also assume that (A3) the precedence is a series-parallel partial order\\n(SPPO) [B√©chet et al. 1997]. Under Assumption (A3), the precedence can be encoded in gDSL'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 16, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='and (A2). Furthermore, we also assume that (A3) the precedence is a series-parallel partial order\\n(SPPO) [B√©chet et al. 1997]. Under Assumption (A3), the precedence can be encoded in gDSL\\nby introducing new nonterminals where necessary. Specifically, if we have a maximal chain\\nf1 ‚âªŒ£ f2 ‚âªŒ£ . . . ‚âªŒ£ fùëòover alternate operators ùëÅ‚Üíf1(. . .)| ¬∑ ¬∑ ¬∑ |fùëò(. . .) in the DSL, then we\\nadd a guarded rule ùëÅ‚Üíf1(. . .) |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤fùëò(. . .). However, if certain alternate operators are left\\nincomparable by the DSL designers, then we introduce a new nonterminal. For example, if f1 ‚âªŒ£ f3\\nand f2 ‚âªŒ£ f3, but there is no preference between f1 and f2, then we introduce a new nonterminal\\nùëÅ‚Ä≤ and have ùëÅ‚ÜíùëÅ‚Ä≤ |‚ä≤f3(. . .) and ùëÅ‚Ä≤ ‚Üíf1(. . .)|f2(. . .) in the gDSL. Any SPPO ‚âªŒ£ can thus be\\nencoded in the gDSL.\\nExample 5.4. Consider the DSL for affine arithmetic from Example 3.1. Suppose the DSL designer\\nwants the precedence input1 ‚âªŒ£ input2. Then, we can replace the two rules for nonterminal'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 16, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Example 5.4. Consider the DSL for affine arithmetic from Example 3.1. Suppose the DSL designer\\nwants the precedence input1 ‚âªŒ£ input2. Then, we can replace the two rules for nonterminal\\naddend by a single guarded rule: addend ‚ÜíTimes(const, input1) |‚ä≤Times(const, input2) to\\nget a gDSL, Dùëî\\nùê¥. In the LPO induced by the extension ‚âªùëí\\nŒ£ of this preference, the program ùëÉ1 :=\\nPlus(Times(3, input1), 1) is preferred over ùëÉ2 := Plus(Times(3, input2), 7).\\n‚ñ°\\nLet ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©be a synthesis task, where R is the rules of a gDSL D. We say a program ùëÉis\\na solution for this task if\\n(a) ùëÉis a solution for the task ‚ü®ùë£out, Rùëê,ùëÜ‚Ü¶‚Üíùëú‚ü©(in the constituent DSL Dùëê), and\\n(b) for any other ùëÉ‚Ä≤ that is a solution in the constituent DSL, ùëÉ‚Ä≤ ‚äÅùëôùëùùëúùëÉ, where ‚âªùëôùëùùëúis the LPO\\ninduced by the extended precedence ‚âªùëí\\nŒ£ coming from the guarded rules.\\nCondition (a) says that ùëÉshould be a solution for the synthesis problem ignoring the precedence.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 16, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='induced by the extended precedence ‚âªùëí\\nŒ£ coming from the guarded rules.\\nCondition (a) says that ùëÉshould be a solution for the synthesis problem ignoring the precedence.\\nCondition (b) says that the ordering in the rules should be interpreted as an ordering on programs\\nusing the induced LPO, and we should ignore programs smaller in this ordering.\\nExample 5.5. Consider the gDSL Dùëî\\nùê¥and programs ùëÉ1, ùëÉ2 from Example 5.4. Consider the synthesis\\ntask ‚ü®ùë£out, R, ‚ü®input1 ‚Ü¶‚Üí2, input2 ‚Ü¶‚Üí0‚ü©‚Ü¶‚Üí7‚ü©from Example 3.2, but with R now coming from the\\ngDSL Dùëî\\nùê¥. Both programs, ùëÉ1 and ùëÉ2, map the input state to 7. However, ùëÉ2 is now not a solution in\\nDùëî\\nùê¥because there exists ùëÉ1 that is preferred. The program ùëÉ3 := 7 also maps the input state to 7.\\nThe (derivations of) ùëÉ1 and ùëÉ3 are incomparable; and in fact, both are solutions in Dùëî\\nùê¥.\\n‚ñ°\\nIf a solution for the unguarded DSL exists, then there will be a solution that is maximal and\\nhence a solution for the gDSL will exist.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 16, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='ùê¥.\\n‚ñ°\\nIf a solution for the unguarded DSL exists, then there will be a solution that is maximal and\\nhence a solution for the gDSL will exist.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 17, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:18\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nGuarded.If\\nPS1 |= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nùëÅ‚Üíùõº1 |‚ä≤ùõº2 ‚ààR\\nPS1 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nGuarded.Else\\nÃ∏|= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nùëÅ‚Üíùõº1 |‚ä≤ùõº2 ‚ààR\\nPS2 |= ‚ü®ùõº2, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nPS2 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nFig. 6. Extending top-down synthesis for guarded rules.\\nTheorem 5.6 (Precedence preserves solvability.). Let D be a gDSL based on precedence ‚âªŒ£\\nthat is a strict partial order. Let ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùë£‚ü©be a synthesis task. This task has a solution in D iff\\nthere is a solution in Dùëê.\\nIf we can compute all solutions for a synthesis task over a gDSL, we can order them by any ‚™∞base\\nordering to solve the PBE synthesis with precedence problem.\\n5.3\\nPBE Synthesis Rules for Guarded DSLs\\nFigure 6 contains two inference rules that describe how guarded rules are handled in top-down,\\nbottom-up, or middle-out synthesis. If ùëÅ‚Üíùõº1 |‚ä≤ùõº2 is a guarded rule in R and we can (recursively)\\nprove ùëÉùëÜ1 |= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, then Rule Guarded.If can be used to assert ùëÉùëÜ1 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. (This'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 17, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='bottom-up, or middle-out synthesis. If ùëÅ‚Üíùõº1 |‚ä≤ùõº2 is a guarded rule in R and we can (recursively)\\nprove ùëÉùëÜ1 |= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, then Rule Guarded.If can be used to assert ùëÉùëÜ1 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. (This\\nassertion will be nontrivial only if ùëÉùëÜ1 ‚â†‚àÖ.) In case there is no program that solves ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©,\\nand we can (recursively) prove ùëÉùëÜ2 |= ‚ü®ùõº2, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, then Rule Guarded.Else can be used to assert\\nùëÉùëÜ2 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©.\\nRecall that the notation ùëÉùëÜ|= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©simply asserts that every program in ùëÉùëÜsolves the\\ngiven synthesis task, and does not require ùëÉùëÜto contain all solutions. The Rule Guarded.Else has a\\ncondition that a certain synthesis task be unsolvable. If we have the ability to compute all solutions\\n(such as, using version-space algebras, or VSAs), then that can help in determining when a problem\\nis unsolvable, but other synthesis approaches that can establish infeasibility can also be used.\\nExample 5.7. Continuing from Example 5.5, consider the task of generating 7 from inputs 2'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 17, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='is unsolvable, but other synthesis approaches that can establish infeasibility can also be used.\\nExample 5.7. Continuing from Example 5.5, consider the task of generating 7 from inputs 2\\nand 0. Say we reduce the original task to the proof obligation ùëã|= ‚ü®addend, R,ùëÜ‚Ü¶‚Üíùë•‚ü©, where\\nwe pick say 6 for ùë•. Now, we have a guarded rule for addend and we can use Rule Guarded.If\\nto get the proof obligation ùëã|= ‚ü®Times(const, input1), R,ùëÜ‚Ü¶‚Üí6‚ü©. This subtask has a solution\\nùëã= {Times(3, input1)}, and hence we will not consider the option Times(const, input2).\\n‚ñ°\\nPerforming synthesis over the gDSL is equivalent to solving PBE synthesis with precedence.\\nTheorem 5.8. Let ‚âªŒ£ be a precedence on Œ£ := F ‚à™T that satisfies Assumptions (A1), (A2), and (A3).\\nLet D := ‚ü®N, T, F, R, Vin, ùë£out‚ü©be a gDSL that encodes ‚âªŒ£. Let Dùëê:= ‚ü®N, T, F, Rùëê, Vin, ùë£out‚ü©be\\nits (unguarded) constituent DSL. Let ‚™∞base be an ordering on programs and let ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©be a\\nsynthesis task. Then, the following are equivalent:'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 17, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='its (unguarded) constituent DSL. Let ‚™∞base be an ordering on programs and let ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©be a\\nsynthesis task. Then, the following are equivalent:\\n(1) {ùëÉ} |= ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and ùëÉis maximal w.r.t ‚™∞base among all such solutions.\\n(2) The program ùëÉis a solution in Dùëêfor the task ‚ü®ùë£out, Rùëê,ùëÜ‚Ü¶‚Üíùëú‚ü©that is maximal w.r.t a lexicographic\\ncombination of ‚âªùëôùëùùëú(induced by ‚âªŒ£) and ‚™∞base.\\nTheorem 5.8 shows that using a ranker ‚™∞base with a gDSL D has the same effect as using a\\ncomplex ranker (lexicographic combination of ‚âªùëôùëùùëúand ‚™∞base) with a regular DSL Dùëê. This shows\\nthat our gDSL-based approach solves the PBE synthesis with precedence problem (under some\\nassumptions). Theorem 5.8 also explains why the ranker ‚™∞base used with a gDSL D can be very\\nsimple compared to what is needed with a regular DSL Dùëê. Designing a good complex ranking\\nfunction has traditionally been very challenging in PBE, taking many developer-months to converge'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 17, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='simple compared to what is needed with a regular DSL Dùëê. Designing a good complex ranking\\nfunction has traditionally been very challenging in PBE, taking many developer-months to converge\\non a good ranking function [Kalyan et al. 2018; Natarajan et al. 2019; Singh and Gulwani 2015]. In\\ncontrast, FlashFill++ uses the power of gDSLs (Theorem 5.8) to reduce the requirements on its base\\nranker, which was developed significantly faster.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 18, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:19\\nlanguage FlashFillPP;\\n@start string output := single |> If(cond, single, output)\\n// conditions\\nbool cond := pred |> And(pred, cond);\\nbool pred := StartsWith(x, matchPattern) |> EndsWith(x, matchPattern)\\n|> Contains(x, matchPattern) |> ...;\\n// single branch\\nstring single := concat | LowerCase(concat) | UpperCase(concat) | ProperCase(concat);\\n// optional concatenation of substrings\\nstring concat := segment |> Concat(segment, concat)\\n// substring logic\\nstring segment := substr | formatNumber | formatDate | constStr;\\n// format substring as a number\\nstring formatNumber := FormatNumber(roundNumber, numFmtDesc);\\ndecimal roundNumber := parseNumber |> RoundNumber(parseNumber, roundNumDesc);\\ndecimal parseNumber := AsNumber(row, columnName) |> ParseNumber(substr, locale);\\n// format substring as a date\\nstring formatDate := FormatDateTime(asDate, dateFmtDesc);'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 18, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='decimal parseNumber := AsNumber(row, columnName) |> ParseNumber(substr, locale);\\n// format substring as a date\\nstring formatDate := FormatDateTime(asDate, dateFmtDesc);\\nDateTime asDate := AsDateTime(row, columnName) |> ParseDateTime(substr, parseDateFmtDesc);\\n// find a substring within the input x\\nstring substr := Split(x, splitDelimiter, splitInstance)\\n|> Slice(x, pos, pos)\\n|> MatchFull(x, matchPattern, matchInstance);\\n// find a position within the input x\\nint pos := End(x) |> Abs(x, absPos)\\n|> Find(x, findDelimiter, findInstance, findOffset)\\n|> Match(x, matchPattern, matchInstance)\\n|> MatchEnd(x, matchPattern, matchInstance);\\n// literal terminals\\nFmtNumDescriptor numFmtDesc; RndNumDescriptor roundNumDesc;\\nFmtDateTimeDescriptor dateFmtDes, parseDateFmtDesc;\\nstring constStr, splitDelimiter, findDelimiter;\\nint splitInstance, findInstance, matchInstance, findOffset;\\nRegex matchPattern; int absPos;'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 18, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FmtDateTimeDescriptor dateFmtDes, parseDateFmtDesc;\\nstring constStr, splitDelimiter, findDelimiter;\\nint splitInstance, findInstance, matchInstance, findOffset;\\nRegex matchPattern; int absPos;\\nFig. 7. A fragment of the gDSL for FlashFill++. | choices are unguarded, |> choices are guarded.\\n5.4\\nGuarded DSL for FlashFill++\\nWe now describe the FlashFill++ gDSL and compare it to FlashFill. Figure 7 shows a major part of the\\nDSL. FlashFill++ shares the top level rules that perform conditional statements, case conversion, and\\nstring concatenation with FlashFill. Conditionals enable if-then-else logic. The condition is one or\\nmore conjunctive predicates based on properties of the input string. Case conversion transforms a\\nstring into lower-, upper-, or proper-case. Concatenation concatenates two strings.\\nAlthough FlashFill can perform some datetime and number operations using text manipulation\\n(such as \"01/01/2020\" ‚Üí\"2020\" or \"10.01\" ‚Üí\"10\"), it is unable to express other sophisticated'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 18, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Although FlashFill can perform some datetime and number operations using text manipulation\\n(such as \"01/01/2020\" ‚Üí\"2020\" or \"10.01\" ‚Üí\"10\"), it is unable to express other sophisticated\\ndatetime and number operations as it does not incorporate operations over those datatypes, but\\nrather treats them as standard strings. For instance, FlashFill cannot get the day of week from a date\\n(such as \"01/01/2020\" ‚Üí\"Wednesday\"), or round up a number (e.g., \"10.49\" ‚Üí\"10.5\"). This\\nmotivates us to add new rules to support richer datetime (rules parseDate and formatDate) and\\nnumber (rules parseNumber and formatNumber) transformations.\\nThe next major differences are in the substr and pos rules. FlashFill has a single Slice operator\\nwhich selects a substring defined by its start and end positions. These positions can be defined\\neither as absolute positions or with the complex RegPos operator which finds the ùëòth place in the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 18, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='which selects a substring defined by its start and end positions. These positions can be defined\\neither as absolute positions or with the complex RegPos operator which finds the ùëòth place in the\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 19, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:20\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nstring where the left- and right substrings match the two given regular expressions. While this is\\nexpressive enough to cover any desired substring selection and all of the operators in FlashFill++\\ncan technically be expressed in terms of it, this introduces a challenge for an industrial synthesizer\\nthat targets different languages for code generation: not all platforms of interest support regular\\nexpressions natively (e.g. Microsoft Excel‚Äôs formula language does not support regular expressions).\\nIn contrast, when designing FlashFill++ we chose a wider collection of more natural operators that\\nare closer to what developers use in practice when working with target languages ‚Äì this removes\\nthe mismatch between synthesis DSL and code generation target language.\\nIn particular, instead of only allowing substrings to be defined as a Slice with their start and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 19, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='the mismatch between synthesis DSL and code generation target language.\\nIn particular, instead of only allowing substrings to be defined as a Slice with their start and\\nend positions, FlashFill++ adds a Split operator to select the ùëòth element in a sequence generated\\nby splitting the input string by some delimiters. We also add a MatchFull operator to find the ùëòth\\nmatch of a regular expression. Additionally, in FlashFill++ the pos rule replaces the operator RegPos\\n(which relies on a pair of regular expressions to identify a position) with a Find of a constant string\\nin the input and a Match/MatchEnd of a regular expression.\\nExample Guarded Rules in the FlashFill++ DSL. We go over a few cases of guarded rules in FlashFill++\\nto show they capture natural intuitions and rules of thumb in the string transformation domain.\\n‚Ä¢ Single segments over concats. The guarded rule segment |‚ä≤Concat(segment, concat) ensures'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 19, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='to show they capture natural intuitions and rules of thumb in the string transformation domain.\\n‚Ä¢ Single segments over concats. The guarded rule segment |‚ä≤Concat(segment, concat) ensures\\nthat we try to synthesize programs that generate the whole output at once, before generating\\nprograms that produce a prefix and a suffix of the output and then combine them. Whenever\\nsuch a program exists, this guarded rule potentially saves the exploration of a huge portion\\nof the program space. This single guarded rule plays a crucial role in keeping FlashFill++\\nperformant since witness function of the concat operator produces 2(ùëõ‚àí1) subtasks, one\\neach for the prefix and suffix at each point where the output string can be split.\\n‚Ä¢ Splits over slices. As illustrated Example 5.1, FlashFill++ strictly prefers programs that use\\nthe Split operator over programs that use the Slice operator. Program in data wrangling\\nscenarios very commonly begin by extracting the appropriate part of the input from a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 19, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='the Split operator over programs that use the Slice operator. Program in data wrangling\\nscenarios very commonly begin by extracting the appropriate part of the input from a\\ndelimited file record (CSVs, TSVs, etc). In all such cases, a split program more closely follows\\nthe human intuition of ‚Äúextract the ùëõùë°‚Ñécolumn delimited by‚Äù as compared to a slice program.\\nNote that the split operator is a ‚Äúhigher level‚Äù construct preferred over the ‚Äúlow-level‚Äù slice.\\n‚Ä¢ Input numbers over rounded numbers. The RoundNumber operator in Figure 7 is guarded by\\nparseNumber, meaning that we can only generate a program that rounds a number if no\\nnumber in the input can be used directly to produce the same output.\\n6\\nEXPERIMENTAL EVALUATION\\nWe now present our experimental results, including an ablation study and survey-based user study.\\n6.1\\nMethodology\\nWe used 3 publicly available benchmark sets ‚Äì Duet string benchmarks [Lee 2021], Playgol [Cropper'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 19, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='6.1\\nMethodology\\nWe used 3 publicly available benchmark sets ‚Äì Duet string benchmarks [Lee 2021], Playgol [Cropper\\n2019], and Prose [PROSE 2022] ‚Äì covering a range of string transformations, including datetime\\nand number formatting.\\nWe compare FlashFill++ with three systems: two publicly available state-of-the-art synthesis\\nsystems that are deployed in productions, namely FlashFill [Gulwani 2011] and SmartFill [Chen\\net al. 2021a; Google 2021], and one, Duet, from a recent publication [Lee 2021]. To experiment with\\nFlashFill, we implemented it on top of the FlashMeta framework [Polozov and Gulwani 2015]. We\\ncarry out our FlashFill, FlashFill++, and Duet experiments on a machine with 2 CPUs & 8GB RAM. To\\nexperiment with SmartFill, we employ Google Sheets in Chrome and use it to solve the subset of\\ntasks that are suitable for its spreadsheet environment.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 20, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:21\\nTable 1. Comparing different tools (rows) on the 3 public benchmarks (columns) based on (1) number of\\nbenchmarks correctly solved (Columns 2‚Äì5) and (2) average number of examples required on the successful\\nbenchmarks (Columns 6‚Äì8). The total number of benchmarks attempted are shown in brackets.\\nNumber benchmarks solved\\nAverage #examples required\\nDuet (205)\\nPlaygol (327)\\nProse (354)\\nTotal (886)\\nDuet\\nPlaygol\\nProse\\nFlashFill\\n139\\n264\\n172\\n575\\n1.41\\n1.26\\n1.54\\nFlashFill++\\n159\\n307\\n353\\n819\\n1.69\\n1.46\\n1.52\\nDuet\\n102\\n211\\n166\\n479\\n1.97\\n2.11\\n2.01\\nSmartFill\\n37 (158)\\n34 (327)\\n18 (296)\\n89 (781)\\n2.75\\n2.85\\n2.83\\nSince SmartFill is not exposed in the Google Sheets API, we rely on browser-automation using\\nSelenium [Selenium 2022] for SmartFill evaluation. Moreover, we remove problematic benchmark\\ntasks and use 158, 327, and 296 tasks from the Duet, Playgol, and Prose benchmarks, respectively,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 20, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Selenium [Selenium 2022] for SmartFill evaluation. Moreover, we remove problematic benchmark\\ntasks and use 158, 327, and 296 tasks from the Duet, Playgol, and Prose benchmarks, respectively,\\nfor SmartFill evaluation. The tasks removed were unsuitable for browser automation (e.g. too many\\nrows or new line characters).\\nThe Duet tool [Lee 2021] accepts a DSL as part of its input. Different Duet benchmarks used\\nslightly different DSLs. For fair comparison, we set a single fixed DSL. We obtained the fixed DSL\\nby taking all commonly-used rules in the string transformation benchmarks of Duet. Second, Duet\\nhas some hyperparameters, for which we picked the best setting after some experimentation.\\n6.2\\nExpressiveness and Performance\\nA key feature of FlashFill++ is improved expressiveness. Table 1 (Columns 2‚Äì5) shows the number\\nof benchmark tasks that the various tools (rows) can correctly solve. FlashFill++ produces a correct'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 20, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='A key feature of FlashFill++ is improved expressiveness. Table 1 (Columns 2‚Äì5) shows the number\\nof benchmark tasks that the various tools (rows) can correctly solve. FlashFill++ produces a correct\\nprogram for most number of benchmarks. FlashFill is limited due to a lack of datetime and number\\nformatting. The DSL used in the Duet tool has no datetime support, and only limited support for\\nnumber and string formatting. The SmartFill tool is a neural-based general purpose tool and has\\nthe weakest numbers here. Since our SmartFill experiments rely on browser-based interaction, it is\\npossible that the underlying synthesizer can solve more benchmarks but these are not exposed to\\nthe UI. However, our setup reflects the experience that a user would face.\\nOur DSL is expressive, covering a large class of string, datetime and number transformations;\\nthus showing the added value from using cuts and guarded rules.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 20, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Our DSL is expressive, covering a large class of string, datetime and number transformations;\\nthus showing the added value from using cuts and guarded rules.\\nWhile increasing expressiveness enables users to accomplish more tasks, there is a risk of reducing\\nperformance on existing tasks. To this end, we consider the minimum number of examples required\\nfor a synthesizer to learn a correct program. To find that number, we use a counter-example guided\\n(CEGIS) loop which provides the next failing I/O example in every iteration to the synthesizer.\\nWe use a time out of 20 seconds. Table 1 Columns 6‚Äì8 present the average number of examples\\nthe various tools used across the benchmarks where they were successful. Here FlashFill has the\\nbest numbers, which indicates that when it works (for string benchmarks), it learns with very\\nfew examples. Our tool FlashFill++ takes only slightly more examples on average, partly because'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 20, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='best numbers, which indicates that when it works (for string benchmarks), it learns with very\\nfew examples. Our tool FlashFill++ takes only slightly more examples on average, partly because\\ndatetime and number formatting typically requires more examples for intent disambiguation. For\\nexample, the string ‚Äò2/3/2020‚Äô can either be the 2ùëõùëëof March or the 3ùëõùëëof February. The Duet and\\nSmartFill tools take more examples in general. We emphasize that the performance of FlashFill++ is\\ngood here because it solves more problems (presumably much harder instances) and yet it doesn‚Äôt\\nuse too many more examples (the harder instances did not make the averages much worse).\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 21, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:22\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\n0\\n2500\\n5000\\n7500\\n10000 12500\\nBest Duet or FF Synthesis Time (ms)\\n‚àí2\\n‚àí1\\n0\\n1\\n2\\nlog10(Best / FlashFill++)\\nOutcome\\nFlashFill++ better\\nFlashFill++ worse\\n(a) log10( min(FlashFill,Duet)\\nFlashFill++\\n) vs min(FlashFill, Duet).\\nBenchmark classes\\nDuet\\nPlaygol\\nProse\\nOverall\\nFlashFill\\n689.7\\n1757.0\\n734.1\\n1116.4\\nFlashFill++\\n203.0\\n217.1\\n246.4\\n228.5\\nDuet\\n264.0\\n558.4\\n1351.7\\n766.74\\n(b) Average time (in ms) taken by the tools\\n(rows) over successful benchmarks from the three\\nsources (columns), and the average for each tool\\nover the entire suite (last column).\\nFig. 8. FlashFill++ is generally faster ‚Äì up to two orders of magnitude ‚Äì than the best of FlashFill and Duet,\\nand the slowdowns are mostly on fast benchmarks.\\nDespite solving more tasks, FlashFill++ continues to require a reasonable number of examples\\ncompared to baselines, showing that it generalizes to unseen examples and does not overfit.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 21, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Despite solving more tasks, FlashFill++ continues to require a reasonable number of examples\\ncompared to baselines, showing that it generalizes to unseen examples and does not overfit.\\nWe next compare synthesis times (averaged over 5 runs). In particular, we compute the synthesis\\ntime when the tools are given the minimum number of examples they required to produce the\\ncorrect program. Figure 8 shows the results. We restrict this experiment to FlashFill, Duet, and\\nFlashFill++, as synthesis time for SmartFill is unobservable through the browser.\\nFigure 8a compares FlashFill++ with the faster of FlashFill and Duet. We focus on benchmarks that\\nare solved by FlashFill++, and by either FlashFill or Duet. The y-axis displays the log base 10 of the\\nratio of the best of FlashFill and Duet synthesis time to the FlashFill++ synthesis time. A higher value\\nrepresents a larger reduction in synthesis time. On the x-axis we display the best of FlashFill and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 21, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='represents a larger reduction in synthesis time. On the x-axis we display the best of FlashFill and\\nDuet synthesis time (in milliseconds) for that benchmark task. FlashFill++ reduces synthesis time\\nin 63% of the benchmarks, and the remaining 37% happen to be benchmarks where synthesis is\\nfast (< 500ùëöùë†in most cases) and slowdown is likely not observable in a user-facing application. In\\n37.3% cases, FlashFill++ is at least one order of magnitude faster and in 18% cases it is at least two\\norders of magnitude faster. Table 8b shows the average synthesis time for various tools across the\\nbenchmark classes. We averaged over the benchmarks that the tool solved. We see that FlashFill++\\nhas better averages despite solving more (presumably harder) benchmarks.\\nFlashFill++ is faster on average despite solving more (presumably harder) benchmarks and better\\nthan the best of the baselines on most hard instances.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 21, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++ is faster on average despite solving more (presumably harder) benchmarks and better\\nthan the best of the baselines on most hard instances.\\nFinally, we evaluate the gain from using gDSLs. We create FlashFill++‚àíby replacing gDSL used\\nin FlashFill++ by a regular DSL ( |‚ä≤operator is treated as the usual |). We compare FlashFill++ and\\nFlashFill++‚àíon synthesis time and minimum examples required metrics. Figure 9b summarize the\\nresults. We note that gDSLs reduce synthesis time in 91% of the benchmark tasks, give more than\\n3x speedup in 20% of cases, and generate better performance across the benchmarks and metrics.\\nPrecedences in gDSLs consistently help FlashFill++ in improving both search (synthesis time)\\nand ranking (minimum number of examples).\\n6.3\\nCode Readability\\nTraditionally DSL design has focused on efficacy of the learning and ranking process, and not on\\nreadable code generation. We evaluated the extent to which FlashFill++ enables such readable code.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 21, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Traditionally DSL design has focused on efficacy of the learning and ranking process, and not on\\nreadable code generation. We evaluated the extent to which FlashFill++ enables such readable code.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 22, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:23\\nDuet\\nPlaygol\\nProse\\nOverall\\nAverage Synthesis Time\\nFlashFill++\\n203.0\\n217.1\\n246.4\\n228.5\\nFlashFill++ ‚àí\\n255.0\\n350.6\\n410.3\\n361.1\\nAverage #examples required\\nFlashFill++\\n1.69\\n1.46\\n1.52\\n1.53\\nFlashFill++ ‚àí\\n1.76\\n1.54\\n1.56\\n1.59\\n(a) Synthesis time and no. of examples required to learn.\\n0\\n2000\\n4000\\n6000\\n8000\\nSynthesis Time FF++ (Ablated) (ms)\\n‚àí0.50\\n‚àí0.25\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\nlog10(FF++ (Ablated) / FlashFill++)\\nOutcome\\nFlashFill++ worse\\nFlashFill++ better\\n(b) Synthesis Time Ratios.\\nFig. 9. FlashFill++ improves over an ablation that removes the use of gDSLs, denoted FlashFill++‚àí.\\nFirst, we compared the code complexity for programs synthesized by FlashFill++ and FlashFill for\\ntwo target languages: Python & PowerFx, the low/no-code language of the Power platform [PowerFx\\n2021]. On average, FlashFill++‚Äôs Python is 76% shorter and uses 83% fewer functions, whereas'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 22, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='two target languages: Python & PowerFx, the low/no-code language of the Power platform [PowerFx\\n2021]. On average, FlashFill++‚Äôs Python is 76% shorter and uses 83% fewer functions, whereas\\nFlashFill++‚Äôs PowerFx is 57% shorter and uses 55% fewer functions. We next compared the Google\\nSheets formula language code generated by SmartFill with Excel code generated by FlashFill++. We\\nfound that FlashFill++ does better in ‚âà60% of the formulas generated and is at parity for ‚âà20% of\\nthe formulas. We did not compare with Duet since it generates programs only in its DSL and not in\\nany target language.\\nNext, we carried out a survey-based user study where we asked users to read and compare\\ndifferent Python functions synthesized by alternative approaches. In our study, we further augment\\nFlashFill++ with a procedure to rename variables. This part of the system is optional, and is only\\nadded to further underscore the benefits of readable code generation. To perform variable renaming'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 22, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++ with a procedure to rename variables. This part of the system is optional, and is only\\nadded to further underscore the benefits of readable code generation. To perform variable renaming\\nwe use the few-shot learning capability of Codex [Chen et al. 2021b], a pretrained large language\\nmodel, and iteratively provide the following prompt [Gao et al. 2021]: (1) two samples of the\\nrenaming task, where each sample contains I/O examples, FlashFill++ program, and the renamed\\nprogram, (2) the current renaming task, which contains the examples and the FlashFill++ program\\nto be renamed, and (3) partially renamed program up to the next non-renamed variable.\\nWe sampled 10 tasks from our benchmarks, with probability proportional to the number of\\nidentifier renaming calls made to Codex. For each task, we displayed the Python code generated by\\nFlashFill, FlashFill++, and FlashFill++ with Codex (anonymized as A, B, and C). For the first 5 tasks,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 22, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill, FlashFill++, and FlashFill++ with Codex (anonymized as A, B, and C). For the first 5 tasks,\\nthe participants were asked (on a 7-point Likert scale) the extent to which they agreed with the\\nstatements ‚ÄúFlashFill++ is more readable than FlashFill‚Äù and ‚ÄúFlashFill++ with Codex is more readable\\nthan FlashFill++‚Äù. For the last 5 tasks, the participants answered (on a 7-point Likert scale) the extent\\nto which they agreed with the statement ‚ÄúX is similar to the code I write‚Äù, where X was replaced\\nwith the corresponding (anonymized) system name.\\nFigure 10a shows that participants found code generated by FlashFill++ (without identifier renam-\\ning) was more readable than code generated by FlashFill for the same task. Adding Codex-based\\nrenaming further improved readability with most participants at least somewhat agreeing.\\nFigure 10b shows that participants strongly disagreed that FlashFill code is similar to the code'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 22, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='renaming further improved readability with most participants at least somewhat agreeing.\\nFigure 10b shows that participants strongly disagreed that FlashFill code is similar to the code\\nthey write. In contrast, most participants at least somewhat agreed that FlashFill++ code is similar\\nto the code they write. Adding identifier renaming resulted in improvements, across all five tasks.\\nWe also provided an open-ended text box for additional feedback with each task. Here are some\\nillustrative excerpts, where we have replaced the anonymized system names (A,B,C) with meaningful\\ncounterparts: FlashFill: ‚Äúis a mess‚Äù, FlashFill++: ‚Äúvery readable‚Äù, FlashFill++ +Codex: \"parameter name is\\nmore self describing‚Äù; ‚ÄúFlashFill is just confusing while FlashFill++ and/or FlashFill++ with Codex are\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 23, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:24\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nD3 D2 D1 N S1 S2 S3\\nResponse\\n0\\n50\\n100\\n150\\n200\\n# of Participants\\ncomparison\\nFF++ > FF\\nFF++ w/Codex > FF++\\n(a) Statement: X is more readable than Y (denoted\\nas ùëã> ùëå). Most participants found FlashFill++\\nbetter than FlashFill. Adding Codex further im-\\nproved it.\\nD3 D2 D1 N S1 S2 S3\\nResponse\\n0\\n50\\n100\\n150\\n# of Participants\\nApproach\\nFF\\nFF++\\nFF++ w/Codex\\n(b) Statement: X is similar to the code I write. Par-\\nticipants did not find FlashFill similar. FlashFill++\\nwas closer, but FlashFill++ +Codex most similar.\\nFig. 10. Survey: D3-S3 Strongly disagree/agree. FF=FlashFill, FF++=FlashFill++, FF++ w/Codex=FlashFill++\\nwith Codex.\\nquite simple and direct‚Äù; and ‚ÄúFlashFill is very badly written, and FlashFill++ with Codex‚Äôs parameter\\nname tells a much better story‚Äù.\\n7\\nDISCUSSION\\n7.1\\nRelated Work\\nCuts are closely related to the widely-studied concept of accelerations in program analysis. Accel-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 23, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='name tells a much better story‚Äù.\\n7\\nDISCUSSION\\n7.1\\nRelated Work\\nCuts are closely related to the widely-studied concept of accelerations in program analysis. Accel-\\nerations are used to capture the effect (transitive closure) of multiple state transitions by a single\\n‚Äúmeta transition‚Äù [Finkel 1987; Karp and Miller 1969]. It has often been used to handle numerical\\nupdates in programs, especially when more classical abstract interpretation techniques either do\\nnot converge or become very imprecise [Bardin et al. 2008; Boigelot 2003; Jeannet et al. 2014].\\nOur use of cuts inherits its motivation and purpose from these works, but applies them to PBE.\\nWhereas in program analysis, accelerations had success mostly on numerical domains, in PBE we\\nfind cuts helpful more generally. Currently, we assume cuts are provided by the DSL designer, but\\nautomatically generating them remains an interesting topic for future research.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 23, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='find cuts helpful more generally. Currently, we assume cuts are provided by the DSL designer, but\\nautomatically generating them remains an interesting topic for future research.\\nIn PBE, cuts help speed-up search (by guiding top-down propagation across non-EI operators\\nbased on abstracting behavior of inner sub-DSLs). Other ways to speed-up search include using\\ntypes and other forms of abstractions [Guo et al. 2020; Osera and Zdancewic 2015; Wang et al.\\n2018], or combining search strategies [Lee 2021]. The difference between cuts and abstraction-based\\nmethods in synthesis is the same as the difference between accelerations and abstraction in program\\nanalysis. We need cuts for only some operators, whereas abstract transformers are required for\\nall operators. Moreover, the methods are not incompatible ‚Äì a promising direction would be to\\ncombine them.\\nMiddle-out synthesis, enabled by cuts, is a new way to combine bottom-up [Alur et al. 2013,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 23, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='all operators. Moreover, the methods are not incompatible ‚Äì a promising direction would be to\\ncombine them.\\nMiddle-out synthesis, enabled by cuts, is a new way to combine bottom-up [Alur et al. 2013,\\n2017] and top-down [Gulwani 2011; Polozov and Gulwani 2015] synthesis. It is very different from\\nthe meet-in-the-middle way of combining them where search starts simultaneously from the bottom\\n(enumerating subprograms that generate new values) and from the top (back propagating the\\noutput) until we find values that connect the two [Gulwani et al. 2011; Lee 2021]. Helped by the\\njump provided by cuts, middle-out synthesis starts at the middle creating two subproblems that can\\nbe solved using either approach. Meet-in-the-middle approach in [Lee 2021] guides the top-down\\nsearch based on the values propagated by bottom-up, similar to middle-out synthesis; however, our\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 24, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:25\\ncuts are more general and can handle more scenarios because they overcome issues (not effectively\\nenumerable operators and large number of constants) that may make partial bottom-up fail.\\nMiddle-out synthesis can be viewed as a divide-and-conquer strategy for synthesis. The coopera-\\ntive synthesis framework in [Huang et al. 2020] proposes 3 such strategies that are used when the\\noriginal synthesis problem remains unsolved. However, [Huang et al. 2020] works on complete\\nlogical specifications, and not on input-output examples.\\nAt a very abstract level, top-down synthesis and middle-out synthesis can both be viewed as\\nan approach that synthesizes a sketch and then fills the sketch in a PBE setting [Feng et al. 2017;\\nPolikarpova et al. 2016; Wang et al. 2017, 2020]. In this context, Scythe [Wang et al. 2017] uses\\noverapproximation of the set of values that are computed by partial programs to synthesize a sketch.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 24, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Polikarpova et al. 2016; Wang et al. 2017, 2020]. In this context, Scythe [Wang et al. 2017] uses\\noverapproximation of the set of values that are computed by partial programs to synthesize a sketch.\\nUnlike our work, [Wang et al. 2017] is exclusively focused on bottom-up synthesis. Since [Wang\\net al. 2017] is bottom-up, its approximations get coarser as the program gets deeper. Scythe, tends\\nto do well for shallow programs. FlashFill++‚Äôs use of cuts is more ‚Äúon-demand‚Äù and thus not affected\\nby depth of programs. In fact, FlashFill++ can synthesize deep programs, with the largest solution in\\nour benchmark suite having a depth of 24, with over 10% of our benchmarks requiring programs\\nwith a depth of at least 10. Furthermore, [Wang et al. 2017] is exclusively focused on SQL ‚Äì its\\nmain contribution is an approach to overapproximate SQL queries that abstracts carefully selected\\nnonterminals. In contrast, our formalization is not fixed to any particular DSL, but relies on the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 24, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='main contribution is an approach to overapproximate SQL queries that abstracts carefully selected\\nnonterminals. In contrast, our formalization is not fixed to any particular DSL, but relies on the\\nDSL designer to provide the cuts.\\nMorpheus [Feng et al. 2017] and Synquid [Polikarpova et al. 2016] overapproximate each com-\\nponent to prune partial programs to synthesize sketches that are subsequently filled. Morpheus\\nis specialized to tables and uses the distinction between value and table transformations. In con-\\ntrast, our framework is more general as it allows the use of approximations (cuts) for only certain\\nfunctions (wherever they are provided); however, we cannot (and do not) prune partial programs.\\nWe always work on concrete values ‚Äì there is no abstract domain involved. We do not use SMT\\nsolvers, whereas SMT solvers are a key component of [Feng et al. 2017; Polikarpova et al. 2016].\\nPrecedence and gDSLs. Precedences or priorities have been used in many grammar formalisms,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 24, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='solvers, whereas SMT solvers are a key component of [Feng et al. 2017; Polikarpova et al. 2016].\\nPrecedence and gDSLs. Precedences or priorities have been used in many grammar formalisms,\\nbut mainly for achieving disambiguation while parsing in ambiguous grammars [Aasa 1995; Aho\\net al. 1973; Earley 1974; Heering et al. 1989]. Disambiguation here refers to preferring the parse\\nùëé+(ùëè‚àóùëê) over (ùëé+ùëè)‚àóùëêfor the same string ùëé+ùëè‚àóùëê. In contrast, we use gDSLs to compare derivations\\nof different strings (programs). Furthermore, in the work on filters and SDF [Heering et al. 1989],\\nthe semantics of the precedence relation ‚âªon rules is different: there ùëÜ1 ‚Üíùë§1 ‚âªùëÜ2 ‚Üíùë§2 means\\nthat one can not use ùëÜ2 ‚Üíùë§2 as a child of ùëÜ1 ‚Üíùë§1 in a parse tree [van den Brand et al. 2002]. In\\nour case, we disallow precedence on rules with different left-hand nonterminals. Nevertheless, our\\nprecedence can be viewed as a specialized filter in the terminology of [van den Brand et al. 2002].'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 24, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='our case, we disallow precedence on rules with different left-hand nonterminals. Nevertheless, our\\nprecedence can be viewed as a specialized filter in the terminology of [van den Brand et al. 2002].\\nFarzan and Nicolet [Farzan and Nicolet 2021] use a sub-grammar to optimize search. This can be\\nmodeled using our precedence operator. They use constraint-based synthesis (using SMT solvers)\\nwhere the interest is in any one correct program. Ranking is not of interest there, whereas we use\\nprecedence in the context of top-down synthesis where we synthesize program sets and rank them.\\nThe interaction of precedence in the grammar and the program ranking is one of our contributions.\\nCasper [Ahmad and Cheung 2018] uses a hierarchy of grammars growing in size for synthesis,\\nmaking search efficient. This hierarchy is dynamically generated - guided by the input-output\\nexample. Our precedence-based approach provides a different mechanism to constrain search. The'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 24, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='making search efficient. This hierarchy is dynamically generated - guided by the input-output\\nexample. Our precedence-based approach provides a different mechanism to constrain search. The\\nvalue of our approach is that it is easily integrated with the underlying synthesis framework, giving\\nsynthesis-engine-builders more flexibility in controlling search and ranking. Precedence is also\\nintuitive for DSL designers because they must think only locally to decide if the operators need a\\nprecedence relation. Technically speaking, our notion of precedence implicitly represents a lattice\\nof grammars rather than a strict linear hierarchy.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 25, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:26\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nEarlier work on ‚ÄúParsing expression grammars‚Äù [Ford 2004] introduces grammars that are like\\nCFGs, but contain features such as prioritized choice (precedence), greedy repetitions, etc., but\\nit does so for parsing, whereas our focus is on top-down, bottom-up, and combination synthesis\\ntechniques. Our novelty is in supporting precedence in our synthesis framework, and formally\\nworking out how it impacts ranking and search.\\nUsing precedence is one way to handle potentially redundant operators in the DSL and prune\\nsearch space. The other way is to explicitly write the equivalence relation on programs and\\nonly consider programs that are canonical representatives of each equivalence class [Osera and\\nZdancewic 2015; Smith and Albarghouthi 2019; Udupa et al. 2013]. The gDSL approach is low\\noverhead, but may consider equivalent programs during search. However, this is by design as our'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 25, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Zdancewic 2015; Smith and Albarghouthi 2019; Udupa et al. 2013]. The gDSL approach is low\\noverhead, but may consider equivalent programs during search. However, this is by design as our\\ngoal is to generate whatever program leads to most naturally readable code.\\nPrecedence of grammar rules can be viewed as a specialized case of probabilistic context-free\\ngrammars (pCFGs) that have been used to bias the enumeration of programs through their gram-\\nmar [Lee et al. 2018; Liang et al. 2010; Menon et al. 2013]. While specialized, precedence is actually\\npreferable in many scenarios where we want to synthesize not just any program that works on the\\ninput-output examples, but one that has other desirable properties, such as, the program generalizes\\nto unseen inputs and has a readable translation in target language. As such, precedence in gDSLs\\ngive designers of synthesizers a clean way to state their ranking preference. Weights in a pCFG'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 25, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='to unseen inputs and has a readable translation in target language. As such, precedence in gDSLs\\ngive designers of synthesizers a clean way to state their ranking preference. Weights in a pCFG\\nhave to be learned from data or manually set ‚Äì both are daunting tasks compared to writing a gDSL.\\nThe contrast between pCFGs and gDSLs is akin to that between neural and symbolic approaches.\\nFlashFill [Gulwani 2011] demonstrated the effectiveness of inductive synthesis at tackling\\ncomplex string transformation. FlashMeta [Polozov and Gulwani 2015] recognized that many\\npopular inductive synthesizers [Gulwani 2011; Le and Gulwani 2014] could be decomposed into\\ndomain-specific features, such as the DSL operators and their semantics, and general (shareable)\\ndeductive steps. We used the FlashMeta framework to implement FlashFill, based on the original\\npaper [Gulwani 2011], for our experiments. We also built FlashFill++ the same way, which extends'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 25, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='deductive steps. We used the FlashMeta framework to implement FlashFill, based on the original\\npaper [Gulwani 2011], for our experiments. We also built FlashFill++ the same way, which extends\\nthe capabilities of FlashFill to include new operations like date and number formatting, and also\\nfocuses on generating readable code.\\nIn recent work [Verbruggen et al. 2021], FlashFill has been combined with a pre-trained language\\nmodel (LM), GPT3, to facilitate semantic transformations, such as converting the city \"San Francisco\"\\nto the state \"CA\". Complementing FlashFill‚Äôs syntactic effectiveness with GPT3‚Äôs ability to do\\nsemantic transformation is interesting, but orthogonal to the problem here. However, we do exploit\\na LM to (optionally) generate meaningful variable names for our user study on code readability.\\nTrust and readability. Wrex [Drosos et al. 2020] argues that readable code is indispensable'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 25, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='a LM to (optionally) generate meaningful variable names for our user study on code readability.\\nTrust and readability. Wrex [Drosos et al. 2020] argues that readable code is indispensable\\nfor users of synthesis technology. Wrex employed hand-written rewrite rules to produce readable\\ncode for their user study. However, this is not an approach that easily extends to all scenarios and\\nlarger languages. FlashFill++ is inspired by Wrex [Drosos et al. 2020] to address the readable code\\ngeneration challenge in a more general and scalable way: redesigning the DSL used with a focus\\non enabling readable code generation, rather than post-processing.\\nZhang et al [Zhang et al. 2021, 2020] introduced a system for interpretable synthesis, where the\\nuser interacts with the synthesizer. This approach is complementary to FlashFill++.\\n7.2\\nLimitations\\nThe concepts of cuts and precedence have been developed exclusively for synthesis approaches'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 25, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='user interacts with the synthesizer. This approach is complementary to FlashFill++.\\n7.2\\nLimitations\\nThe concepts of cuts and precedence have been developed exclusively for synthesis approaches\\nbased on concrete values, so-called version space algebra (VSA) based methods, in this paper. The\\nterm top-down, respectively bottom-up, has been used for techniques that are based on propagation\\nof concrete output (respectively, input) values through partial sketches (respectively, programs)\\ntypically represented using VSAs. In systems that represent approximations of the sets of values\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 26, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:27\\n(at each component) using abstractions or refinement types (e.g., Synquid, Morpheus, etc), cuts\\ncan potentially address the issue of abstractions becoming too coarse as they are composed from\\nabstractions of sub-components. This is similar to how interpolants can be used to compute tight\\ninvariants in program verification when successive application of abstract transformers lead to\\noverly general invariants. Studying broader implications of these ideas is left for future work.\\nNontrivial cuts that go beyond bottom-up or top-down approaches can only be computed if there\\nare nontrivial invariants that hold (about the values that are generated) at certain intermediate\\nnonterminals in the grammar. Moreover, the DSL designer must be aware of these invariants. The\\nDSL designer can choose to write cuts that are incomplete in theory, but reasonable in practice,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 26, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='nonterminals in the grammar. Moreover, the DSL designer must be aware of these invariants. The\\nDSL designer can choose to write cuts that are incomplete in theory, but reasonable in practice,\\nguided by their understanding of the domain. Nontrivial cuts are likely to exist in large DSLs where\\ndifferent forms of values flow on different paths. We have not done a formal study of how difficult\\nit is for a DSL designer to write useful cut functions‚Äîthis is beyond the scope of the current paper\\nwhich just lays down the foundations for cuts.\\nDesigning a guarded DSL requires the DSL designer to have clear preference for certain operators\\nover other alternatives, and moreover, any precedence on operators closer to the start symbol (in the\\ngrammar) should override any precedence on operators closer to the leaves of the program tree. The\\n‚Äúhigher-up‚Äù operators in any DSL typically establish the high-level tactic of the program, and hence'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 26, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='grammar) should override any precedence on operators closer to the leaves of the program tree. The\\n‚Äúhigher-up‚Äù operators in any DSL typically establish the high-level tactic of the program, and hence\\nthis requirement often holds. Precedences in a grammar are likely to exist if it contains redundant\\noperators that are some preferred compositions of other low-level operators in the grammar. Further,\\nthere are certain technical assumptions we make about precedences. The assumption that the\\nprecedence is a series-parallel partial order is not required if we start with the gDSL (rather than\\nstart with precedences), which is what we do in practice. The assumption that the reachability\\nrelation on the grammar nonterminals be acyclic is required only to provide a clean mathematical\\ninterpretation of the ranking induced by gDSL on programs (terms) as a path ordering. In the\\npresence of cycles, the synthesis rule and the full system can still be used without problems, but'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 26, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='interpretation of the ranking induced by gDSL on programs (terms) as a path ordering. In the\\npresence of cycles, the synthesis rule and the full system can still be used without problems, but\\nranking cannot be described in a simple way.\\n8\\nCONCLUSION\\nWe introduced two techniques, cuts and precedence through guarded DSLs, that DSL designers can\\nuse to prune search in programming by example. Cuts enable a novel synthesis strategy: middle-\\nout synthesis. This strategy allows FlashFill++ to support synthesis tasks that require non-EI/EE\\noperators, such as datetime and numeric transformations. The use of precedence through gDSLs\\nallows us to increase the size of our DSL, by incorporating redundant operators, which facilitate\\nreadable code generation in different target languages. We compare our tool to existing state-of-\\nthe-art PBE systems, FlashFill, Duet, and SmartFill, on three public benchmark datasets and show'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 26, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='readable code generation in different target languages. We compare our tool to existing state-of-\\nthe-art PBE systems, FlashFill, Duet, and SmartFill, on three public benchmark datasets and show\\nthat FlashFill++ can solve more tasks, in less time, and without substantially increasing the number\\nof examples required. We also perform a survey-based study on code readability, confirming that\\nthe programs synthesized by FlashFill++ are more readable than those generated by FlashFill.\\nREFERENCES\\nAnnika Aasa. 1995. Precedences in specifications and implementations of programming languages. Theoretical Computer\\nScience 142, 1 (1995), 3‚Äì26.\\nMaaz Bin Safeer Ahmad and Alvin Cheung. 2018. Automatically Leveraging MapReduce Frameworks for Data-Intensive\\nApplications. In Proc. 2018 International Conference on Management of Data, SIGMOD Conference. ACM, 1205‚Äì1220.\\nhttps://doi.org/10.1145/3183713.3196891'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 26, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Applications. In Proc. 2018 International Conference on Management of Data, SIGMOD Conference. ACM, 1205‚Äì1220.\\nhttps://doi.org/10.1145/3183713.3196891\\nAlfred Aho, S. Johnson, and Jeffrey Ullman. 1973. Deterministic parsing of ambiguous grammars. Commun. ACM 18 (01\\n1973), 441‚Äì452. https://doi.org/10.1145/360933.360969\\nRajeev Alur, Rastislav Bod√≠k, Garvit Juniwal, Milo M. K. Martin, Mukund Raghothaman, Sanjit A. Seshia, Rishabh Singh,\\nArmando Solar-Lezama, Emina Torlak, and Abhishek Udupa. 2013. Syntax-Guided Synthesis. In Formal Methods in\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:28\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nComputer-Aided Design, FMCAD 2013. 1‚Äì8.\\nRajeev Alur, Arjun Radhakrishna, and Abhishek Udupa. 2017. Scaling Enumerative Program Synthesis via Divide and\\nConquer. In TACAS. 319‚Äì336.\\nS√©bastien Bardin, Alain Finkel, J√©r√¥me Leroux, and Laure Petrucci. 2008. FAST: acceleration from theory to practice. Int. J.\\nSoftw. Tools Technol. Transf. 10, 5 (2008), 401‚Äì424. https://doi.org/10.1007/s10009-008-0064-3\\nDenis B√©chet, Philippe de Groote, and Christian Retor√©. 1997. A Complete Axiomatisation for the Inclusion of Series-Parallel\\nPartial Orders. In Rewriting Techniques and Applications, 8th Int. Conf., RTA-97 (Lecture Notes in Computer Science,\\nVol. 1232). Springer, 230‚Äì240. https://doi.org/10.1007/3-540-62950-5_74\\nBernard Boigelot. 2003. On iterating linear transformations over recognizable sets of integers. Theor. Comput. Sci. 309, 1-3\\n(2003), 413‚Äì468. https://doi.org/10.1016/S0304-3975(03)00314-1'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Bernard Boigelot. 2003. On iterating linear transformations over recognizable sets of integers. Theor. Comput. Sci. 309, 1-3\\n(2003), 413‚Äì468. https://doi.org/10.1016/S0304-3975(03)00314-1\\nSwarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, and Yisong Yue. 2021. Neu-\\nrosymbolic Programming. Found. Trends Program. Lang. 7, 3 (2021), 158‚Äì243.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards,\\nYuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser,\\nMohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,\\nIgor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua\\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter\\nWelinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021b. Evaluating Large\\nLanguage Models Trained on Code. CoRR abs/2107.03374 (2021). arXiv:2107.03374 https://arxiv.org/abs/2107.03374\\nXinyun Chen, Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, and Denny Zhou. 2021a. Spreadsheet-\\nCoder: Formula Prediction from Semi-structured Context. In Proceedings of the 38th International Conference on Machine\\nLearning (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 1661‚Äì1672.\\nhttps://proceedings.mlr.press/v139/chen21m.html'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Learning (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 1661‚Äì1672.\\nhttps://proceedings.mlr.press/v139/chen21m.html\\nAndrew Cropper. 2019. Playgol: Learning Programs Through Play. In Proceedings of the Twenty-Eighth International Joint\\nConference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, Sarit Kraus (Ed.). ijcai.org, 6074‚Äì6080.\\nhttps://doi.org/10.24963/ijcai.2019/841 https://github.com/andrewcropper/ijcai19-playgol.\\nNachum Dershowitz and Jean-Pierre Jouannaud. 1990. Rewrite Systems. In Handbook of Theoretical Computer Science,\\nVolume B: Formal Models and Semantics. Elsevier and MIT Press, 243‚Äì320.\\nJacob Devlin, Rudy Bunel, Rishabh Singh, Matthew J. Hausknecht, and Pushmeet Kohli. 2017. Neural Program Meta-Induction.\\nIn NIPS, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and\\nRoman Garnett (Eds.). 2080‚Äì2088.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='In NIPS, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and\\nRoman Garnett (Eds.). 2080‚Äì2088.\\nIan Drosos, Titus Barik, Philip J. Guo, Robert DeLine, and Sumit Gulwani. 2020. Wrex: A Unified Programming-by-Example\\nInteraction for Synthesizing Readable Code for Data Scientists. In Proceedings of the 2020 CHI Conference on Human\\nFactors in Computing Systems (Honolulu, HI, USA) (CHI ‚Äô20). Association for Computing Machinery, New York, NY, USA,\\n1‚Äì12. https://doi.org/10.1145/3313831.3376442\\nJay Earley. 1974. Ambiguity and Precedence in Syntax Description. Acta Informatica 4 (1974), 183‚Äì192.\\nAzadeh Farzan and Victor Nicolet. 2021. Phased synthesis of divide and conquer programs. In PLDI ‚Äô21: 42nd ACM SIGPLAN\\nInternational Conference on Programming Language Design and Implementation. ACM, 974‚Äì986. https://doi.org/10.1145/\\n3453483.3454089'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='International Conference on Programming Language Design and Implementation. ACM, 974‚Äì986. https://doi.org/10.1145/\\n3453483.3454089\\nYu Feng, Ruben Martins, Jacob Van Geffen, Isil Dillig, and Swarat Chaudhuri. 2017. Component-based synthesis of table\\nconsolidation and transformation tasks from examples. In Proc. 38th ACM SIGPLAN Conference on Programming Language\\nDesign and Implementation, PLDI. ACM, 422‚Äì436. https://doi.org/10.1145/3062341.3062351\\nAlain Finkel. 1987. A Generalization of the Procedure of Karp and Miller to Well Structured Transition Systems. In Proc.\\n14th Intl. Colloquium on Automata, Languages and Programming, ICALP87 (Lecture Notes in Computer Science, Vol. 267),\\nThomas Ottmann (Ed.). Springer, 499‚Äì508. https://doi.org/10.1007/3-540-18088-5_43\\nBryan Ford. 2004. Parsing expression grammars: a recognition-based syntactic foundation. In Proc. 31st ACM SIGPLAN-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Thomas Ottmann (Ed.). Springer, 499‚Äì508. https://doi.org/10.1007/3-540-18088-5_43\\nBryan Ford. 2004. Parsing expression grammars: a recognition-based syntactic foundation. In Proc. 31st ACM SIGPLAN-\\nSIGACT Symposium on Principles of Programming Languages, POPL. ACM, 111‚Äì122. https://doi.org/10.1145/964001.964011\\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained Language Models Better Few-shot Learners. In\\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\\nConference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, Online,\\n3816‚Äì3830. https://doi.org/10.18653/v1/2021.acl-long.295\\nGoogle. 2021. SpreadSheetCoder. https://github.com/google-research/google-research/tree/master/spreadsheet_coder\\nSumit Gulwani. 2011. Automating string processing in spreadsheets using input-output examples. In POPL. 317‚Äì330.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Sumit Gulwani. 2011. Automating string processing in spreadsheets using input-output examples. In POPL. 317‚Äì330.\\nSumit Gulwani. 2016. Programming by Examples - and its applications in Data Wrangling. In Dependable Software Systems\\nEngineering. 137‚Äì158.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:29\\nSumit Gulwani, William R. Harris, and Rishabh Singh. 2012. Spreadsheet data manipulation using examples. Commun.\\nACM 55, 8 (2012), 97‚Äì105.\\nS. Gulwani, V. Korthikanti, and A. Tiwari. 2011. Synthesizing geometry constructions. In Proc. ACM Conf. on Prgm. Lang.\\nDesgn. and Impl. PLDI. 50‚Äì61.\\nSumit Gulwani, Oleksandr Polozov, and Rishabh Singh. 2017. Program Synthesis. Foundations and Trends in Programming\\nLanguages 4, 1-2 (2017), 1‚Äì119.\\nZheng Guo, Michael James, David Justo, Jiaxiao Zhou, Ziteng Wang, Ranjit Jhala, and Nadia Polikarpova. 2020. Program\\nsynthesis by type-guided abstraction refinement. Proc. ACM Program. Lang. 4, POPL (2020), 12:1‚Äì12:28. https://doi.org/\\n10.1145/3371080\\nJan Heering, P. R. H. Hendriks, Paul Klint, and J. Rekers. 1989. The syntax definition formalism SDF - reference manual.\\nACM SIGPLAN Notices 24, 11 (1989), 43‚Äì75.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='10.1145/3371080\\nJan Heering, P. R. H. Hendriks, Paul Klint, and J. Rekers. 1989. The syntax definition formalism SDF - reference manual.\\nACM SIGPLAN Notices 24, 11 (1989), 43‚Äì75.\\nKangjing Huang, Xiaokang Qiu, Peiyuan Shen, and Yanjun Wang. 2020. Reconciling enumerative and deductive program\\nsynthesis. In Proc. 41st ACM SIGPLAN Intl. Conf. on Programming Language Design and Implementation, PLDI, Alastair F.\\nDonaldson and Emina Torlak (Eds.). ACM, 1159‚Äì1174. https://doi.org/10.1145/3385412.3386027\\nBertrand Jeannet, Peter Schrammel, and Sriram Sankaranarayanan. 2014. Abstract acceleration of general linear loops.\\nIn The 41st Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL. ACM, 529‚Äì540.\\nhttps://doi.org/10.1145/2535838.2535843\\nAshwin Kalyan, Abhishek Mohta, Alex Polozov, Dhruv Batra, Prateek Jain, and Sumit Gulwani. 2018. Neural-Guided\\nDeductive Search for Real-Time Program Synthesis from Examples. In 6th International Conference on Learning Repre-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Deductive Search for Real-Time Program Synthesis from Examples. In 6th International Conference on Learning Repre-\\nsentations (ICLR) (6th international conference on learning representations (iclr) ed.). https://www.microsoft.com/en-\\nus/research/publication/neural-guided-deductive-search-real-time-program-synthesis-examples/\\nRichard M. Karp and Raymond E. Miller. 1969. Parallel Program Schemata. J. Comput. Syst. Sci. 3, 2 (1969), 147‚Äì195.\\nhttps://doi.org/10.1016/S0022-0000(69)80011-5\\nVu Le and Sumit Gulwani. 2014. FlashExtract: A Framework for Data Extraction by Examples. In PLDI. 542‚Äì553.\\nWoosuk Lee. 2021. Combining the top-down propagation and bottom-up enumeration for inductive program synthesis.\\nProc. ACM Program. Lang. 5, POPL (2021), 1‚Äì28. https://doi.org/10.1145/3434335 https://github.com/wslee/duet.\\nWoosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. 2018. Accelerating search-based program synthesis using'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. 2018. Accelerating search-based program synthesis using\\nlearned probabilistic models. In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and\\nImplementation, PLDI, Jeffrey S. Foster and Dan Grossman (Eds.). ACM, 436‚Äì449. https://doi.org/10.1145/3192366.3192410\\nPercy Liang, Michael I. Jordan, and Dan Klein. 2010. Learning Programs: A Hierarchical Bayesian Approach. In Proceedings\\nof the 27th International Conference on Machine Learning (ICML-10), Johannes F√ºrnkranz and Thorsten Joachims (Eds.).\\nOmnipress, 639‚Äì646.\\nDylan Lukes, John Sarracino, Cora Coleman, Hila Peleg, Sorin Lerner, and Nadia Polikarpova. 2021. Synthesis of web\\nlayouts from examples. In ESEC/FSE ‚Äô21: 29th ACM Joint European Software Engineering Conference and Symposium on the\\nFoundations of Software Engineering, Athens, Greece, August 23-28, 2021, Diomidis Spinellis, Georgios Gousios, Marsha'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Foundations of Software Engineering, Athens, Greece, August 23-28, 2021, Diomidis Spinellis, Georgios Gousios, Marsha\\nChechik, and Massimiliano Di Penta (Eds.). ACM, 651‚Äì663.\\nAditya Krishna Menon, Omer Tamuz, Sumit Gulwani, Butler W. Lampson, and Adam Kalai. 2013. A Machine Learning\\nFramework for Programming by Example. In Proceedings of the 30th International Conference on Machine Learning, ICML\\n(JMLR Workshop and Conference Proceedings, Vol. 28). JMLR.org, 187‚Äì195. http://proceedings.mlr.press/v28/menon13.html\\nAnders Miltner, Kathleen Fisher, Benjamin C. Pierce, David Walker, and Steve Zdancewic. 2018. Synthesizing bijective\\nlenses. Proc. ACM Program. Lang. 2, POPL (2018), 1:1‚Äì1:30.\\nNagarajan Natarajan, Danny Simmons, Naren Datha, Prateek Jain, and Sumit Gulwani. 2019. Learning Natural Programs\\nfrom a Few Examples in Real-Time. In AIStats. https://www.microsoft.com/en-us/research/publication/learning-natural-\\nprograms-from-a-few-examples-in-real-time/'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='from a Few Examples in Real-Time. In AIStats. https://www.microsoft.com/en-us/research/publication/learning-natural-\\nprograms-from-a-few-examples-in-real-time/\\nPeter-Michael Osera and Steve Zdancewic. 2015. Type-and-example-directed program synthesis. In Proc. 36th ACM SIGPLAN\\nConf. on Programming Language Design and Implementation. ACM, 619‚Äì630. https://doi.org/10.1145/2737924.2738007\\nRangeet Pan, Vu Le, Nachiappan Nagappan, Sumit Gulwani, Shuvendu K. Lahiri, and Mike Kaufman. 2021. Can Program\\nSynthesis be Used to Learn Merge Conflict Resolutions? An Empirical Analysis. In 43rd IEEE/ACM International Conference\\non Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021. IEEE, 785‚Äì796.\\nNadia Polikarpova, Ivan Kuraj, and Armando Solar-Lezama. 2016. Program synthesis from polymorphic refinement\\ntypes. In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='types. In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation,\\nPLDI 2016, Santa Barbara, CA, USA, June 13-17, 2016, Chandra Krintz and Emery D. Berger (Eds.). ACM, 522‚Äì538.\\nhttps://doi.org/10.1145/2908080.2908093\\nOleksandr Polozov and Sumit Gulwani. 2015. FlashMeta: A Framework for Inductive Program synthesis. In OOPSLA/SPLASH.\\n107‚Äì126.\\nPowerFx 2021. PowerFx: The low code programming language. https://powerapps.microsoft.com/en-us/blog/introducing-\\nmicrosoft-power-fx-the-low-code-programming-language-for-everyone/. Accessed: 2021-11-19.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 29, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:30\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nMicrosoft PROSE. 2022. PROSE public benchmark suite. https://github.com/microsoft/prose-benchmarks.\\nKia Rahmani, Mohammad Raza, Sumit Gulwani, Vu Le, Daniel Morris, Arjun Radhakrishna, Gustavo Soares, and Ashish\\nTiwari. 2021. Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis.\\nProc. ACM Program. Lang. 5, OOPSLA (2021), 1‚Äì29.\\nReudismam Rolim, Gustavo Soares, Loris D‚ÄôAntoni, Oleksandr Polozov, Sumit Gulwani, Rohit Gheyi, Ryo Suzuki, and Bj√∂rn\\nHartmann. 2017. Learning syntactic program transformations from examples. In ICSE. IEEE / ACM, 404‚Äì415.\\nSelenium. 2022. Selenium. https://github.com/SeleniumHQ/selenium\\nNischal Shrestha, Titus Barik, and Chris Parnin. 2018. It‚Äôs Like Python But: Towards Supporting Transfer of Programming\\nLanguage Knowledge. In 2018 IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC, J√°come'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 29, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Language Knowledge. In 2018 IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC, J√°come\\nCunha, Jo√£o Paulo Fernandes, Caitlin Kelleher, Gregor Engels, and Jorge Mendes (Eds.). IEEE Computer Society, 177‚Äì185.\\nhttps://doi.org/10.1109/VLHCC.2018.8506508\\nRishabh Singh and Sumit Gulwani. 2015. Predicting a Correct Program in Programming by Example. In CAV. 398‚Äì414.\\nCalvin Smith and Aws Albarghouthi. 2019. Program Synthesis with Equivalence Reduction. In VMCAI, Constantin Enea\\nand Ruzica Piskac (Eds.).\\nAbhishek Udupa, Arun Raghavan, Jyotirmoy V. Deshmukh, Sela Mador-Haim, Milo M. K. Martin, and Rajeev Alur. 2013.\\nTRANSIT: specifying protocols with concolic snippets. In ACM SIGPLAN Conference on Programming Language Design\\nand Implementation, PLDI, Hans-Juergen Boehm and Cormac Flanagan (Eds.). ACM, 287‚Äì296. https://doi.org/10.1145/\\n2491956.2462174\\nMark van den Brand, Jeroen Scheerder, Jurgen J. Vinju, and Eelco Visser. 2002. Disambiguation Filters for Scannerless'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 29, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='2491956.2462174\\nMark van den Brand, Jeroen Scheerder, Jurgen J. Vinju, and Eelco Visser. 2002. Disambiguation Filters for Scannerless\\nGeneralized LR Parsers. In Compiler Construction, 11th Intl. Conf, CC 2002, Part of ETAPS, Proceedings (Lecture Notes in\\nComputer Science, Vol. 2304). Springer, 143‚Äì158.\\nGust Verbruggen, Vu Le, and Sumit Gulwani. 2021. Semantic programming by example with pre-trained models. Proc. ACM\\nProgram. Lang. 5, OOPSLA (2021), 1‚Äì25. https://doi.org/10.1145/3485477\\nChenglong Wang, Alvin Cheung, and Rastislav Bod√≠k. 2017. Synthesizing highly expressive SQL queries from input-output\\nexamples. In Proc. 38th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI. ACM,\\n452‚Äì466. https://doi.org/10.1145/3062341.3062365\\nXinyu Wang, Isil Dillig, and Rishabh Singh. 2018. Program synthesis using abstraction refinement. Proc. ACM Program.\\nLang. 2, POPL (2018), 63:1‚Äì63:30. https://doi.org/10.1145/3158151'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 29, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Xinyu Wang, Isil Dillig, and Rishabh Singh. 2018. Program synthesis using abstraction refinement. Proc. ACM Program.\\nLang. 2, POPL (2018), 63:1‚Äì63:30. https://doi.org/10.1145/3158151\\nYuepeng Wang, Rushi Shah, Abby Criswell, Rong Pan, and Isil Dillig. 2020. Data Migration using Datalog Program Synthesis.\\nProc. VLDB Endow. 13, 7 (2020), 1006‚Äì1019. https://doi.org/10.14778/3384345.3384350\\nTianyi Zhang, Zhiyang Chen, Yuanli Zhu, Priyan Vaithilingam, Xinyu Wang, and Elena L. Glassman. 2021. Interpretable\\nProgram Synthesis. Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3411764.3445646\\nTianyi Zhang, London Lowmanstone, Xinyu Wang, and Elena L. Glassman. 2020. Interactive Program Synthesis by\\nAugmented Examples. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology\\n(Virtual Event, USA) (UIST ‚Äô20). Association for Computing Machinery, New York, NY, USA, 627‚Äì648. https://doi.org/10.\\n1145/3379337.3415900'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 29, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='(Virtual Event, USA) (UIST ‚Äô20). Association for Computing Machinery, New York, NY, USA, 627‚Äì648. https://doi.org/10.\\n1145/3379337.3415900\\nReceived 2022-07-07; accepted 2022-11-07\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 0, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nContents lists available at ScienceDirect \\nComputer Languages, Systems & Structures \\njournal homepage: www.elsevier.com/locate/cl \\nSystematic mapping study of template-based code generation \\nEugene Syriani ‚àó, Lechanceux Luhunu , Houari Sahraoui \\nDepartment of computer science and operations research (DIRO), University of Montreal, Montreal, Quebec, Canada \\na r t i c l e \\ni n f o \\nArticle history: \\nReceived 18 August 2017 \\nRevised 24 November 2017 \\nAccepted 30 November 2017 \\nAvailable online 7 December 2017 \\nKeywords: \\nCode generation \\nSystematic mapping study \\nModel-driven engineering \\na b s t r a c t \\nContext: Template-based code generation (TBCG) is a synthesis technique that produces \\ncode from high-level speciÔ¨Åcations, called templates. TBCG is a popular technique in \\nmodel-driven engineering (MDE) given that they both emphasize abstraction and automa-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 0, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='code from high-level speciÔ¨Åcations, called templates. TBCG is a popular technique in \\nmodel-driven engineering (MDE) given that they both emphasize abstraction and automa- \\ntion. Given the diversity of tools and approaches, it is necessary to classify existing TBCG \\ntechniques to better guide developers in their choices. \\nObjective: The goal of this article is to better understand the characteristics of TBCG tech- \\nniques and associated tools, identify research trends, and assess the importance of the role \\nof MDE in this code synthesis approach. \\nMethod: We survey the literature to paint an interesting picture about the trends and uses \\nof TBCG in research. To this end, we follow a systematic mapping study process. \\nResults: Our study shows, among other observations, that the research community has \\nbeen diversely using TBCG over the past 16 years. An important observation is that TBCG \\nhas greatly beneÔ¨Åted from MDE. It has favored a template style that is output-based and'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 0, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='been diversely using TBCG over the past 16 years. An important observation is that TBCG \\nhas greatly beneÔ¨Åted from MDE. It has favored a template style that is output-based and \\nhigh-level modeling languages as input. TBCG is mainly used to generate source code and \\nhas been applied to many domains. \\nConclusion: TBCG is now a mature technique and much research work is still conducted \\nin this area. However, some issues remain to be addressed, such as support for template \\ndeÔ¨Ånition and assessment of the correctness and quality of the generated code. \\n¬© 2017 Elsevier Ltd. All rights reserved. \\n1. Introduction \\nCode generation has been around since the 1950s, taking its origin in early compilers [1] . Since then, software organi- \\nzations have been relying on code synthesis techniques in order to reduce development time and increase productivity [2] . \\nAutomatically generating code is an approach where the same generator can be reused to produce many different artifacts'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 0, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Automatically generating code is an approach where the same generator can be reused to produce many different artifacts \\naccording to the varying inputs it receives. It also helps detecting errors in the input artifact early on before the generated \\ncode is compiled, when the output is source code. \\nThere are many techniques to generate code, such as programmatically [3] , using a meta-object protocol [4] , or aspect- \\noriented programming [5] . Since the mid-1990‚Äôs, template-based code generation (TBCG) emerged as an approach requiring \\nless effort for the programmers to develop code generators. Templates favor reuse following the principle of write once, \\nproduce many . The concept was heavily used in web designer software (such as Dreamweaver) to generate web pages \\nand Computer Aided Software Engineering (CASE) tools to generate source code from UML diagrams. Many development \\n‚àóCorresponding author.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 0, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='and Computer Aided Software Engineering (CASE) tools to generate source code from UML diagrams. Many development \\n‚àóCorresponding author. \\nE-mail addresses: syriani@iro.umontreal.ca (E. Syriani), luhunukl@iro.umontreal.ca (L. Luhunu), sahraoui@iro.umontreal.ca (H. Sahraoui). \\nhttps://doi.org/10.1016/j.cl.2017.11.003 \\n1477-8424/¬© 2017 Elsevier Ltd. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='44 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nenvironments started to include a template mechanism in their framework such as Microsoft Text Template Transformation \\nToolkit (T4) 1 for .NET and Velocity 2 for Apache. \\nModel-driven engineering (MDE) has advocated the use of model-to-text transformations as a core component of its \\nparadigm [6] . TBCG is a popular technique in MDE given that they both emphasize abstraction and automation. MDE tools, \\nsuch as Acceleo 3 and Xpand 4 , allow developers to generate code from high-level models without worrying on how to parse \\nand traverse input models. We can Ô¨Ånd today TBCG applied in a plethora of computer science and engineering research. \\nThe software engineering research community has focused essentially on primary studies proposing new TBCG tech- \\nniques, tools and applications. However, to the best of our knowledge, there is no classiÔ¨Åcation, characterization, or'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='niques, tools and applications. However, to the best of our knowledge, there is no classiÔ¨Åcation, characterization, or \\nassessment of these studies available yet. Therefore, in this paper, we conducted a systematic mapping study (SMS) of \\nthe literature in order to understand the trends, identify the characteristics of TBCG, assess the popularity of existing tools \\nwithin the research community, and determine the inÔ¨Çuence that MDE has had on TBCG. We are interested in various \\nfacets of TBCG, such as characterizing the templates, the inputs, and outputs, along with the evolution of the amount of \\npublications using TBCG over the past 16 years. \\nThe remainder of this paper is organized as follows. In Section 2 , we introduce the necessary background on TBCG \\nand discuss related work. In Section 3 , we elaborate on the methodology we followed for this SMS, and on the results of'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='and discuss related work. In Section 3 , we elaborate on the methodology we followed for this SMS, and on the results of \\nthe paper selection phase. The following sections report the results: the trends and evolution of TBCG in Section 4 , the \\ncharacteristics of TBCG according to our classiÔ¨Åcation scheme in Section 5 , the relationships between the different facets \\nin Section 6 , how TBCG tools have been used in primary studies in Section 7 , and the relation between MDE and TBCG \\nin Section 8 . In Section 9 , we answer our research questions and discuss limitations of the study. Finally, we conclude in \\nSection 10 . \\n2. Background and related work \\nWe brieÔ¨Çy explain TBCG in the context of MDE and discuss related work on secondary studies about code generation. \\n2.1. Code generation \\nIn this paper, we view code generation as in automatic programming [1] rather than compilers. The underlying principle'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='2.1. Code generation \\nIn this paper, we view code generation as in automatic programming [1] rather than compilers. The underlying principle \\nof automatic programming is that a user deÔ¨Ånes what he expects from the program and the program should be automat- \\nically generated by a software without any assistance by the user. This generative approach is different from a compiler \\napproach. Compilers produce code executable by a computer from a speciÔ¨Åcation conforming to a programming language, \\nwhereas automatic programming transforms user speciÔ¨Åcations into code which often conforms to a programming language. \\nCompilers have a phase called code generation that retrieves an abstract syntax tree produced by a parser and translates it \\ninto machine code or bytecode executable by a virtual machine. Compared to code generation as in automatic programming, \\ncompilers can be regarded as tasks or services that are incorporated in or post-positioned to code generators [7] .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='compilers can be regarded as tasks or services that are incorporated in or post-positioned to code generators [7] . \\nCode generation is an important model-to-text transformation that ensures the automatic transformation of a model into \\ncode. Organization are adopting the use of code generation since it reduces the development process time and increases \\nthe productivity. Generating the code using the most appropriate technique is even more crucial since it is the key to \\nbeneÔ¨Åt from all the advantages code synthesis offers to an organization. Nowadays, TBCG has raised to be the most popular \\nsynthesis technique available. Using templates can quickly become a complex task especially when the model should satisfy \\na certain condition before a template fragment is executed. \\nAs Balzer [8] states, there are many advantages to code generation. The effort of the user is reduced as he has fewer'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='a certain condition before a template fragment is executed. \\nAs Balzer [8] states, there are many advantages to code generation. The effort of the user is reduced as he has fewer \\nlines to write: speciÔ¨Åcations are shorter than the program that implements them. SpeciÔ¨Åcations are easier to write and \\nto understand for a user, given that they are closer to the application and domain concepts. Writing speciÔ¨Åcations is less \\nerror-prone than writing the program directly, since the expert is the one who writes the speciÔ¨Åcation rather than another \\nprogrammer. \\nThese advantages are in fact the pillar principles of MDE and domain-speciÔ¨Åc modeling. Floch et al. [9] observed many \\nsimilarities between MDE and compilers research and principles. Thus, it is not surprising to see that many, though not \\nexclusively, code generation tools came out of the MDE community. The advantages of code generation should be contrasted'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='exclusively, code generation tools came out of the MDE community. The advantages of code generation should be contrasted \\nwith some of its limitations. For example, there are issues related to integration of generated code with manually written \\ncode and to evolving speciÔ¨Åcations that require to re-generate the code [10] . Sometimes, relying too much on code genera- \\ntors may produce an overly general solution that may not necessarily be optimal for a speciÔ¨Åc problem. \\n1 https://msdn.microsoft.com/en-us/library/bb126445.aspx \\n2 http://velocity.apache.org/ \\n3 http://www.eclipse.org/acceleo/ \\n4 http://wiki.eclipse.org/Xpand'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 2, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n45 \\nconforms to\\nuses\\ninput\\ninput\\ngenerates\\nTemplate\\n<%context class%>\\npublic class <%name%> { String id; }\\nDesign-time input\\nClass\\nname:string\\nRuntime input\\nPerson\\nOutput\\npublic class Person { String id; }\\nTemplate engine\\nFig. 1. Components of TBCG. \\n2.2. Model-to-text transformations \\nIn MDE, model transformations can have different purposes [11] , such as translating, simulating, or reÔ¨Åning models. One \\nparticular kind of model transformation is devoted to code generation with model-to-text transformations (M2T) [12] . In \\ngeneral, M2T transforms a model (often in the form of a graph structure) into a linearized textual representation. M2T is \\nused to produce various text artifacts, e.g., to generate code, to serialize models, to generate documentation and reports, or \\nto visualize and explore models. As such, code generation is a special case of M2T, where the output artifacts are executable'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 2, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='to visualize and explore models. As such, code generation is a special case of M2T, where the output artifacts are executable \\nsource code. There are commonly Ô¨Åve different code generation approaches present in the literature [7,12] : \\nVisitor based approaches consist of programmatically traversing the internal representation of the input, while relying \\non an API dedicated to manipulate the input, to write the output to a text stream. This requires to program directly \\nthe creation of the output Ô¨Åles and storing the strings while navigating through the data structure of the input. The \\nimplementation typically makes use of the visitor design pattern [13] . This approach is used in [3] to generate Java \\ncode from a software architecture description language. \\nMeta-programming is a language extension approach, such as using a meta-object protocol, that relies on introspection'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 2, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='code from a software architecture description language. \\nMeta-programming is a language extension approach, such as using a meta-object protocol, that relies on introspection \\nin order to access the abstract syntax tree of the source program. For example, in OpenJava [4] , a Java meta-program \\ncreates a Java Ô¨Åle, compiles it on the Ô¨Çy, and loads the generated program in its own run-time. \\nIn-line generation relies on a preprocessor that generates additional code to expand the existing one, such as with the \\nC++ standard template library or C macro preprocessor instructions. The generation instructions can be deÔ¨Åned at a \\nhigher-level of abstraction, either using a dedicated language distinct from the base language (e.g., for macros) or as \\na dedicated library as in [14] . \\nCode annotations are in-line descriptions that added to statement declarations (e.g., class deÔ¨Ånition) that can either be'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 2, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='a dedicated library as in [14] . \\nCode annotations are in-line descriptions that added to statement declarations (e.g., class deÔ¨Ånition) that can either be \\ninternally transformed into more expanded code (e.g., attributes in C#) or that are processed by a tool other than the \\ncompiler of the language (e.g., the speciÔ¨Åcation of documentation comments processed by Javadoc). This approach is \\nused in [15] . \\nTemplate based is described below. \\n2.3. Template-based code generation \\nThe literature agrees on a general deÔ¨Ånition of M2T code generation [12] and on templates. J√∂rges [7] identiÔ¨Åes three \\ncomponents in TBCG: the data, the template, and the output. However, there is another component that is not mentioned, \\nwhich is the meta-information the generation logic of the template relies on. Therefore, we conducted this study according \\nto the following notion of TBCG. \\nFigure 1 summarizes the main concepts of TBCG. We consider TBCG as a synthesis technique that uses templates in order'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 2, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='to the following notion of TBCG. \\nFigure 1 summarizes the main concepts of TBCG. We consider TBCG as a synthesis technique that uses templates in order \\nto produce a textual artifact, such as source code, called the output . A template is an abstract and generalized representation \\nof the textual output it describes. It has a static part , text fragments that appear in the output ‚Äúas is‚Äù. It also has a dynamic \\npart embedded with splices of meta-code that encode the generation logic. Templates are executed by the template engine \\n(sometimes refereed to as template processor ) to compute the dynamic part and replace meta-codes by static text according \\nto run-time input . The design-time input deÔ¨Ånes the meta-information which the run-time input conforms to. The dynamic \\npart of a template relies on the design-time input to query the run-time input by Ô¨Åltering the information retrieved and'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 2, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='part of a template relies on the design-time input to query the run-time input by Ô¨Åltering the information retrieved and \\nperforming iterative expansions on it. Therefore, TBCG relies on a design-time input that is used to deÔ¨Åne the template \\nand a run-time input on which the template is applied to produce the output. For example, a TBCG engine that takes as \\nrun-time input an XML document relies on an XML schema as design-time input. DeÔ¨Ånition 1 summarizes our deÔ¨Ånition of \\nTBCG.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='46 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nDeÔ¨Ånition 1. A synthesis technique is a TBCG if it consists of a set of templates speciÔ¨Åed in a formalism, where the speciÔ¨Å- \\ncation of a template is based on a design-time input such that, when executed, it reads a run-time input to produce a textual \\noutput . \\nFor example, the work in [16] generates a C# API from Ecore models using Xpand. According to DeÔ¨Ånition 1 , the tem- \\nplates of this TBCG example are Xpand templates, the design-time input is the metamodel of Ecore, the run-time input is \\nan Ecore model, and the output is a C# project Ô¨Åle and C# classes. \\n2.4. Literature reviews on code generation \\nIn evidence-based software engineering [17] , a systematic literature review is a secondary study that reviews primary \\nstudies with the aim of synthesizing evidence related to a speciÔ¨Åc research question. Several forms of systematic reviews'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='studies with the aim of synthesizing evidence related to a speciÔ¨Åc research question. Several forms of systematic reviews \\nexist depending on the depth of reviewing primary studies and on the speciÔ¨Åcities of research questions. Unlike conven- \\ntional systematic literature reviews that attempt to answer a speciÔ¨Åc question, a SMS aim at classifying and performing a \\nthematic analysis on a topic [18] . SMS is a secondary study method that has been adapted from other disciplines to soft- \\nware engineering in [19] and later evolved by Petersen et al. in [20] . A SMS is designed to provide a wide overview of \\na research area, establish if research evidence exists on a speciÔ¨Åc topic, and provide an indication of the quantity of the \\nevidence speciÔ¨Åc to the domain. \\nOver the years, there have been many primary studies on code generation. However, we could not Ô¨Ånd any secondary \\nstudy on TBCG explicitly. Still, the following are closely related secondary studies.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='study on TBCG explicitly. Still, the following are closely related secondary studies. \\nMehmood et al. [21] performed a SMS regarding the use of aspect-oriented modeling for code generation, which is not \\nbased on templates. They analyzed 65 papers mainly based on three main categories: the focus area, the type of research, \\nand the type of contribution. The authors concluded that this synthesis technique is still immature. The study shows that \\nno work has been reported to use or evaluate any of the techniques proposed. \\nGurunule et al. [22] presented a comparison of aspect orientation and MDE techniques to investigate how they can each \\nbe used for code generation. The authors found that further research in these areas can lead to signiÔ¨Åcant advancements in \\nthe development of software systems. Unlike Mehmood et al. [21] , they did not follow a systematic and repeatable process.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='the development of software systems. Unlike Mehmood et al. [21] , they did not follow a systematic and repeatable process. \\nDominguez et al. [23] performed a systematic literature review of studies that focus on code generation from state ma- \\nchine speciÔ¨Åcations. The study is based on a set of 53 papers, which have been classiÔ¨Åed into two groups: pattern-based \\nand not pattern-based. The authors do not take template-based approaches into consideration. \\nBatot et al. [24] performed a systematic mapping study on model transformations solving a concrete problem that have \\nbeen published in the literature. They analyzed 82 papers based on a classiÔ¨Åcation scheme that is general to any model \\ntransformation approach, which includes M2T. They conclude that concrete model transformations have been pulling out \\nfrom the research literature since 2009 and are being considered as development tasks. They also found that 22% of their'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='from the research literature since 2009 and are being considered as development tasks. They also found that 22% of their \\ncorpus solve concrete problems using reÔ¨Ånement and code synthesis techniques. Finally, they found that research in model \\ntransformations is heading for a more stable and grounded validation. \\nThere are other studies that attempted to classify code generation techniques. However, they did not follow a systematic \\nand repeatable process. For example, Czarnecki et al. [12] proposed a feature model providing a terminology to characterize \\nmodel transformation approaches. They distinguished two categories for M2T approaches: those that are visitor-based and \\nthose that are template-based; the latter being in line with DeÔ¨Ånition 1 . The authors found that many new approaches \\nto model-to-model transformation have been proposed recently, but relatively little experience is available to assess their \\neffectiveness in practical applications.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='to model-to-model transformation have been proposed recently, but relatively little experience is available to assess their \\neffectiveness in practical applications. \\nRose et al. [25] extended the feature model of Czarnecki et al. to focus on template-based M2T tools. Their classiÔ¨Åcation \\nis centered exclusively on tool-dependent features. Their goal is to help developers when they are faced to choose between \\ndifferent tools. This study is close to the work of Czarnecki in [12] but focuses only on a feature model for M2T. The \\ndifference with our study is that it focuses on a feature diagram and deals with tool-dependent features only. \\nThere are also other systematic reviews performed on other related topics. For example, the study in [26] performed a \\nSMS on domain-speciÔ¨Åc modeling languages (DSL). They analyzed 390 papers to portray the published literature on DSLs,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='SMS on domain-speciÔ¨Åc modeling languages (DSL). They analyzed 390 papers to portray the published literature on DSLs, \\nwhich is comparable to the size of our corpus. Also, the study in [27] presents a systematic literature review on software \\nproduct lines engineering in the context of DSLs. Similar to our study, they propose a deÔ¨Ånition for the life-cycle of language \\nproduct lines and use it to analyze how the literature has an impact this life-cycle. Another SMS recently published in \\n[28] presents a taxonomy of source code labeling. \\n3. Research methods \\nIn order to analyze the topic of TBCG, we conducted a SMS following the process deÔ¨Åned by Petersen et al. in [20] and \\nsummarized in Fig. 2 . The deÔ¨Ånition of research question is discussed in Section 3.1 . The search conduction is described \\nin Section 3.2 . We present the screening of papers in Section 3.3 . The relevant papers are obtained based on the criteria'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='in Section 3.2 . We present the screening of papers in Section 3.3 . The relevant papers are obtained based on the criteria \\npresented in Section 3.3.1 and Section 3.3.2 . The elaboration of the classiÔ¨Åcation scheme is described in Section 3.4 . Finally, \\nwe detail the selection of the papers in Section 3.5 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 4, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n47 \\nProcess Steps\\nOutcomes\\nDefinition of\\nResearch Question\\nConduct Search\\nScreening of Papers\\nKeywording using\\nAbstracts\\nData Extraction and\\nMapping Process\\nSystematic Map\\nClassification\\nScheme\\nRelevant Papers\\nAll Papers\\nReview Scope\\nFig. 2. The systematic mapping study process we followed from Peterson et al. \\n3.1. Objectives \\nThe objective of this study is to obtain an overview of the current research in the area of TBCG and to characterize the \\ndifferent approaches that have been developed. We deÔ¨Åned four research questions to set the scope of this study: \\n1. What are the trends in template-based code generation? We are interested to know how this technique has evolved over \\nthe years through research publications. \\n2. What are the characteristics of template-based code generation approaches? We want to identify major characteristics of \\nthese techniques and their tendencies.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 4, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='2. What are the characteristics of template-based code generation approaches? We want to identify major characteristics of \\nthese techniques and their tendencies. \\n3. To what extent are template-based code generation tools being used in research? We are interested in identifying popular \\ntools and their uses. \\n4. What is the place of MDE in template-based code generation? We seek to determine whether and how MDE has inÔ¨Çuenced \\nTBCG. \\n3.2. Selection of source \\nWe delimited the scope of the search to be regular publications that mention TBCG as at least one of the approaches \\nused for code generation and published between 20 0 0‚Äì2016. Therefore, this includes publications where code generation is \\nnot necessarily the main contribution. For example, Buchmann et al. [29] used TBCG to obtain ATL code while their main \\nfocus was implementing a higher-order transformation. Given that not all publications have the term ‚Äúcode generation‚Äù in'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 4, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='focus was implementing a higher-order transformation. Given that not all publications have the term ‚Äúcode generation‚Äù in \\ntheir title, we retrieved publications based on their title, abstract, or full text (when available) mentioning the keywords \\n‚Äútemplate‚Äù and its variations, ‚Äúcode‚Äù, and ‚Äúgeneration‚Äù and synonyms with their variations (e.g., ‚Äúsynthesis‚Äù). We used the \\nfollowing search query for all digital libraries, using their respective syntax: \\ntemplat ‚àóAND ‚Äúcode generat ‚àó‚Äù OR ‚Äúcode synthesi ‚àó‚Äù\\nWe validated our search with a sample of 100 pilot papers we preselected. These papers were chosen from papers we \\napriori knew should be included and resulting from a preliminary pilot search online. We iterated over different versions of \\nthe search strings until all pilot papers could be retrieved from the digital libraries. \\n3.3. Screening procedure \\nScreening is the most crucial phase in a SMS [20] . We followed a two-stage screening procedure: automatic Ô¨Åltering,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 4, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='3.3. Screening procedure \\nScreening is the most crucial phase in a SMS [20] . We followed a two-stage screening procedure: automatic Ô¨Åltering, \\nthen title and abstract screening. In order to avoid the exclusion of papers that should be part of the Ô¨Ånal corpus, we \\nfollowed a strict screening procedure. With four reviewers at our disposal, each article is screened by at least two reviewers \\nindependently. When both reviewers of a paper disagree upon the inclusion or exclusion of the paper, a physical discussion \\nis required. If the conÔ¨Çict is still unresolved, an additional senior reviewer is involved in the discussion until a consensus \\nis reached. To determine a fair exclusion process, a senior reviewer reviews a sample of no less than 20% of the excluded \\npapers at the end of the screening phase, to make sure that no potential paper is missed. \\n3.3.1. Inclusion criteria'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 4, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='papers at the end of the screening phase, to make sure that no potential paper is missed. \\n3.3.1. Inclusion criteria \\nA paper is included if it explicitly indicates the use of TBCG or if it proposes a TBCG technique. We also include papers \\nif the name of a TBCG tool appears in the title, abstract, or content. ‚ÄúUse‚Äù is taken in a large sense when it is explicit that \\na TBCG contributes to the core of the paper (not in the related work). \\n3.3.2. Exclusion criteria \\nResults from the search were Ô¨Årst Ô¨Åltered automatically to discard records that were outside the scope of this study: \\npapers not in computer science, not in the software engineering domain, with less than two pages of length (e.g., proceed-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='48 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nings preface), not peer-reviewed (e.g., white papers), not written in English, or not published between the years 20 0 0 and \\n2016. Then, papers were excluded through manual inspection based on the following criteria: \\n‚Ä¢ No code generation. There is no code generation technique used. \\n‚Ä¢ Not template-based code generation. Code generation is mentioned, but the considered technique is not template-based \\naccording to DeÔ¨Ånition 1 . \\n‚Ä¢ Not a paper. This exclusion criterion spans papers that were not caught by the automatic Ô¨Åltering. For example, some \\npapers had only the abstract written in English and the content of the paper in another language. Additionally, there \\nwere 24 papers where the full text was not accessible online. \\nFor the Ô¨Årst two criteria, when the abstract did not give enough details about the code generation approach, a quick'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='were 24 papers where the full text was not accessible online. \\nFor the Ô¨Årst two criteria, when the abstract did not give enough details about the code generation approach, a quick \\nlook at the full text helped clear any doubts on whether to exclude the paper or not. Reviewers were conservative on that \\nmatter. \\n3.4. ClassiÔ¨Åcation scheme \\nWe elaborated a classiÔ¨Åcation scheme by combining our general knowledge with the information extracted from the \\nabstracts during the screening phase. We classify all papers along different categories that are of interest in order to answer \\nour research questions. This helps analyzing the overall results and gives an overview of the trends and characteristics of \\nTBCG. The categories we classiÔ¨Åed the corpus with are the following: \\n‚Ä¢ Template style : We characterize the level of customization and expressiveness of the templates used in the code gen-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='TBCG. The categories we classiÔ¨Åed the corpus with are the following: \\n‚Ä¢ Template style : We characterize the level of customization and expressiveness of the templates used in the code gen- \\neration approach. PredeÔ¨Åned style is reserved for approaches where the template used for code generation is deÔ¨Åned \\ninternally to the tool. However, a subset of the static part of the template is customizable to vary slightly the gener- \\nated output. This is, for example, the case for common CASE tools where there is a predeÔ¨Åned template to synthesize \\na class diagram into a number of programming languages. Nevertheless, the user can specify what construct to use for \\nmany-cardinality associations, e.g., Array or ArrayList for Java templates. Output-based style covers templates that \\nare syntactically based on the actual target output. In contrast with the previous style, output-based templates offer full'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='are syntactically based on the actual target output. In contrast with the previous style, output-based templates offer full \\ncontrol on how the code is generated, both on the static and dynamic parts. The generation logic is typically encoded in \\nmeta-code as in the example of Fig. 1 . Rule-based style puts the focus of the template on computing the dynamic part \\nwith the static part being implicit. The template lists declarative production rules that are applied on-demand by the \\ntemplate engine to obtain the Ô¨Ånal target output. For example, this is used to render the concrete textual syntax from \\nthe abstract syntax of a model using a grammar. \\n‚Ä¢ Input type : We characterize the language of the design-time input that is necessary to develop templates. The run-time \\ninput is an instance that conforms to it. General-purpose modeling language is for generic languages reusable across dif-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='input is an instance that conforms to it. General-purpose modeling language is for generic languages reusable across dif- \\nferent domains that are not programming languages, such as UML. Domain-speciÔ¨Åc modeling language is for languages \\ntargeted for a particular domain, for example, where the run-time input is a Simulink model. Schema is for structured \\ndata deÔ¨Ånitions, such as XML or database schema. Programming language is for well-deÔ¨Åned programming languages, \\nwhere the run-time input is source code. \\n‚Ä¢ Output type : We characterize the output of the code generator (more than one category can be selected when multiple \\nartifacts are generated). Source code is executable code conforming to a speciÔ¨Åc programming language. Structured data \\nis for code that is not executable, such as HTML. \\n‚Ä¢ Application scale : We characterize the scale of the artifact on which the TBCG approach is applied with respect to the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='is for code that is not executable, such as HTML. \\n‚Ä¢ Application scale : We characterize the scale of the artifact on which the TBCG approach is applied with respect to the \\nevaluation or illustration cases in the paper. The presence of a formal case study that either relied on multiple data \\nsources or subjects qualiÔ¨Åed as large scale application. Small scale was used mainly when there was only one example or \\ntoy examples in the paper that illustrated the TBCG. No application if for when the code generation was not applied on \\nany example. \\n‚Ä¢ Application domain : We classify the general domain TBCG has been applied on. For example, this includes Software engi- \\nneering, Embedded systems, Compilers, Bio-medicine , etc. \\n‚Ä¢ Orientation : We distinguish industrial papers, where at least one author is aÔ¨Éliated to industry, from academic papers \\notherwise. \\n‚Ä¢ Tool : We capture the tool used for TBCG. If a tool is not clearly identiÔ¨Åed in a paper or the TBCG is programmed directly,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='otherwise. \\n‚Ä¢ Tool : We capture the tool used for TBCG. If a tool is not clearly identiÔ¨Åed in a paper or the TBCG is programmed directly, \\nwe classify the tool as unspeciÔ¨Åed . We consider a tool to be popular within the research community when it is used in \\nat least 1% of the papers. Otherwise, we classify it as other . \\n‚Ä¢ MDE : We determine whether the part of the solution where TBCG is applied in the paper follows MDE techniques and \\nprinciples. A good indication is if the design-time input is a metamodel. \\n3.5. Paper selection \\nTable 1 summarizes the Ô¨Çow of information through the selection process of this study. This section explains how we \\nobtained the Ô¨Ånal corpus of papers to classify and analyze.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 6, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n49 \\nTable 1 \\nEvolution of paper corpus during the study process. \\nPhase \\nNumber of papers \\nCollection \\nEngineering Village \\n4043 \\nScopus \\n932 \\nSpringerLink \\n2671 \\nInitial corpus \\n5131 \\nScreening \\nExcluded during screening \\n4553 \\nIncluded \\n578 \\nClassiÔ¨Åcation \\nExcluded during classiÔ¨Åcation \\n99 \\nFinal corpus \\n481 \\n3.5.1. Paper collection \\nThe paper collection step was done in two phases: querying and automatic duplicates removal. There are several \\nonline databases that index software engineering literature. For this study, we considered three main databases to maxi- \\nmize coverage: Engineering Village 5 , Scopus 6 , and SpringerLink 7 . The Ô¨Årst two cover typical software engineering editors \\n( IEEE Xplore , ACM Digital Library , Elsevier ). However, from past experiences [24] , they do not include all of Springer pub-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 6, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='( IEEE Xplore , ACM Digital Library , Elsevier ). However, from past experiences [24] , they do not include all of Springer pub- \\nlications. We used the search string from Section 3.2 to retrieve all papers from these three databases. We obtained 7 646 \\ncandidate papers that satisfy the query and the options of the search stated in Section 3.3.2 . We then removed automatically \\nall duplicates using EndNote software. This resulted in 5 131 candidate papers for the screening phase. \\n3.5.2. Screening \\nBased on the exclusion criteria stated in Section 3.3.2 , each candidate paper was screened by at least two reviewers to \\ndecide on its inclusion. To make the screening phase more eÔ¨Écient, we used a home-made tool [30] . After all the reviewers \\ncompleted screening the papers they were assigned, the tool calculates an inter-rater agreement coeÔ¨Écient. In our case, the \\nCohens Kappa coeÔ¨Écient was 0.813. This high value shows that the reviewers were in almost perfect agreement.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 6, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Cohens Kappa coeÔ¨Écient was 0.813. This high value shows that the reviewers were in almost perfect agreement. \\nAmong the initial corpus of candidate papers, 4556 were excluded, 551 were included and 24 received conÔ¨Çicting ratings. \\nDuring the screening, the senior reviewer systematically veriÔ¨Åed each set of 100 rejected papers for sanity check. A total \\nof 7 more papers were included back hence the rejected papers were reduced to 4549. Almost all cases of conÔ¨Çicts were \\nabout a disagreement on whether the code generation technique of a paper was using templates or not. These conÔ¨Çicts were \\nresolved in physical meetings and 20 of them were Ô¨Ånally included for a total of 578 papers and 4553 excluded. \\nAmong the excluded papers, 52% were rejected because no code generation was used. We were expecting such a high \\nrate because terms such as ‚Äútemplates‚Äù are used in many other Ô¨Åelds, like biometrics. Also, many of these papers were'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 6, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='rate because terms such as ‚Äútemplates‚Äù are used in many other Ô¨Åelds, like biometrics. Also, many of these papers were \\nreferring to the C++ standard template library [31] , which is not about code generation. We counted 34% papers excluded \\nbecause they were not using templates . Examples of such papers are cited in Section 2.2 . Also, more than a quarter of the \\npapers were in the compilers or embedded system domains, where code generation is programmed imperatively rather than \\ndeclaratively speciÔ¨Åed using a template mechanism. Finally, 5% of the papers were considered as not a paper . In fact, this \\ncriterion was in place to catch papers that escaped the automatic Ô¨Åltering from the databases. \\n3.5.3. Eligibility during classiÔ¨Åcation \\nOnce the screening phase over, we thoroughly analyzed the full text of the remaining 578 papers to classify them \\naccording to our classiÔ¨Åcation scheme. Doing so allowed us to conÔ¨Årm that the code generation approach was effectively'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 6, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='according to our classiÔ¨Åcation scheme. Doing so allowed us to conÔ¨Årm that the code generation approach was effectively \\ntemplate-based according to DeÔ¨Ånition 1 . We encountered papers that used multiple TBCG tools: they either compared tools \\nor adopted different tools for different tasks. We classiÔ¨Åed each of these papers as a single publication, but incremented the \\noccurrence corresponding to the tools referred to in the paper. This is the case of [32] where the authors use Velocity and \\nXSLT for code generation. Velocity generates Java and SQL code, while XSLT generates the control code. \\nWe excluded 99 additional papers. During screening, we detected situations where the abstract suggested the imple- \\nmentation of TBCG, whereas the full text proved otherwise. In most of the cases, the meaning of TBCG differed from the \\ndescription presented in Section 2.3 . As shown in [33] the terms template-based and generation are used in the context of'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 6, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='description presented in Section 2.3 . As shown in [33] the terms template-based and generation are used in the context of \\nnetworking and distributed systems. We also encountered circumstances where the tool mentioned in the abstract requires \\nthe explicit use of another component to be considered as TBCG, such as Simulink TLC, as in [34] . \\nThe Ô¨Ånal corpus 8 considered for this study contains 481 papers. \\n5 https://www.engineeringvillage.com/ \\n6 https://www.scopus.com/ \\n7 http://link.springer.com/ \\n8 The complete list of papers is available online at http://www-ens.iro.umontreal.ca/ ‚àºluhunukl/survey/classiÔ¨Åcation.html'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 7, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='50 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\n# of papers\\nFig. 3. Evolution of papers in the corpus. \\nTable 2 \\nMost popular venues. \\nVenue \\nVenue & publication type \\n# Papers \\nModel Driven Engineering Languages and Systems ( Models ) \\nMDE \\nConference \\n26 \\nSoftware and Systems Modeling ( Sosym ) \\nMDE \\nJournal \\n26 \\nEuropean Conference on Modeling Foundations and Applications ( Ecmfa ) \\nMDE \\nConference \\n19 \\nGenerative and Transformational Techniques in Software Engineering ( Gttse ) \\nSoft. eng. \\nConference \\n11 \\nGenerative Programming: Concepts & Experience ( Gpce ) \\nSoft. eng. \\nConference \\n8 \\nInternational Conference on Computational Science and Applications ( Iccsa ) \\nOther \\nConference \\n8 \\nSoftware Language Engineering ( Sle ) \\nMDE \\nConference \\n7 \\nLeveraging Applications of Formal Methods, VeriÔ¨Åcation and Validation ( Isola ) \\nOther \\nConference \\n7 \\nAutomated Software Engineering ( Ase )'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 7, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='MDE \\nConference \\n7 \\nLeveraging Applications of Formal Methods, VeriÔ¨Åcation and Validation ( Isola ) \\nOther \\nConference \\n7 \\nAutomated Software Engineering ( Ase ) \\nSoft. eng. \\nJournal + Conference \\n5 + 2 \\nInternational Conference on Web Engineering ( Icwe ) \\nOther \\nConference \\n6 \\nEvaluation of Novel Approaches to Software Engineering ( Enase ) \\nSoft. eng. \\nConference \\n5 \\n4. Evolution of TBCG \\nWe start with a thorough analysis of the trends in TBCG in order to answer the Ô¨Årst research question. \\n4.1. General trend \\nFigure 3 reports the number of papers per year, averaging around 28. The general trend indicates that the number of \\npublications with at least one TBCG method started increasing in 2002 to reach a Ô¨Årst local maximum in 2005 and then \\nremained relatively constant until 2012. This increase coincides with the early stages of MDE and the Ô¨Årst edition of the \\nModels conference, previous called Uml conference. This is a typical trend where a research community gets carried away'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 7, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Models conference, previous called Uml conference. This is a typical trend where a research community gets carried away \\nby the enthusiasm of a new potentially interesting domain, which leads to more publications. However, the most proliÔ¨Åc \\nperiod was in 2013, where we notice a signiÔ¨Åcant peak with 2.4 times the average numbers of publications observed in the \\nprevious years. Fig. 3 then shows sudden a decrease in 2015. \\nResorting to statistical methods, the high coeÔ¨Écient of variability and modiÔ¨Åed Thompson Tau test indicate that 2013 \\nand 2015 are outliers in the range 2005‚Äì2016, where the average is 37 papers per year. The sudden isolated peak in 2013 \\nis the result of a special event or popularity of TBCG. The following decrease in the amount of papers published should not \\nbe interpreted as a decline in interest in TBCG, but that some event happened around 2013 which boosted publications,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 7, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='be interpreted as a decline in interest in TBCG, but that some event happened around 2013 which boosted publications, \\nand then it went back to the steady rate of publication as previous years. In fact, 2016 is one standard deviation above the \\naverage. \\n4.2. Publications and venues \\nWe analyzed the papers based on the type of publication and the venue of their publication. MDE venues account for \\nonly 22% of the publications, so are software engineering venues, while the majority (56%) were published in other venues. \\nTable 2 shows the most popular venues that have at least Ô¨Åve papers. These top venues account for just more than a quarter \\nof the total number of publications. Among them, MDE venues account for 60% of the papers. Models , Sosym , and Ecmfa \\nare the three most popular venues 9 with a total of 66 publications between them. This is very signiÔ¨Åcant given that the \\n9 We grouped Uml conference with Models and Ecmda-fa with Ecmfa .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 8, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n51 \\nTable 3 \\nDistribution of the template style facet. \\nOutput-based \\nPredeÔ¨Åned \\nRule-based \\n72% \\n24% \\n4% \\n0\\n10\\n20\\n30\\n40\\n50\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nOutput-based\\nPredefined\\nRule-based\\n# of papers\\nFig. 4. Template style evolution. \\naverage is only 1.67 paper per venue with a standard deviation of 2.63. Also, 43% of venues had only one paper using TBCG, \\nwhich is the case for most of the other venues. \\nThe peak in 2013 was mainly inÔ¨Çuenced by MDE and software engineering venues. However the drop in 2015 is the result \\nof an accumulation of the small variations among the other venues. Since 2014, MDE venues account for 10‚Äì12 papers per \\nyear, while only 6‚Äì7 in software engineering. \\nAs for the publication type, conference publications have been dominating at 64%. Journal article account for 24% of all'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 8, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='year, while only 6‚Äì7 in software engineering. \\nAs for the publication type, conference publications have been dominating at 64%. Journal article account for 24% of all \\npapers. The remaining papers were published as book chapters, workshops or other publication type. Interestingly, we notice \\na steady increase in journal articles, reaching a maximum of 15 in 2016. \\n5. Characteristics of template-based code generation \\nWe examine the characteristics of TBCG using the classiÔ¨Åcation scheme presented in Section 3.4 . \\n5.1. Template style \\nAs Table 3 illustrates, the vast majority of the publications follow the output-based style. This consists of papers like [35] , \\nwhere Xpand is used to generate workÔ¨Çow code used to automate modeling tools. There, it is the Ô¨Ånal output target text \\nthat drives the development of the template. This high score is expected since output-based style is the original template'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 8, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='that drives the development of the template. This high score is expected since output-based style is the original template \\nstyle for TBCG as depicted in Fig. 4 . This style has always been the most popular style since 20 0 0. \\nThe predeÔ¨Åned style is the second most popular. Most of these papers generate code using a CASE tool, such as [36] that \\nuses Rhapsody to generate code to map UML2 semantics to Java code with respect to association ends. Apart from CASE \\ntools, we also classiÔ¨Åed papers like [37] as predeÔ¨Åned style since the output code is already Ô¨Åxed as HTML and the pro- \\ngrammer uses the tags to change some values based on the model. Each year, around 28% of the papers were using the \\npredeÔ¨Åned style, except for a peak of 39% in 2005, given the popularity of CASE tools then. \\nWe found 19 publications that used rule-based style templates. This includes papers like [38] which generates Java code'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 8, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='We found 19 publications that used rule-based style templates. This includes papers like [38] which generates Java code \\nwith Stratego from a DSL. A possible explanation of such a low score is that this is the most diÔ¨Écult template style to \\nimplement. It had a maximum of two papers per year throughout the study period. \\n5.2. Input type \\nGeneral purpose languages account for almost half of the design-time input of the publications, as depicted in Table 4 . \\nUML (class) diagrams, which are used as metamodels for code generation, are the most used for 87% of these papers as in \\n[35] . Other popular general-purpose languages that were used are, for example, the architecture analysis and design lan- \\nguage [39] and feature diagrams [40] . The schema category comes second with 21% of the papers. For example, a database \\nTable 4 \\nDistribution of the input type facet. \\nGeneral purpose \\nSchema \\nDomain speciÔ¨Åc \\nProgramming language \\n48% \\n22% \\n20% \\n10%'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 9, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='52 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nGeneral purpose\\nSchema\\nDomain Specific\\nProgramming Language\\n# of papers\\nFig. 5. Evolution of the design-time input type. \\nschema is used as input at design-time in [41] to generate Java for a system that demonstrates that template can im- \\nprove software development. Also, an XML schema is used in [42] as design-time input to produce C programs in order to \\nimplement an approach that can eÔ¨Éciently support all the conÔ¨Åguration options of an application in embedded systems. \\nDSLs are almost at par with schema. They have been gaining popularity and gradually reducing the gap with general-purpose \\nlanguages. For example in [43] , a custom language is given as the design input in order to generate C and C++ to develop \\na TBCG approach dedicated to real-time systems. The least popular design-time input type is programming language . This'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 9, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='a TBCG approach dedicated to real-time systems. The least popular design-time input type is programming language . This \\nincludes papers like [44] where T4 is used to generate hardware description (VHDL) code for conÔ¨Ågurable hardware. In this \\ncase, the input is another program on which the template depends. \\nOver the years, the general-purpose category has dominated the input type facet, as depicted in Fig. 5 . 2003 and 2006 \\nwere the only exceptions where schema obtained slightly more publications. We also notice a shift from schema to domain- \\nspeciÔ¨Åc design-time input types. Domain-speciÔ¨Åc input started increasing in 2009 but never reached the same level as \\ngeneral purpose. Programming language input maintained a constant level, with an average of 1% per year. Interestingly, in \\n2011, there were more programming languages used than DSLs. \\n5.3. Output type \\nAn overwhelming majority of the papers use TBCG to generate source code (81%), in contrast with 19% was structured'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 9, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='2011, there were more programming languages used than DSLs. \\n5.3. Output type \\nAn overwhelming majority of the papers use TBCG to generate source code (81%), in contrast with 19% was structured \\ndata (though some papers output both types). Table 5 shows the distribution of the output languages that appeared in more \\nthan Ô¨Åve papers, representing 74% of the corpus. This includes papers like [45] where Java code is generated an adaptable \\naccess control tool for electronic medical records. Java and C are the most targeted programming languages. Writing a \\nprogram manually often requires proved abilities, especially with system and hardware languages, such as VHDL [46] . This \\nis why 8% of all papers generate low level source codes. Generation of structured data includes TBCG of mainly XML and \\nHTML Ô¨Åles. For example [47] produces both HTML and XML as parts of the web component to ease regression testing. In'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 9, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='HTML Ô¨Åles. For example [47] produces both HTML and XML as parts of the web component to ease regression testing. In \\naddition, we found that around 4% of the papers generate combinations of at least two output types. This includes papers \\nsuch as [48] that generate both C# and HTML from a domain-speciÔ¨Åc model. \\nStructured data output remained constant over the years, unlike source code which follows the general trend. \\n5.4. Application scale \\nAs depicted in Table 6 , most papers applied TBCG on large scale examples. This result indicates that TBCG is a technique \\nwhich scales with larger amounts of data. This includes papers like [49] that uses Acceleo to generate hundreds of lines \\nTable 5 \\nDistribution of the most popular output languages. \\nJava \\nC \\nHTML \\nC ++ \\nC# \\nXML \\nVHDL \\nSQL \\nAspectJ \\nSystemC \\n33% \\n11% \\n7% \\n7% \\n4% \\n3% \\n3% \\n2% \\n2% \\n2% \\nTable 6 \\nDistribution of application scale facet \\nLarge scale \\nSmall scale \\nNo application \\n63% \\n32% \\n5%'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 10, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n53 \\n0\\n10\\n20\\n30\\n40\\n50\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nLarge scale\\nSmall scale\\nNo application\\n# of papers\\nFig. 6. Application scale evolution. \\nFig. 7. Distribution of application domain facet. \\nof aspect-oriented programming code. Small scale obtains 32% of the papers. This is commonly found in research papers \\nthat only need a small and simple example to illustrate their solution. This is the case in [50] in which a small concocted \\nexample shows the generation process with the Epsilon Generation Language (EGL) 10 . No application was used in 5% of the \\npublications. This includes papers like [51] where authors just mention that code synthesis is performed using a tool named \\nMako-template. Even though the number of publications without an actual application is very low, this demonstrates that'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 10, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Mako-template. Even though the number of publications without an actual application is very low, this demonstrates that \\nsome authors have still not adopted good practice to show an example of the implementation. This is important, especially \\nwhen the TBCG approach is performed with a newly developed tool. \\nAs depicted in Fig. 6 , most papers illustrated their TBCG using large scale applications up to 2015. In this period, while \\nit follows the general trend of papers, the other two categories remained constant over the years. However, in 2016, we \\nobserve a major increase of small scale for TBCG. \\n5.5. Application domain \\nThe tree map in Fig. 7 highlights the fact that TBCG is used in many different areas. Software engineering obtains more \\nthan half of the papers with 55% of the publications. We have grouped in this category other related areas like ontolo- \\ngies, information systems or software product lines. This is expected given that the goal of TBCG is to synthesize software'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 10, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='gies, information systems or software product lines. This is expected given that the goal of TBCG is to synthesize software \\napplications. For example, the work in [52] uses the Rational CASE tool to generate Java programs in order to implement an \\napproach that transforms UML state machine to behavioral code. The next category is embedded systems which obtains 13% \\nof papers. Embedded systems often require low level hardware code diÔ¨Écult to write. Some even consider code generation \\nto VHDL as a compilation rather than automatic programming. In this category, we found papers like [53] in which Velocity \\nis used to produce Verilog code to increase the speed of simulation. Web technology related application domains account for \\n8% of the papers. It consists of papers like [54] where the authors worked to enhance the development dynamic web sites. \\nNetworking obtains 4% of the papers, such as [55] where code is generated for a telephony service network. Compiler obtains'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 10, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Networking obtains 4% of the papers, such as [55] where code is generated for a telephony service network. Compiler obtains \\n1% of the papers, such as [56] where a C code is generated and optimized for an Intel C compiler. It is interesting to note \\nthat several papers were applied in domains such as bio-medicine [57] , artiÔ¨Åcial intelligence [58] , and graphics [59] . \\n10 http://www.eclipse.org/epsilon/doc/egl/'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 11, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='54 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nWe combined application domains with a single paper into the other category. This regroups domains such as agronomy, \\neducation, and Ô¨Ånance. It is important to mention that the domain discussed in this category corresponds to the domain of \\napplication of TBCG employed, which differs from the publication venue. \\n5.6. Orientation \\nA quarter (24%) of the papers in the corpus are authored by a researcher from industry . The remaining 76% are written \\nonly by academics . This is a typical distribution since industrials tend to not publish their work. This result shows that TBCG \\nis used in industry as in [16] . Industry oriented papers have gradually increased since 2003 until they reached a peak in \\n2013. \\n6. Relations between characteristics \\nTo further characterize the trends observed in Section 5 , we identiÔ¨Åed signiÔ¨Åcant and interesting relations between the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 11, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='2013. \\n6. Relations between characteristics \\nTo further characterize the trends observed in Section 5 , we identiÔ¨Åed signiÔ¨Åcant and interesting relations between the \\ndifferent facets of the classiÔ¨Åcation scheme, using SPSS. \\n6.1. Statistical correlations \\nA Shapiro‚ÄìWilk test of each category determined that the none of them are normally distributed. Therefore, we opted \\nfor the Spearman two-tailed test of non-parametric correlations with a signiÔ¨Åcance value of 0.05 to identify correlations \\nbetween the trends of each category. The only signiÔ¨Åcantly strong correlations we found statistically are between the two \\ninput types, and between MDE and input type. \\nWith no surprise, the correlation between run-time and design time input is the strongest among all, with a correlation \\ncoeÔ¨Écient of 0.944 and a p- value of less than 0.001. This concurs with the results found in Section 5.2 . An example is'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 11, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='coeÔ¨Écient of 0.944 and a p- value of less than 0.001. This concurs with the results found in Section 5.2 . An example is \\nwhen the design-time input is UML, the run-time input is always a UML diagram as in [57] . Such a strong relationship is \\nalso noticeable in [60] with programming languages and source code, as well as in [58] when a schema design is used for \\nstructured data. As a result, all run-time input categories are correlated to the same categories as for design-time input. We \\nwill therefore treat these two facets together as input type . \\nThere is a strong correlation of coeÔ¨Écient of 0.738 and a p-value of less than 0.001 between input type and MDE . As \\nexpected, more than 90% of the papers using general purpose and domain speciÔ¨Åc inputs are follow the MDE approach. \\n6.2. Other interesting relations \\nWe also found weak but statistically signiÔ¨Åcant correlations between the remaining facets. We discuss the result here. \\n6.2.1. Template style'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 11, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='6.2. Other interesting relations \\nWe also found weak but statistically signiÔ¨Åcant correlations between the remaining facets. We discuss the result here. \\n6.2.1. Template style \\nFigure 8 shows the relationship between template style, design-time input, and output types. We found that for \\nthe predeÔ¨Åned templates, there are twice as many papers that use schema input than domain speciÔ¨Åc. However, for \\n168\\n72\\n79\\n26\\n62\\n13\\n24\\n18\\n13\\n3\\n3\\nOutput-based\\nPredefined\\nRule-based\\nGeneral \\npurpose\\nDomain \\nspecific\\nSchema\\nProg. \\nlanguage\\nSource \\ncode\\nStructured \\ndata\\nDesign-time input type\\nOutput type\\nFig. 8. Relation between template style (vertical) and input/output types (horizontal).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 12, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n55 \\nSource \\ncode\\nStructured \\ndata\\nGeneral \\npurpose\\nDomain \\nspecific\\nSchema\\nProg. \\nlanguage\\nFig. 9. Relation between output (vertical) and design-time input (horizontal) types showing the number of papers in each intersection. \\nTable 7 \\nDistribution of the tool facet. \\nNamed MDE \\nUnspeciÔ¨Åed \\nOther \\nNamed not MDE \\n41% \\n28% \\n23% \\n8% \\noutput-based, domain speciÔ¨Åc inputs are used slightly more often. We also notice that general purpose input is never used \\nwith rule-based templates. The output type follows the same general distribution regardless of the template style. \\nAll rule-based style approaches have included a sample application. Meanwhile, the proportion of small scale was twice \\nmore important for predeÔ¨Åned templates (51%) then for output-based (27%). \\nWe found that popular tools were used twice as often on output-based templates (58%) than on predeÔ¨Åned templates'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 12, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='more important for predeÔ¨Åned templates (51%) then for output-based (27%). \\nWe found that popular tools were used twice as often on output-based templates (58%) than on predeÔ¨Åned templates \\n(23%). Rule-based templates never employed a tool that satisÔ¨Åed our popularity threshold, but used other tools such as \\nStratego. \\nWe found that all papers using a rule-based style template do not follow an MDE approach. On the contrary, 70% of the \\noutput-based style papers and 56% of the predeÔ¨Åned ones follow an MDE approach. \\nFinally, we found that for each template style, the number of papers authored by an industry researcher Ô¨Çuctuated \\nbetween 22‚Äì30%. \\n6.2.2. Input type \\nThe bubble chart in Fig. 9 illustrates the tendencies between input and output types. It is clear that source code is \\nthe dominant generated artifact regardless of the input type. Source code is more often generated from general purpose'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 12, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='the dominant generated artifact regardless of the input type. Source code is more often generated from general purpose \\nand domain speciÔ¨Åc inputs than from schema and programming languages. Also, the largest portion of structured data is \\ngenerated from a schema input. \\nMoving on to input type and application scale, we found that small scales are used 40% of the time when the input is a \\nprogramming language. The number of papers with no sample application is very low (5%) regardless of the template style. \\nFinally, 74% of papers using large scale applications use a domain speciÔ¨Åc input, which is slightly higher than those using a \\ngeneral purpose input with 71%. \\n6.2.3. Output type, application scale, and orientation \\nAs we compared output type to orientation, we found that industrials generate slightly more source code than academics: \\n89% vs. 80%. However, academics generate more structured data than industrials: 18% vs. 6% and 3% vs. 1%., respectively. We'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 12, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='89% vs. 80%. However, academics generate more structured data than industrials: 18% vs. 6% and 3% vs. 1%., respectively. We \\nfound that 65% of the papers without application are from the academy. \\n7. Template-based code generation tools \\nTable 7 shows that half of the papers used a popular TBCG tool ( named ). The other half used less popular tools (the other \\ncategory), did not mention any TBCG tool, or implemented the code generation directly for the purpose of the paper. \\n7.1. Popular tools in research \\nFigure 10 shows the distribution of popular tools used in at least 1% of the papers, i.e., Ô¨Åve papers. We see that only \\n6/14 popular tools follow MDE approaches. Acceleo and Xpand are the most popular with respectively 16% and 15% of the \\npapers using them. Their popularity is probably due to their simple syntax and ease of use [61] and the fact that they are \\nMDE tools [16] . They both have an OCL-like language for the dynamic part and rely on a metamodel speciÔ¨Åed in Ecore as'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 12, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='MDE tools [16] . They both have an OCL-like language for the dynamic part and rely on a metamodel speciÔ¨Åed in Ecore as \\ndesign-time input.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 13, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='56 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nFig. 10. Popular tools. \\nEGL also has a structure similar to the other model-based tools. It is natively integrated with languages from the Epsilon \\nfamily, thus relies on the Epsilon Object Language for its dynamic part. MOFScript is another popular model-based tool that \\nonly differs in syntax from the others. Xtend2 is the least used popular model-based tool. It is both an advanced form of \\nXpand and a simpliÔ¨Åed syntactical version of Java. \\nXSLT is the third most popular tool used. It is suitable for XML documents only. Some use it for models represented in \\ntheir XMI format, as it is the case in [62] . XSLT follows the template and Ô¨Åltering strategy. It matches each tag of the input \\ndocument and applies the corresponding template. \\nJET [63] and Velocity [53] are used as often as each other on top of being quite similar. The main difference is that JET'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 13, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='document and applies the corresponding template. \\nJET [63] and Velocity [53] are used as often as each other on top of being quite similar. The main difference is that JET \\nuses an underlying programming language (Java) for the dynamic part. In JET, templates are used to developers generate the \\nJava code speciÔ¨Åc for the synthesis of code to help developers implement the code generation. \\nStringTemplate [64] has its own template structure. It can be embedded into a Java code where strings to be output are \\ndeÔ¨Åned using templates. Note that all the tools mentioned above use an output-based template style. \\nThe most popular CASE tools for TBCG are Fujaba [65] , Rational [66] , and Rhapsody [67] . One of the features they offer is \\nto generate different target languages from individual UML elements. All CASE tools (even counting the other category) have \\nbeen used in a total of 39 papers, which puts them at par with Xpand. CASE tools are mostly popular for design activities;'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 13, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='been used in a total of 39 papers, which puts them at par with Xpand. CASE tools are mostly popular for design activities; \\ncode generation is only one of their many features. CASE tools have a predeÔ¨Åned template style. \\nSimulink TLC is the only rule-based tool among the most popular ones. As a rule-based approach, it has a different \\nstructure compared to the above mentioned tools. Its main difference is that the developer writes the directives to be \\nfollowed by Simulink in order to render the Ô¨Ånal C code from S-functions. \\nWe notice that the most popular tools are evenly distributed between model-based tools (Acceleo, Xpand) and code- \\nbased tools (JET, XSLT). Surprisingly, XSLT, which has been around the longest, is less popular than Xpand. This is undoubt- \\nedly explained by the advantages that MDE has to offer [7,8] . \\n7.2. UnspeciÔ¨Åed and other tools \\nAs depicted in Table 7 , 28% of the papers did not specify the tool that was used, as in [68] where the authors introduce'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 13, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='7.2. UnspeciÔ¨Åed and other tools \\nAs depicted in Table 7 , 28% of the papers did not specify the tool that was used, as in [68] where the authors introduce \\nthe concept of a meta-framework to resolve issues involved in extending the life of applications. Furthermore, 23% of the \\npapers used less popular tools, present in less than Ô¨Åve papers, such as T4 [44] and Cheetah [56] , a python powered template \\nmainly used for web developing. Like JET, Cheetah templates generate Python classes, while T4 is integrated with .NET \\ntechnology. Some CASE tools were also in this category, such as AndroMDA [69] . Other examples of less popular tools are \\nGroovy template [47] , Meta-Aspect-J [70] , and Jinja2 [71] . The fact that new or less popular tools are still abundantly used \\nsuggests that research in TBCG is still active with new tools being developed or evolved. \\n7.3. Trends of tools used'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 13, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='suggests that research in TBCG is still active with new tools being developed or evolved. \\n7.3. Trends of tools used \\nEach one of these tools had a different evolution over the years. UnspeciÔ¨Åed tools were prevalent before 2004 and then \\nkept a constant rate of usage until a drop since 2014. We notice a similar trend for CASE tools that were the most popular \\nin 2005 before decreasing until 2009. They only appear in at most three papers per year after 2010. The use of the most \\npopular tool, Xpand, gradually increased since 2005 to reach the peak in 2013 before decreasing. The other category main- \\ntained an increasing trend until 2014. Yet, a few other popular tools appeared later on. For example, EGL started appearing \\nin 2008 and had its peak in 2013. Acceleo appeared a year later and was the most popular TBCG tool in 2013‚Äì2014. Finally,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n57 \\nMOFScript had no more than a paper per year since 2005. StringTemplate and T4 were used scarcely since 2006 and 2009, \\nrespectively. \\n7.4. Characteristics of tools \\nWe have also analyzed each popular tool with respect to the characteristics presented in Section 5 . As mentioned earlier, \\nmost of the popular tools implement output-based template technique except the CASE tools which are designed following \\nthe predeÔ¨Åned style. \\nTools such as Acceleo, Xpand, EGL, MOFScript and 97% of the CASE tools papers are only used based on an MDE ap- \\nproach, given that they were created by this community. Nevertheless, there are tools that were never used with MDE \\nprinciples, like JET and Cheetah. Such tools can handle a program code or a schema as metamodel but have no internal \\nsupport for modeling languages. Moreover, the programmer has to write his own stream reader to parse the input, but they'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='support for modeling languages. Moreover, the programmer has to write his own stream reader to parse the input, but they \\nallow for a broader range of artifacts as inputs that do not have to be modeled explicitly. A few code-based tools provide \\ninternal support for model-based approaches. For instance, Velocity, XSLT, and StringTemplate can handle both UML and pro- \\ngrammed metamodel as design-time input. T4 can be integrated with MDE artifacts (e.g., generate code based on a domain- \\nspeciÔ¨Åc language in Visual studio). However, all four papers in the corpus that implemented TBCG in T4 did not use an MDE \\napproach. \\nA surprising result we found is that EGL is the only MDE tool that has its papers mostly published in MDE venues like \\nSosym , Models , and Ecmfa . All the other tools are mostly published in other venues like Icssa , whereas software engineering \\nvenues, like Ase or Icse , and MDE venues account for 26‚Äì33% of the papers for each of the rest of the MDE tools.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='venues, like Ase or Icse , and MDE venues account for 26‚Äì33% of the papers for each of the rest of the MDE tools. \\nCASE tools, MOFScript, Velocity, and Simulink TLC mostly generate program code. The latter is always used in the domain \\nof embedded systems. Papers that use StringTemplate do not include any validation process, so is Velocity in 93% of the \\npapers using it. XSLT has been only used to generate structured data as anticipated. \\nOther tools are the most used TBCG in the industry. This is because the tool is often internal to the company [72] . Among \\nthe most popular tools, Xpand is the most in the industry. \\n7.5. Application scale \\nBetween application scale and tools, we found that 74% of the papers that make use of a popular tool used large scale \\napplication to illustrate their approach. Also, 62% of the papers using unpopular tools 11 use large scale applications. Small \\nscale is likely to be used in unpopular tools rather than popular tools. \\n7.6. Tool support'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='scale is likely to be used in unpopular tools rather than popular tools. \\n7.6. Tool support \\nIn total, we found 82 different tools named in the corpus. Among them 54% are no longer supported (i.e., no release, \\nupdate or commit since the past two years). Interestingly, 55% of the tools have been developed by the industry. 70% of \\nthese industry tools are still supported, in contrast with 51% for the academic tools. As one would expect, the tendency \\nshows that tools that are frequently mentioned in research papers are still supported and are developed by industry. \\n8. MDE and template-based code generation \\nOverall, 64% of the publications followed MDE techniques and principles. For example in [73] , the authors propose a sim- \\nulation environment with an architecture that aims at integrating tools for modeling, simulation, analysis, and collaboration. \\nAs expected, most of the publications using output-based and predeÔ¨Åned techniques are classiÔ¨Åed as model-based papers.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='As expected, most of the publications using output-based and predeÔ¨Åned techniques are classiÔ¨Åed as model-based papers. \\nThe remaining 36% of the publications did not use MDE. This includes all papers that use a rule-based template style as \\nreported in Section 6 . For example, the authors in [40] developed a system that handles the implementation of dependable \\napplications and offers a better certiÔ¨Åcation process for the fault-tolerance mechanisms. \\nAs Fig. 11 shows, the evolution of the MDE category reveals that MDE-based approach started over passing non MDE- \\nbased techniques in 2005, except for 2006. It increased to reach a peak in 2013 and then started decreasing as the general \\ntrend of the corpus. Overall, MDE-based techniques for TBCG have been dominating other techniques in the past 12 years. \\nWe also analyzed the classiÔ¨Åcation of only MDE papers with respect to the characteristics presented in Section 3 . We'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='We also analyzed the classiÔ¨Åcation of only MDE papers with respect to the characteristics presented in Section 3 . We \\nonly focus here on facets with different results compared to the general trend of papers. We found that only half of the \\ntotal number of papers using unspeciÔ¨Åed and other tools are MDE-based papers. We only found one paper that uses a \\nprogramming language as design-time input with MDE [74] . This analysis also shows that the year 2005 clearly marked the \\nshift from schema to domain-speciÔ¨Åc design-time inputs, as witnessed in Section 5.2 . Thus after general purpose, which \\nobtains 69% of the publications, domain speciÔ¨Åc accounts for a better score of 26%, while schema obtains only 4%. With \\nrespect to the run-time category, the use of domain-speciÔ¨Åc models increased to reach a peak in 2013. As expected, no \\nprogram code is used for MDE papers, because MDE typically does not consider them as models, unless a metamodel of the \\nprogramming language is used.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='program code is used for MDE papers, because MDE typically does not consider them as models, unless a metamodel of the \\nprogramming language is used. \\n11 Refers to the union of other and unspeciÔ¨Åed categories of the tool facet.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 15, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='58 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nUsing MDE\\nNot using MDE\\n# of papers\\nFig. 11. Evolution of the MDE facet. \\nInterestingly, MDE venues are only the second most popular after other venues for MDE approaches. Finally, MDE journal \\npapers maintained a linear increase over the years, while MDE conference papers had a heterogeneous evolution similar to \\nthe general trend of papers. \\n9. Discussion \\n9.1. RQ1: What are the trends in TBCG? \\nThe statistical results from this signiÔ¨Åcantly large sample of papers clearly suggest that TBCG has received suÔ¨Écient \\nattention from the research community. The community has maintained a production rate in-line with the last 11 years \\naverage, especially with a constant rate of appearance in journal articles. The only exceptions were a signiÔ¨Åcant boost in'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 15, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='average, especially with a constant rate of appearance in journal articles. The only exceptions were a signiÔ¨Åcant boost in \\n2013 and a dip in 2015. The lack of retention of papers appearing in non MDE may indicate that TBCG is now applied \\nin development projects rather than being a critical research problem to solve. Also, conference papers as well as venues \\noutside MDE and software engineering had a signiÔ¨Åcant impact on the evolution of TBCG. Given that TBCG seems to have \\nreached a steady publication rate since 2005, we can expect contributions from the research community to continue in that \\ntrend. \\n9.2. RQ2: What are the characteristics of TBCG approaches? \\nOur classiÔ¨Åcation scheme constitutes the main source to answer this question. The results clearly indicate the preferences \\nthe research community has regarding TBCG. Output-based templates have always been the most popular style from the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 15, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='the research community has regarding TBCG. Output-based templates have always been the most popular style from the \\nbeginning. Nevertheless, there have been some attempts to propose other template styles, like the rule-based style, but they \\ndid not catch on. Because of its simplicity to use, the predeÔ¨Åned style is probably still popular in practice, but it is less \\nmentioned in research papers. TBCG has been used to synthesize a variety of application code or documents. As expected, \\nthe study shows that modeling language inputs have prevailed over any other type. SpeciÔ¨Åcally for MDE approaches to TBCG, \\nthe input to transform is moving from general purpose to domain-speciÔ¨Åc models. Academic researchers have contributed \\nmost, as expected with a literature review, but we found that industry is actively and continuously using TBCG as well. The \\nstudy also shows that the community is moving from large-scale applications to smaller-sized examples in research papers.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 15, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='study also shows that the community is moving from large-scale applications to smaller-sized examples in research papers. \\nThis concurs with the level of maturity of this synthesis approach. The study conÔ¨Årms that the community uses TBCG to \\ngenerate mainly source code. This trend is set to continue since the automation of computerized tasks is continuing to gain \\nground in all Ô¨Åelds. Finally, TBCG has been implemented in many domains, software engineering and embedded systems \\nbeing the most popular, but also unexpectedly in unrelated domains, such as bio-medicine and Ô¨Ånance. \\n9.3. RQ3: To what extent are TBCG tools being used in research? \\nIn this study, we discovered a total of 82 different tools for TBCG that are mentioned in research papers. Many studies \\nimplemented code generation with a custom-made tool that was never or seldom reused. This indicates that the develop-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 15, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='implemented code generation with a custom-made tool that was never or seldom reused. This indicates that the develop- \\nment of new tools is still very active. MDE tools are the most popular. Since the research community has favored output- \\nbased template style, this has particularly inÔ¨Çuenced the tools implementation. This template style allows for more Ô¨Åne- \\ngrained customization of the synthesis logic which seems to be what users have favored. This particular aspect is also \\ninÔ¨Çuencing the expansion of TBCG into industry. Most popular tool are actively supported by industry. Well-known tools \\nlike Acceleo, Xpand and Velocity are moving from being simple research material to effective development resources in in- \\ndustry. Finally, the study There are many TBCG tools that are popular in industry that fall under the ‚ÄúOther tools‚Äù category \\nbecause they are rarely reported in the scientiÔ¨Åc literature (under 1% of the papers in our corpus). Since this study is a lit-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 15, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='because they are rarely reported in the scientiÔ¨Åc literature (under 1% of the papers in our corpus). Since this study is a lit- \\nerature review, the presence of a tool in this study is biased towards the what is published, and may not reÔ¨Çect the reality \\nin industry. This is a common threat of SMS.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n59 \\n9.4. RQ4: What is the place of MDE in TBCG? \\nAll this analysis clearly concludes that the advent of MDE has been driving TBCG research. In fact, MDE has led to \\nincrease the average number of publications by a factor of four. There are many advantages to code generation, such as \\nreduced development effort, easier to write and understand domain/application concepts and less error-prone [8] . These are, \\nin fact, the pillar principles of MDE and domain-speciÔ¨Åc modeling [2] . Thus, it is not surprising to see that many, though \\nnot exclusively, code generation tools came out from the MDE community. As TBCG became a commonplace in general, \\nthe research in this area is now mostly conducted by the MDE community. Furthermore, MDE has brought very popular \\ntools that have encountered a great success, and they are also contributing to the expansion of TBCG across industry. It'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='tools that have encountered a great success, and they are also contributing to the expansion of TBCG across industry. It \\nis important to mention that the MDE community publishes in speciÔ¨Åc venues like Models , Sosym , or Ecmfa unlike other \\nresearch communities where the venues are very diversiÔ¨Åed. This resulted in three MDE venues at the top of the ranking. \\n9.5. IdentiÔ¨Åed challenges \\nAfter thoroughly analyzing each paper in the corpus, we noted several problems that the research community has ne- \\nglected in TBCG. First, we found 66% of the papers did not provide any assessment of the code generation process or the \\ngenerated output. We only found one paper with a formal veriÔ¨Åcation of the generated code using non-functional require- \\nment analysis [75] . Furthermore, the TBCG can be veriÔ¨Åed through benchmarks as in [55] . Second, we found no paper that \\ninvestigates eÔ¨Éciency of code generation. Researchers may be inspired from other related communities, such as compiler'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='investigates eÔ¨Éciency of code generation. Researchers may be inspired from other related communities, such as compiler \\noptimization [60] . Third, designing templates requires skillful engineering. Good practices, design patterns and other forms \\nof good reusable idioms would be of great value to developers [76] . \\n9.6. Threats to validity \\nThe results presented in this survey have depended on many factors that could potentially threaten its validity. \\n9.6.1. Construction validity \\nIn a strict sense, our Ô¨Åndings are valid only for our sample that we collected from 20 0 0‚Äì2016. This leads to determine \\nwhether the primary studies used in our survey are a good representation of the whole population. From Fig. 3 , we can \\nobserve that our sample can be attributed as a representative sample of the whole population. In particular, the average \\nnumber of identiÔ¨Åed primary studies per year is 28 with standard deviation 15.76. A more systematic selection process'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='number of identiÔ¨Åed primary studies per year is 28 with standard deviation 15.76. A more systematic selection process \\nwould have been diÔ¨Écult to be exhaustive about TBCG. We chose to obtain the best possible coverage at the cost of du- \\nplications. Nevertheless, the size of the corpus we classiÔ¨Åed is about ten times larger than other systematic reviews related \\nto code generation (see Section 2.4 ). We are, therefore, conÔ¨Ådent that this sample is a representative subset of all relevant \\npublications on TBCG. \\nAnother potential limitation is the query formulation for the keyword search. It is diÔ¨Écult to encode a query that is \\nrestrictive enough to discard unrelated publications, but at the same time retrieves all the relevant ones. In order to obtain \\na satisfactory balance, we included synonyms and captured possible declinations. In this study, we are only interested in'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='a satisfactory balance, we included synonyms and captured possible declinations. In this study, we are only interested in \\ncode generation. Therefore we discarded articles where TBCG was used for reporting or mass mailing, for example. We \\nbelieve that our sample is large enough that additional papers will not signiÔ¨Åcantly affect the general trends and results. \\nWe are fully aware that, since TBCG is widely used in practice, industries have their own tools and many have not been \\npublished in academic venues. Our goal was not to be exhaustive, but to get a representative sample. \\n9.6.2. Internal validity \\nA potential limitation is related to data extraction. It is diÔ¨Écult to extract data from relevant publications, especially \\nwhen the quality of the paper is low, when code generation is not the primary contribution of the paper, or when critical \\ninformation for the classiÔ¨Åcation is not directly available in the paper. For example in [77] , the authors only mention the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='information for the classiÔ¨Åcation is not directly available in the paper. For example in [77] , the authors only mention the \\nname of the tool used to generate the code. In order to mitigate this threat, we had to resort to searching for additional \\ninformation about the tool: reading other publications that use the tool, traversing the website of the tool, installing the \\ntool, or discussing with the tools experts. \\nAnother possible threat is the screening of papers based on inclusion and exclusion criteria that we deÔ¨Åned before the \\nstudy was conducted. During this process, we examined only the title, the abstract. Therefore, there is a probability that we \\nexcluded relevant publications such as [55] , that do not include any TBCG terms. In order to mitigate this threat, whenever \\nwe were unsure whether a publication should be excluded or not we conservatively opted to include it. However, during'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='we were unsure whether a publication should be excluded or not we conservatively opted to include it. However, during \\nclassiÔ¨Åcation when reading the whole content of the paper, we may still have excluded it. \\n9.6.3. External validity \\nThe results we obtained are based on TBCG only. Even though our classiÔ¨Åcation scheme includes facets like orientation, \\napplication domain, that are not related to the area, we followed a topic based classiÔ¨Åcation. The core characteristics of our \\nstudy are strictly related to this particular code synthesis technique. We have deÔ¨Åned characteristics like template style and \\nthe two levels of inputs that we believe are exclusive to TBCG. Therefore, the results cannot be generalized to other code \\ngeneration techniques mentioned in Section 2.2 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='60 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n9.6.4. Conclusion validity \\nOur study is based on a large number of primary studies. This helps us mitigate the potential threats related to the \\nconclusions of our study. A missing paper or a wrongly classiÔ¨Åed paper would have a very low impact on the statistics \\ncompared to a smaller number of primary studies. In addition, as a senior reviewer did a sanity check on the rejected \\npapers, we are conÔ¨Ådent that we did not miss a signiÔ¨Åcant number of papers. Hence, the chances for wrong conclusions are \\nsmall. Replication of this study can be achieved as we provided all the details of our research method in Section 3 . Also, our \\nstudy follows the methodology described in [20] . \\n10. Conclusion \\nThis paper reports the results of a large survey we conducted on the topic of TBCG, which has been missing in the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='study follows the methodology described in [20] . \\n10. Conclusion \\nThis paper reports the results of a large survey we conducted on the topic of TBCG, which has been missing in the \\nliterature. The objectives of this study are to better understand the characteristics of TBCG techniques and associated tools, \\nidentify research trends, and assess the importance of the role that MDE plays. The analysis of this corpus is organized into \\nfacets of a novel classiÔ¨Åcation scheme, which is of great value to modeling and software engineering researchers who are \\ninterested in painting an overview of the literature on TBCG. \\nOur study shows that the community has been diversely using TBCG over the past 16 years, and that research and \\ndevelopment is still very active. TBCG has greatly beneÔ¨Åted from MDE in 2005 and 2013 which mark the two peaks of \\nthe evolution of this area, tripling the average number of publications. In addition, TBCG has favored a template style that'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='the evolution of this area, tripling the average number of publications. In addition, TBCG has favored a template style that \\nis output-based and modeling languages as input. It has been applied in a variety of domains. The community has been \\nfavoring the use of custom tools for code generation over popular ones. Most research using TBCG follows an MDE approach. \\nFurthermore, both MDE and non-MDE tools are becoming effective development resources in industry. \\nThe study also revealed that, although the research in TBCG is mature enough, there are many open issues that can \\nbe addressed in the future, upstream and downstream the generation itself. Upstream, the deÔ¨Ånition of templates is not \\na trivial task. Supporting the developers in such a deÔ¨Ånition is a must. Downstream, methods and techniques need to be \\ndeÔ¨Åned to assess the correctness and quality of the generated code.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='a trivial task. Supporting the developers in such a deÔ¨Ånition is a must. Downstream, methods and techniques need to be \\ndeÔ¨Åned to assess the correctness and quality of the generated code. \\nWe believe this survey will be of beneÔ¨Åt to someone familiar with code generation by knowing how their favorite tool \\nranks in popularity within the research community, the relevance and importance of the use of templates, and in which \\ncontext TBCG has been applied (application domain). The relations across categories in Section VI show non-intuitive results \\nas well. The paper also promotes MDE in Ô¨Åelds that have not been traditionally exposed to it. \\nSupplementary material \\nSupplementary material associated with this article can be found, in the online version, at 10.1016/j.cl.2017.11.003 . \\nReferences \\n[1] Rich C , Waters RC . Automatic programming: myths and prospects. Computer 1988;21(8):40‚Äì51 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='References \\n[1] Rich C , Waters RC . Automatic programming: myths and prospects. Computer 1988;21(8):40‚Äì51 . \\n[2] Kelly S , Tolvanen J-P . Domain-speciÔ¨Åc modeling: enabling full code generation. John Wiley & Sons; 2008 . \\n[3] Bonta E , Bernardo M . Padl2java: a Java code generator for process algebraic architectural descriptions. In: Proceedings of the european conference on \\nsoftware architecture. IEEE; 2009. p. 161‚Äì70 . \\n[4] Tatsubori M , Chiba S , Killijian M-O , Itano K . OpenJava: a class-based macro system for Java. In: ReÔ¨Çection and software engineering. In: LNCS, 1826. \\nSpringer; 20 0 0. p. 117‚Äì33 . \\n[5] Lohmann D , Blaschke G , Spinczyk O . Generic advice: on the combination of AOP with generative programming in AspectC++. In: Proceedings of \\ninternational conference on generative programming and component engineering. In: LNCS, 3286. Berlin Heidelberg: Springer; 2004. p. 55‚Äì74 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='international conference on generative programming and component engineering. In: LNCS, 3286. Berlin Heidelberg: Springer; 2004. p. 55‚Äì74 . \\n[6] Kleppe AG , Warmer J , Bast W . MDA explained. The model driven architecture: practice and promise. Addison-Wesley; 2003 . \\n[7] J√∂rges S . Construction and evolution of code generators 7747. Ch 2 The state of the art in code generation. Berlin Heidelberg: Springer; 2013. p. 11‚Äì38 . \\n[8] Balzer R . A 15 year perspective on automatic programming. Trans Softw Eng 1985;11(11):1257‚Äì68 . \\n[9] Floch A , Yuki T , Guy C , Derrien S , Combemale B , Rajopadhye S , et al. Model-driven engineering and optimizing compilers: a bridge too far?. In: Model \\nDriven Engineering Languages and Systems. In: LNCS, 6981. Springer Berlin Heidelberg; 2011. p. 608‚Äì22 . \\n[10] Stahl T , Voelter M , Czarnecki K . Model-driven software development ‚Äì technology, engineering, management. John Wiley & Sons; 2006 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='[10] Stahl T , Voelter M , Czarnecki K . Model-driven software development ‚Äì technology, engineering, management. John Wiley & Sons; 2006 . \\n[11] L√∫cio L , Amrani M , Dingel J , Lambers L , Salay R , Selim GM , et al. Model transformation intents and their properties. Softw Syst Model \\n2014;15(3):685‚Äì705 . \\n[12] Czarnecki K , Helsen S . Feature-based survey of model transformation approaches. IBM Syst J 2006;45(3):621‚Äì45 . \\n[13] Gamma E , Helm R , Johnson R , Vlissides J . Design patterns: elements of reusable object-oriented software. Addison Wesley Professional; 1994 . \\n[14] Beckmann O , Houghton A , Mellor M , Kelly PH . Runtime code generation in C++ as a foundation for domain-speciÔ¨Åc optimisation. In: Domain-SpeciÔ¨Åc \\nProgram Generation. In: LNCS, 3016. Berlin Heidelberg: Springer; 2004. p. 291‚Äì306 . \\n[15] C√≥rdoba I , de Lara J . ANN: a domain-speciÔ¨Åc language for the effective design and validation of Java annotations. Comput Lang Syst Struct \\n2016;45:164‚Äì90 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='[15] C√≥rdoba I , de Lara J . ANN: a domain-speciÔ¨Åc language for the effective design and validation of Java annotations. Comput Lang Syst Struct \\n2016;45:164‚Äì90 . \\n[16] Jugel U , Preu√üner A . A case study on API generation. In: System analysis and modeling: about models. In: LNCS, 6598. Springer; 2011. p. 156‚Äì72 . \\n[17] Kitchenham BA , Dyba T , Jorgensen M . Evidence-based software engineering. In: Proceedings of international conference on software engineering. \\nWashington, DC, USA: IEEE Computer Society; 2004. p. 273‚Äì81 . \\n[18] Kitchenham BA , Budgen D , Brereton OP . Using mapping studies as the basis for further research - a participant-observer case study. Inf Softw Technol \\n2011;53(6):638‚Äì51 . \\n[19] Brereton P , Kitchenham BA , Budgen D , Turner M , Khalil M . Lessons from applying the systematic literature review process within the software engi- \\nneering domain. J Syst Softw 2007;80(4):571‚Äì83 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='neering domain. J Syst Softw 2007;80(4):571‚Äì83 . \\n[20] Petersen K , Feldt R , Mujtaba S , Mattsson M . Systematic mapping studies in software engineering. In: Proceedings of the 12th international conference \\non evaluation and assessment in software engineering, EASE‚Äô08, 17. British Computer Society; 2008. p. 68‚Äì77 . \\n[21] Mehmood A , Jawawi DN . Aspect-oriented model-driven code generation: a systematic mapping study. Inf Softw Technol 2013;55(2):395‚Äì411 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n61 \\n[22] Gurunule D , Nashipudimath M . A review: analysis of aspect orientation and model driven engineering for code generation. Procedia Comput Sci \\n2015;45:852‚Äì61 . \\n[23] Dom√≠guez E , P√©rez B , Rubio AL , Zapata MA . A systematic review of code generation proposals from state machine speciÔ¨Åcations. Inf Softw Technol \\n2012;54(10):1045‚Äì66 . \\n[24] Batot E , Sahraoui H , Syriani E , Molins P , Sboui W . Systematic mapping study of model transformations for concrete problems. In: Model-driven \\nengineering and software development. IEEE; 2016. p. 176‚Äì83 . \\n[25] Rose LM , Matragkas N , Kolovos DS , Paige RF . A feature model for model-to-text transformation languages. In: Modeling in software engineering. IEEE; \\n2012. p. 57‚Äì63 . \\n[26] Kosar T , Bohra S , Mernik M . Domain-speciÔ¨Åc languages: a systematic mapping study. Inf Softw Technol 2016;71:77‚Äì91 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='2012. p. 57‚Äì63 . \\n[26] Kosar T , Bohra S , Mernik M . Domain-speciÔ¨Åc languages: a systematic mapping study. Inf Softw Technol 2016;71:77‚Äì91 . \\n[27] M√©ndez-Acu√±a D , Galindo JA , Degueule T , Combemale B , Baudry B . Leveraging software product lines engineering in the development of external \\nDSLs: a systematic literature review. Comput Lang Syst Struct 2016;46:206‚Äì35 . \\n[28] Mat√∫s Sul√≠r JP . Labeling source code with metadata: a survey and taxonomy. In: Proceedings of federated conference on computer science and infor- \\nmation systems. In: Workshop on Advances in Programming Languages (WAPL‚Äô17), CFP1785N-ART. IEEE; 2017. p. 721‚Äì9 . \\n[29] Buchmann T , Schw√§gerl F . Using meta-code generation to realize higher-order model transformations. In: Proceedings of international conference on \\nsoftware technologies; 2013. p. 536‚Äì41 . \\n[30] Seriai A , Benomar O , Cerat B , Sahraoui H . Validation of software visualization tools: a systematic mapping study. In: Proceedings of IEEE working'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='software technologies; 2013. p. 536‚Äì41 . \\n[30] Seriai A , Benomar O , Cerat B , Sahraoui H . Validation of software visualization tools: a systematic mapping study. In: Proceedings of IEEE working \\nconference on software visualization. VISSOFT; 2014. p. 60‚Äì9 . \\n[31] Liu Q . C++ techniques for high performance Ô¨Ånancial modelling. WIT Trans Model Simul 2006;43:1‚Äì8 . \\n[32] Fang M , Ying J , Wu M . A template engineering based framework for automated software development. In: Proceeding of the 10th international con- \\nference on computer supported cooperative work in design. IEEE; 2006. p. 1‚Äì6 . \\n[33] Singh A , Schaeffer J , Green M . A template-based approach to the generation of distributed applications using a network of workstations. IEEE Trans \\nParallel Distrib Syst 1991;2(1):52‚Äì67 . \\n[34] O‚ÄôHalloran C . Automated veriÔ¨Åcation of code automatically generated from simulink ¬Æ. Autom Softw Eng 2013;20(2):237‚Äì64 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Parallel Distrib Syst 1991;2(1):52‚Äì67 . \\n[34] O‚ÄôHalloran C . Automated veriÔ¨Åcation of code automatically generated from simulink ¬Æ. Autom Softw Eng 2013;20(2):237‚Äì64 . \\n[35] Dahman W , Grabowski J . UML-based speciÔ¨Åcation and generation of executable web services. In: System analysis and modeling. In: LNCS, 6598. \\nSpringer; 2011. p. 91‚Äì107 . \\n[36] Gessenharter D . Mapping the UML2 semantics of associations to a Java code generation model. In: Proceedings of international conference on model \\ndriven engineering languages and systems. In: LNCS, 5301. Springer; 2008. p. 813‚Äì27 . \\n[37] Valderas P , Pelechano V , Pastor O . Towards an end-user development approach for web engineering methods. In: Proceedings of international confer- \\nence on advanced information systems engineering, 4001. Springer; 2006. p. 528‚Äì43 . \\n[38] Hemel Z , Kats LC , Groenewegen DM , Visser E . Code generation by model transformation: a case study in transformation modularity. Softw Syst Model \\n2010;9(3):375‚Äì402 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='[38] Hemel Z , Kats LC , Groenewegen DM , Visser E . Code generation by model transformation: a case study in transformation modularity. Softw Syst Model \\n2010;9(3):375‚Äì402 . \\n[39] Brun M , Delatour J , Trinquet Y . Code generation from AADL to a real-time operating system: an experimentation feedback on the use of model \\ntransformation. In: Engineering of complex computer systems. IEEE; 2008. p. 257‚Äì62 . \\n[40] Buckl C , Knoll A , Schrott G . Development of dependable real-time systems with Zerberus. In: Proceedings of the 11th IEEE paciÔ¨Åc rim international \\nsymposium on dependable computing; 2005. p. 404‚Äì8 . \\n[41] Li J , Xiao H , Yi D . Designing universal template for database application system based on abstract factory. In: Computer science and information \\nprocessing. IEEE; 2012. p. 1167‚Äì70 . \\n[42] Gopinath VS , Sprinkle J , Lysecky R . Modeling of data adaptable reconÔ¨Ågurable embedded systems. In: International conference and workshops on'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='processing. IEEE; 2012. p. 1167‚Äì70 . \\n[42] Gopinath VS , Sprinkle J , Lysecky R . Modeling of data adaptable reconÔ¨Ågurable embedded systems. In: International conference and workshops on \\nengineering of computer based systems. IEEE; 2011. p. 276‚Äì83 . \\n[43] Buckl C , Regensburger M , Knoll A , Schrott G . Models for automatic generation of safety-critical real-time systems. In: Availability, reliability and \\nsecurity. IEEE; 2007. p. 580‚Äì7 . \\n[44] Fischer T , Kollner C , Hardle M , Muller Glaser KD . Product line development for modular FPGA-based embedded systems. In: Proceedings of symposium \\non rapid system prototyping. IEEE; 2014. p. 9‚Äì15 . \\n[45] Chen K , Chang Y-C , Wang D-W . Aspect-oriented design and implementation of adaptable access control for electronic medical records. Int J Med \\nInform 2010;79(3):181‚Äì203 . \\n[46] Brox M , S√°nchez-Solano S , del Toro E , Brox P , Moreno-Velo FJ . CAD tools for hardware implementation of embedded fuzzy systems on FPGAs. IEEE'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Inform 2010;79(3):181‚Äì203 . \\n[46] Brox M , S√°nchez-Solano S , del Toro E , Brox P , Moreno-Velo FJ . CAD tools for hardware implementation of embedded fuzzy systems on FPGAs. IEEE \\nTrans Ind Inform 2013;9(3):1635‚Äì44 . \\n[47] Fraternali P , Tisi M . A higher order generative framework for weaving traceability links into a code generator for web application testing. In: Proceed- \\nings of international conference on web engineering. In: LNCS, 5648. Springer; 2009. p. 340‚Äì54 . \\n[48] Vok√°Àác M , Glattetre JM . Using a domain-speciÔ¨Åc language and custom tools to model a multi-tier service-oriented application experiences and chal- \\nlenges. In: Model Driven Engineering Languages and Systems, 3713. Springer; 2005. p. 492‚Äì506 . \\n[49] Kokar M , Baclawski K , Gao H . Category theory-based synthesis of a higher-level fusion algorithm: an example. In: Proceedings of international confer- \\nence on information fusion; 2006. p. 1‚Äì8 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='ence on information fusion; 2006. p. 1‚Äì8 . \\n[50] Hoisl B , Sobernig S , Strembeck M . Higher-order rewriting of model-to-text templates for integrating domain-speciÔ¨Åc modeling languages. In: Model‚Äì\\nDriven Engineering and Software Development. SCITEPRESS; 2013. p. 49‚Äì61 . \\n[51] Ecker W , Velten M , Zafari L , Goyal A . The metamodeling approach to system level synthesis. In: Proceedings of design, automation & test in Europe \\nconference & exhibition. IEEE; 2014. p. 1‚Äì2 . \\n[52] Behrens T , Richards S . Statelator-behavioral code generation as an instance of a model transformation. In: Proceedings of international conference on \\nadvanced information systems engineering. In: LNCS, 1789. Springer; 20 0 0. p. 401‚Äì16 . \\n[53] Durand SH , Bonato V . A tool to support Bluespec SystemVerilog coding based on UML diagrams. In: Proceedings of annual conference on IEEE indus- \\ntrial electronics society. IEEE; 2012. p. 4670‚Äì5 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='trial electronics society. IEEE; 2012. p. 4670‚Äì5 . \\n[54] Schattkowsky T , Lohmann M . Rapid development of modular dynamic web sites using UML. In: Proceedings of international conference on the uniÔ¨Åed \\nmodeling language. In: LNCS, 2460. Springer; 2002. p. 336‚Äì50 . \\n[55] Buezas N , Guerra E , de Lara J , Mart√≠n J , Monforte M , Mori F , et al. Umbra designer: graphical modelling for telephony services. In: Proceedings of \\neuropean conference on modelling foundations and applications. In: LNCS, 7949. Berlin Heidelberg: Springer; 2013. p. 179‚Äì91 . \\n[56] Manley R , Gregg D . A program generator for intel AES-NI instructions. In: Proceedings of international conference on cryptology. In: LNCS, 6498. \\nSpringer; 2010. p. 311‚Äì27 . \\n[57] Phillips J , Chilukuri R , Fragoso G , Warzel D , Covitz PA . The caCORE software development kit: streamlining construction of interoperable biomedical \\ninformation services. BMC Med Inform Decis Mak 2006;6(2):1‚Äì16 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='information services. BMC Med Inform Decis Mak 2006;6(2):1‚Äì16 . \\n[58] Fu J , Bastani FB , Yen I-L . Automated AI planning and code pattern based code synthesis. In: Proceedings of international conference on tools with \\nartiÔ¨Åcial intelligence. IEEE; 2006. p. 540‚Äì6 . \\n[59] Possatto MA , Lucr√©dio D . Automatically propagating changes from reference implementations to code generation templates. Inf Softw Technol \\n2015;67:65‚Äì78 . \\n[60] Ghodrat MA , Givargis T , Nicolau A . Control Ô¨Çow optimization in loops using interval analysis. In: Proceedings of international conference on compilers, \\narchitectures and synthesis for embedded systems. ACM; 2008. p. 157‚Äì66 . \\n[61] Guduvan A-R , Waeselynck H , Wiels V , Durrieu G , Fusero Y , Schieber M . A meta-model for tests of avionics embedded systems. In: Proceedings of \\ninternational conference on model-driven engineering and software development; 2013. p. 5‚Äì13 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='international conference on model-driven engineering and software development; 2013. p. 5‚Äì13 . \\n[62] Adamko A . Modeling data-oriented web applications using UML. In: Proceedings of the international conference on computer as a tool, EUROCON \\n2005, 1. IEEE; 2005. p. 752‚Äì5 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 19, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='62 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n[63] K√∂vi A , Varr√≥ D . An eclipse-based framework for AIS service conÔ¨Ågurations. In: Proceedings of the 4th international symposium on service availability, \\nISAS. In: LNCS, 4526. Springer; 2007. p. 110‚Äì26 . \\n[64] Anjorin A , Saller K , Rose S , Sch√ºrr A . A framework for bidirectional model-to-platform transformations. In: Proceedings of the 5th international con- \\nference on software language engineering, SLE 2012. In: LNCS, 7745. Berlin Heidelberg: Springer; 2013. p. 124‚Äì43 . \\n[65] Burmester S , Giese H , Sch√§fer W . Model-driven architecture for hard real-time systems: from platform independent models to code. In: Proceedings \\nof European conference on model driven architecture-foundations and applications. In: LNCS, 3748. Berlin Heidelberg: Springer; 2005. p. 25‚Äì40 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 19, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='of European conference on model driven architecture-foundations and applications. In: LNCS, 3748. Berlin Heidelberg: Springer; 2005. p. 25‚Äì40 . \\n[66] Brown AW , Conallen J , Tropeano D . Introduction: models, modeling, and model-driven architecture (MDA). In: Proceedings of international conference \\non model-driven software development. Berlin Heidelberg: Springer; 2005. p. 1‚Äì16 . \\n[67] Basu AS , Lajolo M , Prevostini M . A methodology for bridging the gap between UML and codesign. In: UML for SOC design. US: Springer; 2005. \\np. 119‚Äì46 . \\n[68] Furusawa T . Attempting to increase longevity of applications based on new SaaS/cloud technology. Fujitsu Sci Tech J 2010;46:223‚Äì8 . \\n[69] Muller P-A , Studer P , Fondement F , B√©zivin J . Platform independent web application modeling and development with Netsilon. Softw Syst Model \\n2005;4(4):424‚Äì42 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 19, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='[69] Muller P-A , Studer P , Fondement F , B√©zivin J . Platform independent web application modeling and development with Netsilon. Softw Syst Model \\n2005;4(4):424‚Äì42 . \\n[70] Antkiewicz M , Czarnecki K . Framework-speciÔ¨Åc modeling languages with round-trip engineering. In: Model driven engineering languages and systems. \\nIn: LNCS, 4199. Berlin Heidelberg: Springer; 2006. p. 692‚Äì706 . \\n[71] Hinkel G , Denninger O , Krach S , Groenda H . Experiences with model-driven engineering in neurorobotics. In: Proceedings of the 12th european con- \\nference on modelling foundations and applications, ECMFA 2016. Cham: Springer International Publishing; 2016. p. 217‚Äì28 . \\n[72] Kulkarni V , Barat S , Ramteerthkar U . Early experience with agile methodology in a model-driven approach. In: Model driven engineering languages \\nand systems. In: LNCS, 6981. Springer; 2011. p. 578‚Äì90 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 19, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='and systems. In: LNCS, 6981. Springer; 2011. p. 578‚Äì90 . \\n[73] Touraille L , Traor√© MK , Hill DR . A model-driven software environment for modeling, simulation and analysis of complex systems. In: Proceedings of \\nsymposium on theory of modeling & simulation. SCSC; 2011. p. 229‚Äì37 . \\n[74] Fertalj K , Kalpic D , Mornar V . Source code generator based on a proprietary speciÔ¨Åcation language. In: Proceedings of Hawaii international conference \\non system sciences, 9. IEEE; 2002. p. 3696‚Äì704 . \\n[75] Yen I-L, Goluguri J, Bastani F, Khan L, Linn J. A component-based approach for embedded software development. In: International symposium on \\nobject-oriented real-time distributed computing. ISORC 2002. IEEE Computer Society; 2002. p. 402‚Äì10. doi: 10.1109/ISORC.2002.1003805 . \\n[76] Luhunu L , Syriani E . Comparison of the expressiveness and performance of template-based code generation tools. In: Software Language Engineering. \\nACM; 2017 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 19, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='[76] Luhunu L , Syriani E . Comparison of the expressiveness and performance of template-based code generation tools. In: Software Language Engineering. \\nACM; 2017 . \\n[77] Ma M , Meissner M , Hedrich L . A case study: automatic topology synthesis for analog circuit from an ASDEX speciÔ¨Åcation. In: Synthesis, modeling, \\nanalysis and simulation methods and applications to circuit design. IEEE; 2012. p. 9‚Äì12 .')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3691b2a2",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5110ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b63a8f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 1003.70it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded successfully. embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x24ad75c1550>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name:str=\"all-MiniLM-L6-v2\"):\n",
    "        self.model_name=model_name\n",
    "        self.model=None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"loading embedding model: {self.model_name}\")\n",
    "            self.model=SentenceTransformer(self.model_name)\n",
    "            print(f\"model loaded successfully. embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error model model {self.model_name}:{e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) ->np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"model not loaded\")\n",
    "        \n",
    "        print(f\"generating embeddings for {len(texts)} texts\")\n",
    "        embeddings=self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"generated embeddings with shape : {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee5d41",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec98cc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector store initialized. collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x24ad9325be0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, collection_name: str=\"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        self.collection_name=collection_name\n",
    "        self.persist_directory=persist_directory\n",
    "        self.client=None\n",
    "        self.collection=None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection=self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={'description':\"pdf document embeddings for RAG\"}\n",
    "        )\n",
    "            print(f\"vector store initialized. collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        if len(documents)!=len(embeddings):\n",
    "            raise ValueError(\"number of documents must match number of embeddings\")\n",
    "\n",
    "        print(f\"adding {len(documents)} documents to vector store\")\n",
    "\n",
    "        ids=[]\n",
    "        metadatas=[]\n",
    "        document_texts=[]\n",
    "        embeddings_list=[]\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id=f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata=dict(doc.metadata)\n",
    "            metadata['doc_index']=i\n",
    "            metadata['content_length']=len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            document_texts.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=document_texts\n",
    "            )\n",
    "            print(f\"successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"total documents in collection : {self.collection.count()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e950c5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 0, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1\\nA Survey on Large Language Models for Code Generation\\nJUYONG JIANG‚àó, The Hong Kong University of Science and Technology (Guangzhou), China\\nFAN WANG‚àó, The Hong Kong University of Science and Technology (Guangzhou), China\\nJIASI SHEN‚Ä†, The Hong Kong University of Science and Technology, China\\nSUNGJU KIM‚Ä†, NAVER Cloud, South Korea\\nSUNGHUN KIM‚Ä†, The Hong Kong University of Science and Technology (Guangzhou), China\\nLarge Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks,\\nknown as Code LLMs, particularly in code generation that generates source code with LLM from natural\\nlanguage descriptions. This burgeoning field has captured significant interest from both academic researchers\\nand industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite\\nthe active exploration of LLMs for a variety of code tasks, either from the perspective of natural language'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 0, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language\\nprocessing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive\\nand up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge\\nthis gap by providing a systematic literature review that serves as a valuable reference for researchers\\ninvestigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize\\nand discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest\\nadvances, performance evaluation, ethical implications, environmental impact, and real-world applications.\\nIn addition, we present a historical overview of the evolution of LLMs for code generation and offer an\\nempirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 0, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels\\nof difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities\\nfor code generation. We identify critical challenges and promising opportunities regarding the gap between\\nacademia and practical development. Furthermore, we have established a dedicated resource GitHub page\\n(https://github.com/juyongjiang/CodeLLMSurvey) to continuously document and disseminate the most recent\\nadvances in the field.\\nCCS Concepts: ‚Ä¢ General and reference ‚ÜíSurveys and overviews; ‚Ä¢ Software and its engineering ‚Üí\\nSoftware development techniques; ‚Ä¢ Computing methodologies ‚ÜíArtificial intelligence.\\nAdditional Key Words and Phrases: Large Language Models, Code Large Language Models, Code Generation\\nACM Reference Format:\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2018. A Survey on Large Language Models'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 0, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ACM Reference Format:\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2018. A Survey on Large Language Models\\nfor Code Generation. J. ACM 37, 4, Article 1 (August 2018), 70 pages. https://doi.org/XXXXXXX.XXXXXXX\\n‚àóEqually major contributors.\\n‚Ä†Corresponding authors.\\nAuthors‚Äô addresses: Juyong Jiang, jjiang472@connect.hkust-gz.edu.cn, The Hong Kong University of Science and Technology\\n(Guangzhou), Guangzhou, China; Fan Wang, fwang380@connect.hkust-gz.edu.cn, The Hong Kong University of Science\\nand Technology (Guangzhou), Guangzhou, China; Jiasi Shen, sjs@cse.ust.hk, The Hong Kong University of Science and\\nTechnology, Hong Kong, China; Sungju Kim, sungju.kim@navercorp.com, NAVER Cloud, Seoul, South Korea; Sunghun\\nKim, hunkim@cse.ust.hk, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 0, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\\n¬© 2018 Association for Computing Machinery.\\n0004-5411/2018/8-ART1 $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.\\narXiv:2406.00515v2  [cs.CL]  10 Nov 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 1, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:2\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n1\\nINTRODUCTION\\nThe advent of Large Language Models (LLMs) such as ChatGPT1 [196] has profoundly transformed\\nthe landscape of automated code-related tasks [48], including code completion [87, 171, 270, 282],\\ncode translation [52, 135, 245], and code repair [75, 126, 195, 204, 291, 310]. A particularly intriguing\\napplication of LLMs is code generation, a task that involves producing source code from natural\\nlanguage descriptions. Despite varying definitions across studies [51, 221, 238, 269], for the main\\nscope of this survey, we focus on the code generation task and adopt a consistent definition of code\\ngeneration as the natural-language-to-code (NL2Code) task [16, 17, 307]. To enhance clarity, the\\ndifferentiation between code generation and other code-related tasks, along with a more nuanced\\ndefinition, is summarized in Table 1. This area has garnered substantial interest from both academia'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 1, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='differentiation between code generation and other code-related tasks, along with a more nuanced\\ndefinition, is summarized in Table 1. This area has garnered substantial interest from both academia\\nand industry, as evidenced by the development of tools like GitHub Copilot2 [48], CodeGeeX3 [321],\\nand Amazon CodeWhisperer4, which leverage groundbreaking code LLMs to facilitate software\\ndevelopment.\\nInitial investigations into code generation primarily utilized heuristic rules or expert systems,\\nsuch as probabilistic grammar-based frameworks [10, 62, 119, 125, 288] and specialized language\\nmodels [64, 83, 117]. These early techniques were typically rigid and difficult to scale. However,\\nthe introduction of Transformer-based LLMs has shifted the paradigm, establishing them as the\\npreferred method due to their superior proficiency and versatility. One remarkable aspect of LLMs is\\ntheir capability to follow instructions [56, 187, 200, 275, 289], enabling even novice programmers to'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 1, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='their capability to follow instructions [56, 187, 200, 275, 289], enabling even novice programmers to\\nwrite code by simply articulating their requirements. This emergent ability has democratized coding,\\nmaking it accessible to a broader audience [307]. The performance of LLMs on code generation\\ntasks has seen remarkable improvements, as illustrated by the HumanEval leaderboard5, which\\nshowcases the evolution from PaLM 8B [54] of 3.6% to LDB [325] of 95.1% on Pass@1 metrics.\\nAs can be seen, the HumanEval benchmark [48] has been established as a de facto standard for\\nevaluating the coding proficiency of LLMs [48].\\nTo offer a comprehensive chronological evolution, we present an overview of the development\\nof LLMs for code generation, as illustrated in Figure 1. The landscape of LLMs for code generation\\nis characterized by a spectrum of models, with certain models like ChatGPT [200], GPT4 [5],\\nLLaMA [252, 253], and Claude 3 [14] serving general-purpose applications, while others such'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 1, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='is characterized by a spectrum of models, with certain models like ChatGPT [200], GPT4 [5],\\nLLaMA [252, 253], and Claude 3 [14] serving general-purpose applications, while others such\\nas StarCoder [147, 170], Code LLaMA [227], DeepSeek-Coder [88], and Code Gemma [59] are\\ntailored specifically for code-centric tasks. The convergence of code generation with the latest LLM\\nadvancements is pivotal, especially when programming languages can be considered as distinct\\ndialects of multilingual natural language [16, 321]. These models are not only tested against software\\nengineering (SE) requirements but also propel the advancement of LLMs into practical production\\n[317].\\nWhile recent surveys have shed light on code LLMs from the lenses of Natural Language Pro-\\ncessing (NLP), Software Engineering (SE), or a combination of both disciplines [74, 101, 174, 307,\\n317, 324], they have often encompassed a broad range of code-related tasks. There remains a dearth'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 1, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='cessing (NLP), Software Engineering (SE), or a combination of both disciplines [74, 101, 174, 307,\\n317, 324], they have often encompassed a broad range of code-related tasks. There remains a dearth\\nof literature specifically reviewing advanced topics in code generation, such as meticulous data\\ncuration, instruction tuning, alignment with feedback, prompting techniques, the development of\\nautonomous coding agents, retrieval augmented code generation, LLM-as-a-Judge for code genera-\\ntion, among others. A notably pertinent study [16, 307] also concentrates on LLMs for text-to-code\\ngeneration (NL2Code), yet it primarily examines models released from 2020 to 2022. Consequently,\\n1https://chat.openai.com\\n2https://github.com/features/copilot\\n3https://codegeex.cn/en-US\\n4https://aws.amazon.com/codewhisperer\\n5https://paperswithcode.com/sota/code-generation-on-humaneval\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 2, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:3\\nTable 1. The applications of code LLMs in various code-related understanding and generation tasks. The I-O\\ncolumn indicates the type of input and output for each task, where C, NL, and K represent code, natural\\nlanguage, and label, respectively. Note that the detailed definitions of each task aligns with the descriptions\\nin [7, 16, 17, 194, 307]. The main scope of this survey focuses on code generation while it may involve code\\ncompletion in Section 5.7 and 5.8, aiming to illustrate the corresponding advancements.\\nType\\nI-O\\nTask\\nDefinition\\nUnderstanding\\nC-K\\nCode Classification\\nClassify code snippets based on functionality, purpose, or attributes\\nto aid in organization and analysis.\\nBug Detection\\nDetect and diagnose bugs or vulnerabilities in code to ensure\\nfunctionality and security.\\nClone Detection\\nIdentifying duplicate or similar code snippets in software to enhance\\nmaintainability, reduce redundancy, and check plagiarism.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 2, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='functionality and security.\\nClone Detection\\nIdentifying duplicate or similar code snippets in software to enhance\\nmaintainability, reduce redundancy, and check plagiarism.\\nException Type Prediction\\nPredict different exception types in code to manage and handle\\nexceptions effectively.\\nC-C\\nCode-to-Code Retrieval\\nRetrieve relevant code snippets based on a given\\ncode query for reuse or analysis.\\nNL-C\\nCode Search\\nFind relevant code snippets based on natural language\\nqueries to facilitate coding and development tasks.\\nGeneration\\nC-C\\nCode Completion\\nPredict and suggest the next portion of code, given contextual\\ninformation from the prefix (and suffix), while typing to enhance\\ndevelopment speed and accuracy.\\nCode Translation\\nTranslate the code from one programming language to another\\nwhile preserving functionality and logic.\\nCode Repair\\nIdentify and fix bugs in code by generating the correct version to\\nimprove functionality and reliability.\\nMutant Generation'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 2, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='while preserving functionality and logic.\\nCode Repair\\nIdentify and fix bugs in code by generating the correct version to\\nimprove functionality and reliability.\\nMutant Generation\\nGenerate modified versions of code to test and evaluate the\\neffectiveness of testing strategies.\\nTest Generation\\nGenerate test cases to validate code functionality, performance,\\nand robustness.\\nC-NL\\nCode Summarization\\nGenerate concise textual descriptions or explanations of code to\\nenhance understanding and documentation.\\nNL-C\\nCode Generation\\nGenerate source code from natural language descriptions to\\nstreamline development and reduce manual coding efforts.\\nthis noticeable temporal gap has resulted in an absence of up-to-date literature reviews that con-\\ntemplate the latest advancements, including models like CodeQwen [249], WizardCoder [173],\\nCodeFusion [241], and PPOCoder [238], as well as the comprehensive exploration of the advanced\\ntopics previously mentioned.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 2, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='CodeFusion [241], and PPOCoder [238], as well as the comprehensive exploration of the advanced\\ntopics previously mentioned.\\nRecognizing the need for a dedicated and up-to-date literature review, this survey endeavors to fill\\nthat void. We provide a systematic review that will serve as a foundational reference for researchers\\nquickly exploring the latest progress in LLMs for code generation. A taxonomy is introduced to\\ncategorize and examine recent advancements, encompassing data curation [173, 268, 278], advanced\\ntopics [45, 51, 104, 139, 163, 171, 187, 190, 205, 239, 309], evaluation methods [48, 95, 123, 332], and\\npractical applications [48, 321]. This category aligns with the comprehensive lifecycle of an LLM for\\ncode generation. Furthermore, we pinpoint critical challenges and identify promising opportunities\\nto bridge the research-practicality divide. Therefore, this survey allows NLP and SE researchers'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 2, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='code generation. Furthermore, we pinpoint critical challenges and identify promising opportunities\\nto bridge the research-practicality divide. Therefore, this survey allows NLP and SE researchers\\nto seamlessly equip with a thorough understanding of LLM for code generation, highlighting\\ncutting-edge directions and current hurdles and prospects.\\nThe remainder of the survey is organized following the structure outlined in our taxonomy in\\nFigure 6. In Section 2, we introduce the preliminaries of LLM with Transformer architecture and\\nformulate the task of LLM for code generation. Section 3, we detail the systematic methodologies\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 3, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:4\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nemployed in conducting literature reviews. Then, in Section 4, we propose a taxonomy, categorizing\\nthe complete process of LLMs in code generation. Section 5 delves into the specifics of LLMs\\nfor code generation within this taxonomy framework. In Section 6, we underscore the critical\\nchallenges and promising opportunities for bridging the research-practicality gap and conclude\\nthis work in Section 7.\\n2\\nBACKGROUND\\n2.1\\nLarge Language Models\\nThe effectiveness of large language models (LLMs) is fundamentally attributed to their substantial\\nquantity of model parameters, large-scale and diversified datasets, and the immense computational\\npower utilized during training [97, 127]. Generally, scaling up language models consistently results\\nin enhanced performance and sample efficiency across a broad array of downstream tasks [275, 319].'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 3, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='power utilized during training [97, 127]. Generally, scaling up language models consistently results\\nin enhanced performance and sample efficiency across a broad array of downstream tasks [275, 319].\\nHowever, with the expansion of the model size to a certain extent (e.g., GPT-3 [33] with 175B-\\nparameters and PaLM [54] with 540B), LLMs have exhibited an unpredictable phenomenon known\\nas emergent abilities6, including instruction following [200], in-context learning [70], and step-\\nby-step reasoning [105, 276], which are absent in smaller models but apparent in larger ones\\n[275].\\nAdhering to the same architectures of the Transformer [257] in LLMs, code LLMs are specifically\\npre-trained (or continually pre-trained on general LLMs) using large-scale unlabeled code corpora\\nwith a smaller portion of text (and math) data, whereas general-purpose LLMs are pre-trained\\nprimarily on large-scale text data, incorporating a smaller amount of code and math data to'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 3, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='with a smaller portion of text (and math) data, whereas general-purpose LLMs are pre-trained\\nprimarily on large-scale text data, incorporating a smaller amount of code and math data to\\nenhance logical reasoning capabilities. Additionally, some code LLMs, such as Qwen2.5-Coder\\n[109], incorporate synthetic data in their training processes, a practice that is attracting increasing\\nattention from both industry and academia. Analogous to LLMs, Code LLMs can also be classified\\ninto three architectural categories: encoder-only models, decoder-only models, and encoder-decoder\\nmodels. For encoder-only models, such as CodeBERT [76], they are typically suitable for code\\ncomprehension tasks including type prediction, code retrieval, and clone detection. For decoder-\\nonly models, such as StarCoder [33], they predominantly excel in generation tasks, such as code\\ngeneration, code translation, and code summarization. Encoder-decoder models, such as CodeT5'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 3, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='only models, such as StarCoder [33], they predominantly excel in generation tasks, such as code\\ngeneration, code translation, and code summarization. Encoder-decoder models, such as CodeT5\\n[271], can accommodate both code understanding and generation tasks but do not necessarily\\noutperform encoder-only or decoder-only models. The overall architectures of the different Code\\nLLMs for code generation are depicted in Figure 2.\\nIn the following subsection, we will delineate the key modules of the Transformer layers in Code\\nLLMs.\\n2.1.1\\nMulti-Head Self-Attention Modules. Each Transformer layer incorporates a multi-head self-\\nattention (MHSA) mechanism to discern the inherent semantic relationships within a sequence\\nof tokens across ‚Ñédistinct latent representation spaces. Formally, the MHSA employed by the\\nTransformer can be formulated as follows:\\nh(ùëô) = MultiHeadSelfAttn(Q, K, V) = Concat {Headùëñ}‚Ñé\\nùëñ=1 WO,\\n(1)\\nHeadùëñ= Attention(H(ùëô‚àí1)WQ\\nùëñ\\n|      {z      }\\nQ\\n, H(ùëô‚àí1)WK\\nùëñ\\n|      {z      }\\nK'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 3, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Transformer can be formulated as follows:\\nh(ùëô) = MultiHeadSelfAttn(Q, K, V) = Concat {Headùëñ}‚Ñé\\nùëñ=1 WO,\\n(1)\\nHeadùëñ= Attention(H(ùëô‚àí1)WQ\\nùëñ\\n|      {z      }\\nQ\\n, H(ùëô‚àí1)WK\\nùëñ\\n|      {z      }\\nK\\n, H(ùëô‚àí1)WV\\nùëñ\\n|      {z      }\\nV\\n),\\n(2)\\n6It should be noted that an LLM is not necessarily superior to a smaller language model, and emergent abilities may not\\nmanifest in all LLMs [319].\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 4, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:5\\nphi-2\\n2021\\nMay\\nGPT-C\\nCodeGPT\\nFeb.\\nMar.\\nMay\\nGPT-Neo PLBART\\nGPT-J\\nJul.\\nCodex\\nSep.\\nCodeT5\\nNov.\\nCodeParrot\\nPolyCoder\\nAlphaCode\\nJuPyT5\\nCodeGen\\nGPT-NeoX\\nPaLM-Coder InCoder\\nCodeRL\\nPanGu-Coder\\nPyCodeGPT\\nCodeGeeX\\nBLOOM\\nERNIE-Code\\nSantaCoder\\nJan.\\nPPOCoder\\nLLaMA\\nFeb.\\nMay\\nCodeGen2\\nreplit-code\\nStarCoder\\nCodeT5+\\nCodeTF\\nJun.\\nWizardCoder\\nphi-1\\nJul.\\nChainCoder\\nCodeGeeX2\\nPanGu-Coder2\\nAug.\\nOctoPack\\nSep.\\nMFTCoder\\nOct.\\nCodeShell\\nphi-1.5\\nCodeFusion\\nNov.\\nDeepSeek-Coder\\nDec.\\nMagicoder\\nAlphaCode 2\\nWaveCoder\\nJan.\\nFeb.\\nAST-T5\\nToolGen\\nStableCode\\nAlphaCodium\\nStepCoder OpenCodeInterpreter StarCoder2\\nMar.\\nDevin\\nOpenDevin\\nCodeS\\nApr.\\nProCoder\\nCodeQwen1.5\\nCodeGemma\\nCode Llama\\nApr.\\nSelf-Debugging\\nJan.\\nFeb.\\nJun.\\nJul.\\nSep.\\nNov.\\nDec.\\nApr.\\nMar.\\n2020\\n2022\\n2023\\n2024\\nChatGPT\\nMar.\\nGPT4\\nLlama 2\\nLlama 3\\nClaude 3\\nCodeT\\nSelfEvolve\\nLEVER\\nRLTF\\nOct.\\nPyMT5\\nStarCoder2-Instruct\\nOpen Source Closed Source\\n9\\n3\\n6\\n6\\n5\\n3\\n1\\n4\\nMar.\\nCodestral'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 4, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Nov.\\nDec.\\nApr.\\nMar.\\n2020\\n2022\\n2023\\n2024\\nChatGPT\\nMar.\\nGPT4\\nLlama 2\\nLlama 3\\nClaude 3\\nCodeT\\nSelfEvolve\\nLEVER\\nRLTF\\nOct.\\nPyMT5\\nStarCoder2-Instruct\\nOpen Source Closed Source\\n9\\n3\\n6\\n6\\n5\\n3\\n1\\n4\\nMar.\\nCodestral\\nFig. 1. A chronological overview of large language models (LLMs) for code generation in recent years. The\\ntimeline was established mainly according to the release date. The models with publicly available model\\ncheckpoints are highlighted in green color.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 5, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:6\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nAttention(Q, K, V) = softmax\\n \\nQKùëá\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñé\\n!\\nV,\\n(3)\\nwhere H(ùëô‚àí1) ‚ààRùëõ√óùëëùëöùëúùëëùëíùëôdenotes the input to the ùëô-th Transformer layer, while h(ùëô) ‚ààRùëõ√óùëëùëöùëúùëëùëíùëô\\nrepresents the output of MHSA sub-layer. The quantity of distinct attention heads is represented\\nby ‚Ñé, and ùëëùëöùëúùëëùëíùëôrefers to the model dimension. The set of projections\\nn\\nWQ\\nùëñ, WK\\nùëñ, WV\\nùëñ, WO\\nùëñ\\no\\n‚àà\\nRùëëùëöùëúùëëùëíùëô√óùëëùëöùëúùëëùëíùëô/‚Ñéencompasses the affine transformation parameters for each attention head Headùëñ,\\ntransforming the Query Q, Key K, Value V, and the output of the attention sub-layer. The softmax\\nfunction is applied in a row-wise manner. The dot-products of queries and keys are divided by\\na scaling factor\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñéto counteract the potential risk of excessive large inner products and\\ncorrespondingly diminished gradients in the softmax function, thus encouraging a more balanced\\nattention landscape.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 5, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñéto counteract the potential risk of excessive large inner products and\\ncorrespondingly diminished gradients in the softmax function, thus encouraging a more balanced\\nattention landscape.\\nIn addition to multi-head self-attention, there are two other types of attention based on the\\nsource of queries and key-value pairs:\\n‚Ä¢ Masked Multi-Head Self-Attention. Within the decoder layers of the Transformer, the\\nself-attention mechanism is constrained by introducing an attention mask, ensuring that\\nqueries at each position can only attend to all key-value pairs up to and inclusive of that\\nposition. To facilitate parallel training, this is typically executed by assigning a value of 0\\nto the lower triangular part and setting the remaining elements to ‚àí‚àû. Consequently, each\\nitem attends only to its predecessors and itself. Formally, this modification in Equation 3 can\\nbe depicted as follows:\\nAttention(Q, K, V) = softmax\\n \\nQKùëá\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñé\\n+ Mùëöùëéùë†ùëò\\n!\\nV,\\n(4)\\nMùëöùëéùë†ùëò=\\n\\x10\\nùëöùëñùëó\\n\\x11\\nùëõ√óùëõ=\\n\\x10\\nI(ùëñ‚â•ùëó)\\n\\x11'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 5, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='be depicted as follows:\\nAttention(Q, K, V) = softmax\\n \\nQKùëá\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñé\\n+ Mùëöùëéùë†ùëò\\n!\\nV,\\n(4)\\nMùëöùëéùë†ùëò=\\n\\x10\\nùëöùëñùëó\\n\\x11\\nùëõ√óùëõ=\\n\\x10\\nI(ùëñ‚â•ùëó)\\n\\x11\\nùëõ√óùëõ=\\n(\\n0\\nfor ùëñ‚â•ùëó\\n‚àí‚àû\\notherwise ,\\n(5)\\nThis form of self-attention is commonly denoted as autoregressive or causal attention [157].\\n‚Ä¢ Cross-Layer Multi-Head Self-Attention. The queries are derived from the outputs of the\\npreceding (decoder) layer, while the keys and values are projected from the outputs of the\\nencoder.\\n2.1.2\\nPosition-wise Feed-Forward Networks. Within each Transformer layer, a Position-wise Feed-\\nForward Network (PFFN) is leveraged following the MHSA sub-layer to refine the sequence\\nembeddings at each position ùëñin a separate and identical manner, thereby encoding more intricate\\nfeature representations. The PFFN is composed of a pair of linear transformations, interspersed\\nwith a ReLU activation function. Formally,\\nPFFN(‚Ñé(ùëô)) =\\n\\x10\\nConcat\\nn\\nFFN(‚Ñé(ùëô)\\nùëñ)ùëáoùëõ\\nùëñ=1\\n\\x11ùëá\\n,\\n(6)\\nFFN(‚Ñé(ùëô)\\nùëñ) = ReLU(‚Ñé(ùëô)\\nùëñW(1) + ùëè(1))W(2) + ùëè(2),\\n(7)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 5, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='with a ReLU activation function. Formally,\\nPFFN(‚Ñé(ùëô)) =\\n\\x10\\nConcat\\nn\\nFFN(‚Ñé(ùëô)\\nùëñ)ùëáoùëõ\\nùëñ=1\\n\\x11ùëá\\n,\\n(6)\\nFFN(‚Ñé(ùëô)\\nùëñ) = ReLU(‚Ñé(ùëô)\\nùëñW(1) + ùëè(1))W(2) + ùëè(2),\\n(7)\\nwhere ‚Ñé(ùëô) ‚ààRùëõ√óùëëùëöùëúùëëùëíùëôis the outputs of MHSA sub-layer in ùëô-th Transformer layer, and ‚Ñé(ùëô)\\nùëñ\\n‚àà\\nRùëëùëöùëúùëëùëíùëôdenotes the latent representation at each sequence position. The projection matrices\\n\\x08\\nW(1), (W(2))ùëá\\t\\n‚ààRùëëùëöùëúùëëùëíùëô√ó4ùëëùëöùëúùëëùëíùëôand bias vectors {b(1), b(2)} ‚ààRùëëùëöùëúùëëùëíùëôare parameters learned\\nduring training. These parameters remain consistent across all positions while are individually\\ninitialized from layer to layer. In this context, ùëárepresents the transpose operation on a matrix.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 6, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:7\\nMasked\\nMulti-Head\\nSelf-Attention\\nMulti-Head\\nSelf-Attention\\n+\\n+\\n+\\n+\\n+\\nLayer Norm\\nPosition-wise\\nFeed Forward\\nLinear & Softmax\\nToken & Position\\nEmbedding\\nInputs\\nOutputs (Shifted Right)\\nMulti-Head\\nSelf-Attention\\nLayer Norm\\nLayer Norm\\nLayer Norm\\nLayer Norm\\nPosition-wise\\nFeed Forward\\nToken & Position\\nEmbedding\\nOutput Probabilities\\nùëÅ√ó\\nùëÅ√ó\\n(a) Encoder-Decoder Models\\nMasked\\nMulti-Head\\nSelf-Attention\\nPosition-wise\\nFeed Forward\\n+\\n+\\nLinear & Softmax\\nToken & Position\\nEmbedding\\nInputs\\nLayer Norm\\nLayer Norm\\nOutput Probabilities\\nùëÅ√ó\\n(b) Decoder-only Models\\nFig. 2. The overview of large language models (LLMs) with encoder-decoder and decoder-only Transformer\\narchitecture for code generation, adapted from [257].\\n2.1.3\\nResidual Connection and Normalization. To alleviate the issue of vanishing or exploding\\ngradients resulting from network deepening, the Transformer model incorporates a residual con-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 6, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='2.1.3\\nResidual Connection and Normalization. To alleviate the issue of vanishing or exploding\\ngradients resulting from network deepening, the Transformer model incorporates a residual con-\\nnection [94] around each of the aforementioned modules, followed by Layer Normalization [18].\\nFor the placement of Layer Normalization operation, there are two widely used approaches: 1)\\nPost-Norm: Layer normalization is implemented subsequent to the element-wise residual addition,\\nin accordance with the vanilla Transformer [257]. 2) Pre-Norm: Layer normalization is applied to\\nthe input of each sub-layer, as seen in models like GPT-2 [214]. Formally, it can be formulated as:\\nPost-Norm : H(l) = LayerNorm(PFFN(h(l)) + h(l)),\\nh(l) = LayerNorm(MHSA(H(l‚àí1)) + H(l‚àí1))\\n(8)\\nPre-Norm : H(l) = PFFN(LayerNorm(h(l))) + h(l),\\nh(l) = MHSA(LayerNorm(H(l‚àí1))) + H(l‚àí1)\\n(9)\\n2.1.4\\nPositional Encoding. Given that self-attention alone cannot discern the positional information'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 6, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='(8)\\nPre-Norm : H(l) = PFFN(LayerNorm(h(l))) + h(l),\\nh(l) = MHSA(LayerNorm(H(l‚àí1))) + H(l‚àí1)\\n(9)\\n2.1.4\\nPositional Encoding. Given that self-attention alone cannot discern the positional information\\nof each input token, the vanilla Transformer introduces an absolute positional encoding method to\\nsupplement this positional information, known as sinusoidal position embeddings [257]. Specifically,\\nfor a token at position ùëùùëúùë†, the position embedding is defined as:\\npùëùùëúùë†,2ùëñ= sin(\\nùëùùëúùë†\\n100002ùëñ/ùëëùëöùëúùëëùëíùëô),\\n(10)\\npùëùùëúùë†,2ùëñ+1 = cos(\\nùëùùëúùë†\\n100002ùëñ/ùëëùëöùëúùëëùëíùëô),\\n(11)\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 7, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:8\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nwhere 2ùëñ, 2ùëñ+1 represent the dimensions of the position embedding, while ùëëùëöùëúùëëùëíùëôdenotes the model\\ndimension. Subsequently, each position embedding is added to the corresponding token embedding,\\nand the sum is fed into the Transformer. Since the inception of this method, a variety of innovative\\npositional encoding approaches have emerged, such as learnable embeddings [66], relative position\\nembeddings [232], RoPE [243], and ALiBi [211]. For more detailed descriptions of each method,\\nplease consult [157, 318].\\n2.1.5\\nArchitecture. There are two types of Transformer architecture for code generation task,\\nincluding encoder-decoder and decoder-only. For the encoder-decoder architecture, it consists of\\nboth an encoder and a decoder, in which the encoder processes the input data and generates a\\nset of representations, which are then used by the decoder to produce the output. However, for'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 7, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='both an encoder and a decoder, in which the encoder processes the input data and generates a\\nset of representations, which are then used by the decoder to produce the output. However, for\\ndecoder-only architecture, it consists only of the decoder part of the transformer, where it uses\\na single stack of layers to both process input data and generate output. Therefore, the encoder-\\ndecoder architecture is suited for tasks requiring mapping between different input and output\\ndomains, while the decoder-only architecture is designed for tasks focused on sequence generation\\nand continuation. The overview of LLMs with these two architectures are illustrated in Figure 2.\\n2.2\\nCode Generation\\nLarge language models (LLMs) for code generation refer to the use of LLM to generate source\\ncode from natural language descriptions, a process also known as a natural-language-to-code\\ntask. Typically, these natural language descriptions encompass programming problem statements'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 7, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='code from natural language descriptions, a process also known as a natural-language-to-code\\ntask. Typically, these natural language descriptions encompass programming problem statements\\n(or docstrings) and may optionally include some programming context (e.g., function signatures,\\nassertions, etc.). Formally, these natural language (NL) descriptions can be represented as x. Given x,\\nthe use of an LLM with model parameters ùúÉto generate a code solution y can be denoted as ùëÉùúÉ(y | x).\\nThe advent of in-context learning abilities in LLM [275] has led to the appending of exemplars to\\nthe natural language description x as demonstrations to enhance code generation performance or\\nconstrain the generation format [145, 206]. A fixed set of ùëÄexemplars is denoted as {(xi, yi)}ùëÄ\\nùëñ=1.\\nConsequently, following [190], a more general formulation of LLMs for code generation with\\nfew-shot (or zero-shot) exemplars can be revised as:\\nùëÉùúÉ(y | x) ‚áíùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(12)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 7, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='few-shot (or zero-shot) exemplars can be revised as:\\nùëÉùúÉ(y | x) ‚áíùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(12)\\nwhere prompt(x, {(xi, yi)}ùëò\\nùëñ=1)) is a string representation of the overall input, and {(xi, yi)}ùëò\\nùëñ=1\\ndenotes a set of ùëòexemplars randomly selected from {(xi, yi)}ùëÄ\\nùëñ=1. In particular, when ùëò= 0, this\\ndenotes zero-shot code generation, equivalent to vanilla ones without in-context learning. In the\\ndecoding process, a variety of decoding strategies can be performed for code generation, including\\ndeterministic-based strategies (e.g., greedy search and beam search) and sampling-based strategies\\n(e.g., temperature sampling, top-k sampling, and top-p (nucleus) sampling). For more detailed\\ndescriptions of each decoding strategy, please consult [99]. For example, the greedy search and\\nsampling-based decoding strategies can be formulated as follows:\\nGreedy Search : y‚àó= argmax\\ny\\nùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(13)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 7, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='sampling-based decoding strategies can be formulated as follows:\\nGreedy Search : y‚àó= argmax\\ny\\nùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(13)\\nSampling : y ‚àºùëÉùúÉ(y | prompt(x, {(xi, yùëñ)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(14)\\nTo verify the functionality correctness of the generated code solution, y is subsequently executed\\nvia a compiler or interpreter, represented as Exe(¬∑), on a suit of unit tests T. The feedback from\\nthis execution can be denoted as Feedback(Exe(y, T)). If the generated code solution fails to pass\\nall test cases, the error feedback can be iteratively utilized to refine the code by leveraging the\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 8, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:9\\nResearch\\nQuestions\\nLarge Language\\nModel (LLM)\\nCode Generation\\nTop-tier LLMs\\nand SE Venues\\nTotal 235 Papers\\nSnowballing\\nSearch\\nQuality\\nAssessment\\nInclusion and\\nExclusion Criteria\\nAutomatic\\nFiltering\\nSearch Strings\\nAutomated Search\\nManual Search\\n235\\n247\\n351\\n294\\n73\\n36\\n261\\nFig. 3. Overview of the paper search and collection process.\\nprevious attempt (yùëùùëüùëí) and the associated feedback. Formally,\\ny ‚àºùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1, yùëùùëüùëí, Feedback(Exe(y, T)))),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(15)\\nFurther details and relevant studies on using feedback to improve code generation are comprehen-\\nsively discussed in Section 5.5 and 5.6.\\n3\\nMETHODOLOGY\\nIn this section, we detail the systematic methodologies employed in conducting literature reviews.\\nWe follow the systematic literature review methodology outlined by [131], which has been widely\\nadopted in numerous software engineering literature reviews [101, 146, 169, 219, 262]. The overall'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 8, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='We follow the systematic literature review methodology outlined by [131], which has been widely\\nadopted in numerous software engineering literature reviews [101, 146, 169, 219, 262]. The overall\\nprocess is illustrated in Figure 3, and the detailed steps in our methodology are documented below.\\n3.1\\nResearch Questions\\nTo deliver a comprehensive and up-to-date literature review on the latest advancements in large\\nlanguage models (LLMs) for code generation, this systematic literature review addresses the\\nfollowing research questions (RQs):\\nRQ1: How can we categorize and evaluate the latest advances in LLMs for code genera-\\ntion? The recent proliferation of LLMs has resulted in many of these models being adapted for code\\ngeneration task. While the adaptation of LLMs for code generation essentially follows the evolution\\nof LLMs, this evolution encompasses a broad spectrum of research directions and advancements.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 8, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='generation task. While the adaptation of LLMs for code generation essentially follows the evolution\\nof LLMs, this evolution encompasses a broad spectrum of research directions and advancements.\\nFor software engineering (SE) researchers, it can be challenging and time-consuming to fully grasp\\nthe comprehensive research landscape of LLMs and their adaptation to code generation. RQ1 aims\\nto propose a taxonomy that serves as a comprehensive reference for researchers, enabling them to\\nquickly familiarize themselves with the state-of-the-art in this dynamic field and identify specific\\nresearch problems and directions of interest.\\nRQ2: What are the key insights into LLMs for code generation? RQ2 seeks to assist\\nresearchers in establishing a comprehensive, up-to-date, and advanced understanding of LLMs for\\ncode generation. This includes discussing various aspects of this rapidly evolving domain, such as'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 8, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='researchers in establishing a comprehensive, up-to-date, and advanced understanding of LLMs for\\ncode generation. This includes discussing various aspects of this rapidly evolving domain, such as\\ndata curation, latest advancements, performance evaluation, ethical and environmental implications,\\nand real-world applications. A historical overview of the evolution of LLMs for code generation is\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 9, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:10\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 2. Publication venues for conference proceedings and journals articles for manual search.\\nDomain\\nVenue\\nAcronym\\nLLMs\\nInternational Conference on Learning Representations\\nICLR\\nConference on Neural Information Processing Systems\\nNeurIPS\\nInternational Conference on Machine Learning\\nICML\\nAnnual Meeting of the Association for Computational Linguistics\\nACL\\nConference on Empirical Methods in Natural Language Processing\\nEMNLP\\nInternational Joint Conference on Artificial Intelligence\\nNAACL\\nAAAI Conference on Artificial Intelligence\\nAAAI\\nSE\\nInternational Conference on Software Engineering\\nICSE\\nJoint European Software Engineering Conference and Symposium on the Foundations of Software Engineering\\nESEC/FSE\\nInternational Conference on Automated Software Engineering\\nASE\\nTransactions on Software Engineering and Methodology\\nTOSEM\\nTransactions on Software Engineering\\nTSE\\nInternational Symposium on Software Testing and Analysis'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 9, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ASE\\nTransactions on Software Engineering and Methodology\\nTOSEM\\nTransactions on Software Engineering\\nTSE\\nInternational Symposium on Software Testing and Analysis\\nISSTA\\nTable 3. Keywords related to LLMs and code generation task for automated search.\\nKeywords Related to LLMs\\nKeywords Related to Code Generation Task\\nCode Large Language Model‚àó, Code LLMs, Code Language Model,\\nCode LMs, Large Language Model‚àó, LLM, Language Model‚àó, LM,\\nPre-trained Language Model‚àó, PLM, Pre-trained model,\\nNatural Language Processing, NLP, GPT-3, ChatGPT, GPT-4, LLaMA,\\nCodeLlama, PaLM‚àó, CodeT5, Codex, CodeGen, InstructGPT\\nCode Generation, Program Synthesis, Code Intelligence,\\n‚àóCoder‚àó, natural-language-to-code, NL2Code, Programming\\nprovided, along with an empirical comparison using the widely recognized HumanEval and MBPP\\nbenchmarks, as well as the more practical and challenging BigCodeBench benchmark, to highlight\\nthe progressive enhancements in LLM capabilities for code generation. RQ2 offers an in-depth'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 9, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='benchmarks, as well as the more practical and challenging BigCodeBench benchmark, to highlight\\nthe progressive enhancements in LLM capabilities for code generation. RQ2 offers an in-depth\\nanalysis of critical insights related to LLMs for code generation.\\nRQ3: What are the critical challenges and promising research opportunities in LLMs for\\ncode generation? Despite the revolutionary impact of LLMs on the paradigm of code generation\\nand their remarkable performance, numerous challenges remain unaddressed. These challenges\\nprimarily stem from the gap between academic research and practical development. For instance,\\nwhile the HumanEval benchmark is established as a de facto standard for evaluating the coding\\nproficiency of LLMs in academia, it has been shown that this evaluation does not adequately reflect\\npractical development scenarios [68, 72, 123, 162]. RQ3 aims to identify critical challenges and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 9, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='proficiency of LLMs in academia, it has been shown that this evaluation does not adequately reflect\\npractical development scenarios [68, 72, 123, 162]. RQ3 aims to identify critical challenges and\\nhighlight promising opportunities to bridge the gap between research and practical application.\\n3.2\\nSearch Process\\n3.2.1\\nSearch Strings. To address the aforementioned three research questions (RQs), we initiate a\\nmanual review of conference proceedings and journal articles from top-tier venues in the fields of\\nLLMs and SE, as detailed in Table 2. This process allowed us to identify relevant studies and derive\\nsearch strings, which are subsequently utilized for an automated search across various scientific\\ndatabases. The complete set of search keywords is presented in Table 3.\\n3.2.2\\nSearch Databases. Following the development of search strings, we executed an automated\\nsearch using four popular scientific databases: the ACM Digital Library, IEEE Xplore Digital Library,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 9, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='3.2.2\\nSearch Databases. Following the development of search strings, we executed an automated\\nsearch using four popular scientific databases: the ACM Digital Library, IEEE Xplore Digital Library,\\narXiv, and DBLP. Our search focus on identifying papers whose titles contain keywords pertinent\\nto LLMs and code generation. This approach enhances the likelihood of retrieving relevant papers\\nsince both sets of keywords must be present in the title. Although this title-based search strategy\\neffectively retrieves a large volume of papers, it is important to note that in some instances [238],\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 10, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:11\\nthe scope of code generation can be broader, encompassing areas such as code completion, code\\ntranslation, and program synthesis. As outlined in Section 1, this survey adopts a prevalent definition\\nof code generation as the natural-language-to-code (NL2Code) task [16, 17, 307].\\nConsequently, we conduct further automatic filtering based on the content of the papers. Papers\\nfocusing on ‚Äúcode completion‚Äù and ‚Äúcode translation‚Äù are excluded unless they pertain to the\\nspecific topics discussed in Section 5.7 and Section 5.8, where code completion is a primary focus.\\nAfter completing the automated search, the results from each database are merged and deduplicated\\nusing scripts. This process yields 294 papers from arXiv, 73 papers from the ACM Digital Library,\\n36 papers from IEEE Xplore, and 261 papers from DBLP.\\n3.3\\nInclusion and Exclusion Criteria'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 10, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='using scripts. This process yields 294 papers from arXiv, 73 papers from the ACM Digital Library,\\n36 papers from IEEE Xplore, and 261 papers from DBLP.\\n3.3\\nInclusion and Exclusion Criteria\\nThe search process conducted across various databases and venues is intentionally broad to gather\\na comprehensive pool of candidate papers. This approach maximizes the collection of potentially\\nrelevant studies. However, such inclusivity may lead to the inclusion of papers that do not align\\nwith the scope of this survey, as well as duplicate entries from multiple sources. To address this,\\nwe have established a clear set of inclusion and exclusion criteria, based on the guidelines from\\n[101, 260]. These criteria are applied to each paper to ensure alignment with our research scope\\nand questions, and to eliminate irrelevant studies.\\nInclusion Criteria. A paper will be included if it meets any of the following criteria:\\n‚Ä¢ It is available in full text.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 10, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and questions, and to eliminate irrelevant studies.\\nInclusion Criteria. A paper will be included if it meets any of the following criteria:\\n‚Ä¢ It is available in full text.\\n‚Ä¢ It presents a dataset or benchmark specifically designed for code generation with LLMs.\\n‚Ä¢ It explores specific LLM techniques, such as pre-training or instruction tuning, for code\\ngeneration.\\n‚Ä¢ It provides an empirical study or evaluation related to the use of LLMs for code generation.\\n‚Ä¢ It discusses the ethical considerations and environmental impact of deploying LLMs for code\\ngeneration.\\n‚Ä¢ It proposes tools or applications powered by LLMs for code generation.\\nExclusion Criteria. Conversely, papers will be excluded if they meet any of the following\\nconditions:\\n‚Ä¢ They are not written in English.\\n‚Ä¢ They are found in books, theses, monographs, keynotes, panels, or venues (excluding arXiv)\\nthat do not undergo a full peer-review process.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 10, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='conditions:\\n‚Ä¢ They are not written in English.\\n‚Ä¢ They are found in books, theses, monographs, keynotes, panels, or venues (excluding arXiv)\\nthat do not undergo a full peer-review process.\\n‚Ä¢ They are duplicate papers or different versions of similar studies by the same authors.\\n‚Ä¢ They focus on text generation rather than source code generation, such as generating code\\ncomments, questions, test cases, or summarization.\\n‚Ä¢ They do not address the task of code generation, for instance, focusing on code translation\\ninstead.\\n‚Ä¢ They leverage software engineering methods to enhance code generation without emphasiz-\\ning LLMs.\\n‚Ä¢ They do not utilize LLMs, opting for other models like Long Short-Term Memory (LSTM)\\nnetworks.\\n‚Ä¢ They use encoder-only language models, such as BERT, which are not directly applicable to\\ncode generation task.\\n‚Ä¢ LLMs are mentioned only in future work or discussions without being central to the proposed\\napproach.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 10, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='code generation task.\\n‚Ä¢ LLMs are mentioned only in future work or discussions without being central to the proposed\\napproach.\\nPapers identified through both manual and automated searches undergo a detailed manual review\\nto ensure they meet the inclusion criteria and do not fall under the exclusion criteria. Specifically,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 11, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:12\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nthe first two authors independently review each paper to determine its eligibility. In cases of\\ndisagreement, the third author makes the final inclusion decision.\\n3.4\\nQuality Assessment\\nTo ensure the inclusion of high-quality studies, we have developed a comprehensive set of ten\\nQuality Assessment Criteria (QAC) following [101]. These QAC are designed to evaluate the\\nrelevance, clarity, validity, and significance of the papers considered for our review.\\nIn accordance with [101], the first three QAC assess the study‚Äôs alignment with our objectives.\\nThese criteria are rated as ‚Äúirrelevant/unmet‚Äù, ‚Äúpartially relevant/met‚Äù, or ‚Äúrelevant/fully met‚Äù,\\ncorresponding to scores of -1, 0, and 1, respectively. If a study receive a score of -1 across these\\ninitial three criteria, it is deemed ineligible for further consideration and subsequently excluded\\nfrom our review process.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 11, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='initial three criteria, it is deemed ineligible for further consideration and subsequently excluded\\nfrom our review process.\\nThe subsequent seven QAC focus on a more detailed content evaluation, employing a scoring\\nrange of -1 to 2, representing ‚Äúpoor‚Äù, ‚Äúfair‚Äù, ‚Äúgood‚Äù, and ‚Äúexcellent‚Äù. We compute a cumulative score\\nbased on the responses to QAC4 through QAC10 for each paper. For published works, the maximum\\nachievable score is 14 (2 points per question). We retain those with a score of 11.2 (80% of the total\\nscore) or higher. For unpublished papers available on arXiv, QAC4 defaults to a score of 0, making\\nthe maximum possible score for the remaining criteria 12. Accordingly, we retain papers scoring\\n9.6 (80% of the adjusted total score) or above .\\n‚Ä¢ QAC1: Is the research not classified as a secondary study, such as a systematic literature\\nreview or survey? (-1, 0, 1)\\n‚Ä¢ QAC2: Does the study incorporate the use of LLMs? (-1, 0, 1)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 11, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ QAC1: Is the research not classified as a secondary study, such as a systematic literature\\nreview or survey? (-1, 0, 1)\\n‚Ä¢ QAC2: Does the study incorporate the use of LLMs? (-1, 0, 1)\\n‚Ä¢ QAC3: Is the study relevant to the code generation task? (-1, 0, 1)\\n‚Ä¢ QAC4: Is the research published in a prestigious venue? (-1, 0, 1, 2)\\n‚Ä¢ QAC5: Does the study present a clear research motivation? (-1, 0, 1, 2)\\n‚Ä¢ QAC6: Are the key contributions and limitations of the study discussed? (-1, 0, 1, 2)\\n‚Ä¢ QAC7: Does the study contribute to the academic or industrial community? (-1, 0, 1, 2)\\n‚Ä¢ QAC8: Are the LLM techniques employed in the study clearly described? (-1, 0, 1, 2)\\n‚Ä¢ QAC9: Are the experimental setups, including experimental environments and dataset infor-\\nmation, thoroughly detailed? (-1, 0, 1, 2)\\n‚Ä¢ QAC10: Does the study clearly confirm its experimental findings? (-1, 0, 1, 2)\\n3.5\\nSnowballing Search'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 11, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='mation, thoroughly detailed? (-1, 0, 1, 2)\\n‚Ä¢ QAC10: Does the study clearly confirm its experimental findings? (-1, 0, 1, 2)\\n3.5\\nSnowballing Search\\nFollowing the quality assessment, we establish an initial set of papers for our study. To minimize\\nthe risk of excluding pertinent literature, we implement a snowballing search strategy. Snowballing\\nsearch involves utilizing a paper‚Äôs reference list or its citations to discover additional relevant\\nstudies, known as backward and forward snowballing, respectively. In this survey, we exclusively\\nemployed backward snowballing following [260]. Despite this effort, no additional studies are\\nidentified through this method. This could be attributed to the task-specific nature of the code\\ngeneration (natural-language-to-code), where reference studies are typically published earlier.\\nConsequently, our methodology, which encompassed an extensive manual and automated search,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 11, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='generation (natural-language-to-code), where reference studies are typically published earlier.\\nConsequently, our methodology, which encompassed an extensive manual and automated search,\\nlikely covered the relevant literature comprehensively, explaining the lack of additional studies\\nthrough snowballing search.\\n3.6\\nData Collection and Analysis\\nThe data collection process for our study, illustrated in Figure 3, began with a manual search\\nthrough conference proceedings and journal articles from leading venues in LLMs and SE. This\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 12, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:13\\n2018\\n2019\\n2020\\n2021\\n2022\\n2023\\n2024\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n# Number of Papers\\n1\\n1\\n1\\n6\\n11\\n75\\n140\\nVenue\\nTrend\\nAAAI\\nACL\\nCAV\\nCCS\\nCHI\\nCVPR\\nEMNLP\\nFSE\\nICLR\\nICML\\nICSE\\nISSTA\\nKDD\\nNAACL\\nNeurIPS\\nOthers\\nTACL\\nTOSEM\\nTSE\\nUSENIX\\narXiv\\nPre-Training & \\nFoundation Model (21.5%)\\nFine-tuning (7.5%)\\nReinforcement Learning (4.8%)\\nPrompting (11.8%)\\nEvaluation &\\n Benchmark (24.1%)\\nData\\n Synthesis (1.8%)\\nRepository\\n Level (5.7%)\\nRetrieval Augmented (3.1%)\\nOthers (8.3%)\\nCode LLMs\\n Alignment (7.0%)\\nAutonomous Coding \\nAgents (4.4%)\\nTotal Papers:\\n235\\nResearch Topics\\nPre-Training & Foundation Model\\nFine-tuning\\nReinforcement Learning\\nPrompting\\nEvaluation & Benchmark\\nData Synthesis\\nRepository Level\\nRetrieval Augmented\\nOthers\\nCode LLMs Alignment\\nAutonomous Coding Agents\\nFig. 4. Data qualitative analysis. Top: Annual distribution of selected papers across various publication'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 12, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Repository Level\\nRetrieval Augmented\\nOthers\\nCode LLMs Alignment\\nAutonomous Coding Agents\\nFig. 4. Data qualitative analysis. Top: Annual distribution of selected papers across various publication\\nvenues. Bottom: Distribution analysis of research topics covered in the included papers.\\ninitial step yielded 42 papers, from which we extracted relevant search strings. Following this,\\nwe performed an automated search across four academic databases using keyword-based queries,\\nresulting in the retrieval of 664 papers. After performing automatic filtering (351 papers), applying\\ninclusion and exclusion criteria (247 papers), conducting quality assessments (235 papers), and\\nutilizing snowballing search (235 papers), we finalize a collection of 235 papers focusing on LLMs\\nfor code generation.\\nTo provide insights from the selected papers, we begin by presenting an overview of their distribu-\\ntion across publication venues each year, as illustrated at the top of Figure 4. Our analysis indicates'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 12, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='tion across publication venues each year, as illustrated at the top of Figure 4. Our analysis indicates\\nthat 14% of the papers are published in LLM-specific venues and 7% in SE venues. Remarkably, 49%\\nof the papers remain unpublished in peer-reviewed venues and are available on arXiv. This trend is\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 13, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:14\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nunderstandable given the emerging nature of this field, with many works being recent and pending\\nformal submission. Despite the absence of peer review on arXiv, our quality assessment process\\nensures that only high-quality papers are included, thereby maintaining the integrity of this survey.\\nFurthermore, the annual trend in the number of collected papers indicates nearly exponential\\ngrowth in the field. From a single paper in the period 2018 to 2020, the numbers increased to 6 in\\n2021, 11 in 2022, 75 in 2023, and 140 in 2024. This trend reflects growing interest and attention\\nin this research area, with expectations for continued expansion in the future. Additionally, to\\ncapture the breadth of advancements in LLMs for code generation, we conducted a distribution\\nanalysis of the research topics covered in the included papers, as shown at the bottom of Figure 4.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 13, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='capture the breadth of advancements in LLMs for code generation, we conducted a distribution\\nanalysis of the research topics covered in the included papers, as shown at the bottom of Figure 4.\\nWe observe that the development of LLMs for code generation closely aligns with broader trends\\nin general-purpose LLM research. Notably, the most prevalent research topics are Pre-training and\\nFoundation Models (21.5%), Prompting (11.8%), and Evaluation and Benchmarks (24.1%). These\\nareas hold significant promise for enhancing, refining, and evaluating LLM-driven code generation.\\n4\\nTAXONOMY\\nThe recent surge in the development of LLMs has led to a significant number of these models\\nbeing repurposed for code generation task through continual pre-training or fine-tuning. This\\ntrend is particularly observable in the realm of open-source models. For instance, Meta AI initially\\nmade the LLaMA [252] model publicly available, which was followed by the release of Code Llama'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 13, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='trend is particularly observable in the realm of open-source models. For instance, Meta AI initially\\nmade the LLaMA [252] model publicly available, which was followed by the release of Code Llama\\n[227], designed specifically for code generation. Similarly, DeepSeek LLM [26] developed and\\nreleased by DeepSeek has been extended to create DeepSeek Coder [88], a variant tailored for code\\ngeneration. The Qwen team has developed and released Code Qwen [249], building on their original\\nQwen [20] model. Microsoft, on the other hand, has unveiled WizardLM [289] and is exploring its\\ncoding-oriented counterpart, WizardCoder [173]. Google has joined the fray by releasing Gemma\\n[248], subsequently followed by Code Gemma [59]. Beyond simply adapting general-purpose LLMs\\nfor code-related tasks, there has been a proliferation of models specifically engineered for code\\ngeneration. Notable examples include StarCoder [147], OctoCoder [187], and CodeGen [193]. These'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 13, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='for code-related tasks, there has been a proliferation of models specifically engineered for code\\ngeneration. Notable examples include StarCoder [147], OctoCoder [187], and CodeGen [193]. These\\nmodels underscore the trend of LLMs being developed with a focus on code generation.\\nRecognizing the importance of these developments, we conduct a thorough analysis of selected\\npapers on LLMs for code generation, sourced from widely used scientific databases as mentioned\\nin Section 3. Based on this analysis, we propose a taxonomy that categorizes and evaluates the\\nlatest advancements in LLMs for code generation. This taxonomy, depicted in Figure 6, serves as a\\ncomprehensive reference for researchers seeking to quickly familiarize themselves with the state-\\nof-the-art in this dynamic field. It is important to highlight that the category of recent advances\\nemphasizes the core techniques used in the current state-of-the-art code LLMs.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 13, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='of-the-art in this dynamic field. It is important to highlight that the category of recent advances\\nemphasizes the core techniques used in the current state-of-the-art code LLMs.\\nIn the subsequent sections, we will provide an in-depth analysis of each category related to code\\ngeneration. This will encompass a definition of the problem, the challenges to be addressed, and a\\ncomparison of the most prominent models and their performance evaluation.\\n5\\nLARGE LANGAUGE MODELS FOR CODE GENERATION\\nLLMs with Transformer architecture have revolutionized a multitude of fields, and their application\\nin code generation has been particularly impactful. These models follow a comprehensive process\\nthat starts with the curation and synthesis of code data, followed by a structured training approach\\nthat includes pre-training and fine-tuning (instruction tuning), reinforcement learning with various\\nfeedback, and the use of sophisticated prompt engineering techniques. Recent advancements have'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 13, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='that includes pre-training and fine-tuning (instruction tuning), reinforcement learning with various\\nfeedback, and the use of sophisticated prompt engineering techniques. Recent advancements have\\nseen the integration of repository-level and retrieval-augmented code generation, as well as the\\ndevelopment of autonomous coding agents. Furthermore, the evaluation of coding abilities of LLMs\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 14, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:15\\nPretrained\\n(Base) LLM\\nInstruct\\nCode LLM\\nSupervised\\nFine-tuning\\n(SFT)\\nHuman Preference\\nAlignment with RL\\n(e.g., RLHF)\\n(Optional)\\nPre-training Database\\nInstruction Database\\nPreference Database\\nStage ‚ë†\\nStage ‚ë¢\\nStage ‚ë£\\nPre-training Database\\nStage ‚ë°\\nContinual\\nPre-training\\n(Optional)\\nTask\\nDescription\\nGenerated\\nSource Code\\nInference\\nEvaluation\\nBenchmark\\nFig. 5. A diagram illustrating the general training, inference, and evaluation workflow for Code LLMs and\\ntheir associated databases. The training workflow is mainly divided into four distinct stages: Stage 1‚óãand 2‚óã\\nare the pre-training phase, whereas Stages 3‚óãand 4‚óãrepresent the post-training phases. It is important to\\nnote that Stage 2‚óãand 4‚óãare optional. For instance, StarCoder [147] incorporates only Stage 1‚óã. WizardCoder\\n[173], fine-tuned upon StarCoder, includes only Stage 3‚óã, while Code Llama [227], continually pre-trained on'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 14, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[173], fine-tuned upon StarCoder, includes only Stage 3‚óã, while Code Llama [227], continually pre-trained on\\nLlama 2, encompasses Stages 2‚óãand 3‚óã. DeepSeek-Coder-V2 [331], continually pre-trained on DeepSeek-V2,\\ncovers Stages 2‚óã, 3‚óã, and 4‚óã. Note that pre-trained model can be directly used for inference through prompt\\nengineering.\\nhas become a critical component of this research area. Figure 5 illustrates the general training,\\ninference, and evaluation workflow for Code LLMs and their associated databases.\\nIn the forthcoming sections, we will explore these dimensions of LLMs in the context of code\\ngeneration in detail. Section 5.1 will address the data curation and processing strategies employed\\nthroughout the various stages of LLM development. Section 5.2 will discuss data synthesis methods\\ndesigned to mitigate the scarcity of high-quality data. Section 5.3 will outline the prevalent model\\narchitectures used in LLMs for code generation. Moving to Section 5.4, we will examine the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 14, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='designed to mitigate the scarcity of high-quality data. Section 5.3 will outline the prevalent model\\narchitectures used in LLMs for code generation. Moving to Section 5.4, we will examine the\\ntechniques for full parameter fine-tuning and parameter-efficient fine-tuning, which are essential\\nfor tailoring LLMs to code generation task. Section 5.5 will shed light on enhancing code quality\\nthrough reinforcement learning, utilizing the power of feedback. Section 5.6 will delve into the\\nstrategic use of prompts to maximize the coding capabilities of LLMs. The innovative approaches\\nof repository-level and retrieval-augmented code generation will be elaborated in Sections 5.7 and\\n5.8, respectively. Additionally, Section 5.9 will discuss the exciting field of autonomous coding\\nagents. Section 5.10 discusses various evaluation strategies and offer an empirical comparison using\\nthe widely recognized HumanEval, MBPP, and the more practical and challenging BigCodeBench'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 14, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='agents. Section 5.10 discusses various evaluation strategies and offer an empirical comparison using\\nthe widely recognized HumanEval, MBPP, and the more practical and challenging BigCodeBench\\nbenchmarks to highlight the progressive enhancements in LLM capabilities for code generation.\\nFurthermore, the ethical implications and the environmental impact of using LLMs for code\\ngeneration are discussed in Section 5.11, aiming to establish a trustworthiness, responsibility, safety,\\nefficiency, and green of LLM for code generation. Lastly, Section 5.12 will provide insights into some\\nof the practical applications that leverage LLMs for code generation, demonstrating the real-world\\nimpact of these sophisticated models. Through this comprehensive exploration, we aim to highlight\\nthe significance and potential of LLMs within the domain of automated code generation.\\n5.1\\nData Curation & Processing\\nThe exceptional performance of LLMs can be attributed to their training on large-scale and diverse'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 14, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='5.1\\nData Curation & Processing\\nThe exceptional performance of LLMs can be attributed to their training on large-scale and diverse\\ndatasets [307]. Meanwhile, the extensive parameters of these models necessitate substantial data to\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 15, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:16\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nLLMs for Code Generation\\nData\\nCuration\\n(Sec. 5.1)\\nPre-training\\nCodeSearchNet[110], Google BigQuery[96], The Pile[78], CodeParrot[254], GitHub Code[254]\\nROOTS[137], The Stack[132], The Stack v2[170]\\nInstruction\\nTuning\\nCommitPackFT [187], Code Alpaca[43], OA-Leet[63], OSS-Instruct[278], Evol-instruction[225]\\nSelf-OSS-Instruct-SC2-Exec-Filter[304]\\nBenchmarks\\nGeneral\\nHumanEval[48], HumanEval+[162], HumanEvalPack[187], MBPP[17]\\nMBPP+[162], CoNaLa[297], Spider[300], CONCODE[113], ODEX[273]\\nCoderEval[299], ReCode[263], StudentEval[19]\\nCompetitions\\nAPPS[95], CodeContests[151]\\nData Science\\nDSP[41], DS-1000[136], ExeDS[107]\\nMultilingual\\nMBXP[16], Multilingual HumanEval[16], HumanEval-X[321], MultiPL-E[39]\\nxCodeEval[128]\\nReasoning\\nMathQA-X[16], MathQA-Python[17], GSM8K[58], GSM-HARD[79]\\nRepository\\nRepoEval[309], Stack-Repo[239], Repobench[167], EvoCodeBench[144]\\nSWE-bench[123], CrossCodeEval[68], SketchEval[308]\\nRecent\\nAdvances'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 15, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Repository\\nRepoEval[309], Stack-Repo[239], Repobench[167], EvoCodeBench[144]\\nSWE-bench[123], CrossCodeEval[68], SketchEval[308]\\nRecent\\nAdvances\\nData\\nSynthesis\\n(Sec. 5.2)\\nSelf-Instruct [268], Evol-Instruct [289], Phi-1[84], Code Alpaca[43], WizardCoder[173]\\nMagicoder[278], StarCoder2-instruct [304]\\nPre-training\\n(Sec. 5.3)\\nModel\\nArchitectures\\nEncoder-Decoder\\nPyMT5[57], PLBART[7], CodeT5[271], JuPyT5[41]\\nAlphaCode[151], CodeRL[139], ERNIE-Code[40]\\nPPOCoder[238], CodeT5+[269], CodeFusion[241]\\nAST-T5[81]\\nDecoder-Only\\nGPT-C[244], GPT-Neo[30], GPT-J[258], Codex[48]\\nCodeGPT[172], CodeParrot[254], PolyCoder[290]\\nCodeGen[193], GPT-NeoX[29], PaLM-Coder[54]\\nInCoder[77], PanGu-Coder[55], PyCodeGPT[306]\\nCodeGeeX[321], BLOOM[140], ChatGPT[196]\\nSantaCoder[9], LLaMA[252], GPT-4[5]\\nCodeGen2[192], replit-code[223], StarCoder[147]\\nWizardCoder[173], phi-1[84], ChainCoder[323]\\nCodeGeeX2[321], PanGu-Coder2[234], Llama 2[253]\\nOctoPack[187], Code Llama[227], MFTCoder[160]'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 15, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='CodeGen2[192], replit-code[223], StarCoder[147]\\nWizardCoder[173], phi-1[84], ChainCoder[323]\\nCodeGeeX2[321], PanGu-Coder2[234], Llama 2[253]\\nOctoPack[187], Code Llama[227], MFTCoder[160]\\nphi-1.5[150], CodeShell[285], Magicoder[278]\\nAlphaCode 2[11], StableCode[210], WaveCoder[301]\\nphi-2[182], DeepSeek-Coder[88], StepCoder[71]\\nOpenCodeInterpreter[322], StarCoder 2[170]\\nClaude 3[14], ProCoder[27], CodeGemma[59]\\nCodeQwen[249], Llama3[180]\\nStarCoder2-Instruct[304], Codestral[181]\\nPre-training\\nTasks\\nCLM[88, 147, 173, 278], DAE[7, 269, 271], Auxiliary[40, 269, 271]\\nFine-tuning\\nInstruction\\nTuning\\n(Sec. 5.4)\\nFull Parameter\\nFine-tuning\\nCode Alpaca[43], CodeT5+[271], WizardCoder[173]\\nStarCoder[147], Pangu-Coder2[234], OctoPack[187]\\nCodeGeeX2[321], Magicoder[278], CodeGemma[59]\\nStarCoder2-instruct[304]\\nParameter\\nEfficient\\nFine-tuning\\nCodeUp[121], ASTRAIOS[334]\\nReinforcement\\nLearning\\nwith Feedback\\n(Sec. 5.5)\\nCodeRL[139], CompCoder[266], PPOCoder[238], RLTF[163]\\nPanGu-Coder2[234], StepCoder[71]'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 15, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Parameter\\nEfficient\\nFine-tuning\\nCodeUp[121], ASTRAIOS[334]\\nReinforcement\\nLearning\\nwith Feedback\\n(Sec. 5.5)\\nCodeRL[139], CompCoder[266], PPOCoder[238], RLTF[163]\\nPanGu-Coder2[234], StepCoder[71]\\nPrompting\\nEngineering\\n(Sec. 5.6)\\nReflexion[236], LATS[327], Self-Debugging[51], SelfEvolve[122]\\nTheo X. et al.[195], CodeT[45], LEVER[190], AlphaCodium[224]\\nRepository\\nLevel & Long\\nContext\\n(Sec. 5.7)\\nRepoCoder[309], CoCoMIC[69], RepoHyper[209], RLPG[240]\\nRepoformer[282], RepoFusion[239], ToolGen[259], CodePlan[22]\\nCodeS[308]\\nRetrieval\\nAugmented\\n(Sec. 5.8)\\nHGNN[166], REDCODER[205], ReACC[171], DocPrompting[330]\\nRepoCoder[309], Su et al.[242]\\nAutonomous\\nCoding Agents\\n(Sec. 5.9)\\nAgentCoder [104], MetaGPT[100], CodeAct [265], AutoCodeRover [316], Devin[61]\\nOpenDevin[199], SWE-agent[124], L2MAC[98], OpenDevin CodeAct 1.0[287]\\nEvaluation\\n(Sec. 5.10)\\nMetrics\\nExact Match, BLEU[203], ROUGE[156], METEOR[23], CodeBLEU[221], pass@k[48]'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 15, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='OpenDevin[199], SWE-agent[124], L2MAC[98], OpenDevin CodeAct 1.0[287]\\nEvaluation\\n(Sec. 5.10)\\nMetrics\\nExact Match, BLEU[203], ROUGE[156], METEOR[23], CodeBLEU[221], pass@k[48]\\nn@k[151], test case average[95], execution accuracy[218], pass@t[195], perplexity[116]\\nHuman\\nEvaluation\\nCodePlan[22], RepoFusion[239], CodeBLEU[221]\\nLLM-as-a-Judge\\nAlpacaEval[148], MT-bench[320], ICE-Score[332]\\nCode LLMs\\nAlignment\\n(Sec. 5.10.3)\\nGreen[235, 277], Responsibility[168, 292], Efficiency[293], Safety[8, 9, 77, 91, 231, 294, 302], Trustworthiness[120, 202]\\nApplication\\n(Sec. 5.12)\\nGitHub Copilot[48], CodeGeeX[321], CodeWhisperer[12], Codeium[60], CodeArts Snap[234], TabNine[246], Replit[222]\\nFig. 6. Taxonomy of LLMs for code generation.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 16, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:17\\nunlock their full potential, in alignment with established scaling law [97, 127]. For a general-purpose\\nLLM, amassing a large-scale corpus of natural language from a variety of sources is imperative.\\nSuch sources include webpages, conversation data, books and news, scientific data, and code\\n[20, 33, 54, 252, 253, 298], while these data are often crawled from the web and must undergo\\nmeticulous and aggressive pre-processing [217, 317]. Fortunately, multiple platforms and websites\\noffer large-scale, open-source, and permissively licensed code corpora, such as GitHub7 and Stack\\nOverflow8. Notably, the number of stars or forks of GitHub repositories has emerged as a valuable\\nmetric for filtering high-quality code datasets. In a similar vein, the quantity of votes on Stack\\nOverflow can serve to discern the most relevant and superior answers.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 16, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='metric for filtering high-quality code datasets. In a similar vein, the quantity of votes on Stack\\nOverflow can serve to discern the most relevant and superior answers.\\nNonetheless, raw datasets are frequently laden with redundant, noisy data and personal infor-\\nmation, eliciting concerns regarding privacy leakage, which may include the names and email\\naddresses of repository contributors [8, 37, 137]. Consequently, it is essential to undertake rigorous\\ndata-cleaning procedures. Typically, this process encompasses exact match deduplication, code\\ndata filtering based on average line length and a defined threshold for the fraction of alphanumeric\\ncharacters, the removal of auto-generated files through keyword searches, and the expunction of\\npersonal user data [132, 254]. Specifically, the standard data preprocessing workflow is depicted in\\nFigure 7.\\nThe development of a proficient LLM for code generation necessitates the utilization of various'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 16, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Figure 7.\\nThe development of a proficient LLM for code generation necessitates the utilization of various\\ntypes of code data at different developmental stages. Therefore, we categorize code data into three\\ndistinct classes: pre-training datasets, instruction-tuning datasets, and benchmarks for performance\\nevaluation. The subsequent subsections will provide a detailed illustration of code data within each\\nclassification.\\n5.1.1\\nPre-training. The remarkable success of bidirectional pre-trained language models (PLMs)\\nsuch as BERT [66] and unidirectional PLMs like GPT [213] has firmly established the practice of\\npre-training on large-scale unlabeled datasets to endow models with a broad spectrum of general\\nknowledge. Extending this principle to the realm of code generation enables LLMs to assimilate\\nfundamental coding principles, including the understanding of code structure dependencies, the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 16, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='knowledge. Extending this principle to the realm of code generation enables LLMs to assimilate\\nfundamental coding principles, including the understanding of code structure dependencies, the\\nsemantics of code identifiers, and the intrinsic logic of code sequences [48, 85, 269, 271]. In light of\\nthis advancement, there has been a proliferation of large-scale unlabeled code datasets proposed\\nto serve as the foundational training ground for LLMs to develop coding proficiency. A brief\\nintroduction of these datasets is as follows, with the statistics available in Table 4.\\n‚Ä¢ CodeSearchNet [110]: CodeSearchNet corpus is a comprehensive dataset, consisting of 2\\nmillion (comment, code) pairs from open-source repositories on GitHub. It includes code\\nand documentation in several programming languages including Go, Java, PHP, Python,\\nJavaScript, and Ruby. The dataset was primarily compiled to promote research into the\\nproblem of code retrieval using natural language.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 16, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='JavaScript, and Ruby. The dataset was primarily compiled to promote research into the\\nproblem of code retrieval using natural language.\\n‚Ä¢ Google BigQuery [96]: the Google BigQuery Public Datasets program offers a full snapshot\\nof the content of more than 2.8 million open source GitHub repositories in BigQuery.\\n‚Ä¢ The Pile [78]: the Pile is an 825 GiB diverse and open source language modeling dataset\\naggregating 22 smaller, high-quality datasets including GitHub, Books3, and Wikipedia (en).\\nIt aims to encompass text from as many modalities as possible, thereby facilitating the\\ndevelopment of models with broader generalization capabilities. For code generation, the\\nGitHub composite is specifically utilized.\\n‚Ä¢ CodeParrot [254]: the CodeParrot dataset contains Python files used to train the code genera-\\ntion model in Chapter 10: Training Transformers from Scratch in the ‚ÄúNLP with Transformers\\n7https://github.com\\n8https://stackoverflow.com'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 16, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='tion model in Chapter 10: Training Transformers from Scratch in the ‚ÄúNLP with Transformers\\n7https://github.com\\n8https://stackoverflow.com\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 17, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:18\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nQuality Filtering\\n‚Ä¢\\nProgramming Language \\n‚Ä¢\\nStatistic Number\\n‚Ä¢\\nMetric Threshold\\n‚Ä¢\\nKeyword Search\\nDe-duplication\\n‚Ä¢\\nExact Match\\n‚Ä¢\\nSimilarity Metrics\\n‚Ä¢\\nFunction Level\\nPrivacy Reduction\\n‚Ä¢\\nDetect Personally Identifiable \\nInformation (PII)\\n‚Ä¢\\nDelete PII\\nRaw Corpus\\nTokenization\\n‚Ä¢\\nOpen Source Tokenizer\\n‚Ä¢\\nSentencePiece\\n‚Ä¢\\nByte-level BPE\\nPre-training Database\\n# Sum numbers from 1 to 10 \\nand print the result\\ntotal = sum(range(1, 11))\\nprint(total)\\ntotal = 0\\nfor i in range(1, 11):\\ntotal += i\\ntotal = sum(range(1, 11))\\n# Copyright 2024 @ John\\n# Email: csjohn@gmail.com\\n# Institution: HKUST\\ninputs = tokenizer.encode([\"def \\nprint_hello_world():\",...], \\nreturn_tensors=\"pt\").to(\"cuda\")\\n[\\n[755, 1194, 97824, \\n32892, 4658],\\n[755, 4062, 18942, \\n11179, 997, 262, 4304, \\n10442, 264, 1160, 315, \\n5219, 304, 36488, 2015, \\n1701, 279, 17697, 6354, \\n371, 12384,...],...\\n]\\n. . .'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 17, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[\\n[755, 1194, 97824, \\n32892, 4658],\\n[755, 4062, 18942, \\n11179, 997, 262, 4304, \\n10442, 264, 1160, 315, \\n5219, 304, 36488, 2015, \\n1701, 279, 17697, 6354, \\n371, 12384,...],...\\n]\\n. . .\\nFig. 7. A diagram depicting the standard data preprocessing workflow utilized in the pre-training phase of\\nLLMs for code generation.\\nbook‚Äù [254]. Created with the GitHub dataset available via Google‚Äôs BigQuery, the CodeParrot\\ndataset includes approximately 22 million Python files and is 180 GB (50 GB compressed) big.\\n‚Ä¢ GitHub Code [254]: the GitHub Code dataset comprises 115M code files derived from GitHub,\\nspanning 32 programming languages and 60 extensions totaling 1TB of data. The dataset\\nwas created from the public GitHub dataset on Google BiqQuery.\\n‚Ä¢ ROOTS [137]: the BigScience ROOTS Corpus is a 1.6TB dataset spanning 59 languages that\\nwas used to train the 176B BigScience Large Open-science Open-access Multilingual (BLOOM)'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 17, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ ROOTS [137]: the BigScience ROOTS Corpus is a 1.6TB dataset spanning 59 languages that\\nwas used to train the 176B BigScience Large Open-science Open-access Multilingual (BLOOM)\\nlanguage model. For the code generation task, the code subset of the ROOTS Corpus will be\\nspecifically utilized.\\n‚Ä¢ The Stack [132]: the Stack contains over 6TB of permissively licensed source code files that\\ncover 358 programming languages. The dataset was compiled as part of the BigCode Project,\\nan open scientific collaboration working on the responsible development of Large Language\\nModels for Code (Code LLMs).\\n‚Ä¢ The Stack v2 [170]: The Stack v2, a dataset created as part of the BigCode Project, contains\\nover 3B files across more than 600 programming and markup languages. The dataset is\\nderived from the Software Heritage archive9, the largest public archive of software source\\ncode and accompanying development history.\\n5.1.2'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 17, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='derived from the Software Heritage archive9, the largest public archive of software source\\ncode and accompanying development history.\\n5.1.2\\nInstruction Tuning. Instruction tuning refers to the process of supervised fine-tuning LLMs\\nusing a collection of datasets structured as various instructions, with the purpose of following a\\nwide range of task instructions [56, 200, 229, 274]. This method has demonstrated a considerable\\nimprovement in model performance and an enhanced ability to generalize to unseen tasks that the\\n9https://archive.softwareheritage.org\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 18, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:19\\nTable 4. The statistics of some commonly-used pre-training datasets for LLMs aimed at code generation. The\\ncolumn labeled ‚Äò#PL‚Äô indicates the number of programming languages included in each dataset. It should\\nbe noted that in the CodeSearchNet [110] dataset, each file represents a function, and for the Pile [78] and\\nROOTS [137] datasets, only the code components are considered.\\nDataset\\nSize (GB)\\nFiles (M)\\n#PL\\nDate\\nLink\\nCodeSearchNet [110]\\n20\\n6.5\\n6\\n2022-01\\nhttps://huggingface.co/datasets/code_search_net\\nGoogle BigQuery[96]\\n-\\n-\\n-\\n2016-06\\ngithub-on-bigquery-analyze-all-the-open-source-code\\nThe Pile [78]\\n95\\n19\\n-\\n2022-01\\nhttps://huggingface.co/datasets/EleutherAI/pile\\nCodeParrot [254]\\n180\\n22\\n1\\n2021-08\\nhttps://huggingface.co/datasets/transformersbook/codeparrot\\nGitHub Code[254]\\n1,024\\n115\\n32\\n2022-02\\nhttps://huggingface.co/datasets/codeparrot/github-code\\nROOTS [137]\\n163\\n15\\n13\\n2023-03\\nhttps://huggingface.co/bigscience-data'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 18, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='GitHub Code[254]\\n1,024\\n115\\n32\\n2022-02\\nhttps://huggingface.co/datasets/codeparrot/github-code\\nROOTS [137]\\n163\\n15\\n13\\n2023-03\\nhttps://huggingface.co/bigscience-data\\nThe Stack [132]\\n3,136\\n317\\n30\\n2022-10\\nhttps://huggingface.co/datasets/bigcode/the-stack\\nThe Stack v2 [170]\\n32K\\n3K\\n619\\n2024-04\\nhttps://huggingface.co/datasets/bigcode/the-stack-v2\\nTable 5. The statistics of several representative datasets used in instruction-tuning LLMs for code generation.\\nThe column labeled ‚Äò#PL‚Äô indicates the number of programming languages encompassed by each dataset.\\nDataset\\nSize\\n#PL\\nDate\\nLink\\nCodeAlpaca-20K [43]\\n20k\\n-\\n2023-03\\nhttps://huggingface.co/datasets/sahil2801/CodeAlpaca-20k\\nCommitPackFT [187]\\n2GB\\n277\\n2023-08\\nhttps://huggingface.co/datasets/bigcode/commitpackft\\nEvol-Instruct-Code-80k [225]\\n80k\\n-\\n2023-07\\nhttps://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1\\nevol-codealpaca-v1 [251]\\n110K\\n-\\n2023-07\\nhttps://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 18, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='80k\\n-\\n2023-07\\nhttps://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1\\nevol-codealpaca-v1 [251]\\n110K\\n-\\n2023-07\\nhttps://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1\\nMagicoder-OSS-Instruct-75k [278]\\n75k\\nPython, Shell,\\nTypeScript, C++,\\nRust, PHP, Java,\\nSwift, C#\\n2023-12\\nhttps://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K\\nSelf-OSS-Instruct-SC2-Exec-Filter-50k [304]\\n50k\\nPython\\n2024-04\\nhttps://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k\\nmodel has not previously encountered, as evidenced by recent studies [56, 200]. Leveraging the\\nbenefits of instruction tuning, instruction tuning has been expanded into coding domains, especially\\nfor code generation, which involves the automatic generation of the intended code from a natural\\nlanguage description. The promise of instruction tuning in this area has led numerous researchers\\nto develop large-scale instruction-tuning datasets tailored for code generation. Below, we provide an'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 18, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='language description. The promise of instruction tuning in this area has led numerous researchers\\nto develop large-scale instruction-tuning datasets tailored for code generation. Below, we provide an\\noverview of several notable datasets tailored for instruction tuning, with their respective statistics\\ndetailed in Table 5.\\n‚Ä¢ CodeAlpaca-20k [43]: CodeAlpaca-20k is a collection of 20K instruction-following data\\ngenerated using the data synthesis techniques termed Self-Instruct outlined in [268], with\\nmodifications for code generation, editing, and optimization tasks instead of general tasks.\\n‚Ä¢ CommitPackFT [187]: CommitPackFT is a 2GB refined version of CommitPack. It is filtered\\nto only include high-quality commit messages that resemble natural language instructions.\\n‚Ä¢ Evol-Instruct-Code-80k [225]: Evol-Instruct-Code-80k is an open-source implementation of\\nEvol-Instruct-Code described in the WizardCoder paper [173], which enhances the fine-tuning'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 18, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Evol-Instruct-Code-80k [225]: Evol-Instruct-Code-80k is an open-source implementation of\\nEvol-Instruct-Code described in the WizardCoder paper [173], which enhances the fine-tuning\\neffect of pre-trained code large models by adding complex code instructions.\\n‚Ä¢ Magicoder-OSS-Instruct-75k [278]: is a 75k synthetic data generated through OSS-Instruct\\nwith gpt-3.5-turbo-1106 and used to train both Magicoder and Magicoder-S series models.\\n‚Ä¢ Self-OSS-Instruct-SC2-Exec-Filter-50k [304]: Self-OSS-Instruct-SC2-Exec-Filter-50k is gen-\\nerated by StarCoder2-15B using the OSS-Instruct [278] data synthesis approach. It was\\nsubsequently used to fine-tune StarCoder-15B without any human annotations or distilled\\ndata from huge and proprietary LLMs.\\n5.1.3\\nBenchmarks. To rigorously assess the efficacy of LLMs for code generation, the research\\ncommunity has introduced a variety of high-quality benchmarks in recent years. Building on\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 19, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:20\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nthe foundational work by [48], numerous variations of the HumanEval dataset and additional\\nbenchmarks have emerged, aiming to evaluate a broader spectrum of code generation capabilities\\nin LLMs. We roughly divide these benchmarks into six distinct categories based on their application\\ncontexts, including general-purpose, competitive programming, data science, multilingual, logical\\nreasoning, and repository-level. It is important to highlight that logical reasoning encompasses math-\\nrelated benchmarks, as it aims to create ‚Äúcode-based solutions‚Äù for solving complex mathematical\\nproblems [50, 79, 326]. This strategy can therefore mitigate the limitations of LLMs in performing\\nintricate mathematical computations. The statistics for these benchmarks are presented in Table 6.\\nGeneral\\n‚Ä¢ HumanEval [48]: HumanEval comprises 164 manually scripted Python programming prob-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 19, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='intricate mathematical computations. The statistics for these benchmarks are presented in Table 6.\\nGeneral\\n‚Ä¢ HumanEval [48]: HumanEval comprises 164 manually scripted Python programming prob-\\nlems, each featuring a function signature, docstring, body, and multiple unit tests.\\n‚Ä¢ HumanEval+ [162]: HumanEval+ extends the original HumanEval [48] benchmark by in-\\ncreasing the scale of the test cases by 80 times. As the test cases increase, HumanEval+ can\\ncatch significant amounts of previously undetected incorrect code synthesized by LLMs.\\n‚Ä¢ HumanEvalPack [187]: expands HumanEval [48] by extending it to encompass three coding\\ntasks across six programming languages, namely code synthesis, code repair, and code\\nexplanation.\\n‚Ä¢ MBPP [17]: MBPP is a collection of approximately 974 Python programming problems, crowd-\\nsourced and designed for entry-level programmers. Each problem comes with an English\\ntask description, a code solution, and three automated test cases.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 19, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='sourced and designed for entry-level programmers. Each problem comes with an English\\ntask description, a code solution, and three automated test cases.\\n‚Ä¢ MBPP+ [162]: MBPP+ enhances MBPP [17] by eliminating ill-formed problems and rectifying\\nproblems with incorrect implementations. The test scale of MBPP+ is also expanded by 35\\ntimes for test augmentation.\\n‚Ä¢ CoNaLa [297]: CoNaLa contains almost 597K data samples for evaluating Python code\\ngeneration. The curated part of CoNaLa is crawled from Stack Overflow, automatically\\nfiltered, and then curated by annotators. The mined part of CoNaLais automatically mined,\\nwith almost 600k examples.\\n‚Ä¢ Spider [300]: Spider is large-scale complex text-to-SQL dataset covering 138 different domains.\\nIt has over 10K questions and 5.6K complex SQL queries on 200 databases. This dataset aims\\nto test a model‚Äôs ability to generalize to SQL queries, database schemas, and new domains.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 19, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='It has over 10K questions and 5.6K complex SQL queries on 200 databases. This dataset aims\\nto test a model‚Äôs ability to generalize to SQL queries, database schemas, and new domains.\\n‚Ä¢ CONCODE [113]: CONCODE is a dataset with over 100K samples consisting of Java classes\\nfrom public GitHub repositories. It provides near zero-shot conditions that can test the\\nmodel‚Äôs ability to generalize to unseen natural language tokens with unseen environments.\\n‚Ä¢ ODEX [273]: ODEX is an open-domain dataset focused on the execution-based generation\\nof Python code from natural language. It features 945 pairs of natural language queries and\\ntheir corresponding Python code, all extracted from StackOverflow forums.\\n‚Ä¢ CoderEval [299]: CoderEval is a pragmatic code generation benchmark that includes 230\\nPython and 230 Java code generation problems. It can be used to evaluate the model perfor-\\nmance in generating pragmatic code beyond just generating standalone functions.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 19, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Python and 230 Java code generation problems. It can be used to evaluate the model perfor-\\nmance in generating pragmatic code beyond just generating standalone functions.\\n‚Ä¢ ReCode [263]: Recode serves as a comprehensive robustness evaluation benchmark. ReCode\\napplies perturbations to docstrings, function and variable names, code syntax, and code\\nformat, thereby providing multifaceted assessments of a model‚Äôs robustness performance.\\n‚Ä¢ StudentEval [19]: StudentEval is a dataset of 1,749 prompts for 48 problems, authored by 80\\nstudents who have only completed a one-semester Python programming class. Unlike many\\nother benchmarks, it has multiple prompts per problem and multiple attempts by the same\\nparticipant, each problem is also accompanied by a set of instructor-written test cases.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 20, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:21\\n‚Ä¢ BigCodeBench [333]: BigCodeBench has 1,140 complex Python programming tasks, covering\\n723 function calls from 139 popular libraries across 7 domains. This benchmark is specifically\\ndesigned to assess LLMs‚Äô ability to call multiple functions from cross-domain libraries and\\nfollow complex instructions to solve programming tasks, helping to bridge the evaluation\\ngap between isolated coding exercises and the real-world programming scenario.\\n‚Ä¢ ClassEval [72]: ClassEval is a manually-crafted benchmark consisting of 100 classes and\\n412 methods for evaluating LLMs in the class-level code generation scenario. Particularly,\\nthe task samples of ClassEval present higher complexities, involving long code generation\\nand sophisticated docstring information, thereby benefiting the evaluation of the LLMs‚Äô\\ncapabilities in generating complicated code.\\n‚Ä¢ NaturalCodeBench [314]: NaturalCodeBench is a comprehensive code benchmark featuring'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 20, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='capabilities in generating complicated code.\\n‚Ä¢ NaturalCodeBench [314]: NaturalCodeBench is a comprehensive code benchmark featuring\\n402 high-quality problems in Python and Java. These problems are selected from natural\\nuser queries from online coding services and span 6 distinct domains, shaping an evaluation\\nenvironment aligned with real-world applications.\\nCompetitions\\n‚Ä¢ APPS [95]: The APPS benchmark is composed of 10K Python problems, spanning three levels\\nof difficulty: introductory, interview, and competition. Each entry in the dataset includes a\\nprogramming problem described in English, corresponding ground truth Python solutions,\\ntest cases defined by their inputs and outputs or function names if provided.\\n‚Ä¢ CodeContests [151]: CodeContests is a competitive programming dataset consisting of sam-\\nples from various sources including Aizu, AtCoder, CodeChef, Codeforces, and HackerEarth.\\nThe dataset encompasses programming problems accompanied by test cases in the form of'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 20, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ples from various sources including Aizu, AtCoder, CodeChef, Codeforces, and HackerEarth.\\nThe dataset encompasses programming problems accompanied by test cases in the form of\\npaired inputs and outputs, along with both correct and incorrect human solutions in multiple\\nprogramming languages.\\n‚Ä¢ LiveCodeBench [188]: LiveCodeBench is a comprehensive and contamination-free benchmark\\nfor evaluating a wide array of code-related capabilities of LLMs, including code generation,\\nself-repair, code execution, and test output prediction. It continuously gathers new coding\\nproblems from contests across three reputable competition platforms: LeetCode, AtCoder,\\nand CodeForces. The latest release of the dataset includes 713 problems that were released\\nbetween May 2023 and September 2024.\\nData Science\\n‚Ä¢ DSP [41]: DSP allows for model evaluation based on real data science pedagogical notebooks.\\nIt includes well-structured problems, along with unit tests to verify the correctness of solutions'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 20, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ DSP [41]: DSP allows for model evaluation based on real data science pedagogical notebooks.\\nIt includes well-structured problems, along with unit tests to verify the correctness of solutions\\nand a Docker environment for reproducible execution.\\n‚Ä¢ DS-1000 [136]: DS-1000 has 1K science questions from seven Python libraries, namely NumPy,\\nPandas, TensorFlow, PyTorch, SciPy, Scikit-learn, and Matplotlib. The DS-1000 benchmark\\nfeatures: (1) realistic problems with diverse contexts (2) implementation of multi-criteria\\nevaluation metrics, and (3) defense against memorization.\\n‚Ä¢ ExeDS [107]: ExeDS is a data science code generation dataset specifically designed for execu-\\ntion evaluation. It contains 534 problems with execution outputs from Jupyter Notebooks, as\\nwell as 123K examples for training and validation.\\nMultilingual\\n‚Ä¢ MBXP [16]: MBXP is a multilingual adaptation of the original MBPP [17] dataset. It is created'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 20, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='well as 123K examples for training and validation.\\nMultilingual\\n‚Ä¢ MBXP [16]: MBXP is a multilingual adaptation of the original MBPP [17] dataset. It is created\\nusing a framework that translates prompts and test cases from the original Python datasets\\ninto the corresponding data in the targeted programming language.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 21, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:22\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n‚Ä¢ Multilingual HumanEval [16]: Multilingual HumanEval is a dataset derived from HumanEval\\n[48]. It is designed to assess the performance of models in a multilingual context. It helps\\nuncover the generalization ability of the given model on languages that are out-of-domain.\\n‚Ä¢ HumanEval-X [321]: HumanEval-X is developed for evaluating the multilingual ability of\\ncode generation models with 820 hand-writing data samples in C++, Java, JavaScript, and Go.\\n‚Ä¢ MultiPL-E [39]: MultiPL-E is a dataset for evaluating LLMs for code generation across 18 pro-\\ngramming languages. It adopts the HumanEval [48] and the MBPP [17] Python benchmarks\\nand uses little compilers to translate them to other languages.\\n‚Ä¢ xCodeEval [128]: xCodeEval is an executable multilingual multitask benchmark consisting of\\n25M examples covering 17 programming languages. Its tasks include code understanding,\\ngeneration, translation, and retrieval.\\nReasoning'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 21, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='25M examples covering 17 programming languages. Its tasks include code understanding,\\ngeneration, translation, and retrieval.\\nReasoning\\n‚Ä¢ MathQA-X [16] MathQA-X is the multilingual version of MathQA [13]. It is generated by\\nutilizing a conversion framework that converts samples from Python datasets into the target\\nlanguage.\\n‚Ä¢ MathQA-Python [17] MathQA-Python is a Python version of the MathQA benchmark[13].\\nThe benchmark, containing more than 23K problems, is designed to assess the capability of\\nmodels to synthesize code from complex textual descriptions.\\n‚Ä¢ GSM8K [58]: GSM8K is a dataset of 8.5K linguistically diverse grade school math problems.\\nThe dataset is crafted to facilitate the task of question answering on basic mathematical\\nproblems that requires multi-step reasoning.\\n‚Ä¢ GSM-HARD [79]: GSM-HARD is a more challenging version of the GSM8K [58] dataset. It\\nreplaces the numbers in the GSM8K questions with larger, less common numbers, thereby'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 21, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ GSM-HARD [79]: GSM-HARD is a more challenging version of the GSM8K [58] dataset. It\\nreplaces the numbers in the GSM8K questions with larger, less common numbers, thereby\\nincreasing the complexity and difficulty level of the problems.\\n‚Ä¢ CRUXEval [82]: CRUXEval contains 800 Python functions, each paired with an input-output\\nexample. This benchmark supports two tasks: input prediction and output prediction, designed\\nto evaluate the code reasoning, understanding, and execution capabilities of code LLMs.\\nRepository\\n‚Ä¢ RepoEval [309]: RepoEval enables the evaluation of repository-level code completion. It can\\noffer different levels of granularity and improved evaluation accuracy through the use of unit\\ntests.\\n‚Ä¢ Stack-Repo [239]: Stack-Repo is a dataset of 200 Java repositories from GitHub with near-\\ndeduplicated files. These files are augmented with three types of repository contexts: prompt\\nproposal contexts, BM25 Contexts (based on BM25 similarity scores), and RandomNN Con-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 21, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='deduplicated files. These files are augmented with three types of repository contexts: prompt\\nproposal contexts, BM25 Contexts (based on BM25 similarity scores), and RandomNN Con-\\ntexts (obtained using the nearest neighbors in the representation space of an embedding\\nmodel).\\n‚Ä¢ Repobench [167]: Repobench is a benchmark specifically used for evaluating repository-\\nlevel code auto-completion systems. Supporting both Python and Java, it consists of three\\ninterconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion),\\nand RepoBench-P (Pipeline).\\n‚Ä¢ EvoCodeBench [144]: EvoCodeBench is an evolutionary code generation benchmark, con-\\nstructed through a rigorous pipeline and aligned with real-world repositories. This benchmark\\nalso provides comprehensive annotations and robust evaluation metrics.\\n‚Ä¢ SWE-bench [123]: SWE-bench is a dataset that tests a model‚Äôs ability to automatically solve'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 21, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='also provides comprehensive annotations and robust evaluation metrics.\\n‚Ä¢ SWE-bench [123]: SWE-bench is a dataset that tests a model‚Äôs ability to automatically solve\\nGitHub issues. The dataset has 2,294 Issue-Pull Request pairs from 12 popular Python reposi-\\ntories.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 22, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:23\\nTable 6. The detailed statistics of commonly-used benchmarks used in evaluating LLMs for code generation.\\nThe column labeled ‚Äò#PL‚Äô indicates the number of programming languages included in each dataset. For the\\nsake of brevity, we list the programming languages (PLs) for benchmarks that support fewer than or include\\nfive PLs. For benchmarks with six or more PLs, we provide only a numerical count of the PLs supported.\\nScenario\\nBenchmark\\nSize\\n#PL\\nDate\\nLink\\nGeneral\\nHumanEval [48]\\n164\\nPython\\n2021-07\\nhttps://huggingface.co/datasets/openai_humaneval\\nHumanEval+ [162]\\n164\\nPython\\n2023-05\\nhttps://huggingface.co/datasets/evalplus/humanevalplus\\nHumanEvalPack [187]\\n164\\n6\\n2023-08\\nhttps://huggingface.co/datasets/bigcode/humanevalpack\\nMBPP [17]\\n974\\nPython\\n2021-08\\nhttps://huggingface.co/datasets/mbpp\\nMBPP+ [162]\\n378\\nPython\\n2023-05\\nhttps://huggingface.co/datasets/evalplus/mbppplus\\nCoNaLa [297]\\n596.88K\\nPython\\n2018-05'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 22, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='MBPP [17]\\n974\\nPython\\n2021-08\\nhttps://huggingface.co/datasets/mbpp\\nMBPP+ [162]\\n378\\nPython\\n2023-05\\nhttps://huggingface.co/datasets/evalplus/mbppplus\\nCoNaLa [297]\\n596.88K\\nPython\\n2018-05\\nhttps://huggingface.co/datasets/neulab/conala\\nSpider [300]\\n8,034\\nSQL\\n2018-09\\nhttps://huggingface.co/datasets/xlangai/spider\\nCONCODE [113]\\n104K\\nJava\\n2018-08\\nhttps://huggingface.co/datasets/AhmedSSoliman/CONCOD\\nODEX [273]\\n945\\nPython\\n2022-12\\nhttps://huggingface.co/datasets/neulab/odex\\nCoderEval [299]\\n460\\nPython, Java\\n2023-02\\nhttps://github.com/CoderEval/CoderEval\\nReCode [263]\\n1,138\\nPython\\n2022-12\\nhttps://github.com/amazon-science/recode\\nStudentEval [19]\\n1,749\\nPython\\n2023-06\\nhttps://huggingface.co/datasets/wellesley-easel/StudentEval\\nBigCodeBench [333]\\n1,140\\nPython\\n2024-06\\nhttps://huggingface.co/datasets/bigcode/bigcodebench\\nClassEval [72]\\n100\\nPython\\n2023-08\\nhttps://huggingface.co/datasets/FudanSELab/ClassEval\\nNaturalCodeBench [314]\\n402\\nPython, Java\\n2024-05\\nhttps://github.com/THUDM/NaturalCodeBench'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 22, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ClassEval [72]\\n100\\nPython\\n2023-08\\nhttps://huggingface.co/datasets/FudanSELab/ClassEval\\nNaturalCodeBench [314]\\n402\\nPython, Java\\n2024-05\\nhttps://github.com/THUDM/NaturalCodeBench\\nCompetitions\\nAPPS [95]\\n10,000\\nPython\\n2021-05\\nhttps://huggingface.co/datasets/codeparrot/apps\\nCodeContests [151]\\n13,610\\nC++, Python,\\nJava\\n2022-02\\nhttps://huggingface.co/datasets/deepmind/code_contests\\nLiveCodeBench [188]\\n713\\nUpdating\\nPython\\n2024-03\\nhttps://github.com/LiveCodeBench/LiveCodeBench\\nData Science\\nDSP [41]\\n1,119\\nPython\\n2022-01\\nhttps://github.com/microsoft/DataScienceProblems\\nDS-1000 [136]\\n1,000\\nPython\\n2022-11\\nhttps://huggingface.co/datasets/xlangai/DS-1000\\nExeDS [107]\\n534\\nPython\\n2022-11\\nhttps://github.com/Jun-jie-Huang/ExeDS\\nMultilingual\\nMBXP [16]\\n12.4K\\n13\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/mbxp\\nMultilingual HumanEval [16]\\n1.9K\\n12\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/multi-humaneval\\nHumanEval-X [321]\\n820\\nPython, C++,\\nJava, JavaScript,\\nGo\\n2023-03'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 22, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Multilingual HumanEval [16]\\n1.9K\\n12\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/multi-humaneval\\nHumanEval-X [321]\\n820\\nPython, C++,\\nJava, JavaScript,\\nGo\\n2023-03\\nhttps://huggingface.co/datasets/THUDM/humaneval-x\\nMultiPL-E [39]\\n161\\n18\\n2022-08\\nhttps://huggingface.co/datasets/nuprl/MultiPL-E\\nxCodeEval [128]\\n5.5M\\n11\\n2023-03\\nhttps://github.com/ntunlp/xCodeEval\\nReasoning\\nMathQA-X [16]\\n5.6K\\nPython, Java,\\nJavaScript\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/mathqa-x\\nMathQA-Python [17]\\n23,914\\nPython\\n2021-08\\nhttps://github.com/google-research/google-research\\nGSM8K [58]\\n8.5K\\nPython\\n2021-10\\nhttps://huggingface.co/datasets/gsm8k\\nGSM-HARD [79]\\n1.32K\\nPython\\n2022-11\\nhttps://huggingface.co/datasets/reasoning-machines/gsm-hard\\nCRUXEval [82]\\n800\\nPython\\n2024-01\\nhttps://huggingface.co/datasets/cruxeval-org/cruxeval\\nRepository\\nRepoEval [309]\\n3,573\\nPython, Java\\n2023-03\\nhttps://paperswithcode.com/dataset/repoeval\\nStack-Repo [239]\\n200\\nJava\\n2023-06\\nhttps://huggingface.co/datasets/RepoFusion/Stack-Repo'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 22, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Repository\\nRepoEval [309]\\n3,573\\nPython, Java\\n2023-03\\nhttps://paperswithcode.com/dataset/repoeval\\nStack-Repo [239]\\n200\\nJava\\n2023-06\\nhttps://huggingface.co/datasets/RepoFusion/Stack-Repo\\nRepobench [167]\\n27k\\nPython, Java\\n2023-01\\nhttps://github.com/Leolty/repobench\\nEvoCodeBench [144]\\n275\\nPython\\n2024-03\\nhttps://huggingface.co/datasets/LJ0815/EvoCodeBench\\nSWE-bench [123]\\n2,294\\nPython\\n2023-10\\nhttps://huggingface.co/datasets/princeton-nlp/SWE-bench\\nCrossCodeEval [68]\\n10K\\nPython, Java,\\nTypeScript, C#\\n2023-10\\nhttps://github.com/amazon-science/cceval\\nSketchEval [308]\\n20,355\\nPython\\n2024-03\\nhttps://github.com/nl2code/codes\\n‚Ä¢ CrossCodeEval [68]: CrossCodeEval is a diverse and multilingual scope completion dataset\\ncovering four languages: Python, Java, TypeScript, and C#. This benchmark tests the model‚Äôs\\nability to understand in-depth cross-file information and accurately complete the code.\\n‚Ä¢ SketchEval [308]: SketchEval is a repository-oriented benchmark that encompasses data from'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 22, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ability to understand in-depth cross-file information and accurately complete the code.\\n‚Ä¢ SketchEval [308]: SketchEval is a repository-oriented benchmark that encompasses data from\\n19 repositories, each varying in complexity. In addition to the dataset, SketchEval introduces\\na metric, known as SketchBLEU, to measure the similarity between two repositories based\\non their structures and semantics.\\n5.2\\nData Synthesis\\nNumerous studies have demonstrated that high-quality datasets are integral to enhancing the\\nperformance of LLMs in various downstream tasks [33, 133, 179, 280, 286, 328]. For instance, the\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 23, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:24\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nSeed Problem\\n<Problem, Solution>\\nEvolved Problem\\nSelf-Instruct\\nEvol-Instruct\\nSolution\\nSeed Problem\\nEvolution Type\\n<Debiased Problem, Solution>\\nOSS-Instruct\\nSeed Code Snippet\\nùêøùêøùëÄ\\nùêøùêøùëÄ\\nùêøùêøùëÄ\\nFig. 8. The comparison among three representative data synthesis methods used for generating instruction\\ndata with LLMs. The Code Alpaca [43] employs the self-instruct method, whereas WizardCoder [173] and\\nMagicoder [278] utilize the Evol-Instruct and OSS-Instruct methods, respectively.\\nLIMA model, a 65B parameter LLaMa language model fine-tuned with a standard supervised loss\\non a mere 1,000 meticulously curated prompts and responses, achieved performance on par with, or\\neven superior to, GPT-4 in 43% of evaluated cases. This figure rose to 58% when compared to Bard\\nand 65% against DaVinci003, all without the use of reinforcement learning or human preference'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 23, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='even superior to, GPT-4 in 43% of evaluated cases. This figure rose to 58% when compared to Bard\\nand 65% against DaVinci003, all without the use of reinforcement learning or human preference\\nmodeling [328]. The QuRating initiative strategically selects pre-training data embodying four key\\ntextual qualities ‚Äî writing style, facts & trivia, required expertise, and educational value ‚Äî that\\nresonate with human intuition. Training a 1.3B parameter model on such data resulted in reduced\\nperplexity and stronger in-context learning compared to baseline models [280].\\nDespite these advancements, acquiring quality data remains a significant challenge due to issues\\nsuch as data scarcity, privacy concerns, and prohibitive costs [165, 268]. Human-generated data is\\noften labor-intensive and expensive to produce, and it may lack the necessary scope and detail to\\nnavigate complex, rare, or ambiguous scenarios. As a resolution to these challenges, synthetic data'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 23, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='often labor-intensive and expensive to produce, and it may lack the necessary scope and detail to\\nnavigate complex, rare, or ambiguous scenarios. As a resolution to these challenges, synthetic data\\nhas emerged as a viable alternative. By generating artificial datasets that replicate the intricacies\\nof real-world information, models such as GPT-3.5-turbo [196] and GPT-4 [5] have enabled the\\ncreation of rich datasets without the need for human annotation [92, 138, 165, 268]. This approach\\nis particularly beneficial in enhancing the instruction-following capabilities of LLMs, with a focus\\non generating synthetic instruction-based data.\\nA notable example of this approach is the Self-Instruct [268] framework, which employs an off-the-\\nshelf language model to generate a suite of instructions, inputs, and outputs. This data is then refined\\nby removing invalid or redundant entries before being used to fine-tune the model. The empirical'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 23, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='by removing invalid or redundant entries before being used to fine-tune the model. The empirical\\nevidence supports the efficacy of this synthetic data generation methodology. Building upon this\\nconcept, the Alpaca [247] model, fine-tuned on 52k pieces of instruction-following data from a 7B\\nparameter LLaMa [252] model, exhibits performance comparable to the text-davinci-003 model.\\nWizardLM [289] introduced the Evol-Instruct technique, which incrementally transforms simple\\ninstructions into more complex variants. The fine-tuned LLaMa model using this technique has\\nshown promising results in comparison to established proprietary LLMs such as ChatGPT [196] and\\nGPT-4 [5], to some extent. Moreover, Microsoft has contributed to this field with their Phi series of\\nmodels, predominantly trained on synthetic high-quality data, which includes Phi-1 (1.3B) [84]\\nfor Python coding, Phi-1.5 (1.3B) [150] for common sense reasoning and language understanding,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 23, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='models, predominantly trained on synthetic high-quality data, which includes Phi-1 (1.3B) [84]\\nfor Python coding, Phi-1.5 (1.3B) [150] for common sense reasoning and language understanding,\\nPhi-2 (2.7B) [182] for advanced reasoning and language understanding, and Phi-3 (3.8B) [4] for\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 24, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:25\\ngeneral purposes. These models have consistently outperformed larger counterparts across various\\nbenchmarks, demonstrating the efficacy of synthetic data in model training.\\nDrawing on the successes of data synthesis for general-purpose LLMs, researchers have expanded\\nthe application of synthetic data to the realm of code generation. The Code Alpaca model, as de-\\nscribed in [43], has been fine-tuned on a 7B and 13B LLaMA model using a dataset of 20k instruction-\\nfollowing examples for code generation. This dataset was created by text-davinci-00310 and\\nemployed the Self-Instruct technique [268]. Building on this, the WizardCoder 15B [173] utilizes\\nthe Evol-Instruct technique to create an enhanced dataset of 78k evolved code instruction examples.\\nThis dataset originates from the initial 20k instruction-following dataset used by Code Alpaca\\n[43], which was also generated by text-davinci-003. The WizardCoder model, fine-tuned on'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 24, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='This dataset originates from the initial 20k instruction-following dataset used by Code Alpaca\\n[43], which was also generated by text-davinci-003. The WizardCoder model, fine-tuned on\\nthe StarCoder [147] base model, achieved a 57.3% pass@1 on the HumanEval benchmarks. This\\nperformance not only surpasses all other open-source Code LLMs by a significant margin but also\\noutperforms leading closed LLMs such as Anthropic‚Äôs Claude and Google‚Äôs Bard. In a similar vein,\\nMagicoder [278] introduces a novel data synthesis approach termed OSS-INSTRUCT which enlight-\\nens LLMs with open-source code snippets to generate high-quality instruction data for coding tasks.\\nIt aims to address the inherent biases often present in synthetic data produced by LLMs. Building\\nupon CodeLlama [227], the MagicoderS-CL-7B model ‚Äî fine-tuned with 75k synthetic instruction\\ndata using the OSS-INSTRUCT technique and with gpt-3.5-turbo-1106 as the data generator ‚Äî'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 24, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='upon CodeLlama [227], the MagicoderS-CL-7B model ‚Äî fine-tuned with 75k synthetic instruction\\ndata using the OSS-INSTRUCT technique and with gpt-3.5-turbo-1106 as the data generator ‚Äî\\nhas outperformed the prominent ChatGPT on the HumanEval Plus benchmark, achieving pass@1\\nof 66.5% versus 65.9%. In a noteworthy development, Microsoft has introduced the phi-1 model [84],\\na more compact LLM of only 1.3B parameters. Despite its smaller size, phi-1 has been trained on\\nhigh-quality textbook data sourced from the web (comprising 6 billion tokens) and supplemented\\nwith synthetic textbooks and exercises generated with GPT-3.5 (1 billion tokens). It has achieved\\npass@1 of 50.6% on HumanEval and 55.5% on MBPP, setting a new state-of-the-art for Python\\ncoding performance among existing small language models (SLMs). The latest contribution to\\nthis field is from the BigCode team, which has presented StarCoder2-15B-instruct [304], the first'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 24, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='coding performance among existing small language models (SLMs). The latest contribution to\\nthis field is from the BigCode team, which has presented StarCoder2-15B-instruct [304], the first\\nentirely self-aligned code LLM trained with a transparent and permissive pipeline. This model\\naligns closely with the OSS-INSTRUCT principles established by Magicoder, generating instructions\\nbased on seed functions filtered from the Stack v1 dataset [132] and producing responses through\\nself-validation. Unlike Magicoder, StarCoder2-15B-instruct employs its base model, StarCoder2-15B,\\nas the data generator, thus avoiding reliance on large and proprietary LLMs like GPT-3.5-turbo\\n[196]. Figure 8 illustrates the comparison between Self-Instruct, Evol-Instruct, and OSS-Instruct\\ndata synthesis methods.\\nWhile synthetic data has demonstrated its potential across both small- and large-scale LMs for a\\nvariety of general and specialized tasks, including code generation, it also poses several challenges'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 24, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='While synthetic data has demonstrated its potential across both small- and large-scale LMs for a\\nvariety of general and specialized tasks, including code generation, it also poses several challenges\\nthat must be addressed. These challenges include a lack of data diversity [280], the need to ensure\\nthe factuality and fidelity of the information [256, 281], and the potential to amplify existing biases\\nor introduce new ones [24, 89].\\n5.3\\nPre-Training\\n5.3.1\\nModel Architectures. Since the inception of the Transformer architecture for machine transla-\\ntion [257], it has become the de facto backbone for a multitude of LLMs that address a wide range of\\ndownstream tasks. The Transformer and its derivatives owe their prominence to their exceptional\\nability to parallelize computation and their powerful representational capacities [298, 319]. Through\\ninnovative scaling techniques, such as Mixture-of-Experts (MoE) [35, 233] and Depth-Up-Scaling'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 24, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ability to parallelize computation and their powerful representational capacities [298, 319]. Through\\ninnovative scaling techniques, such as Mixture-of-Experts (MoE) [35, 233] and Depth-Up-Scaling\\n(DUS) [130], the capacity of Transformer-based LLMs has expanded to encompass hundreds of\\n10https://platform.openai.com\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 25, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:26\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 7. The overview of LLMs with encoder-decoder architectures for code generation.\\nModel\\nInstitution\\nSize\\nVocabulary\\nContext\\nWindow\\nDate\\nOpen Source\\nPyMT5[57]\\nMicrosoft\\n374M\\n50K\\n1024+1024\\n2020-10\\nPLBART[7]\\nUCLA\\n140M\\n50K\\n1024+1024\\n2021-03\\n\"\\nCodeT5 [271]\\nSalesforce\\n60M, 220M, 770M\\n32K\\n512+256\\n2021-09\\n\"\\nJuPyT5[41]\\nMicrosoft\\n350M\\n50K\\n1024+1024\\n2022-01\\nAlphaCode[151]\\nDeepMind\\n284M, 1.1B, 2.8B,\\n8.7B, 41.1B\\n8K\\n1536+768\\n2022-02\\nCodeRL[139]\\nSalesforce\\n770M\\n32K\\n512+256\\n2022-06\\n\"\\nERNIE-Code[40]\\nBaidu\\n560M\\n250K\\n1024+1024\\n2022-12\\n\"\\nPPOCoder[238]\\nVirginia Tech\\n770M\\n32K\\n512+256\\n2023-01\\nCodeT5+[269]\\nSalesforce\\n220M, 770M, 2B,\\n6B, 16B\\n50K\\n2048+2048\\n2023-05\\n\"\\nCodeFusion[241]\\nMicrosoft\\n75M\\n32k\\n128+128\\n2023-10\\n\"\\nAST-T5[81]\\nUC Berkeley\\n226M\\n32k\\n512+200/300\\n2024-01\\n\"\\nbillions or even trillions of parameters. These scaled-up models have exhibited a range of emergent'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 25, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Microsoft\\n75M\\n32k\\n128+128\\n2023-10\\n\"\\nAST-T5[81]\\nUC Berkeley\\n226M\\n32k\\n512+200/300\\n2024-01\\n\"\\nbillions or even trillions of parameters. These scaled-up models have exhibited a range of emergent\\nabilities [97, 127, 275], such as instruction following [200], in-context learning [70], and step-by-step\\nreasoning [105, 276] that were previously unforeseen.\\nIn the domain of code generation using LLMs, the architecture of contemporary models generally\\nfalls into one of two categories: encoder-decoder models, such as CodeT5 [271], CodeT5+ [269],\\nand CodeRL [139]; or decoder-only models, such as Codex [48], StarCoder [147], Code Llama [227],\\nand CodeGemma [59]. These architectures are depicted in Figure 2(b) and (c), respectively. For a\\ncomprehensive overview, Table 7 details the encoder-decoder architectures, while Table 8 focuses\\non the decoder-only models utilized in code generation.\\n5.3.2\\nPre-training Tasks. In the initial phase, language models for code generation are typically'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 25, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='on the decoder-only models utilized in code generation.\\n5.3.2\\nPre-training Tasks. In the initial phase, language models for code generation are typically\\ntrained from scratch using datasets consisting of manually annotated pairs of natural language\\ndescriptions and corresponding code snippets, within a supervised learning framework. However,\\nmanual annotation is not only laborious and time-consuming, but the efficacy of the resulting\\nmodels is also constrained by both the volume and the quality of the available annotated data. This\\nlimitation is especially pronounced in the context of low-resource programming languages, such\\nas Swahili and Yoruba, where annotated examples are scarce [38, 46]. In light of these challenges,\\nthere has been a shift towards an alternative training strategy that involves pre-training models on\\nextensive and unlabelled code corpora. This method is aimed at imbuing the models with a broad'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 25, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='there has been a shift towards an alternative training strategy that involves pre-training models on\\nextensive and unlabelled code corpora. This method is aimed at imbuing the models with a broad\\nunderstanding of programming knowledge, encompassing elements like identifiers, code structure,\\nand underlying semantics [48]. In this regard, two pre-training tasks have gained prominence\\nfor their effectiveness, namely Causal Language Modeling (CLM), also known as unidirectional\\nlanguage modeling or next-token prediction, and Denoising Autoencoding (DAE). The CLM task\\ncan be applied to both decoder-only and encoder-decoder model architectures, while DAE tasks are\\nspecifically designed for encoder-decoder frameworks. It should also be noted that there is a variety\\nof additional auxiliary pre-training tasks that can further enhance model performance. These\\ninclude Masked Identifier Prediction, Identifier Tagging, Bimodal Dual Generation [271], Text-Code'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 25, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='of additional auxiliary pre-training tasks that can further enhance model performance. These\\ninclude Masked Identifier Prediction, Identifier Tagging, Bimodal Dual Generation [271], Text-Code\\nMatching, and Text-Code Contrastive Learning [269]. These tasks contribute to a more nuanced\\nand comprehensive pre-training process, equipping the models with the capabilities necessary to\\nhandle a wide range of code generation scenarios.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 26, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:27\\nTable 8. The overview of LLMs with decoder-only architectures for code generation.\\nModel\\nInstitution\\nSize\\nVocabulary\\nContext\\nWindow\\nDate\\nOpen Source\\nGPT-C [244]\\nMicrosoft\\n366M\\n60K\\n1024\\n2020-05\\nCodeGPT [172]\\nMicrosoft\\n124M\\n50K\\n1024\\n2021-02\\n\"\\nGPT-Neo[30]\\nEleutherAI\\n125M, 1.3B, 2.7B\\n50k\\n2048\\n2021-03\\n\"\\nGPT-J [258]\\nEleutherAI\\n6B\\n50k\\n2048\\n2021-05\\n\"\\nCodex [48]\\nOpenAI\\n12M, 25M, 42M,\\n85M, 300M, 679M,\\n2.5B, 12B\\n-\\n4096\\n2021-07\\nCodeParrot [254]\\nHugging Face\\n110M, 1.5B\\n33k\\n1024\\n2021-11\\n\"\\nPolyCoder [290]\\nCMU\\n160M, 400M, 2.7B\\n50k\\n2048\\n2022-02\\n\"\\nCodeGen [193]\\nSalesforce\\n350M, 2.7B, 6.1B,\\n16.1B\\n51k\\n2048\\n2022-03\\n\"\\nGPT-NeoX [29]\\nEleutherAI\\n20B\\n50k\\n2048\\n2022-04\\n\"\\nPaLM-Coder [54]\\nGoogle\\n8B, 62B, 540B\\n256k\\n2048\\n2022-04\\nInCoder [77]\\nMeta\\n1.3B, 6.7B\\n50k\\n2049\\n2022-04\\n\"\\nPanGu-Coder [55]\\nHuawei\\n317M, 2.6B\\n42k\\n1024\\n2022-07\\nPyCodeGPT [306]\\nMicrosoft\\n110M\\n32k\\n1024\\n2022-06\\n\"\\nCodeGeeX [321]\\nTsinghua\\n13B\\n52k\\n2048\\n2022-09\\n\"\\nBLOOM [140]\\nBigScience\\n176B\\n251k\\n-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 26, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='2049\\n2022-04\\n\"\\nPanGu-Coder [55]\\nHuawei\\n317M, 2.6B\\n42k\\n1024\\n2022-07\\nPyCodeGPT [306]\\nMicrosoft\\n110M\\n32k\\n1024\\n2022-06\\n\"\\nCodeGeeX [321]\\nTsinghua\\n13B\\n52k\\n2048\\n2022-09\\n\"\\nBLOOM [140]\\nBigScience\\n176B\\n251k\\n-\\n2022-11\\n\"\\nChatGPT [196]\\nOpenAI\\n-\\n-\\n16k\\n2022-11\\n\"\\nSantaCoder [9]\\nHugging Face\\n1.1B\\n49k\\n2048\\n2022-12\\n\"\\nLLaMA [252]\\nMeta\\n6.7B, 13.0B, 32.5B,\\n65.2B\\n32K\\n2048\\n2023-02\\n\"\\nGPT-4 [5]\\nOpenAI\\n-\\n-\\n32K\\n2023-03\\nCodeGen2 [192]\\nSalesforce\\n1B, 3.7B, 7B, 16B\\n51k\\n2048\\n2023-05\\n\"\\nreplit-code [223]\\nreplit\\n3B\\n33k\\n2048\\n2023-05\\n\"\\nStarCoder [147]\\nHugging Face\\n15.5B\\n49k\\n8192\\n2023-05\\n\"\\nWizardCoder [173]\\nMicrosoft\\n15B, 34B\\n49k\\n8192\\n2023-06\\n\"\\nphi-1 [84]\\nMicrosoft\\n1.3B\\n51k\\n2048\\n2023-06\\n\"\\nCodeGeeX2 [321]\\nTsinghua\\n6B\\n65k\\n8192\\n2023-07\\n\"\\nPanGu-Coder2 [234]\\nHuawei\\n15B\\n42k\\n1024\\n2023-07\\nLlama 2 [253]\\nMeta\\n7B, 13B, 70B\\n32K\\n4096\\n2023-07\\n\"\\nOctoCoder [187]\\nHugging Face\\n15.5B\\n49k\\n8192\\n2023-08\\n\"\\nCode Llama [227]\\nMeta\\n7B, 13B, 34B\\n32k\\n16384\\n2023-08\\n\"\\nCodeFuse [160]\\nAnt Group\\n350M, 13B, 34B\\n101k\\n4096\\n2023-09\\n\"\\nphi-1.5 [150]\\nMicrosoft'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 26, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='\"\\nOctoCoder [187]\\nHugging Face\\n15.5B\\n49k\\n8192\\n2023-08\\n\"\\nCode Llama [227]\\nMeta\\n7B, 13B, 34B\\n32k\\n16384\\n2023-08\\n\"\\nCodeFuse [160]\\nAnt Group\\n350M, 13B, 34B\\n101k\\n4096\\n2023-09\\n\"\\nphi-1.5 [150]\\nMicrosoft\\n1.3B\\n51k\\n2048\\n2023-09\\n\"\\nCodeShell [285]\\nPeking University\\n7B\\n70k\\n8192\\n2023-10\\n\"\\nMagicoder [278]\\nUIUC\\n7B\\n32k\\n16384\\n2023-12\\n\"\\nAlphaCode 2 [11]\\nGoogle DeepMind\\n-\\n-\\n-\\n2023-12\\nStableCode [210]\\nStabilityAI\\n3B\\n50k\\n16384\\n2024-01\\n\"\\nWaveCoder [301]\\nMicrosoft\\n6.7B\\n32k\\n16384\\n2023-12\\n\"\\nphi-2 [182]\\nMicrosoft\\n2.7B\\n51k\\n2048\\n2023-12\\n\"\\nDeepSeek-Coder [88]\\nDeepSeek\\n1.3B, 6.7B, 33B\\n32k\\n16384\\n2023-11\\n\"\\nStarCoder 2 [170]\\nHugging Face\\n15B\\n49k\\n16384\\n2024-02\\n\"\\nClaude 3 [14]\\nAnthropic\\n-\\n-\\n200K\\n2024-03\\nCodeGemma [59]\\nGoogle\\n2B, 7B\\n25.6k\\n8192\\n2024-04\\n\"\\nCode-Qwen [249]\\nQwen Group\\n7B\\n92K\\n65536\\n2024-04\\n\"\\nLlama3 [180]\\nMeta\\n8B, 70B\\n128K\\n8192\\n2024-04\\n\"\\nStarCoder2-Instruct [304]\\nHugging Face\\n15.5B\\n49K\\n16384\\n2024-04\\n\"\\nCodestral [181]\\nMistral AI\\n22B\\n33k\\n32k\\n2024-05\\n\"'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 26, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Qwen Group\\n7B\\n92K\\n65536\\n2024-04\\n\"\\nLlama3 [180]\\nMeta\\n8B, 70B\\n128K\\n8192\\n2024-04\\n\"\\nStarCoder2-Instruct [304]\\nHugging Face\\n15.5B\\n49K\\n16384\\n2024-04\\n\"\\nCodestral [181]\\nMistral AI\\n22B\\n33k\\n32k\\n2024-05\\n\"\\nCausal Language Modeling. In decoder-only LLMs, given a sequence of tokens x = {ùë•1, . . . ,ùë•ùëõ},\\nthe CLM task refers to autoregressively predict the target tokens ùë•ùëñbased on the preceding tokens\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 27, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:28\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nùë•<ùëñin a sequence. The causal language modeling objective for training decoder LLMs is to minimize\\nthe following likelihood:\\nLùê∑ùëíùëêùëúùëëùëíùëü‚àíùëúùëõùëôùë¶\\nùê∂ùêøùëÄ\\n(x) = ‚àílog(\\nùëõ\\n√ñ\\nùëñ=1\\nùëÉùúÉ(ùë•ùëñ| x<ùëñ)) =\\nùëõ\\n‚àëÔ∏Å\\nùëñ=1\\n‚àílog ùëÉùúÉ(ùë•ùëñ| x<ùëñ)\\n(16)\\nwhere x<ùëñrepresents the sequence of preceding tokens {ùë•1, . . . ,ùë•ùëñ‚àí1} before xùëñin the input, ùúÉ\\ndenotes the model parameters. The conditional probability ùëÉùúÉ(ùë•ùëñ|x<ùëñ)) is modeled by adding a\\ncausal attention mask to the multi-head self-attention matrix of each Transformer block. To be\\nspecific, causal attention masking is implemented by setting the lower triangular part of the\\nmatrix to 0 and the remaining elements to ‚àí‚àû, ensuring that each token ùë•ùëñattends only to its\\npredecessors and itself. On the contrary, in encoder-decoder LLMs, a pivot token ùë•ùëòis randomly\\nselected in a sequence of tokens and then regarding the context before it as the source sequence'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 27, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='predecessors and itself. On the contrary, in encoder-decoder LLMs, a pivot token ùë•ùëòis randomly\\nselected in a sequence of tokens and then regarding the context before it as the source sequence\\nxùëñùëõ= {ùë•1, . . . ,ùë•ùëò} of the encoder and the sequence after it as the target output xùëúùë¢ùë°= {ùë•ùëò+1, . . . ,ùë•ùëõ}\\nof decoder. Formally, the causal language modeling objective for training encoder-decoder LLMs is\\nto minimize loss function as follows:\\nLùê∏ùëõùëêùëúùëëùëíùëü‚àíùê∑ùëíùëêùëúùëëùëíùëü\\nùê∂ùêøùëÄ\\n(x) = ‚àílog(\\nùëõ\\n√ñ\\nùëñ=ùëò+1\\nùëÉùúÉ(ùë•ùëñ| x‚â§ùëò, x<ùëñ)) =\\nùëõ\\n‚àëÔ∏Å\\nùëñ=ùëò+1\\n‚àílog ùëÉùúÉ(ùë•ùëñ| x‚â§ùëò, x<ùëñ)\\n(17)\\nwhere x‚â§ùëòis the source sequence input and x<ùëñdenotes the target sequence autoregressively\\ngenerated so far. During the inference phase, pre-trained LLMs that have been trained on large-\\nscale code corpus can generate code in a zero-shot manner without the need for fine-tuning. This\\nis achieved through the technique of prompt engineering, which guides the model to produce the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 27, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='scale code corpus can generate code in a zero-shot manner without the need for fine-tuning. This\\nis achieved through the technique of prompt engineering, which guides the model to produce the\\ndesired output11 [33, 214]. Additionally, recent studies have explored the use of few-shot learning,\\nalso referred to as in-context learning, to enhance model performance further [145, 206].\\nDenoising Autoencoding. In addition to causal language modeling (CLM), the denoising\\nautoencoding (DAE) task has been extensively applied in pre-training encoder-decoder architectures\\nfor code generation, such as PLBART [7], CodeT5 [271], and its enhanced successor, CodeT5+ [269].\\nFollowing T5 [217] and CodeT5 [271], the DAE refers to initially perturbing the source sequence\\nby introducing randomly masked spans of varying lengths. This corrupted sequence serves as the\\ninput for the encoder. Subsequently, the decoder employs an autoregressive strategy to reconstruct'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 27, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='by introducing randomly masked spans of varying lengths. This corrupted sequence serves as the\\ninput for the encoder. Subsequently, the decoder employs an autoregressive strategy to reconstruct\\nthe masked spans, integrating sentinel tokens to facilitate the generation process. This method\\nhas proven effective in improving the model‚Äôs ability to generate semantically and syntactically\\naccurate code by learning robust contextual representations [269, 271]. Formally, the denoising\\nautoencoding objective for training encoder-decoder LLMs is to minimize the following likelihood:\\nLùê∏ùëõùëêùëúùëëùëíùëü‚àíùê∑ùëíùëêùëúùëëùëíùëü\\nùê∑ùê¥ùê∏\\n(x) =\\nùëò\\n‚àëÔ∏Å\\nùëñ=1\\n‚àílog ùëÉùúÉ(xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\nùëñ\\n| x\\\\ùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†, xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\n<ùëñ\\n)\\n(18)\\nwhere ùúÉdenotes the model parameters, x\\\\ùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†is the noisy input with masked spans,\\nxùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†is the masked spans to predict from the decoder with ùëòdenoting the number of\\ntokens in xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†, and xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\n<ùëñ\\nis the span sequence autoregressively generated so far.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 27, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†is the masked spans to predict from the decoder with ùëòdenoting the number of\\ntokens in xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†, and xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\n<ùëñ\\nis the span sequence autoregressively generated so far.\\nCompared with CLM, the DAE task presents a more challenging scenario, as it necessitates a deeper\\nunderstanding and capture of the intrinsic semantic relationships among token sequences by LLMs\\n[217].\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 28, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:29\\nModel learns to perform\\nmany code tasks via natural\\nlanguage instructions\\nInference on\\nunseen code\\ntask\\n(A) Train from Scratch (Transformer)\\n‚Ä¢ Randomly initialized\\nmodel parameters\\n‚Ä¢ Typically requires many\\ncode task-specific examples\\n‚Ä¢ One specialized code model\\nfor each code task\\nImprove performance via\\nfew-shot prompting or\\nprompt engineering\\nUse RL to align\\nwith human\\npreferences\\nInference on\\nunseen code\\ntask\\n(B) Prompting (StarCoder)\\n(C) Pretrain-Finetune (CodeBERT, CodeT5)\\n(E) RLHF (InstructGPT)\\n‚Ä¢ Typically requires many\\ncode task-specific examples\\n‚Ä¢ One specialized code model for\\neach code task\\n(D) Instruction Tuning (WizardCoder)\\nPretrained\\nLM\\nPretrained\\nLM\\nPretrained\\nLM\\nPretrained\\nLM\\nInference\\non task A\\nInference\\non task A\\nInference\\non task A\\nInference\\non task A\\nInference\\non task A\\nTrained LM\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nFinetune\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nRLHF'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 28, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Inference\\non task A\\nInference\\non task A\\nInference\\non task A\\nTrained LM\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nFinetune\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nRLHF\\nTask-specific Knowledge\\nWorld/General Knowledge\\nInstruction-following with\\nMulti-task Learning\\nHuman Preference Alignment\\nModel learns to perform\\nmany code tasks via natural\\nlanguage instructions\\nFig. 9. Comparison of instruction tuning with various fine-tuning strategies and prompting for code tasks,\\nadapted from [274]. For (A), which involves training a Transformer from scratch, please refer to [6] for its use\\nin source code summarization task. In the case of (E), we utilize a representative RLHF [200] as an example.\\nAdditional reinforcement learning methods, such as DPO [216], are also applicable at this stage.\\n5.4\\nInstruction Tuning\\nAfter pre-training LLMs on large-scale datasets, the next phase typically involves augmenting the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 28, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='5.4\\nInstruction Tuning\\nAfter pre-training LLMs on large-scale datasets, the next phase typically involves augmenting the\\nmodel‚Äôs ability to process and follow various instructions, known as instruction tuning. Instruction\\ntuning generally refers to the supervised fine-tuning of pre-trained LLMs using datasets comprised\\nof structured examples framed as various natural language instructions [114, 200, 274, 313]. The\\ncomparison of instruction tuning with various fine-tuning strategies and prompting for code tasks\\nis depicted in Figure 9. Two exemplars of instruction data sampled from Code Alpaca [43] are\\ndemonstrated in Figure 10. It capitalizes on the heterogeneity of instruction types, positioning\\ninstruction tuning as a form of multi-task prompted training that significantly enhances the model‚Äôs\\ngeneralization to unseen tasks [56, 200, 229, 274].\\nIn the realm of code generation, natural language descriptions serve as the instructions guiding'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 28, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='generalization to unseen tasks [56, 200, 229, 274].\\nIn the realm of code generation, natural language descriptions serve as the instructions guiding\\nthe model to generate corresponding code snippets. Consequently, a line of research on instruction\\ntuning LLMs for code generation has garnered substantial interest across academia and industry.\\nTo perform instruction tuning, instruction data are typically compiled from source code with\\npermissive licenses [110, 132, 170] (refer to Section 5.1.2) or are constructed from synthetic code\\ndata [173, 278, 304] (refer to Section 5.2). These datasets are then utilized to fine-tune LLMs through\\na supervised learning paradigm. However, the substantial computational resources required for\\nfull parameter fine-tuning (FFT) LLM pose a notable challenge, particularly in scenarios with\\nconstrained resources [67, 153]. To mitigate this issue, parameter-efficient fine-tuning (PEFT) has'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 28, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='full parameter fine-tuning (FFT) LLM pose a notable challenge, particularly in scenarios with\\nconstrained resources [67, 153]. To mitigate this issue, parameter-efficient fine-tuning (PEFT) has\\nemerged as a compelling alternative strategy, gaining increasing attention for its potential to reduce\\nresource consumption [67]. In the following subsection, we categorize existing works based on\\ntheir instruction-tuning strategies to provide a comprehensive and systematic review.\\n11For more information on prompt engineering, visit https://www.promptingguide.ai\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 29, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:30\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\ndef find_primes(n): \\n    prime_list = [2] \\n    for number in range(2, n + 1): \\n        is_prime = True\\n        for k in range(2, number): \\n            if number % k == 0: \\n                is_prime = False \\n        if is_prime: \\n            prime_list.append(number) \\n    return prime_list\\nOutput:\\nInput:\\nInstruction:\\nWrite code to create a list of all \\nprime numbers between 2 and 100.\\nimport re\\nstring = \"This string contains some \\nurls such as https://www.google.com and \\nhttps://www.facebook.com.\"\\nurls = re.findall(\\'http[s]?://(?:[a-zA-\\nZ]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-\\nfA-F][0-9a-fA-F]))+\\', string) \\nprint(urls)\\nN/A\\nThis string contains some urls such as \\nhttps://www.google.com and \\nhttps://www.facebook.com.\\nGenerate a snippet of code to extract \\nall the URLs from the given string.\\nOutput:\\nInput:\\nInstruction:\\nFig. 10. Two exemplars of instruction data sampled from Code Alpaca [43] used to instruction-tune pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 29, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='all the URLs from the given string.\\nOutput:\\nInput:\\nInstruction:\\nFig. 10. Two exemplars of instruction data sampled from Code Alpaca [43] used to instruction-tune pre-\\ntrained code LLM to enhance their alignment with natural language instructions. The instruction corpus\\nencompasses a variety of tasks, each accompanied by distinct instructions, such as prime numbers generation\\nand URLs extraction.\\n5.4.1\\nFull Parameter Fine-tuning. Full parameter fine-tuning (FFT) involves updating all parameters\\nwithin a pre-trained model, as shown in Figure 11(a). This approach is often preferred when ample\\ncomputational resources and substantial training data are available, as it typically leads to better\\nperformance. [271] introduces an encoder-decoder pre-trained language model for code generation,\\nnamed CodeT5+. They instruction-tune this model on a dataset comprising 20k instruction samples\\nfrom Code Alpaca [43], resulting in an instruction-following model called InstructCodeT5+, which'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 29, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='named CodeT5+. They instruction-tune this model on a dataset comprising 20k instruction samples\\nfrom Code Alpaca [43], resulting in an instruction-following model called InstructCodeT5+, which\\nexhibited improved capabilities in code generation. [173] leverages the Evol-Instruct data synthesis\\ntechnique from WizardLM [289] to evolve 20K code Alpaca [43] instruction samples into a 78K\\ncode instruction dataset. This enriched dataset is then used to fine-tune the StarCoder base model,\\nresulting in WizardCoder, which showcases notable advancements in code generation. In a similar\\nvein, inspired by the successes of WizardCoder [173] and RRHF [303], Pangu-Coder 2 [234] applies\\nthe Evol-Instruct method to generate 68k high-quality instruction samples from the initial 20k Code\\nAlpaca [43] instruction samples. Additionally, they introduces a novel reinforcement learning via\\nRank Responses to align Test & Teacher Feedback (RRTF), which further enhances the performance'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 29, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Alpaca [43] instruction samples. Additionally, they introduces a novel reinforcement learning via\\nRank Responses to align Test & Teacher Feedback (RRTF), which further enhances the performance\\nof Pangu-Coder 2 in code generation. Diverging from synthetic instruction data generation methods,\\nOctoPack [187] utilizes real-world data by curating CommitPack from the natural structure of\\nGit commits, which inherently pair code changes with human-written instructions. This dataset,\\nconsisting of 4 terabytes of Git commits across 350 programming languages, is employed to fine-\\ntune StarCoder [147] and CodeGeeX2 [321], leading to the instruction-following code models of\\nOctoCoder and OctoGeeX for code generation, respectively. The most recent innovation comes\\nfrom Magicoder [278], who proposes OSS-INSTRUCT, a novel data synthesis method that leverages\\nopen-source code snippets to generate high-quality instruction data for code generation. This'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 29, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='from Magicoder [278], who proposes OSS-INSTRUCT, a novel data synthesis method that leverages\\nopen-source code snippets to generate high-quality instruction data for code generation. This\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 30, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:31\\napproach seeks to reduce the bias often present in synthetic data generated by LLM. In line with\\nOSS-INSTRUCT, the BigCode team introduces StarCoder2-15B-instruct [304], which they claim\\nto be the first entirely self-aligned LLM for code generation, trained with a fully permissive and\\ntransparent pipeline. Moreover, [59] harnesses open-source mathematics datasets, such as MATH\\n[95] and GSM8k [58], along with synthetically generated code following the OSS-INSTRUCT\\n[278] paradigm, to instruction-tune CodeGemma 7B, yielding exceptional results in mathematical\\nreasoning and code generation tasks.\\n5.4.2\\nParameter-Efficient Fine-tuning. To mitigate the extensive computational and resource de-\\nmands inherent in fine-tuning LLMs, the concept of parameter-efficient fine-tuning (PEFT) has\\nemerged to focus on updating a minimal subset of parameters, which may either be a selection of'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 30, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='mands inherent in fine-tuning LLMs, the concept of parameter-efficient fine-tuning (PEFT) has\\nemerged to focus on updating a minimal subset of parameters, which may either be a selection of\\nthe model‚Äôs parameters or an array of additional parameters specifically introduced for the tuning\\nprocess [67, 153]. The categorization of these methods is depicted in Figure 11(b), (c), and (d). A\\nplethora of innovative PEFT approaches have been developed, among which BitFit [305], Adapter\\n[102], Prompt tuning [142], Prefix-tuning [149], LoRA [103], IA3 [161], QLoRA [65], and AdaLoRA\\n[312] are particularly noteworthy. A seminal study in this field, LoRA [103], proposes a parameter\\nupdate mechanism for a pre-trained weight matrix ‚Äî such as those found in the key or value\\nprojection matrices of a Transformer block‚Äôs multi-head self-attention layer ‚Äî by factorizing the\\nupdate into two low-rank matrices. Crucially, all original model parameters remain frozen, with'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 30, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='projection matrices of a Transformer block‚Äôs multi-head self-attention layer ‚Äî by factorizing the\\nupdate into two low-rank matrices. Crucially, all original model parameters remain frozen, with\\nonly the pair of low-rank matrices being trainable. After fine-tuning, the product of these low-rank\\nmatrices can be seamlessly incorporated into the existing weight matrix through an element-wise\\naddition. This process can be formally described as:\\n(W0 + ŒîW)ùë•= W0ùë•+ ŒîWùë•= Wùëìùëüùëúùëßùëíùëõ\\n0\\nùë•+ ùõº\\nùëüBùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùë¢ùëù\\nAùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùëëùëúùë§ùëõ\\n|                     {z                     }\\nŒîW\\nùë•\\n(19)\\nwhere W0 ‚ààRùëë√óùëòdenotes a pre-trained weight matrix, Bùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùë¢ùëù\\n‚ààRùëë√óùëüand Aùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùëëùëúùë§ùëõ\\n‚ààRùëü√óùëòare\\ntwo trainable low-rank matrixes and initialized by a zero matrix and a random Gaussian distribution\\nN (0, ùúé2) respectively, to ensure ŒîW = 0 at the beginning of training. The rank ùëü‚â™min(ùëë,ùëò), the\\nùõº\\nùëüis a scaling coefficient to balance the importance of the LoRA module, like a learning rate.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 30, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='N (0, ùúé2) respectively, to ensure ŒîW = 0 at the beginning of training. The rank ùëü‚â™min(ùëë,ùëò), the\\nùõº\\nùëüis a scaling coefficient to balance the importance of the LoRA module, like a learning rate.\\nDespite the advancements in PEFT methods, their application in code generation remains limited.\\nFor instance, [121] pioneered the use of parameter-efficient instruction-tuning on a Llama 2 [253]\\nmodel with a single RTX 3090 GPU, leading to the development of a multilingual code generation\\nmodel called CodeUp. More recently, ASTRAIOS [334] conducted a thorough empirical examination\\nof parameter-efficient instruction tuning for code comprehension and generation tasks. This study\\nyielded several perceptive observations and conclusions, contributing valuable insights to the\\ndomain.\\n5.5\\nReinforcement Learning with Feedback\\nLLMs have exhibited remarkable instruction-following capabilities through instruction tuning.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 30, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='domain.\\n5.5\\nReinforcement Learning with Feedback\\nLLMs have exhibited remarkable instruction-following capabilities through instruction tuning.\\nHowever, they often produce outputs that are unexpected, toxic, biased, or hallucinated outputs that\\ndo not align with users‚Äô intentions or preferences [118, 200, 272]. Consequently, aligning LLMs with\\nhuman preference has emerged as a pivotal area of research. A notable work is InstructGPT [200],\\nwhich further fine-tunes an instruction-tuned model utilizing reinforcement learning with human\\nfeedback (RLHF) on a dataset where labelers have ranked model outputs in order of quality, from\\nbest to worst. This method has been instrumental in the development of advanced conversational\\nlanguage models, such as ChatGPT [196] and Bard [177]. Despite its success, acquiring high-quality\\nhuman preference ranking data is a resource-intensive process [141]. To address this, Reinforcement\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 31, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:32\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTrainable\\nInput\\nLayer N+1\\nüî•\\nLayer N\\nLayer 1\\nLayer 2\\n...\\nInput\\nLayer 2\\nüî•\\nLayer N\\nLayer 1\\n...\\nüî•\\nFrozen\\nInput\\n...\\nLayer 2\\nüî•\\nLayer 1\\nüî•\\nLayer N\\nüî•\\nInput\\nLayer N\\nLayer 1\\nLayer 2\\n‚ñ≥WN\\nüî•\\n‚ñ≥W2\\nüî•\\n‚ñ≥W1\\nüî•\\n...\\n...\\n(a) Full Fine-tuning\\n(b) Specification\\n(c) Addition\\n(d) Reparameterization\\nFig. 11. An illustration of full parameter fine-tuning (FFT) and parameter-efficient fine-tuning (PEFT) methods.\\n(a) refers to the Full Fine-tuning method, which updates all parameters of the base model during fine-tuning.\\n(b) stands for the Specification-based PEFT method that conditionally fine-tunes a small subset of the model\\nparameters while freezing the rest of the model, e.g. BitFit [305]. (c) represents the Addition-based PEFT\\nmethod that fine-tunes the incremental parameters introduced into the base model or input, e.g. Adapter\\n[102], Prefix-tuning [149], and Prompt-tuning [142]. (d) symbolizes the Reparameterization-based method'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 31, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[102], Prefix-tuning [149], and Prompt-tuning [142]. (d) symbolizes the Reparameterization-based method\\nwhich reparameterizes existing model parameters by low-rank transformation, e.g. LoRA [103], QLoRA [65],\\nand AdaLoRA [312].\\nLearning from AI Feedback (RLAIF) [21, 141] has been proposed to leverage powerful off-the-shelf\\nLLMs (e.g., ChatGPT [196] and GPT-4 [5]) to simulate human annotators by generating preference\\ndata.\\nBuilding on RLHF‚Äôs success, researchers have explored reinforcement learning with feedback to\\nenhance code generation in LLMs. Unlike RLHF, which relies on human feedback, this approach\\nemploys compilers or interpreters to automatically provide feedback on code samples through code\\nexecution on unit test cases, catalyzing the advancement of this research domain. CodeRL [139]\\nintroduced an actor-critic reinforcement learning framework for code generation. In this setup, the\\nlanguage model serves as the actor-network, while a token-level functional correctness reward'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 31, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='introduced an actor-critic reinforcement learning framework for code generation. In this setup, the\\nlanguage model serves as the actor-network, while a token-level functional correctness reward\\npredictor acts as the critic. Generated code is assessed through unit test signals from a compiler,\\nwhich can indicate compiler errors, runtime errors, unit test failures, or passes. CompCoder [266]\\nenhances code compilability by employing compiler feedback, including language model fine-\\ntuning, compilability reinforcement, and compilability discrimination strategies. Subsequently,\\nPPOCoder [238] integrates pre-trained code model CodeT5 [271] with Proximal Policy Optimization\\n(PPO) [230]. This integration not only utilizes execution (i.e., compilers or interpreters) feedback to\\nassess syntactic and functional correctness but also incorporates a reward function that evaluates\\nthe syntactic and semantic congruence between abstract syntax tree (AST) sub-trees and data flow'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 31, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='assess syntactic and functional correctness but also incorporates a reward function that evaluates\\nthe syntactic and semantic congruence between abstract syntax tree (AST) sub-trees and data flow\\ngraph (DFG) edges in the generated code against the ground truth. Additionally, the framework\\napplies a KL-divergence penalty to maintain fidelity between the actively learned policy and the\\nreferenced pre-trained model, enhancing the optimization process. More recently, RLTF [163] has\\nproposed an online reinforcement learning framework that provides fine-grained feedback based\\non compiler error information and location, along with adaptive feedback that considers the ratio\\nof passed test cases.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 32, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:33\\nDespite these successes, reinforcement learning algorithms face inherent limitations such as\\ninefficiency, instability, extensive resource requirements, and complex hyperparameter tuning,\\nwhich can impede the performance and scalability of LLMs. To overcome these challenges, recent\\nstudies have introduced various variants of RL methods that do not rely on PPO, including DPO\\n[216], RRHF [303], and sDPO [129]. In essence, these methods aim to maximize the likelihood\\nbetween the logarithm of conditional probabilities of preferred and rejected responses, which may\\nbe produced by LLMs with varying capabilities. Inspired by RRHF [303], PanGu-Coder 2 [234]\\nleverages a novel framework, Reinforcement Learning via Rank Responses to align Test & Teacher\\nFeedback (RRTF), significantly enhancing code generation capabilities, as evidenced by pass@1 of\\n62.20% on the HumanEval benchmark.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 32, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Feedback (RRTF), significantly enhancing code generation capabilities, as evidenced by pass@1 of\\n62.20% on the HumanEval benchmark.\\nTaking a step forward, the integration of more non-differentiable code features, such as coding\\nstyle [44, 178] and readability [34], into the reinforcement learning feedback for LLM-based code\\ngeneration, presents an exciting avenue for future research.\\n5.6\\nPrompting Engineering\\nLarge-scale language models (LLMs) such as GPT-3 and its successors have been trained on large-\\nscale data corpora, endowing them with substantial world knowledge [33, 200, 274]. Despite this,\\ncrafting an effective prompting as a means of communicating with LLMs to harness their full\\npotential remains a long-standing challenge [164]. Recent advancements in prompting engineering\\nhave expanded the capabilities of LLMs, enabling more sophisticated task completion and enhancing\\nboth reliability and performance. Notable techniques include Chain-of-Thought (CoT) [276], Self-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 32, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='have expanded the capabilities of LLMs, enabling more sophisticated task completion and enhancing\\nboth reliability and performance. Notable techniques include Chain-of-Thought (CoT) [276], Self-\\nConsistency [267], Tree-of-Thought (ToT) [295], Program of Thoughts (PoT) [50], Reasoning via\\nPlanning (RAP) [93], ReAct [296], Self-Refine [176], Reflexion [236], and LATS [327]. For instance,\\nCoT significantly improves the LLMs‚Äô ability to perform complex reasoning by providing a few\\nchain-of-thought demonstrations as exemplars in prompting.\\nPrompting engineering is particularly advantageous as it bypasses the need for additional training\\nand can significantly elevate performance. Consequently, numerous studies have leveraged this\\ntechnique for iterative and self-improving (refining) code generation within proprietary LLMs\\nsuch as ChatGPT and GPT-4. Figure 12 illustrates the general pipeline for self-improving code'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 32, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='technique for iterative and self-improving (refining) code generation within proprietary LLMs\\nsuch as ChatGPT and GPT-4. Figure 12 illustrates the general pipeline for self-improving code\\ngeneration with LLMs. For instance, Self-Debugging [51] involves prompting an LLM to iteratively\\nrefine a predicted program by utilizing feedback composed of code explanations combined with\\nexecution results, which assists in identifying and rectifying errors. When unit tests are unavailable,\\nthis feedback can rely solely on code explanations. Similarly, LDB [325] prompts LLMs to refine\\ngenerated code by incorporating debugging feedback, which consists of the evaluation of the\\ncorrectness of variable values throughout runtime execution, as assessed by the LLMs. In parallel,\\nSelfEvolve [122] employs a two-stage process where LLMs first generate domain-specific knowledge\\nfor a problem, followed by a trial code. This code is then iteratively refined through interactive'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 32, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='SelfEvolve [122] employs a two-stage process where LLMs first generate domain-specific knowledge\\nfor a problem, followed by a trial code. This code is then iteratively refined through interactive\\nprompting and execution feedback. An empirical investigation by [195] provides a comprehensive\\nanalysis of the self-repairing capabilities for code generation in models like Code Llama, GPT-\\n3.5, and GPT-4, using problem sets from HumanEval and APPS. This study yields a series of\\ninsightful observations and findings, shedding light on the self-refinement effectiveness of these\\nLLMs. Moreover, Reflexion [236] introduces a general approach for code generation wherein LLM-\\npowered agents engage in verbal self-reflection on task feedback signals, storing these reflections\\nin an episodic memory buffer to inform and improve decision-making in subsequent interactions.\\nLATS [327] adopts a novel strategy, utilizing LLMs as agents, value functions, and optimizers. It'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 32, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='in an episodic memory buffer to inform and improve decision-making in subsequent interactions.\\nLATS [327] adopts a novel strategy, utilizing LLMs as agents, value functions, and optimizers. It\\nenhances decision-making by meticulously constructing trajectories through Monte Carlo Tree\\nSearch (MCTS) algorithms, integrating external feedback, and learning from experience. This\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 33, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content=\"1:34\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nStep 2: Trajectory\\nStep 3: Evaluation\\nStep 4: Self-Reflection\\nStep 1: Code Task\\nFeedback\\nCode LLM\\nExecutor\\nWrite a Python script to \\nprint all unique elements \\nin a list.\\ndef unique_elements(lst):\\nresult = set(lst)\\n    return list(result)\\nassert unique_elements\\n(['a', 'b', 'c', 'a', 'd']) \\n== ['a', 'b', 'c', 'd'] \\n[‚Ä¶] does not work as \\nexpected because it uses \\nthe built-in `set()` \\nfunction in Python, which \\ndoes not maintain the order \\nof elements.[‚Ä¶]\\n(Optional)\\nassert unique_elements([1, \\n2, 3, 4, 4]) == [1, 2, 3, 4]\\n(Code) LLM\\nFig. 12. An illustration of the self-improving code generation pipeline using prompts for LLMs. This process\\nincorporates iterative self-refinement by integrating execution outcomes and includes an optional self-\\nreflection mechanism to enhance generation quality.\\napproach has demonstrated remarkable results in code generation, achieving a pass@1 of 94.4% on\"),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 33, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='reflection mechanism to enhance generation quality.\\napproach has demonstrated remarkable results in code generation, achieving a pass@1 of 94.4% on\\nthe HumanEval benchmark with GPT-4.\\nDistinct from the aforementioned methods, CodeT [45] and LEVER [190] prompt LLMs to\\ngenerate numerous code samples, which are then re-ranked based on execution outcomes to select\\nthe optimal solution. Notably, these approaches do not incorporate a self-refinement step to further\\nimprove code generation.\\n5.7\\nRepository Level & Long Context\\nIn contemporary software engineering practices, modifications to a code repository are widespread\\nand encompass a range of activities, including package migration, temporary code edits, and the\\nresolution of GitHub issues. While LLMs showcase impressive prowess in function-level code\\ngeneration, they often falter when grappling with the broader context inherent to a repository,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 33, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='resolution of GitHub issues. While LLMs showcase impressive prowess in function-level code\\ngeneration, they often falter when grappling with the broader context inherent to a repository,\\nsuch as import dependencies, parent classes, and files bearing similar names. These deficiencies\\nresult in suboptimal performance in repository-level code generation, as identified in recent studies\\n[239, 240]. The challenges faced by LLMs in this domain are primarily due to the following factors:\\n‚Ä¢ Code repositories typically contain intricate interdependencies scattered across various\\nfiles, including shared utilities, configurations, and cross-API invocations, which arise from\\nmodular design principles [22, 309].\\n‚Ä¢ Repositories are characterized by their unique structures, naming conventions, and coding\\nstyles, which are essential for maintaining clarity and facilitating ongoing maintenance [44].\\n‚Ä¢ The vast context of an entire repository often exceeds the context length limitations of LLMs,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 33, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='styles, which are essential for maintaining clarity and facilitating ongoing maintenance [44].\\n‚Ä¢ The vast context of an entire repository often exceeds the context length limitations of LLMs,\\nthus hindering their ability to integrate comprehensive contextual information [22].\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 34, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:35\\n‚Ä¢ LLMs may not have been adequately trained on extensive sets of repository data, such as\\nproprietary software or projects that are still in development [239].\\nGiven that the scope of a typical software repository encompasses hundreds of thousands of\\ntokens, it is imperative to enhance the capacity of LLMs to handle extensive contexts when they\\nare employed for repository-level code generation. Fortunately, recent advancements in positional\\nencoding techniques, such as ALiBi [211] and RoPE [243], have shown promise in improving the\\nTransformer‚Äôs ability to generalize from shorter training sequences to longer inference sequences\\n[318]. This progress addresses the third challenge mentioned above to a certain degree, thereby\\nenabling better contextualization of coding activities within full repositories.\\nTo further refine LLMs for repository-level code completion, several innovative approaches have'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 34, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='enabling better contextualization of coding activities within full repositories.\\nTo further refine LLMs for repository-level code completion, several innovative approaches have\\nbeen introduced. RepoCoder [309] leverages a similarity-based retrieval system within an iterative\\nretrieval-generation paradigm to enrich the context and enhance code completion quality. In a\\nsimilar vein, CoCoMIC [69] employs a cross-file context finder named CCFINDER to pinpoint and\\nretrieve the most relevant cross-file contexts within a repository. RepoHyper [209] introduces a\\nsemantic graph structure, termed RSG, to encapsulate the expansive context of code repositories\\nand uses an ‚ÄúExpand and Refine‚Äù retrieval method to obtain relevant code snippets. Moreover, a\\nframework known as RLPG [240] has been proposed to generate repository-level prompts that\\nintegrate the repository‚Äôs structure with the relevant context across all files. However, the constant'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 34, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='framework known as RLPG [240] has been proposed to generate repository-level prompts that\\nintegrate the repository‚Äôs structure with the relevant context across all files. However, the constant\\nreliance on retrieval mechanisms has raised concerns regarding efficiency and robustness, as some\\nretrieved contexts may prove unhelpful or harmful. In response, Repoformer [282] introduces a\\nselective Retrieval-Augmented Generation (RAG) framework that judiciously bypasses retrieval\\nwhen it is deemed redundant. This approach incorporates a self-supervised learning strategy that\\nequips a code LLM with the ability to perform a self-assessment on the utility of retrieval for\\nenhancing the quality of its output, thereby effectively utilizing potentially noisy retrieved contexts.\\nAdditionally, RepoFusion [239] has been developed to train models to combine multiple relevant\\ncontexts from a repository, aiming to produce more precise and context-aware code completions.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 34, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Additionally, RepoFusion [239] has been developed to train models to combine multiple relevant\\ncontexts from a repository, aiming to produce more precise and context-aware code completions.\\nIn a novel approach, Microsoft‚Äôs CodePlan [22] frames repository-level coding tasks as a planning\\nproblem, generating a multi-step chain of edits (plan) where each step involves invoking an LLM on a\\nspecific code location, considering context from the entire repository, preceding code modifications,\\nand task-specific instructions.\\nAdvancing the state-of-the-art, [308] tackles the formidable challenge of NL2Repo, an endeavor\\nthat seeks to create a complete code repository from natural language requirements. To address\\nthis complex task, they introduce the CodeS framework, which strategically breaks down NL2Repo\\ninto a series of manageable sub-tasks using a multi-layer sketch approach. The CodeS framework'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 34, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='this complex task, they introduce the CodeS framework, which strategically breaks down NL2Repo\\ninto a series of manageable sub-tasks using a multi-layer sketch approach. The CodeS framework\\ncomprises three distinct modules: 1) RepoSketcher, for creating a directory structure of the reposi-\\ntory based on given requirements; 2) FileSketcher, for sketching out each file within that structure;\\nand 3) SketchFiller, for fleshing out the specifics of each function within the file sketches [308].\\nAccordingly, a surge of benchmarks tailored for repository-level code generation has emerged,\\nsuch as RepoEval [309], Stack-Repo [239], Repobench [167], EvoCodeBench [144], SWE-bench\\n[123], CrossCodeEval [68], and SketchEval [308]. The detailed statistics and comparisons of these\\nbenchmarks are presented in Table 6.\\nDespite the progress made by these methods in repository-level code generation, significant chal-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 34, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='benchmarks are presented in Table 6.\\nDespite the progress made by these methods in repository-level code generation, significant chal-\\nlenges remain to be addressed. Programming developers are often required to invest considerable\\ntime in editing and debugging [25, 28, 186, 239, 255]. However, the advent of LLM-powered coding\\nagents, such as AutoCodeRover [316], SWE-Agent [124], and OpenDevin [199], has demonstrated\\ntheir potential to tackle complex problems, paving the way for future exploration in this field (for\\nmore details, see Section 5.9).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 35, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:36\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nEmbedding \\nModel\\nQuery\\nRetrieved Context\\nCreate a quick-\\nsort algorithm \\nin Python.\\nCombine Prompts and Context\\nCreate a quick-sort algorithm \\nin Python.\\nPlease solve the above problem \\nbased on the following context:\\n{context}\\ndef quick_sort(arr):\\n\"\"\"Sort a list of numbers in ascending \\norder using the Quick-Sort algorithm\"\"\"\\nif len(arr) == 0:\\nreturn []\\npivot = arr[0]\\nleft_arr = [x for x in arr if x < pivot]\\nright_arr = [x for x in arr if x > pivot]\\nreturn quick_sort(left_arr) + [pivot] + \\nquick_sort(right_arr)\\nCode LLM\\nEmbedding \\nModel\\nVector \\nDatabase\\nCode Solution\\nCode Data \\nChunks\\nOpen \\nSource\\nStage 1: Retrieval\\nStage 2: Generation\\nAlgorithm:\\n1. If the input \\narray...already \\nsorted....\\n5. Recursively \\ncall quicksort...\\nFig. 13. A workflow illustration of the Retrieval-Augmented Code Generation (RACG). Upon receiving a query'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 35, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Algorithm:\\n1. If the input \\narray...already \\nsorted....\\n5. Recursively \\ncall quicksort...\\nFig. 13. A workflow illustration of the Retrieval-Augmented Code Generation (RACG). Upon receiving a query\\n(instruction), the retriever selects the relevant contexts from a large-scale vector database. Subsequently, the\\nretrieved contexts are merged with the query, and this combined input is fed into the generator (LLM) to\\nproduce the target code solution.\\n5.8\\nRetrieval Augmented\\nLLMs have exhibited impressive capabilities but are hindered by several critical issues such as\\nhallucination [154, 315], obsolescence of knowledge [115], and non-transparent [32], untraceable\\nreasoning processes [80, 106, 276, 329]. While techniques like instruction-tuning (see Section 5.4)\\nand reinforcement learning with feedback (see Section 5.5) mitigate these issues, they also introduce\\nnew challenges, such as catastrophic forgetting and the requirement for substantial computational'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 35, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and reinforcement learning with feedback (see Section 5.5) mitigate these issues, they also introduce\\nnew challenges, such as catastrophic forgetting and the requirement for substantial computational\\nresources during training [90, 201].\\nRecently, Retrieval-Augmented Generation (RAG) has emerged as an innovative approach to\\novercoming these limitations by integrating knowledge from external databases. Formally defined,\\nRAG denotes a model that, in response to queries, initially sources relevant information from\\nan extensive corpus of documents, and then leverages this retrieved information in conjunction\\nwith the original query to enhance the response‚Äôs quality and accuracy, especially for knowledge-\\nintensive tasks. The RAG framework typically consists of a vector database, a retriever, a re-ranker,\\nand a generator. It is commonly implemented using tools such as LangChain12 and LLamaIndex13.\\nBy performing continuous knowledge updates of the database and the incorporation of domain-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 35, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and a generator. It is commonly implemented using tools such as LangChain12 and LLamaIndex13.\\nBy performing continuous knowledge updates of the database and the incorporation of domain-\\nspecific data, RAG circumvents the need for re-training LLMs from scratch [80]. Consequently,\\nRAG has substantially advanced LLM performance across a variety of tasks [47, 143].\\nDue to the nature of code, code LLMs are also susceptible to the aforementioned issues that\\naffect general-purpose LLMs. For instance, they may exhibit a hallucination phenomenon when\\ninstructions fall outside the scope of their training data or necessitate the latest programming\\npackages. Given the dynamic nature of publicly available source-code libraries like PyTorch, which\\nundergo frequent expansion and updates, deprecated calling methods can become a significant\\nchallenge. If Code LLMs are not updated in tandem with the latest functions and APIs, this can'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 35, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='undergo frequent expansion and updates, deprecated calling methods can become a significant\\nchallenge. If Code LLMs are not updated in tandem with the latest functions and APIs, this can\\nintroduce potential errors and safety risks. Retrieval-Augmented Code Generation (RACG) stands\\n12LangChain facilitates the development of LLM-powered applications. https://www.langchain.com\\n13LLamaIndex is a leading data framework for building LLM applications. https://www.llamaindex.ai\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 36, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:37\\nas a promising solution to these concerns. A workflow illustration of the RACG is depicted in\\nFigure 13.\\nDespite its potential, the adoption of RAG for code generation remains limited. Drawing in-\\nspiration from the common practice among programmers of referencing related code snippets,\\n[166] introduced a novel retrieval-augmented mechanism with graph neural networks (GNNs),\\ntermed HGNN, which unites the advantages of similar examples retrieval with the generalization\\ncapabilities of generative models for code summarization, which is the reverse process of code\\ngeneration. [205] pioneered a retrieval augmented framework named REDCODER for code gener-\\nation by retrieving and integrating relevant code snippets from a source-code database, thereby\\nproviding supplementary context for the generation process. Subsequently, a retrieval-augmented'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 36, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='ation by retrieving and integrating relevant code snippets from a source-code database, thereby\\nproviding supplementary context for the generation process. Subsequently, a retrieval-augmented\\ncode completion framework termed ReACC [171] is proposed to leverage both lexical copying and\\nsemantic referencing of related code, achieving state-of-the-art performance on the CodeXGLUE\\nbenchmark [172]. In the spirit of how programmers often consult textual resources such as code\\nmanuals and documentation to comprehend functionalities, DocPrompting [330] explicitly utilizes\\ncode documentation by retrieving the relevant documentation pieces based on a natural language\\nquery and then generating the target code by blending the query with the retrieved information.\\nMore recently, RepoCoder [309], an iterative retrieval-generation framework, is proposed for\\nenhancing repository-level code completion by effectively utilizing code analogies across different'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 36, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='More recently, RepoCoder [309], an iterative retrieval-generation framework, is proposed for\\nenhancing repository-level code completion by effectively utilizing code analogies across different\\nfiles within a repository to inform and improve code suggestions. Furthermore, breaking away\\nfrom reliance on a singular source of retrieval, [242] developed a multi-faceted ‚Äúknowledge soup‚Äù\\nthat integrates web searches, documentation, execution feedback, and evolved code snippets. Then,\\nit incorporates an active retrieval strategy that iteratively refines the query and enriches the\\nknowledge soup, expanding the scope of information available for code generation.\\nDespite these advancements, several limitations in retrieval-augmented code generation warrant\\nfurther exploration: 1) the quality of the retrieved information significantly impacts overall perfor-\\nmance; 2) the effective integration of retrieved code information with the query needs optimization;'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 36, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='mance; 2) the effective integration of retrieved code information with the query needs optimization;\\n3) an over-reliance on retrieved information may lead to inadequate responses that fail to address\\nthe query‚Äôs intent; 4) additional retrieved information necessitates larger context windows for the\\nLLM, resulting in increased computational demands.\\n5.9\\nAutonomous Coding Agents\\nThe advent of LLMs has marked the beginning of a new era of potential pathways toward artificial\\ngeneral intelligence (AGI), capturing significant attention in both academia and industry [108, 261,\\n279, 284]. A rapidly expanding array of applications for LLM-based autonomous agents, including\\nAutoGPT [2], AgentGPT [1], BabyAGI [3], and AutoGen [283], underlines the promise of this\\ntechnology.\\nLLM-powered autonomous agents are systems endowed with sophisticated reasoning abilities,\\nleveraging an LLM as a central computational engine or controller. This allows them to formulate'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 36, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='technology.\\nLLM-powered autonomous agents are systems endowed with sophisticated reasoning abilities,\\nleveraging an LLM as a central computational engine or controller. This allows them to formulate\\nand execute problem-solving plans through a series of tool-enabled functions or API calls. Moreover,\\nthese agents are designed to function within a shared environment where they can communicate\\nand engage in cooperative, competitive, or negotiating interactions [104, 261, 283]. The typical\\narchitecture of such an agent encompasses an LLM-based Agent, a memory module, a planning\\ncomponent, and a tool utilization module, as depicted in Figure 14.\\nIn the realm of automated code generation, LLM-powered autonomous agents have demon-\\nstrated remarkable proficiency. For instance, AgentCoder [104] achieved a groundbreaking pass@1\\nof 96.3% on the HumanEval benchmark, forwarding a step closer to the future of automated soft-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 36, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='strated remarkable proficiency. For instance, AgentCoder [104] achieved a groundbreaking pass@1\\nof 96.3% on the HumanEval benchmark, forwarding a step closer to the future of automated soft-\\nware development [111]. The innovative meta-programming framework termed MetaGPT [100]\\nintegrates human workflow efficiencies into LLM-based multi-agent collaboration, as shown in\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 37, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:38\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nAgent\\nMemory\\nAction\\nTools\\nPlanning\\nShort-term Memory\\nLong-term Memory\\nCalendar ( )\\nCalculator ( )\\nCode Interpreter ( )\\nSearch ( )\\n...more\\nReflection\\nSelf-critics\\nChain of Thoughts\\nSubgoal Decomposition\\nFig. 14. The general architecture of an LLM-powered autonomous agent system, adapted from [279]. Plan-\\nning: The agent decomposes large tasks into smaller, manageable sub-goals or engages in self-criticism\\nand self-reflection on past actions to learn from mistakes and improve future performance. Memory: This\\ncomponent enables the agent to store and retrieve past information. Tools: The agent is trained to invoke\\nexternal functions or APIs. Action: The agent executes actions, with or without the use of tools, to interact\\nwith the environment. The gray dashed lines represent the data flow within the system.\\nFig. 15. MetaGPT integrates human workflow efficiencies into LLM-based multi-agent collaboration to break'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 37, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='with the environment. The gray dashed lines represent the data flow within the system.\\nFig. 15. MetaGPT integrates human workflow efficiencies into LLM-based multi-agent collaboration to break\\ndown complex code-related tasks into specific, actionable procedures. These procedures are then assigned to\\nvarious roles, such as Product Manager, Architect, and Engineer played by LLM. The image is sourced from\\nthe original paper [100].\\nFigure 15. Furthermore, [104] introduces AgentCoder, a multi-agent framework composed of three\\nspecialized agents, each with distinct roles and capabilities. These roles include a programmer agent\\nresponsible for code generation, a test designer agent tasked with generating unit test cases, and\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 38, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:39\\na test executor agent that executes the code and provides feedback. This division of labor within\\nAgentCoder promotes more efficient and effective code generation. CodeAct [265] distinguishes\\nitself by utilizing executable Python code to consolidate LLM agent actions within a unified action\\nspace, in contrast to the generation of JSON or textual formats. Additionally, AutoCodeRover [316]\\nis proposed to autonomously resolve GitHub issues for program enhancement.\\nTo address the complexity of tasks within software engineering, two innovative autonomous AI\\nsoftware engineers Devin14[61] and OpenDevin15[199], have been released and rapidly garnered\\nconsiderable interest within the software engineering (SE) and artificial general intelligence (AGI)\\ncommunity. Subsequently, an autonomous system, SWE-agent [124], leverages a language model'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 38, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='considerable interest within the software engineering (SE) and artificial general intelligence (AGI)\\ncommunity. Subsequently, an autonomous system, SWE-agent [124], leverages a language model\\nto interact with a computer to address software engineering tasks, successfully resolving 12.5% of\\nissues on the SWE-bench benchmark [123]. L2MAC [98] has been introduced as the first practical,\\nLLM-based, multi-agent, general-purpose stored-program automatic computer that utilizes a von\\nNeumann architecture, designed specifically for the generation of long and consistent outputs.\\nAt the time of writing this survey, OpenDevin has enhanced CodeAct with bash command-based\\ntools, leading to the release of OpenDevin CodeAct 1.0 [287], which sets a new state-of-the-art\\nperformance on the SWE-Bench Lite benchmark [123].\\nDespite these remarkable advancements, the journey toward fully realized AI software engineers\\nemploying LLM-powered autonomous agents is far from complete [261, 284]. Critical aspects'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 38, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Despite these remarkable advancements, the journey toward fully realized AI software engineers\\nemploying LLM-powered autonomous agents is far from complete [261, 284]. Critical aspects\\nsuch as prompt design, context length, agent count, and toolsets call for further refinement and\\noptimization, especially as problem complexities escalate [111].\\n5.10\\nEvaluation\\nDespite the impressive capabilities of LLMs, they exhibit a range of behaviors that are both beneficial\\nand potentially risky. These behaviors can enhance performance across various downstream tasks\\nbut may also introduce reliability and trustworthiness concerns in LLM deployment [42, 48, 290].\\nConsequently, it is imperative to develop precise evaluation approaches to discern the qualitative\\nand quantitive differences between models, thereby encouraging further advancements in LLM\\ncapabilities.\\nEvaluation strategies for LLMs in code generation mirror those for general-purpose LLMs and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 38, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and quantitive differences between models, thereby encouraging further advancements in LLM\\ncapabilities.\\nEvaluation strategies for LLMs in code generation mirror those for general-purpose LLMs and\\ncan be divided into three principal categories: metrics-based, human-centered, and LLM-based\\napproaches. Detailed benchmarks for these evaluation strategies are presented in Section 5.1.3 and\\nsummarized in Table 6. Subsequent subsections will provide a thorough analysis of each approach.\\n5.10.1\\nMetrics. The pursuit of effective and reliable automatic evaluation metrics for generated\\ncontent is a long-standing challenge within the field of natural language processing (NLP) [49, 156,\\n203]. At the early stage, most works directly leverage token-matching-based metrics, such as Exact\\nMatch, BLEU [203], ROUGE [156], and METEOR [23], which are prevalent in text generation of\\nNLP, to assess the quality of code generation.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 38, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Match, BLEU [203], ROUGE [156], and METEOR [23], which are prevalent in text generation of\\nNLP, to assess the quality of code generation.\\nWhile these metrics offer a rapid and cost-effective approach for assessing the quality of gener-\\nated code, they often fall short of capturing the syntactical and functional correctness, as well as\\nthe semantic features of the code. To eliminate this limitation, CodeBLEU [221] was introduced, en-\\nhancing the traditional BLEU metric [203] by incorporating syntactic information through abstract\\nsyntax trees (AST) and semantic understanding via data-flow graph (DFG). Despite these improve-\\nments, the metric does not fully resolve issues pertaining to execution errors or discrepancies in\\nthe execution results of the generated code. In light of these challenges, execution-based metrics\\nhave gained prominence for evaluating code generation, including pass@k [48], n@k [151], test\\n14https://www.cognition.ai/introducing-devin'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 38, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='have gained prominence for evaluating code generation, including pass@k [48], n@k [151], test\\n14https://www.cognition.ai/introducing-devin\\n15https://github.com/OpenDevin/OpenDevin\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 39, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:40\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 9. The performance comparison of LLMs for code generation on the HumanEval [48] benchmark,\\nmeasured by Pass@1. For models with various sizes, we report only the largest size version of each model\\nwith a magnitude of B parameters. ‚Ä° denotes instruction-tuned models.\\nModel\\nSize\\npass@1 (%)\\nAvailability\\nClosed Source\\nGPT-4o-0513 [197]\\n-\\n91.0\\n[API Access]\\nGPT-4-Turbo-0409 [198]\\n-\\n88.2\\n[API Access]\\nGPT-4-1106 [5]\\n-\\n87.8\\n[API Access]\\nGPT-3.5-Turbo-0125 [196]\\n-\\n76.2\\n[API Access]\\nClaude-3.5-Sonnet [14]\\n-\\n92.0\\n[API Access]\\nClaude-3-Opus [14]\\n-\\n84.9\\n[API Access]\\nClaude-3-Sonnet [14]\\n-\\n73.0\\n[API Access]\\nClaude-3-Haiku [14]\\n-\\n75.9\\n[API Access]\\nGemini-1.5-Pro [220]\\n-\\n84.1\\n[API Access]\\nGemini-1.5-Flash [220]\\n-\\n74.3\\n[API Access]\\nGemini-1.0-Ultra [220]\\n-\\n74.4\\n[API Access]\\nGemini-1.0-Pro [220]\\n-\\n67.7\\n[API Access]\\n‚Ä°PanGu-Coder2 [234]\\n15B\\n61.64\\n-\\nPanGu-Coder [55]\\n2.6B\\n23.78\\n-\\nCodex [48]\\n12B\\n28.81\\nDeprecated\\nPaLM-Coder [54]\\n540B\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 39, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='-\\n74.4\\n[API Access]\\nGemini-1.0-Pro [220]\\n-\\n67.7\\n[API Access]\\n‚Ä°PanGu-Coder2 [234]\\n15B\\n61.64\\n-\\nPanGu-Coder [55]\\n2.6B\\n23.78\\n-\\nCodex [48]\\n12B\\n28.81\\nDeprecated\\nPaLM-Coder [54]\\n540B\\n36\\n-\\nAlphaCode [151]\\n1.1B\\n17.1\\n-\\nOpen Source\\n‚Ä°Codestral [181]\\n22B\\n81.1\\n[Checkpoint Download]\\n‚Ä°DeepSeek-Coder-V2-Instruct [331]\\n21B (236B)\\n90.2\\n[Checkpoint Download]\\n‚Ä°Qwen2.5-Coder-Instruct [109]\\n7B\\n88.4\\n[Checkpoint Download]\\nQwen2.5-Coder [109]\\n7B\\n61.6\\n[Checkpoint Download]\\n‚Ä°StarCoder2-Instruct [304]\\n15.5B\\n72.6\\n[Checkpoint Download]\\n‚Ä°CodeGemma-Instruct [59]\\n7B\\n56.1\\n[Checkpoint Download]\\nCodeGemma [59]\\n7B\\n44.5\\n[Checkpoint Download]\\nStarCoder 2 [170]\\n15B\\n46.3\\n[Checkpoint Download]\\n‚Ä°WaveCoder-Ultra [301]\\n6.7B\\n79.9\\n[Checkpoint Download]\\n‚Ä°WaveCoder-Pro [301]\\n6.7B\\n74.4\\n[Checkpoint Download]\\n‚Ä°WaveCoder-DS [301]\\n6.7B\\n65.8\\n[Checkpoint Download]\\nStableCode [210]\\n3B\\n29.3\\n[Checkpoint Download]\\nCodeShell [285]\\n7B\\n34.32\\n[Checkpoint Download]\\n‚Ä°CodeQwen1.5-Chat [249]\\n7B\\n83.5\\n[Checkpoint Download]\\nCodeQwen1.5 [249]\\n7B\\n51.8'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 39, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[Checkpoint Download]\\nStableCode [210]\\n3B\\n29.3\\n[Checkpoint Download]\\nCodeShell [285]\\n7B\\n34.32\\n[Checkpoint Download]\\n‚Ä°CodeQwen1.5-Chat [249]\\n7B\\n83.5\\n[Checkpoint Download]\\nCodeQwen1.5 [249]\\n7B\\n51.8\\n[Checkpoint Download]\\n‚Ä°DeepSeek-Coder-Instruct [88]\\n33B\\n79.3\\n[Checkpoint Download]\\nDeepSeek-Coder [88]\\n33B\\n56.1\\n[Checkpoint Download]\\nreplit-code [223]\\n3B\\n20.12\\n[Checkpoint Download]\\n‚Ä°MagicoderùëÜ-CL [278]\\n7B\\n70.7\\n[Checkpoint Download]\\n‚Ä°Magicoder-CL [278]\\n7B\\n60.4\\n[Checkpoint Download]\\n‚Ä°WizardCoder [173]\\n33B\\n79.9\\n[Checkpoint Download]\\nCodeFuse [160]\\n34B\\n74.4\\n[Checkpoint Download]\\nPhi-1 [84]\\n1.3B\\n50.6\\n[Checkpoint Download]\\n‚Ä°Code Llama-Instruct [227]\\n70B\\n67.8\\n[Checkpoint Download]\\nCode Llama [227]\\n70B\\n53.0\\n[Checkpoint Download]\\n‚Ä°OctoCoder [187]\\n15.5B\\n46.2\\n[Checkpoint Download]\\nCodeGeeX2 [321]\\n6B\\n35.9\\n[Checkpoint Download]\\n‚Ä°InstructCodeT5+ [269]\\n16B\\n35.0\\n[Checkpoint Download]\\nCodeGen-NL [193]\\n16.1B\\n14.24\\n[Checkpoint Download]\\nCodeGen-Multi [193]\\n16.1B\\n18.32\\n[Checkpoint Download]\\nCodeGen-Mono [193]'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 39, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[Checkpoint Download]\\n‚Ä°InstructCodeT5+ [269]\\n16B\\n35.0\\n[Checkpoint Download]\\nCodeGen-NL [193]\\n16.1B\\n14.24\\n[Checkpoint Download]\\nCodeGen-Multi [193]\\n16.1B\\n18.32\\n[Checkpoint Download]\\nCodeGen-Mono [193]\\n16.1B\\n29.28\\n[Checkpoint Download]\\nStarCoder [147]\\n15B\\n33.60\\n[Checkpoint Download]\\nCodeT5+ [271]\\n16B\\n30.9\\n[Checkpoint Download]\\nCodeGen2 [192]\\n16B\\n20.46\\n[Checkpoint Download]\\nSantaCoder [9]\\n1.1B\\n14.0\\n[Checkpoint Download]\\nInCoder [77]\\n6.7B\\n15.2\\n[Checkpoint Download]\\nPolyCoder [290]\\n2.7B\\n5.59\\n[Checkpoint Download]\\nCodeParrot [254]\\n1.5B\\n3.99\\n[Checkpoint Download]\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 40, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:41\\ncase average [95], execution accuracy [218], and pass@t [195]. In particular, the pass@k, serving\\nas a principal evaluation metric, assesses the probability that at least one out of ùëòcode samples\\ngenerated by a model will pass all unit tests. An unbiased estimator for pass@k introduced by [48]\\nis defined as:\\npass@k B Etask\\n\"\\n1 ‚àí\\n\\x00ùëõ‚àíùëê\\nùëò\\n\\x01\\n\\x00ùëõ\\nùëò\\n\\x01\\n#\\n(20)\\nwhere ùëõis the total number of sampled candidate code solutions, ùëòis the number of randomly\\nselected code solutions from these candidates for each programming problem, with ùëõ‚â•ùëò, and ùëêis\\nthe count of correct samples within the ùëòselected.\\nNevertheless, these execution-based methods are heavily dependent on the quality of unit tests\\nand are limited to evaluating executable code [307]. Consequently, when unit tests are unavailable,\\ntoken-matching-based metrics are often employed as an alternative for evaluation. Furthermore, in'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 40, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and are limited to evaluating executable code [307]. Consequently, when unit tests are unavailable,\\ntoken-matching-based metrics are often employed as an alternative for evaluation. Furthermore, in\\nscenarios lacking a ground truth label, unsupervised metrics such as perplexity (PPL) [116] can\\nserve as evaluative tools. Perplexity quantifies an LLM‚Äôs uncertainty in predicting new content,\\nthus providing an indirect measure of the model‚Äôs generalization capabilities and the quality of the\\ngenerated code.\\nTaken together, while the aforementioned methods primarily focus on the functional correctness\\nof code, they do not provide a holistic evaluation that encompasses other critical dimensions such\\nas code vulnerability [189], maintainability [15], readability [34], complexity and efficiency [208],\\nstylistic consistency [178], and execution stability [215]. A comprehensive evaluation framework'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 40, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='as code vulnerability [189], maintainability [15], readability [34], complexity and efficiency [208],\\nstylistic consistency [178], and execution stability [215]. A comprehensive evaluation framework\\nthat integrates these aspects remains an open area for future research and development in the field\\nof code generation assessment.\\n5.10.2\\nHuman Evaluation. Given the intrinsic characteristics of code, the aforementioned automatic\\nevaluation metrics are inherently limited in their capacity to fully assess code quality. For instance,\\nmetrics specifically designed to measure code style consistency are challenging to develop and\\noften fail to capture this aspect adequately [44]. When it comes to repository-level code generation,\\nthe evaluation of overall code quality is substantially complicated due to the larger scale of the\\ntask, which involves cross-file designs and intricate internal as well as external dependencies, as\\ndiscussed by [22, 239].'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 40, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='task, which involves cross-file designs and intricate internal as well as external dependencies, as\\ndiscussed by [22, 239].\\nTo overcome these challenges, conducting human evaluations becomes necessary, as it yields\\nrelatively robust and reliable results. Human assessments also offer greater adaptability across\\nvarious tasks, enabling the simplification of complex and multi-step evaluations. Moreover, human\\nevaluations are essential for demonstrating the effectiveness of certain token-matching-based\\nmetrics, such as CodeBLEU [221]. These studies typically conduct experiments to evaluate the\\ncorrelation coefficient between proposed metrics and quality scores assigned by actual users,\\ndemonstrating their superiority over existing metrics.\\nMoreover, in an effort to better align LLMs with human preferences and intentions, InstructGPT\\n[200] employs human-written prompts and demonstrations, and model output ranking in the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 40, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Moreover, in an effort to better align LLMs with human preferences and intentions, InstructGPT\\n[200] employs human-written prompts and demonstrations, and model output ranking in the\\nfine-tuning of LLMs using reinforcement learning from human feedback (RLHF). Although similar\\nalignment learning techniques have been applied to code generation, the feedback in this domain\\ntypically comes from a compiler or interpreter, which offers execution feedback, rather than from\\nhuman evaluators. Notable examples include CodeRL [139], PPOCoder [238], RLTF [163], and\\nPanGu-Coder2 [234]. Further information on this topic is available in Section 5.5.\\nNonetheless, human evaluations are not without drawbacks, as they can be prone to certain\\nissues that may compromise their accuracy and consistency. For instance, 1) personalized tastes\\nand varying levels of expertise among human evaluators can introduce biases and inconsistencies'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 40, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='issues that may compromise their accuracy and consistency. For instance, 1) personalized tastes\\nand varying levels of expertise among human evaluators can introduce biases and inconsistencies\\ninto the evaluation process; 2) conducting comprehensive and reliable human evaluations often\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 41, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:42\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nInstruction\\nCode LLM\\n(Generator)\\n(Code) LLM\\n(Judge)\\nWhich response is better?\\nInstruction: {instruction}\\nResponse 1: {response 1}\\nResponse 2: {response 2}\\nPairwise Comparison\\nRate the response on a scale\\nof 1 to 10.\\nInstruction: {instruction}\\nResponse: {response 1/2}\\nSingle Answer Grading\\nResponse 2 is better\\nThe score is 7\\nResponse 1\\nResponse 2\\nFig. 16. The pipeline of (Code) LLM-as-a-judge for evaluating generated code by Code LLMs. There are\\nprimarily two types of approaches: pairwise comparison and single answer grading.\\nnecessitates a substantial number of evaluators, leading to significant expenses and time-consuming;\\n3) the reproducibility of human evaluations is often limited, which presents challenges in extending\\nprevious evaluation outcomes or monitoring the progress of LLMs, as highlighted by [319].\\n5.10.3\\nLLM-as-a-Judge. The powerful instruction-following capabilities of LLMs have stimulated'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 41, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='previous evaluation outcomes or monitoring the progress of LLMs, as highlighted by [319].\\n5.10.3\\nLLM-as-a-Judge. The powerful instruction-following capabilities of LLMs have stimulated\\nresearchers to innovatively investigate the potential of LLM-based evaluations. The LLM-as-a-Judge\\n[320] refers to the application of advanced proprietary LLMs (e.g., GPT4, Gemini, and Claud 3)\\nas proxies for human evaluators. This involves designing prompts with specific requirements\\nto guide LLMs in conducting evaluations, as demonstrated by AlpacaEval [148] and MT-bench\\n[320]. This method reduces reliance on human participation, thereby facilitating more efficient\\nand scalable evaluations. Moreover, LLMs can offer insightful explanations for the assigned rating\\nscores, thereby augmenting the interpretability of evaluations [319].\\nNevertheless, the use of LLM-based evaluation for code generation remains relatively underex-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 41, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='scores, thereby augmenting the interpretability of evaluations [319].\\nNevertheless, the use of LLM-based evaluation for code generation remains relatively underex-\\nplored compared with general-purpose LLM. The pipeline of (Code) LLM-as-a-judge for evaluating\\ngenerated code by Code LLMs is depicted in Figure 16. A recent work [332] introduces the ICE-Score\\nevaluation metric, which instructs LLM for code assessments. This approach attains superior corre-\\nlations with functional correctness and human preferences, thereby eliminating the requirement\\nfor test oracles or references. As the capabilities of LLM continue to improve, we anticipate seeing\\nmore research in this direction.\\nDespite their scalability and explainability, the effectiveness of LLM-based evaluation is con-\\nstrained by the inherent limitations of the chosen LLM. Several studies have shown that most LLMs,\\nincluding GPT-4, suffer from several issues, including position, verbosity, and self-enhancement'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 41, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='strained by the inherent limitations of the chosen LLM. Several studies have shown that most LLMs,\\nincluding GPT-4, suffer from several issues, including position, verbosity, and self-enhancement\\nbiases, as well as restricted reasoning ability [320]. Specifically, position bias refers to the tendency\\nof LLMs to disproportionately favor responses that are presented in certain positions, which can\\nskew the perceived quality of answers based on their order of presentation. Meanwhile, verbosity\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 42, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:43\\n1.1\\n5.5\\n13 15\\n33\\n70\\n# Parameters (B)\\n0\\n20\\n40\\n60\\n80\\nPass@1 (%)\\nGPT-3.5-Turbo\\nClaude-3-Sonnet\\nClaude-3-Haiku\\nClaude-3-Opus\\nQwen2.5-Coder\\nCodeGemma\\nStarCoder 2\\nWaveCoder\\nCodeFuse\\nCodeShell\\nCodeQwen1.5\\nDeepSeek-Coder\\nphi-1\\nCode Llama\\nCodeGeeX2\\nCodeGeeX\\nPanGu-Coder\\nCodeGen-NL\\nCodeGen-Multi\\nCodeGen-Mono\\nStarCoder\\nCodeT5+\\nSantaCoder\\nInCoder\\nPolyCoder\\nCodeParrot\\nCodestral\\nQwen2.5-Coder-Instruct\\nStarCoder2-Instruct\\nCodeGemma-Instruct\\nCodeQwen1.5-Chat\\nDeepSeek-Coder-Instruct\\nMagicoderS-CL\\nMagicoder-CL\\nWizardCoder\\nCode Llama-Instruct\\nBase\\nInstruct\\nFig. 17. The performance comparison of LLMs for code generation on the MBPP [17] benchmark, measured by\\nPass@1. For models with various sizes, we report only the largest size version of each model with a magnitude\\nof B parameters.\\nbias describes the inclination of LLMs to prefer lengthier responses, even when these are not'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 42, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='of B parameters.\\nbias describes the inclination of LLMs to prefer lengthier responses, even when these are not\\nnecessarily of higher quality compared to more concise ones. Self-enhancement bias, on the other\\nhand, is observed when LLMs consistently overvalue the quality of the text they generate [319, 320].\\nMoreover, due to their inherent limitations in tackling complex reasoning challenges, LLMs may not\\nbe entirely reliable as evaluators for tasks that require intensive reasoning, such as those involving\\nmathematical problem-solving. However, these shortcomings can be partially addressed through\\nthe application of deliberate prompt engineering and fine-tuning techniques, as suggested by [320].\\n5.10.4\\nEmpirical Comparison. In this section, we present a performance comparison of LLMs for\\ncode generation using the well-regarded HumanEval, MBPP, and the more practical and chal-\\nlenging BigCodeBench benchmarks. This empirical comparison aims to highlight the progressive'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 42, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='code generation using the well-regarded HumanEval, MBPP, and the more practical and chal-\\nlenging BigCodeBench benchmarks. This empirical comparison aims to highlight the progressive\\nenhancements in LLM capabilities for code generation. These benchmarks assess an LLM‚Äôs ability to\\ngenerate source code across various levels of difficulty and types of programming tasks. Specifically,\\nHumanEval focuses on complex code generation, MBPP targets basic programming tasks, and\\nBigCodeBench emphasizes practical and challenging programming tasks.\\nDue to the limitations in computational resources we faced, we have cited experimental results\\nfrom original papers or widely recognized open-source leaderboards within the research community,\\nsuch as the HumanEval Leaderboard 16, EvalPlus Leaderboard 17, Big Code Models Leaderboard\\n18, and BigCodeBench Leaderboard 19. We report performance on HumanEval using the pass@1'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 42, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='such as the HumanEval Leaderboard 16, EvalPlus Leaderboard 17, Big Code Models Leaderboard\\n18, and BigCodeBench Leaderboard 19. We report performance on HumanEval using the pass@1\\nmetric, as shown in Table 9, while MBPP and BigCodeBench results are presented with pass@1 in\\nFigures 17 and 18, respectively.\\nWe offer the following insights:\\n16https://paperswithcode.com/sota/code-generation-on-humaneval\\n17https://evalplus.github.io/leaderboard.html\\n18https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard\\n19https://bigcode-bench.github.io/\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 43, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:44\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nCodeGemma-7B\\nStarCoder 2-15B\\nCodeGemma-Instruct-7B\\nCodeQwen1.5-Chat-7B\\nWaveCoder-Ultra-6.7B\\nCode Llama-70B\\nStarCoder2-Instruct-15B\\nCodeQwen1.5-7B\\nDeepSeek-Coder-33B\\nMagicoder-S-DS-7B\\nPhi-3-Medium-128K-Instruct-14B\\nQwen2.5-Coder-Instruct-7B\\nCodeGeeX4-9B\\nCode Llama-Instruct-70B\\nClaude-3-Haiku\\nGPT-3.5-Turbo-0125\\nDeepSeek-Coder-Instruct-33B\\nCodestral-22B\\nClaude-3-Sonnet\\nGemini-1.5-Flash\\nGPT-4-0613\\nClaude-3-Opus\\nGemini-1.5-Pro\\nGPT-4-Turbo-0409\\nClaude-3.5-Sonnet\\nDeepSeek-Coder-V2-Instruct-21B (236B)\\nGPT-4o-0513\\n40\\n45\\n50\\n55\\n60\\nPass@1 (%)\\n38.3\\n38.4\\n39.3\\n43.6\\n43.7\\n44\\n45.1\\n45.6\\n46.6\\n47.6\\n48.7\\n48.8\\n49\\n49.6\\n50.1\\n50.6\\n51.1\\n52.5\\n53.8\\n55.1\\n57.2\\n57.4\\n57.5\\n58.2\\n58.6\\n59.7\\n61.1\\nGPT-3.5-Turbo-0125\\nClosed Source\\nOpen Source\\nFig. 18. The performance comparison of LLMs for code generation on the BigCodeBench [333] benchmark,\\nmeasured by Pass@1. For models with various sizes, we report only the largest size version of each model'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 43, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='measured by Pass@1. For models with various sizes, we report only the largest size version of each model\\nwith a magnitude of B parameters.\\n‚Ä¢ The performance gap between open-source and closed-source models across the three bench-\\nmarks is gradually narrowing. For instance, on the HumanEval benchmark, DeepSeek-Coder-\\nV2-Instruct with 21B activation parameters and Qwen2.5-Coder-Instruct 7B achieve 90% and\\n88.4% pass@1, respectively. These results are comparable to the much larger closed-source\\nLLMs, such as Claude-3.5-Sonnet, which achieves 92.0% pass@1. On the MBPP benchmark,\\nQwen2.5-Coder-Instruct 7B with 83.5% pass@1 significantly outperforms GPT-3.5-Turbo\\nwith 52.2% pass@1 and closely rivals the closed-source Claude-3-Opus with 86.4% pass@1.\\nOn the BigCodeBench, DeepSeek-Coder-V2-Instruct achieves 59.7%, surpassing all compared\\nclosed-source and open-source LLMs except for slightly falling behind GPT-4o-0513, which\\nachieves 61.1%.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 43, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='On the BigCodeBench, DeepSeek-Coder-V2-Instruct achieves 59.7%, surpassing all compared\\nclosed-source and open-source LLMs except for slightly falling behind GPT-4o-0513, which\\nachieves 61.1%.\\n‚Ä¢ Generally, as the number of model parameters increases, the performance of code LLMs\\nimproves. However, Qwen2.5-Coder-Instruct 7B achieves 88.4% pass@1, outperforming larger\\nmodels like StarCoder2-Instruct 15.5B with 72.6% pass@1, DeepSeek-Coder-Instruct 33B\\nwith 79.3% pass@1, and Code Llama-Instruct 70B with 67.8% pass@1 on the HumanEval\\nbenchmark. Similar trends are observed across the other two benchmarks, suggesting that\\ncode LLMs with 7B parameters may be sufficiently capable for code generation task.\\n‚Ä¢ Instruction-tuned models consistently outperform their base (pretrained) counterparts across\\nthe HumanEval and MBPP benchmarks. For instance, Qwen2.5-Coder-Instruct surpasses\\nQwen2.5-Coder by an average of 26.04%, StarCoder2-Instruct improves upon StarCoder 2'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 43, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='the HumanEval and MBPP benchmarks. For instance, Qwen2.5-Coder-Instruct surpasses\\nQwen2.5-Coder by an average of 26.04%, StarCoder2-Instruct improves upon StarCoder 2\\nby an average of 35.20%, and CodeGemma-Instruct enhances CodeGemma by an average\\nof 11.26%. Additionally, DeepSeek-Coder-Instruct outperforms DeepSeek-Coder by an aver-\\nage of 23.71%, while Code Llama-Instruct shows a 13.80% improvement over Code Llama.\\nDetailed results can be found in Table 10. These findings underscore the effectiveness of\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 44, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:45\\nTable 10. The performance improvement of instruction-tuned models over their pretrained counterparts\\non the HumanEval, MBPP, and BigCodeBench benchmarks. The last two rows demonstrate the average\\nimprovement on the first two benchmarks and the three benchmarks, respectively.\\nQwen2.5-Coder-\\nInstruct 7B\\nStarCoder2-\\nInstruct 15.5B\\nCodeGemma-\\nInstruct 7B\\nDeepSeek-Coder-\\nInstruct 33B\\nCode Llama-\\nInstruct 70B\\nHumanEval\\n43.51%\\n56.80%\\n26.07%\\n41.35%\\n27.92%\\nMBPP\\n8.58%\\n13.60%\\n-3.56%\\n6.06%\\n-0.32%\\nBigCodeBench\\n-\\n17.45%\\n2.61%\\n9.66%\\n12.73%\\n# Avg. Imp. H. M.\\n26.04%\\n35.20%\\n11.26%\\n23.71%\\n13.80%\\n# Avg. Imp. H. M. B.\\n-\\n29.28%\\n8.37%\\n19.02%\\n13.44%\\ninstruction tuning, although the quality of the instruction tuning dataset plays a critical role\\nin determining model performance [173, 328].\\n‚Ä¢ Performance on the HumanEval benchmark is nearly saturated. However, MBPP, which'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 44, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='in determining model performance [173, 328].\\n‚Ä¢ Performance on the HumanEval benchmark is nearly saturated. However, MBPP, which\\ninvolves basic programming tasks, and BigCodeBench, which involves more practical and\\nchallenging programming tasks, demand more capable code LLMs. Additionally, while these\\nbenchmarks primarily evaluate the functional correctness of code, they do not provide\\na comprehensive assessment across other critical dimensions. Developing a more holistic\\nevaluation framework that integrates various aspects remains an open area for future research\\nand development in LLMs for code generation evaluation.\\nDiscussion: We discuss certain code LLMs in Table 9 for clarity: (1) General LLMs accessed via\\nAPI are not specifically trained on large code corpora but achieve state-of-the-art performance\\nin code generation, such as Claude-3.5-Sonnet with 92.0% pass@1 on HumanEval benchmark.\\n(2) AlphaCode targets code generation for more complex and unseen problems that require a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 44, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='in code generation, such as Claude-3.5-Sonnet with 92.0% pass@1 on HumanEval benchmark.\\n(2) AlphaCode targets code generation for more complex and unseen problems that require a\\ndeep understanding of algorithms and intricate natural language, such as those encountered in\\ncompetitive programming. The authors of AlphaCode found that large-scale model sampling to\\nnavigate the search space, such as 1M samples per problem for CodeContests, followed by filtering\\nbased on program behavior to produce a smaller set of submissions, is crucial for achieving good and\\nreliable performance on problems that necessitate advanced reasoning. (3) Phi-1 1.3B is a specialized\\nLLM for code, trained on ‚Äútextbook quality‚Äù data from the web (6B tokens) and synthetically\\ngenerated textbooks and exercises using GPT-3.5 (1B tokens). (4) Code Llama 70B is initialized with\\nLlama 2 model weights and continually pre-trained on 1T tokens from a code-heavy dataset and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 44, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='generated textbooks and exercises using GPT-3.5 (1B tokens). (4) Code Llama 70B is initialized with\\nLlama 2 model weights and continually pre-trained on 1T tokens from a code-heavy dataset and\\nlong-context fine-tuned with approximately 20B tokens. However, Code Llama-Instruct 70B is fine-\\ntuned from Code Llama-Python 70B without long-context fine-tuning, using an additional 260M\\ntokens to better follow human instructions. Surprisingly, these models underperform compared to\\nsmaller parameter Code LLMs like Qwen2.5-Coder-Instruct 7B, DeepSeek-Coder-V2-Instruct 21B,\\nand Codestral 22B across all three benchmarks. The underlying reasons for this discrepancy remain\\nunclear and warrant further exploration. (5) Unlike other open-source Code LLMs, DeepSeek-Coder-\\nV2-Instruct is further pre-trained on DeepSeek-V2 [159], which employs a Mixture-of-Experts (MoE)\\narchitecture with only 21B activation parameters out of 236B parameters, using an additional 6'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 44, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='V2-Instruct is further pre-trained on DeepSeek-V2 [159], which employs a Mixture-of-Experts (MoE)\\narchitecture with only 21B activation parameters out of 236B parameters, using an additional 6\\ntrillion tokens composed of 60% source code, 10% math corpus, and 30% natural language corpus.\\nFor a comprehensive understanding of MoE in LLMs, please refer to [36].\\n5.11\\nCode LLMs Alignment\\nThe pre-training of LLMs for next-token prediction, aimed at maximizing conditional generation\\nlikelihood across vast textual corpora, equips these models with extensive world knowledge and\\nemergent capabilities [33]. This training approach enables the generation of coherent and fluent\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 45, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:46\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\ntext in response to diverse instructions. Nonetheless, LLMs can sometimes misinterpret human\\ninstructions, produce biased content, or generate factually incorrect information (commonly referred\\nto as hallucinations), which may limit their practical utility [118, 272, 319].\\nAligning LLMs with human intentions and values, known as LLM alignment, has consequently\\nbecome a critical research focus [118, 272]. Key objectives frequently discussed in the context of LLM\\nalignment include robustness, interpretability, controllability, ethicality, trustworthiness, security,\\nprivacy, fairness, and safety. In recent years, significant efforts have been made by researchers\\nto achieve this alignment, employing techniques such as Reinforcement Learning with Human\\nFeedback (RLHF) [200].\\nHowever, the alignment of Code LLMs has not been extensively explored. Compared to text'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 45, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Feedback (RLHF) [200].\\nHowever, the alignment of Code LLMs has not been extensively explored. Compared to text\\ngeneration, aligning code generation with human intentions and values is even more crucial. For\\ninstance, users without programming expertise might prompt Code LLM to generate source code\\nand subsequently execute it on their computers, potentially causing catastrophic damage. Some\\npotential risks include:\\n‚Ä¢ Malware Infection: The code could contain viruses, worms, or trojans that compromise our\\nsystem‚Äôs security.\\n‚Ä¢ Data Loss: It might delete or corrupt important files and data.\\n‚Ä¢ Unauthorized Access: It can create backdoors, allowing attackers to access our system\\nremotely.\\n‚Ä¢ Performance Issues: The code might consume excessive resources, slowing down our\\nsystem.\\n‚Ä¢ Privacy Breaches: Sensitive information, such as passwords or personal data, might be\\nstolen.\\n‚Ä¢ System Damage: It may alter system settings or damage hardware components.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 45, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='system.\\n‚Ä¢ Privacy Breaches: Sensitive information, such as passwords or personal data, might be\\nstolen.\\n‚Ä¢ System Damage: It may alter system settings or damage hardware components.\\n‚Ä¢ Network Spread: It could propagate across networks, affecting other devices.\\n‚Ä¢ Financial Loss: If the code is ransomware, it might encrypt data and demand payment for\\ndecryption.\\n‚Ä¢ Legal Consequences: Running certain types of malicious code can lead to legal repercus-\\nsions.\\nAs illustrated, aligning Code LLMs to produce source code consistent with human preferences and\\nvalues is of paramount importance in software development. A recent study [293] provides the first\\nsystematic literature review identifying seven critical non-functional properties of LLMs for code,\\nbeyond accuracy, including robustness, security, privacy, explainability, efficiency, and usability.\\nThis study is highly pertinent to the alignment of Code LLMs. We recommend readers refer to this\\nsurvey for more detailed insights.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 45, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='This study is highly pertinent to the alignment of Code LLMs. We recommend readers refer to this\\nsurvey for more detailed insights.\\nIn this survey, we identify five core principles that serve as the key objectives for aligning Code\\nLLMs: Green, Responsibility, Efficiency, Safety, and Trustworthiness (collectively referred to as\\nGREST). These principles are examined from a broader perspective. Each category encompasses\\nvarious concepts and properties, which are summarized in Table 11. In the following, we define\\neach principle and briefly introduce a few notable works to enhance understanding.\\nGreen: The Green principle underscores the importance of environmental sustainability in\\nthe development and deployment of LLMs for code generation. This involves optimizing energy\\nconsumption and reducing both the carbon footprint and financial costs associated with training\\nand inference processes. Currently, training, inference, and deployment of Code LLMs are notably'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 45, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='consumption and reducing both the carbon footprint and financial costs associated with training\\nand inference processes. Currently, training, inference, and deployment of Code LLMs are notably\\nresource-intensive. For example, training GPT-3, with its 175 billion parameters, required the\\nequivalent of 355 years of single-processor computing time and consumed 284,000 kWh of energy,\\nresulting in an estimated 552.1 tons of CO2 emissions [228]. Furthermore, a ChatGPT-like application,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:47\\nTable 11. Five core principles serve as the key objectives for Code LLMs alignment: Green, Responsibility,\\nEfficiency, Safety, and Trustworthiness (collectively referred to as GREST).\\nPrinciples\\nInvolved Concepts and Properties\\nGreen\\nEnergy Efficiency: Minimizing computational energy use and reduce environmental impact and financial costs.\\nSustainable Materials: Leveraging eco-friendly infrastructure and servers for code generation, lowering long-term expenses.\\nCarbon Footprint: Reducing emissions associated with model training and inference to enhance efficiency and save costs.\\nResource Optimization: Efficiently utilizing computational resources to minimize waste and reduce expenses in code generation.\\nRecycling Management: Responsibly dispose of hardware used in model development to reduce waste management costs.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Recycling Management: Responsibly dispose of hardware used in model development to reduce waste management costs.\\nRenewable Energy: Utilizing renewable energy sources for powering training and inference processes to decrease energy costs.\\nLifecycle Assessment: Evaluating the environmental and financial impacts of models from creation to deployment and disposal.\\nResponsibility\\nEthical Considerations: Adhering to ethical guidelines to ensure responsible use and deployment of generated code.\\nAccountability: Establishing clear lines of responsibility for code generation outcomes and potential impacts.\\nUser Education: Providing resources and guidance to help users understand and responsibly use generated code.\\nImpact Assessment: Evaluating the social and technical implications of code generation to minimize negative effects.\\nRegulatory Compliance: Ensuring that generated code adheres to relevant laws (e.g., copyright) and industry regulations.\\nEfficiency'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Regulatory Compliance: Ensuring that generated code adheres to relevant laws (e.g., copyright) and industry regulations.\\nEfficiency\\nModel Optimization: Streamlining models to reduce computational load and improve speed.\\nPrompt Engineering: Designing effective prompts to generate accurate code efficiently.\\nResource Management: Allocating computational resources wisely to balance speed and cost.\\nInference Optimization: Enhancing the inference process to quickly generate code with minimal latency.\\nParallel Processing: Utilizing parallelism to speed up code generation tasks.\\nCaching Mechanisms: Implementing caching to reuse previous results and reduce redundant computations.\\nEvaluation Metrics: Using precise metrics to assess and improve the efficiency of code outputs.\\nSafety\\nInput Validation: Ensuring inputs (prompts) are safe and sanitized to prevent malicious exploitation.\\nSecurity Audits: Regularly reviewing generated code for vulnerabilities and potential exploits.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Input Validation: Ensuring inputs (prompts) are safe and sanitized to prevent malicious exploitation.\\nSecurity Audits: Regularly reviewing generated code for vulnerabilities and potential exploits.\\nMonitoring and Logging: Keeping track of generation outputs to quickly identify and address safety issues.\\nUser Access Control: Limiting access to generation capabilities to trusted users to minimize risk.\\nContinuous Updates: Regularly updating models with the latest safety protocols and security patches.\\nEthical Guidelines: Implementing ethical standards to guide safe and responsible code generation.\\nTrustworthiness\\nReliability: Ensuring that generated code consistently meets functional requirements and performs as expected.\\nTransparency: Providing clear explanations of how code is generated to build user confidence.\\nVerification and Testing: Using rigorous testing frameworks to ensure the generated code accuracy and reliability.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Verification and Testing: Using rigorous testing frameworks to ensure the generated code accuracy and reliability.\\nBias Mitigation: Actively working to identify and reduce biases in code generation to ensure fairness and impartiality.\\nUser Feedback Integration: Continuously incorporating user feedback to refine and improve code generation processes.\\nDocumentation: Providing comprehensive documentation for generated code to enhance understanding and trust.\\nwith an estimated usage of 11 million requests per hour, can produce emissions of 12.8k metric\\ntons of CO2 per year, which is 25 times the carbon emissions associated with training GPT-3\\n[53]. To mitigate these costs, several techniques are often employed, such as the development of\\nspecialized hardware (e.g., Tensor Processing Units (TPUs) and Neural Processing Units (NPUs)),\\nmodel compression methods (e.g., quantization and knowledge distillation), parameter-efficient'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='specialized hardware (e.g., Tensor Processing Units (TPUs) and Neural Processing Units (NPUs)),\\nmodel compression methods (e.g., quantization and knowledge distillation), parameter-efficient\\nfine-tuning (PEFT), and the use of renewable energy sources. For instance, Shi et al. [235] applied\\nknowledge distillation to reduce the size of CodeBERT [76] and GraphCodeBERT [86], resulting in\\noptimized models of just 3MB. These models are 160 times smaller than the original large models\\nand significantly reduce energy consumption by up to 184 times and carbon footprint by up to 157\\ntimes. Similarly, Wei et al. [277] utilized quantization techniques for Code LLMs such as CodeGen\\n[193] and Incoder [77] by employing lower-bit integers (e.g., int8). This approach reduced storage\\nrequirements by 67.3% to 70.8%, carbon footprint by 28.8% to 55.0%, and pricing costs by 28.9% to\\n55.0%.\\nResponsibility: The Responsibility principle in the context of Code LLMs underscores the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 46, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='requirements by 67.3% to 70.8%, carbon footprint by 28.8% to 55.0%, and pricing costs by 28.9% to\\n55.0%.\\nResponsibility: The Responsibility principle in the context of Code LLMs underscores the\\nimportance of ethical considerations, fairness, and accountability throughout their lifecycle. This\\ninvolves addressing biases in training data, ensuring fairness and transparency in model decision-\\nmaking, maintaining accountability for outputs, adhering to applicable laws (e.g., copyright),\\nimplementing safeguards against misuse, and providing clear communication about the model‚Äôs\\ncapabilities and limitations. Specifically,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 47, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:48\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n‚Ä¢ Bias Mitigation. Biases in code generation can lead to flawed software and reinforce stereo-\\ntypes, potentially causing significant societal impacts. For example, an Code LLM that in-\\nherits biases from its training data may produce source code/software that inadvertently\\ndiscriminates against certain user groups. This can result in applications that fail to meet the\\ndiverse needs of users, promoting exclusionary practices and reinforcing existing stereotypes\\n[168, 185].\\n‚Ä¢ Fairness and Transparency. A lack of fairness and transparency in Code LLM decision-making\\ncan result in biased or suboptimal code solutions. If the model‚Äôs decision-making process is\\nopaque, developers might unknowingly introduce code that favors specific frameworks or\\nlibraries, thereby limiting innovation and diversity in software development. This opacity'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 47, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='opaque, developers might unknowingly introduce code that favors specific frameworks or\\nlibraries, thereby limiting innovation and diversity in software development. This opacity\\ncan create unfair advantages and hinder collaborative efforts within tech communities [31].\\n‚Ä¢ Legal Compliance. Compliance with relevant laws, such as licensing and copyright, is crucial\\nwhen using Code LLMs for code generation to avoid legal complications. If an Code LLM\\ngenerates code snippets that inadvertently infringe on existing copyrights, it can lead to\\nlegal disputes and financial liabilities for developers and organizations [292]. Such risks may\\ndiscourage the use of advanced AI tools, thus stifling innovation and affecting growth and\\ncollaboration within the tech community.\\n‚Ä¢ Accountability. Without accountability for code generated by Code LLMs, addressing bugs\\nor security vulnerabilities becomes challenging. If a model generates faulty code leading to'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 47, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Accountability. Without accountability for code generated by Code LLMs, addressing bugs\\nor security vulnerabilities becomes challenging. If a model generates faulty code leading to\\na security breach, the absence of clear accountability can result in significant financial and\\nreputational damage for companies. This uncertainty can delay critical issue resolution and\\nimpede trust in AI-assisted development [155].\\n‚Ä¢ Misuse Prevention. Failing to implement mechanisms to prevent the misuse of Code LLMs can\\nenable the creation of harmful software. For example, models could be exploited to generate\\nmalware or unauthorized scripts, posing cybersecurity risks. Without proper safeguards,\\nthese models can facilitate malicious activities, threatening both individual and organizational\\nsecurity [184].\\n‚Ä¢ Clear Communication. Without clear communication about a model‚Äôs capabilities and limita-\\ntions, developers may misuse the model or overestimate its abilities. Relying on the model'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 47, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='security [184].\\n‚Ä¢ Clear Communication. Without clear communication about a model‚Äôs capabilities and limita-\\ntions, developers may misuse the model or overestimate its abilities. Relying on the model\\nto generate complex, mission-critical code without human oversight can lead to significant\\nsoftware failures. Misunderstanding its limitations can result in faulty implementations and\\nlost productivity [226].\\nTo adhere to this principle, potential mitigation methods include bias detection and mitigation,\\nquantification and evaluation, and adherence to ethical guidelines. Liu et al. [168] propose a new\\nparadigm for constructing code prompts, successfully uncovering social biases in code generation\\nmodels, and developing a dataset along with three metrics to evaluate overall social bias. Recently,\\nXu et al. [292] introduced LiCoEval, an evaluation benchmark for assessing the license compliance\\ncapabilities of LLMs. Additionally, incorporating diverse perspectives in development teams and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 47, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Xu et al. [292] introduced LiCoEval, an evaluation benchmark for assessing the license compliance\\ncapabilities of LLMs. Additionally, incorporating diverse perspectives in development teams and\\nengaging with stakeholders from various communities can further align Code LLM outputs with\\nethical standards and societal values.\\nEfficiency: The Efficiency principle emphasizes optimizing the performance and speed of Code\\nLLMs for code generation while minimizing the computational resources required for training\\nand inference. For instance, training the GPT-3 model, which consists of 175 billion parameters,\\ndemands substantial resources. It requires approximately 1,024 NVIDIA V100 GPUs, costing around\\n4.6 million and taking approximately 34 days to complete the training process. To address these\\nchallenges, various techniques are employed, including model compression methods such as\\npruning, quantization, and knowledge distillation. Additionally, optimized algorithms like AdamW,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 47, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='challenges, various techniques are employed, including model compression methods such as\\npruning, quantization, and knowledge distillation. Additionally, optimized algorithms like AdamW,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 48, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:49\\nparallel strategies such as tensor, pipeline, and data parallelism, and parameter-efficient fine-tuning\\n(PEFT) (see Section 5.4.2) are often utilized. For a comprehensive and detailed discussion on methods\\nto enhance the efficiency of Code LLMs for code generation, please refer to Section 4.5.2, ‚ÄúEfficiency\\nEnhancement‚Äù, in [293].\\nSafety: The Safety principle of Code LLMs is of utmost importance due to their potential\\nto introduce vulnerabilities, errors, or privacy breaches into software systems. Ensuring safety\\ninvolves comprehensive testing and validation processes to detect and mitigate these risks. For\\ninstance, attackers might compromise the training process of LLMs by injecting malicious examples\\ninto the training data, a method known as data poisoning attacks [231]. Even when attackers\\nlack access to the training process, they may employ techniques like the black-box inversion'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 48, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='into the training data, a method known as data poisoning attacks [231]. Even when attackers\\nlack access to the training process, they may employ techniques like the black-box inversion\\napproach introduced by Hajipour et al. [91]. This method uses few-shot prompting to identify\\nprompts that coax black-box code generation models into producing vulnerable code. Furthermore,\\nYang et al. [294] and Al-Kaswan et al. [8] reveals that Code LLMs, such as CodeParrot [73], can\\nmemorize training data, potentially outputting personally identifiable information like emails,\\nnames, and IP addresses, thereby posing significant privacy risks. Additionally, Yuan et al. [302]\\ndemonstrate that engaging with ChatGPT and GPT-4 in non-natural languages can circumvent\\nsafety alignment measures, leading to unsafe outcomes, such as ‚ÄúThe steps involved in stealing\\nmoney from a bank.‚Äù. To bolster the safety of LLMs in code generation, it is crucial to detect and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 48, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='safety alignment measures, leading to unsafe outcomes, such as ‚ÄúThe steps involved in stealing\\nmoney from a bank.‚Äù. To bolster the safety of LLMs in code generation, it is crucial to detect and\\neliminate privacy-related information from training datasets. For example, approaches outlined in\\n[77] and [9] utilize carefully crafted regular expressions to identify and remove private information\\nfrom training data. To counteract black-box inversion, implementing prompt filtering mechanisms\\nis recommended to identify and block prompts that might result in insecure code generation.\\nMoreover, adversarial training can enhance the model‚Äôs resilience to malicious prompts. Employing\\nreinforcement learning methods can further align Code LLMs with human preferences, thereby\\nreducing the likelihood of producing harmful outputs.\\nTrustworthiness: The Trustworthiness principle focuses on developing Code LLMs that users'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 48, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='reducing the likelihood of producing harmful outputs.\\nTrustworthiness: The Trustworthiness principle focuses on developing Code LLMs that users\\ncan depend on for accurate and reliable code generation, which is crucial for their acceptance and\\nwidespread adoption. Achieving this requires ensuring model transparency, providing explanations\\nfor decisions, and maintaining consistent performance across various scenarios. For instance,\\nJi et al. [120] propose a causal graph-based representation of prompts and generated code to\\nidentify the causal relationships between them. This approach offers insights into the effectiveness\\nof Code LLMs and assists end-users in understanding the generation. Similarly, Palacio et al.\\n[202] introduce ASTxplainer, a tool that extracts and aggregates normalized model logits within\\nAbstract Syntax Tree (AST) structures. This alignment of token predictions with AST nodes'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 48, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[202] introduce ASTxplainer, a tool that extracts and aggregates normalized model logits within\\nAbstract Syntax Tree (AST) structures. This alignment of token predictions with AST nodes\\nprovides visualizations that enhance end-user understanding of Code LLM predictions. Therefore,\\nby prioritizing trustworthiness, we can bolster user confidence and facilitate the integration of\\nCode LLMs into diverse coding environments. By adhering to the aforementioned principles as key\\nobjectives for aligning Code LLMs, researchers and developers can create LLMs for code generation\\nthat are not only capable but also ethical, sustainable, and user-centric.\\n5.12\\nApplications\\nCode LLMs have been integrated with development tools and platforms, such as integrated de-\\nvelopment environments (IDEs) and version control systems, improving programming efficiency\\nsubstantially. In this section, we will briefly introduce several widely used applications as coding'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 48, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='velopment environments (IDEs) and version control systems, improving programming efficiency\\nsubstantially. In this section, we will briefly introduce several widely used applications as coding\\nassistants. The statistics of these applications are provided in Table 12.\\nGitHub Copilot. GitHub Copilot, powered by OpenAI‚Äôs Codex, is an AI pair programmer that\\nhelps you write better code faster. Copilot suggests whole lines or blocks of code as you type, based\\non the context provided by your existing code and comments. It‚Äôs trained on a dataset that includes\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 49, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:50\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 12. The overview of code assistant applications powered by LLMs. The column labeled ‚ÄòPLs‚Äô and ‚ÄòIDEs‚Äô\\nindicate programming languages and integrated development environments, respectively [307].\\nInstitution\\nProducts\\nModel\\nSupported Features\\nSupported PLs\\nSupported IDEs\\nGitHub & OpenAI\\nGitHub Copilot [48]\\nCodex\\nCode Completions, Code Generation,\\nCoding Questions Answering,\\nCode Refactoring, Code Issues Fix,\\nUnit Test Cases Generation,\\nCode Documentation Generation\\nJava, Python, JavaScript, TypeScript,\\nPerl, R, PowerShell, Rust, SQL, CSS,\\nRuby, Julia, C#, PHP, Swift, C++,Go,\\nHTML, JSON, SCSS, .NET, Less,\\nT-SQL, Markdown\\nVisual Studio, VS Code, Neovim,\\nJetBrains IDE\\nZhipu AI\\nCodeGeeX [321]\\nCodeGeeX\\nCode Generation, Code Translation,\\nCode Completion, Code Interpretation,\\nCode Bugs Fix, Comment Generation,\\nAI Chatbot\\nPHP, Go, C, C#, C++, Rust, Perl, CSS,\\nJava, Python, JavaScript, TypeScript,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 49, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Code Generation, Code Translation,\\nCode Completion, Code Interpretation,\\nCode Bugs Fix, Comment Generation,\\nAI Chatbot\\nPHP, Go, C, C#, C++, Rust, Perl, CSS,\\nJava, Python, JavaScript, TypeScript,\\nObjective C++, Objective C, Pascal,\\nHTML, SQL, Kotlin, R, Shell, Cuda,\\nFortran, Tex, Lean, Scala\\nClion, RubyMine, AppCode, Aqua,\\nIntelliJ IDEA, VS Code, PyCharm,\\nAndroid Studio, WebStorm, Rider,\\nGoLand, DataGrip, DataSpell\\nAmazon\\nCodeWhisperer [12]\\n‚àí\\nCode Completion, Code Explanation,\\nCode Translation,\\nCode Security Identification,\\nCode Suggestion\\nJava, Python, TypeScript, JavaScript,\\nC#\\nJetBrains IDE, VS Code, AWS Cloud9,\\nAWS Lambda\\nCodeium\\nCodeium [60]\\n‚àí\\nCode Completion, Bug Detection,\\nCode Suggestions, AI Chatbot,\\nTest Type Generation,\\nTest Plan Creation,\\nCodebase Search\\nMore than 70 languages in total,\\nincluding but not limited to:\\nC, C#, C++, Dart, CSS, Go, Elixir,\\nHTML, Haskell, Julia, Java, JavaScript,\\nLisp, Kotlin, Lua, Objective-C,\\nPerl, Pascal, PHP, Protobuf,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 49, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='More than 70 languages in total,\\nincluding but not limited to:\\nC, C#, C++, Dart, CSS, Go, Elixir,\\nHTML, Haskell, Julia, Java, JavaScript,\\nLisp, Kotlin, Lua, Objective-C,\\nPerl, Pascal, PHP, Protobuf,\\nR, Python, Ruby, Scala, Rust,\\nSwift, SQL, TS, Vue\\nJetBrains, VSCode, Visual Studio,\\nColab, Jupyter, Deepnote,\\nNotebooks, Databricks, Chrome,\\nVim, Neovim, Eclipse, Emacs,\\nVSCode Web IDEs, Sublime Text\\nHuawei\\nCodeArts Snap [234]\\nPanGu-Coder\\nCode Generation, Code Explanation\\nResearch and Development Knowledge\\nQuestion and Answer\\nCode Comment, Code Debug\\nUnit Test Case Generation\\nJava, Python\\nPyCharm, VS Code, IntelliJ\\nTabnine\\nTabNine [246]\\n‚àí\\nCode Generation, Code Completion,\\nCode Explanation, Bug Fix,\\nCode Recommendation, Code Refactoring,\\nCode Test Generation,\\nDocstring Generation\\nPython, Javascript, Java, TypeScript,\\nHTML, Haskell, Matlab, Kotlin, Sass,\\nGo, PHP, Ruby, C, C#, C++, Swift,\\nRust, CSS, Perl, Angular, Dart, React,\\nObjective C, NodeJS, Scala,\\nSublime, PyCharm, Neovim, Rider,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 49, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='HTML, Haskell, Matlab, Kotlin, Sass,\\nGo, PHP, Ruby, C, C#, C++, Swift,\\nRust, CSS, Perl, Angular, Dart, React,\\nObjective C, NodeJS, Scala,\\nSublime, PyCharm, Neovim, Rider,\\nVS Code, IntelliJ IDE, Visual Studio,\\nPhpStorm, Vim, RubyMine, DataGrip,\\nAndroid Studio, WebStorm, Emacs,\\nClion, Jupyter Notebook, JupyterLab,\\nEclipse, GoLand, AppCode\\nReplit\\nReplit[222]\\nreplit-code\\nCode Completion, Code Editing,\\nCode Generation, Code Explanation,\\nCode Suggestion, Code Test Generation\\nC#, Bash, C, CSS, C++, Java, Go,\\nHTML, JavaScript, Perl, PHP,\\nRuby, Python, R, SQL, Rust\\n‚àí\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 50, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:51\\nFig. 19. An exemplar of GitHub Copilot to demonstrate how to use development tools powered by LLMs,\\nincluding powerful GPT 4o, o1-preview (Preview), and o1-mini (Preview). To illustrate its capabilities, we input\\nthe description of the ‚Äú5. Longest Palindromic Substring‚Äù problem from LeetCode into Copilot‚Äôs chat box. The\\ncode generated by Copilot is then submitted to the online judge platform, where it is successfully accepted.\\na significant portion of the public code available on GitHub, which enables it to understand a wide\\nrange of programming languages and coding styles. Copilot not only improves productivity but\\nalso serves as a learning tool by providing programmers with examples of how certain functions\\ncan be implemented or how specific problems can be solved.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 51, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:52\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nCodeGeeX. CodeGeeX stands out as a multifaceted programming assistant, proficient in code\\ncompletion, comment generation, code translation, and developer interactions. Its underlying code\\ngeneration LLM has been refined with extensive training on vast amounts of code data, exhibiting\\nsuperior performance on benchmarks like HumanEval, HumanEval-X, and DS1000. Renowned for\\nsupporting multilingual code generation, CodeGeeX plays a pivotal role in enhancing the efficiency\\nof code development.\\nCodeWhisperer. Amazon‚Äôs CodeWhisperer is a versatile, machine learning-driven code genera-\\ntor that offers on-the-fly code recommendations. Tailored to your coding patterns and comments,\\nCodeWhisperer provides personalized suggestions that range from succinct comments to complex\\nfunctions, all aimed at streamlining your coding workflow.\\nCodeium. Codeium is an AI-accelerated coding toolkit that offers a suite of functions, including'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 51, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='functions, all aimed at streamlining your coding workflow.\\nCodeium. Codeium is an AI-accelerated coding toolkit that offers a suite of functions, including\\ncode completion, explanation, translation, search, and user chatting. Compatible with over 70\\nprogramming languages, Codeium delivers fast and cutting-edge solutions to coding challenges,\\nsimplifying the development process for its users.\\nCodeArts Snap. Huawei‚Äôs CodeArts Snap is capable of generating comprehensive function-level\\ncode from both Chinese and English descriptions. This tool not only reduces the monotony of\\nmanual coding but also efficiently generates test code, in addition to providing automatic code\\nanalysis and repair services.\\nTabnine. Tabnine is an AI coding assistant that empowers development teams to leverage\\nAI for streamlining the software development lifecycle while maintaining strict standards for\\nprivacy, security, and compliance. With a focus on enhancing coding efficiency, code quality, and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 51, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='AI for streamlining the software development lifecycle while maintaining strict standards for\\nprivacy, security, and compliance. With a focus on enhancing coding efficiency, code quality, and\\ndeveloper satisfaction, Tabnine offers AI-driven automation that is tailored to the needs of your\\nteam. Supporting over one million developers worldwide, Tabnine is applicable across various\\nindustries.\\nReplit. Replit is a multifunctional platform that caters to a diverse array of software development\\nneeds. As a complimentary online IDE, it facilitates code collaboration, and cloud services, and\\nfosters a thriving developer community. Replit also enables users to compile and execute code in\\nmore than 50 programming languages directly within a web browser, eliminating the need for local\\nsoftware installations.\\nTo illustrate the use of development tools powered by LLMs, we employ GitHub Copilot within\\nVisual Studio Code (VS Code) as our example. Note that'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 51, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='software installations.\\nTo illustrate the use of development tools powered by LLMs, we employ GitHub Copilot within\\nVisual Studio Code (VS Code) as our example. Note that\\n1‚óãFor details on using the GitHub Copilot extension in VS Code, please refer to the useful\\ndocument at https://code.visualstudio.com/docs/copilot/overview.\\n2‚óãIf you would like to get free access to Copilot as a student, teacher, or open-source maintainer,\\nplease refer to this tutorial at https://docs.github.com/en/copilot/managing-copilot/managing-\\ncopilot-as-an-individual-subscriber/managing-your-copilot-subscription/getting-free-access-\\nto-copilot-as-a-student-teacher-or-maintainer and GitHub education application portal at\\nhttps://education.github.com/discount_requests/application.\\nAs depicted in the upper section of Figure 19, users can interact with Copilot through the chat box in\\nthe lower left corner, where they can inquire about various coding-related tasks. This feature is now'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 51, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='the lower left corner, where they can inquire about various coding-related tasks. This feature is now\\nsupported by the advanced capabilities of GPT-4o, o1-preview (Preview), and o1-mini (Preview).\\nFrom the generated content, Copilot demonstrates the ability to plan solutions to coding problems.\\nIt can write code and subsequently explain the generated code to enhance user comprehension.\\nWithin the right-side workspace, users can engage in inline chat conversations to generate or\\nrefactor source code, conduct code explanations, fix coding errors, resolve issues encountered\\nduring terminal command executions, produce documentation comments, and generate unit tests.\\nTo illustrate its capabilities, we input the description of the ‚Äú5. Longest Palindromic Substring‚Äù\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 52, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:53\\nproblem from LeetCode into Copilot‚Äôs chat box. The code generated by Copilot is then submitted\\nto the online judge platform, where it is successfully accepted, as shown at the lower section of\\nFigure 19.\\n6\\nCHALLENGES & OPPORTUNITIES\\nAccording to our investigations, the LLMs have revolutionized the paradigm of code generation\\nand achieved remarkable performance. Despite this promising progress, there are still numerous\\nchallenges that need to be addressed. These challenges are mainly caused by the gap between\\nacademia and practical development. For example, in academia, the HumanEval benchmark has\\nbeen established as a de facto standard for evaluating the coding proficiency of LLMs. However,\\nmany works have illustrated the evaluation of HumanEval can‚Äôt reflect the scenario of practical\\ndevelopment [68, 72, 123, 162]. In contrast, these serious challenges offer substantial opportunities'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 52, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='many works have illustrated the evaluation of HumanEval can‚Äôt reflect the scenario of practical\\ndevelopment [68, 72, 123, 162]. In contrast, these serious challenges offer substantial opportunities\\nfor further research and applications. In this section, we pinpoint critical challenges and identify\\npromising opportunities, aiming to bridge the research-practicality divide.\\nEnhancing complex code generation at repository and software scale. In practical de-\\nvelopment scenarios, it often involves a large number of complex programming problems of\\nvarying difficulty levels [151, 311]. While LLMs have shown proficiency in generating function-\\nlevel code snippets, these models often struggle with more complex, unseen programming problems,\\nrepository- and software-level problems that are commonplace in real-world software develop-\\nment. To this end, it requires strong problem-solving skills in LLM beyond simply functional-level'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 52, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='repository- and software-level problems that are commonplace in real-world software develop-\\nment. To this end, it requires strong problem-solving skills in LLM beyond simply functional-level\\ncode generation. For example, AlphaCode [151] achieved an average ranking in the top 54.3% in\\nprogramming competitions where an understanding of algorithms and complex natural language is\\nrequired to solve competitive programming problems. [123] argues that existing LLMs can‚Äôt resolve\\nreal-world GitHub issues well since the best-performing model, Claude 2, is able to solve a mere\\n1.96% of the issues. The reason for poor performance is mainly attributed to the weak reasoning\\ncapabilities [105], complex internal- and external- dependencies [22], and context length limitation\\nof LLMs [22]. Therefore, the pursuit of models that can handle more complex, repository- and\\nsoftware-level code generation opens up new avenues for automation in software development'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 52, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='of LLMs [22]. Therefore, the pursuit of models that can handle more complex, repository- and\\nsoftware-level code generation opens up new avenues for automation in software development\\nand makes programming more productive and accessible.\\nInnovating model architectures tuned to code structures. Due to their scalability and effec-\\ntiveness, Transformer-based LLM architectures have become dominant in solving code generation\\ntask. Nevertheless, they might not be optimally designed to capture the inherent structure and\\nsyntax of programming languages (PLs) [85, 86, 134, 175]. Code has a highly structured nature,\\nwith a syntax that is more rigid than natural language. This presents a unique challenge for LLMs,\\nwhich are often derived from models that were originally designed for natural language processing\\n(NLP). The development of novel model architectures that inherently understand and integrate the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 52, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='which are often derived from models that were originally designed for natural language processing\\n(NLP). The development of novel model architectures that inherently understand and integrate the\\nstructural properties of code represents a significant opportunity to improve code generation and\\ncomprehension. Innovations such as tree-based neural networks [183], which mirror the abstract\\nsyntax tree (AST) representation of code, can offer a more natural way for models to learn and\\ngenerate programming languages. Additionally, leveraging techniques from the compiler theory,\\nsuch as intermediate representations (IR) [152], could enable models to operate on a more abstract\\nand generalizable level, making them effective across multiple programming languages [207]. By\\nexploring architectures beyond the traditional sequential models, researchers can unlock new\\npotentials in code generation.\\nCurating high-quality code data for pre-training and fine-tuning of LLMs. The efficacy'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 52, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='potentials in code generation.\\nCurating high-quality code data for pre-training and fine-tuning of LLMs. The efficacy\\nof LLMs largely depends on the quality and diversity of code datasets used during pre-training and\\nfine-tuning phases [133, 280, 328]. Currently, there is a scarcity of large, high-quality datasets that\\nencompass a wide range of programming tasks, styles, and languages. This limitation constrains the\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 53, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:54\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nability of LLMs to generalize across unseen programming tasks, different coding environments, and\\nreal-world software development scenarios. The development of more sophisticated data acquisition\\ntechniques, such as automated code repositories mining [158], advanced filtering algorithms, and\\ncode data synthesis [165] (see Section 5.2), can lead to the creation of richer datasets. Collaborations\\nwith industry partners (e.g., GitHub) could also facilitate access to proprietary codebases, thereby\\nenhancing the practical relevance of the training material. Furthermore, the adoption of open-source\\nmodels for dataset sharing can accelerate the collective effort to improve the breadth and depth of\\ncode data available for LLM research.\\nDeveloping comprehensive benchmarks and metrics for coding proficiency evaluation\\nin LLMs. Current benchmarks like HumanEval may not capture the full spectrum of coding'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 53, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Developing comprehensive benchmarks and metrics for coding proficiency evaluation\\nin LLMs. Current benchmarks like HumanEval may not capture the full spectrum of coding\\nskills required for practical software development [191]. Additionally, metrics often focus on\\nsyntactic correctness or functional accuracy, neglecting aspects such as code efficiency [208],\\nstyle [44], readability [34], or maintainability [15]. The design of comprehensive benchmarks that\\nsimulate real-world software development challenges could provide a more accurate assessment\\nof LLMs‚Äô coding capabilities. These benchmarks should include diverse programming tasks of\\nvarying difficulty levels, such as debugging [325], refactoring [237], and optimization [112], and\\nshould be complemented by metrics that evaluate qualitative aspects of code. The establishment of\\ncommunity-driven benchmarking platforms could facilitate continuous evaluation and comparison\\nof LLMs for code generation across the industry and academia.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 53, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='community-driven benchmarking platforms could facilitate continuous evaluation and comparison\\nof LLMs for code generation across the industry and academia.\\nSupport for low-resource, low-level, and domain-specific programming languages. LLMs\\nare predominantly trained in popular high-level programming languages, leaving low-resource, low-\\nlevel, and domain-specific languages underrepresented. This lack of focus restricts the applicability\\nof LLMs in certain specialized fields and systems programming [250]. Intensifying research on\\ntransfer learning and meta-learning approaches may enable LLMs to leverage knowledge from\\nhigh-resource languages to enhance their performance on less common ones [38, 46]. Additionally,\\npartnerships with domain experts can guide the creation of targeted datasets and fine-tuning\\nstrategies to better serve niche markets. The development of LLMs with a capacity for multilingual'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 53, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='partnerships with domain experts can guide the creation of targeted datasets and fine-tuning\\nstrategies to better serve niche markets. The development of LLMs with a capacity for multilingual\\ncode generation also presents a significant opportunity for broadening the scope of applications.\\nContinuous learning for LLMs to keep pace with evolving coding knowledge. The\\nsoftware development landscape is continuously evolving, with new languages, frameworks, and\\nbest practices emerging regularly. LLMs risk becoming outdated if they cannot adapt to these\\nchanges and incorporate the latest programming knowledge [115, 264]. While retrieval augmented\\ncode generation mitigates these issues, the performance is limited by the quality of the retrieval\\ncontext While retrieval-augmented code generation offers a partial solution to these issues, its\\neffectiveness is inherently constrained by the quality of retrieved context. [171, 309, 330]. Therefore,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 53, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='effectiveness is inherently constrained by the quality of retrieved context. [171, 309, 330]. Therefore,\\nestablishing mechanisms for continuous learning and updating of LLMs can help maintain their\\nrelevance over time. This could involve real-time monitoring of code repositories to identify trends\\nand innovations, as well as the creation of incremental learning systems that can assimilate new\\ninformation without forgetting previously acquired knowledge. Engaging the LLMs in active\\nlearning scenarios where they interact with human developers may also foster ongoing knowledge\\nacquisition.\\nEnsuring code safety and aligning LLM outputs with human coding preferences. Ensuring\\nthe safety and security of code generated by LLMs is a paramount concern, as is their ability to\\nalign with human preferences and ethical standards. Current models may inadvertently introduce\\nvulnerabilities or generate code that does not adhere to desired norms [48, 293]. Research into'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 53, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='align with human preferences and ethical standards. Current models may inadvertently introduce\\nvulnerabilities or generate code that does not adhere to desired norms [48, 293]. Research into\\nthe integration of formal verification tools within the LLM pipeline can enhance the safety of the\\nproduced code. Additionally, developing frameworks for alignment learning that capture and reflect\\nhuman ethical preferences can ensure that the code generation process aligns with societal values\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 54, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:55\\n[200, 212]. Transparent and explainable AI methodologies can also contribute to building trust in\\nthe LLM-generated code by making the decision-making process more accessible to developers.\\n7\\nCONCLUSION\\nIn this survey, we provide a systematic literature review, serving as a valuable reference for\\nresearchers investigating the cutting-edge progress in LLMs for code generation. A thorough\\nintroduction and analysis for data curation, the latest advances, performance evaluation, ethical\\nimplications, environmental impact, and real-world applications are illustrated. In addition, we\\npresent a historical overview of the evolution of LLMs for code generation in recent years and\\noffer an empirical comparison using the widely recognized HumanEval, MBPP, and the more\\npractical and challenging BigCodeBench benchmarks to highlight the progressive enhancements'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 54, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='offer an empirical comparison using the widely recognized HumanEval, MBPP, and the more\\npractical and challenging BigCodeBench benchmarks to highlight the progressive enhancements\\nin LLM capabilities for code generation. Critical challenges and promising opportunities regarding\\nthe gap between academia and practical development are also identified for future investigation.\\nFurthermore, we have established a dedicated resource website to continuously document and\\ndisseminate the most recent advances in the field. We hope this survey can contribute to a compre-\\nhensive and systematic overview of LLM for code generation and promote its thriving evolution.\\nWe optimistically believe that LLM will ultimately change all aspects of coding and automatically\\nwrite safe, helpful, accurate, trustworthy, and controllable code, like professional programmers,\\nand even solve coding problems that currently cannot be solved by humans.\\nREFERENCES'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 54, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='write safe, helpful, accurate, trustworthy, and controllable code, like professional programmers,\\nand even solve coding problems that currently cannot be solved by humans.\\nREFERENCES\\n[1] 2023. AgentGPT: Assemble, configure, and deploy autonomous AI Agents in your browser. https://github.com/\\nreworkd/AgentGPT.\\n[2] 2023. AutoGPT is the vision of accessible AI for everyone, to use and to build on. https://github.com/Significant-\\nGravitas/AutoGPT.\\n[3] 2023. BabyAGI. https://github.com/yoheinakajima/babyagi.\\n[4] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach,\\nAmit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: A highly capable language model\\nlocally on your phone. arXiv preprint arXiv:2404.14219 (2024).\\n[5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 54, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='locally on your phone. arXiv preprint arXiv:2404.14219 (2024).\\n[5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\\n(2023).\\n[6] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A Transformer-based Approach for\\nSource Code Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\\n4998‚Äì5007.\\n[7] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified pre-training for program\\nunderstanding and generation. arXiv preprint arXiv:2103.06333 (2021).\\n[8] Ali Al-Kaswan, Maliheh Izadi, and Arie Van Deursen. 2024. Traces of memorisation in large language models for\\ncode. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. 1‚Äì12.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 54, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='code. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. 1‚Äì12.\\n[9] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas\\nMuennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. SantaCoder: don‚Äôt reach for the stars! arXiv preprint\\narXiv:2301.03988 (2023).\\n[10] Miltiadis Allamanis and Charles Sutton. 2014. Mining idioms from source code. In Proceedings of the 22nd acm sigsoft\\ninternational symposium on foundations of software engineering. 472‚Äì483.\\n[11] Google DeepMind AlphaCode Team. 2023. AlphaCode 2 Technical Report. https://storage.googleapis.com/deepmind-\\nmedia/AlphaCode2/AlphaCode2_Tech_Report.pdf.\\n[12] Amazon. 2022. What is CodeWhisperer? https://docs.aws.amazon.com/codewhisperer/latest/userguide/what-is-\\ncwspr.html.\\n[13] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 54, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='cwspr.html.\\n[13] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019.\\nMathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. In Proceedings of the\\n2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers). 2357‚Äì2367.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:56\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[14] Anthropic. 2024.\\nThe Claude 3 Model Family: Opus, Sonnet, Haiku.\\nhttps://www-cdn.anthropic.com/\\nde8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.\\n[15] Luca Ardito, Riccardo Coppola, Luca Barbato, and Diego Verga. 2020. A tool-based perspective on software code\\nmaintainability metrics: a systematic literature review. Scientific Programming 2020 (2020), 1‚Äì26.\\n[16] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad,\\nShiqi Wang, Qing Sun, Mingyue Shang, et al. 2022. Multi-lingual evaluation of code generation models. arXiv preprint\\narXiv:2210.14868 (2022).\\n[17] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie\\nCai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732\\n(2021).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732\\n(2021).\\n[18] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450\\n(2016).\\n[19] Hannah McLean Babe, Sydney Nguyen, Yangtian Zi, Arjun Guha, Molly Q Feldman, and Carolyn Jane Anderson. 2023.\\nStudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code. arXiv:2306.04556 [cs.LG]\\n[20] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang,\\net al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023).\\n[21] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna\\nGoldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv\\npreprint arXiv:2212.08073 (2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv\\npreprint arXiv:2212.08073 (2022).\\n[22] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok,\\nShashank Shet, et al. 2023. Codeplan: Repository-level coding using llms and planning. arXiv preprint arXiv:2309.12499\\n(2023).\\n[23] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation\\nwith human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine\\ntranslation and/or summarization. 65‚Äì72.\\n[24] Enrico Barbierato, Marco L Della Vedova, Daniele Tessera, Daniele Toti, and Nicola Vanoli. 2022. A methodology for\\ncontrolling bias and fairness in synthetic data generation. Applied Sciences 12, 9 (2022), 4619.\\n[25] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[25] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with\\ncode-generating models. Proceedings of the ACM on Programming Languages 7, OOPSLA1 (2023), 85‚Äì111.\\n[26] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi\\nDu, Zhe Fu, et al. 2024. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint\\narXiv:2401.02954 (2024).\\n[27] Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan, Fangxin Lu, Zili Zhang, Yulei Sui, Xuanhua Shi,\\nand Hai Jin. 2024. Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler\\nFeedback. arXiv preprint arXiv:2403.16792 (2024).\\n[28] Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini Kalliamvakou, Travis Lowdermilk, and\\nIdan Gazit. 2022. Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools.\\nQueue 20, 6 (2022), 35‚Äì57.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Idan Gazit. 2022. Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools.\\nQueue 20, 6 (2022), 35‚Äì57.\\n[29] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy,\\nKyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. arXiv\\npreprint arXiv:2204.06745 (2022).\\n[30] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive\\nLanguage Modeling with Mesh-Tensorflow. https://doi.org/10.5281/zenodo.5297715 If you use this software, please\\ncite it using these metadata..\\n[31] Veronika Bogina, Alan Hartman, Tsvi Kuflik, and Avital Shulner-Tal. 2022. Educating software and AI stakeholders\\nabout algorithmic fairness, accountability, transparency and ethics. International Journal of Artificial Intelligence in\\nEducation (2022), 1‚Äì26.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='about algorithmic fairness, accountability, transparency and ethics. International Journal of Artificial Intelligence in\\nEducation (2022), 1‚Äì26.\\n[32] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,\\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models.\\narXiv preprint arXiv:2108.07258 (2021).\\n[33] Tom B Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).\\n[34] Raymond PL Buse and Westley R Weimer. 2009. Learning a metric for code readability. IEEE Transactions on software\\nengineering 36, 4 (2009), 546‚Äì558.\\n[35] Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, and Jiayi Huang. 2024. Shortcut-connected Expert\\nParallelism for Accelerating Mixture-of-Experts. arXiv preprint arXiv:2404.05019 (2024).\\n[36] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. A survey on mixture of experts.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 55, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[36] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. A survey on mixture of experts.\\narXiv preprint arXiv:2407.06204 (2024).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:57\\n[37] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts,\\nTom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th\\nUSENIX Security Symposium (USENIX Security 21). 2633‚Äì2650.\\n[38] Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg,\\nAbhinav Jangda, and Arjun Guha. 2023. Knowledge Transfer from High-Resource to Low-Resource Programming\\nLanguages for Code LLMs. arXiv preprint arXiv:2308.09895 (2023).\\n[39] Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho\\nYee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. 2022. A scalable and extensible approach to\\nbenchmarking nl2code for 18 programming languages. arXiv preprint arXiv:2208.08227 (2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. 2022. A scalable and extensible approach to\\nbenchmarking nl2code for 18 programming languages. arXiv preprint arXiv:2208.08227 (2022).\\n[40] Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, and Hua Wu. 2022. ERNIE-Code: Beyond english-centric\\ncross-lingual pretraining for programming languages. arXiv preprint arXiv:2212.06742 (2022).\\n[41] Shubham Chandel, Colin B Clement, Guillermo Serrato, and Neel Sundaresan. 2022. Training and evaluating a jupyter\\nnotebook data science assistant. arXiv preprint arXiv:2201.12901 (2022).\\n[42] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang,\\nYidong Wang, et al. 2024. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems\\nand Technology 15, 3 (2024), 1‚Äì45.\\n[43] Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA model for code generation. https://github.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and Technology 15, 3 (2024), 1‚Äì45.\\n[43] Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA model for code generation. https://github.\\ncom/sahil280114/codealpaca.\\n[44] Binger Chen and Ziawasch Abedjan. 2023. DUETCS: Code Style Transfer through Generation and Retrieval. In 2023\\nIEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2362‚Äì2373.\\n[45] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. Codet:\\nCode generation with generated tests. arXiv preprint arXiv:2207.10397 (2022).\\n[46] Fuxiang Chen, Fatemeh H Fard, David Lo, and Timofey Bryksin. 2022. On the transferability of pre-trained language\\nmodels for low-resource programming languages. In Proceedings of the 30th IEEE/ACM International Conference on\\nProgram Comprehension. 401‚Äì412.\\n[47] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Program Comprehension. 401‚Äì412.\\n[47] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented\\ngeneration. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 17754‚Äì17762.\\n[48] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\\nYuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv\\npreprint arXiv:2107.03374 (2021).\\n[49] Stanley F Chen, Douglas Beeferman, and Roni Rosenfeld. 1998. Evaluation metrics for language models. (1998).\\n[50] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling\\ncomputation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588 (2022).\\n[51] Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, and Denny Zhou. 2023. Teaching large language models to self-debug.\\narXiv preprint arXiv:2304.05128 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[51] Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, and Denny Zhou. 2023. Teaching large language models to self-debug.\\narXiv preprint arXiv:2304.05128 (2023).\\n[52] Xinyun Chen, Chang Liu, and Dawn Song. 2018. Tree-to-tree neural networks for program translation. Advances in\\nneural information processing systems 31 (2018).\\n[53] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. 2023. Reducing\\nthe Carbon Impact of Generative AI Inference (today and in 2035). In Proceedings of the 2nd workshop on sustainable\\ncomputer systems. 1‚Äì7.\\n[54] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.\\nJournal of Machine Learning Research 24, 240 (2023), 1‚Äì113.\\n[55] Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Journal of Machine Learning Research 24, 240 (2023), 1‚Äì113.\\n[55] Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng\\nXiao, Bo Shen, Lin Li, et al. 2022. Pangu-coder: Program synthesis with function-level language modeling. arXiv\\npreprint arXiv:2207.11280 (2022).\\n[56] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa\\nDehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine\\nLearning Research 25, 70 (2024), 1‚Äì53.\\n[57] Colin B Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan. 2020. PyMT5:\\nmulti-mode translation of natural language and Python code with transformers. arXiv preprint arXiv:2010.03150\\n(2020).\\n[58] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 56, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='(2020).\\n[58] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\\nTworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint\\narXiv:2110.14168 (2021).\\n[59] CodeGemma Team, Ale Jakse Hartman, Andrea Hu, Christopher A. Choquette-Choo, Heri Zhao, Jane Fine, Jeffrey\\nHui, Jingyue Shen, Joe Kelley, Joshua Howland, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Nam Nguyen, Paul Michel,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:58\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nPeter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Siqi Zuo, Tris Warkentin, and Zhitao\\net al. Gong. 2024. CodeGemma: Open Code Models Based on Gemma. (2024). https://goo.gle/codegemma\\n[60] Codeium. 2023. Free, ultrafast Copilot alternative for Vim and Neovim. https://github.com/Exafunction/codeium.vim.\\n[61] Cognition. 2024. Introducing Devin, the first AI software engineer. https://www.cognition.ai/introducing-devin.\\n[62] Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. The Journal of\\nMachine Learning Research 11 (2010), 3053‚Äì3096.\\n[63] Cognitive Computations. 2023. oa_leet10k. https://huggingface.co/datasets/cognitivecomputations/oa_leet10k.\\n[64] Leonardo De Moura and Nikolaj Bj√∏rner. 2008. Z3: An efficient SMT solver. In International conference on Tools and\\nAlgorithms for the Construction and Analysis of Systems. Springer, 337‚Äì340.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[64] Leonardo De Moura and Nikolaj Bj√∏rner. 2008. Z3: An efficient SMT solver. In International conference on Tools and\\nAlgorithms for the Construction and Analysis of Systems. Springer, 337‚Äì340.\\n[65] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized\\nllms. Advances in Neural Information Processing Systems 36 (2024).\\n[66] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\\n[67] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min\\nChan, Weize Chen, et al. 2022. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained\\nlanguage models. arXiv preprint arXiv:2203.06904 (2022).\\n[68] Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='language models. arXiv preprint arXiv:2203.06904 (2022).\\n[68] Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh\\nNallapati, Parminder Bhatia, Dan Roth, et al. 2024. Crosscodeeval: A diverse and multilingual benchmark for cross-file\\ncode completion. Advances in Neural Information Processing Systems 36 (2024).\\n[69] Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia,\\nDan Roth, and Bing Xiang. 2022. Cocomic: Code completion by jointly modeling in-file and cross-file context. arXiv\\npreprint arXiv:2212.10007 (2022).\\n[70] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022.\\nA survey on in-context learning. arXiv preprint arXiv:2301.00234 (2022).\\n[71] Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie Shan, Caishuang Huang, Wei Shen, Xiaoran Fan,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A survey on in-context learning. arXiv preprint arXiv:2301.00234 (2022).\\n[71] Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie Shan, Caishuang Huang, Wei Shen, Xiaoran Fan,\\nZhiheng Xi, et al. 2024. StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback.\\narXiv preprint arXiv:2402.01391 (2024).\\n[72] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng,\\nand Yiling Lou. 2024. Evaluating large language models in class-level code generation. In Proceedings of the IEEE/ACM\\n46th International Conference on Software Engineering. 1‚Äì13.\\n[73] Hugging Face. 2023. Training CodeParrot from Scratch. https://github.com/huggingface/blog/blob/main/codeparrot.\\nmd.\\n[74] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M Zhang. 2023. Large\\nlanguage models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='language models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on\\nSoftware Engineering: Future of Software Engineering (ICSE-FoSE). IEEE, 31‚Äì53.\\n[75] Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei Tan. 2023. Automated repair of programs\\nfrom large language models. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE,\\n1469‚Äì1481.\\n[76] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu,\\nDaxin Jiang, et al. 2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint\\narXiv:2002.08155 (2020).\\n[77] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke\\nZettlemoyer, and Mike Lewis. 2022. Incoder: A generative model for code infilling and synthesis. arXiv preprint\\narXiv:2204.05999 (2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Zettlemoyer, and Mike Lewis. 2022. Incoder: A generative model for code infilling and synthesis. arXiv preprint\\narXiv:2204.05999 (2022).\\n[78] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish\\nThite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint\\narXiv:2101.00027 (2020).\\n[79] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.\\nPal: Program-aided language models. In International Conference on Machine Learning. PMLR, 10764‚Äì10799.\\n[80] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023.\\nRetrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023).\\n[81] Linyuan Gong, Mostafa Elhoushi, and Alvin Cheung. 2024. AST-T5: Structure-Aware Pretraining for Code Generation'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 57, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[81] Linyuan Gong, Mostafa Elhoushi, and Alvin Cheung. 2024. AST-T5: Structure-Aware Pretraining for Code Generation\\nand Understanding. arXiv preprint arXiv:2401.03003 (2024).\\n[82] Alex Gu, Baptiste Rozi√®re, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I Wang. 2024. Cruxeval:\\nA benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065 (2024).\\n[83] Sumit Gulwani. 2010. Dimensions in program synthesis. In Proceedings of the 12th international ACM SIGPLAN\\nsymposium on Principles and practice of declarative programming. 13‚Äì24.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:59\\n[84] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan\\nJavaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks are all you need. arXiv preprint\\narXiv:2306.11644 (2023).\\n[85] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022. UniXcoder: Unified Cross-Modal\\nPre-training for Code Representation. In Proceedings of the 60th Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers). 7212‚Äì7225.\\n[86] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svy-\\natkovskiy, Shengyu Fu, et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint\\narXiv:2009.08366 (2020).\\n[87] Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023. Longcoder: A long-range pre-trained language'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='arXiv:2009.08366 (2020).\\n[87] Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023. Longcoder: A long-range pre-trained language\\nmodel for code completion. In International Conference on Machine Learning. PMLR, 12098‚Äì12107.\\n[88] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li,\\net al. 2024. DeepSeek-Coder: When the Large Language Model Meets Programming‚ÄìThe Rise of Code Intelligence.\\narXiv preprint arXiv:2401.14196 (2024).\\n[89] Aman Gupta, Deepak Bhatt, and Anubha Pandey. 2021. Transitioning from Real to Synthetic data: Quantifying the\\nbias in model. arXiv preprint arXiv:2105.04144 (2021).\\n[90] Aman Gupta, Anup Shirgaonkar, Angels de Luis Balaguer, Bruno Silva, Daniel Holstein, Dawei Li, Jennifer Marsman,\\nLeonardo O Nunes, Mahsa Rouzbahman, Morris Sharp, et al. 2024. RAG vs Fine-tuning: Pipelines, Tradeoffs, and a\\nCase Study on Agriculture. arXiv preprint arXiv:2401.08406 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Leonardo O Nunes, Mahsa Rouzbahman, Morris Sharp, et al. 2024. RAG vs Fine-tuning: Pipelines, Tradeoffs, and a\\nCase Study on Agriculture. arXiv preprint arXiv:2401.08406 (2024).\\n[91] Hossein Hajipour, Keno Hassler, Thorsten Holz, Lea Sch√∂nherr, and Mario Fritz. 2024. CodeLMSec Benchmark:\\nSystematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models. In 2024 IEEE\\nConference on Secure and Trustworthy Machine Learning (SaTML). IEEE, 684‚Äì709.\\n[92] Perttu H√§m√§l√§inen, Mikke Tavast, and Anton Kunnari. 2023. Evaluating large language models in generating synthetic\\nhci research data: a case study. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems.\\n1‚Äì19.\\n[93] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning\\nwith language model is planning with world model. arXiv preprint arXiv:2305.14992 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='with language model is planning with world model. arXiv preprint arXiv:2305.14992 (2023).\\n[94] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In\\nProceedings of the IEEE conference on computer vision and pattern recognition. 770‚Äì778.\\n[95] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\\nSteinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874\\n(2021).\\n[96] Felipe Hoffa. 2016. GitHub on BigQuery: Analyze all the open source code. URL: https://cloud.google.com/blog/topics/\\npublic-datasets/github-on-bigquery-analyze-all-the-open-source-code (2016).\\n[97] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las\\nCasas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language\\nmodels. arXiv preprint arXiv:2203.15556 (2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language\\nmodels. arXiv preprint arXiv:2203.15556 (2022).\\n[98] Samuel Holt, Max Ruiz Luyten, and Mihaela van der Schaar. 2023. L2MAC: Large Language Model Automatic\\nComputer for Unbounded Code Generation. In The Twelfth International Conference on Learning Representations.\\n[99] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration.\\narXiv preprint arXiv:1904.09751 (2019).\\n[100] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing\\nYau, Zijuan Lin, Liyang Zhou, et al. 2023. Metagpt: Meta programming for multi-agent collaborative framework.\\narXiv preprint arXiv:2308.00352 (2023).\\n[101] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='arXiv preprint arXiv:2308.00352 (2023).\\n[101] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang.\\n2024. Large Language Models for Software Engineering: A Systematic Literature Review. arXiv:2308.10620 [cs.SE]\\n[102] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,\\nMona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International conference on\\nmachine learning. PMLR, 2790‚Äì2799.\\n[103] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\n2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).\\n[104] Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and Heming Cui. 2023. AgentCoder: Multi-Agent-based Code\\nGeneration with Iterative Testing and Optimisation. arXiv preprint arXiv:2312.13010 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 58, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Generation with Iterative Testing and Optimisation. arXiv preprint arXiv:2312.13010 (2023).\\n[105] Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. arXiv preprint\\narXiv:2212.10403 (2022).\\n[106] Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards Reasoning in Large Language Models: A Survey. In 61st\\nAnnual Meeting of the Association for Computational Linguistics, ACL 2023. Association for Computational Linguistics\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:60\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n(ACL), 1049‚Äì1065.\\n[107] Junjie Huang, Chenglong Wang, Jipeng Zhang, Cong Yan, Haotian Cui, Jeevana Priya Inala, Colin Clement, Nan\\nDuan, and Jianfeng Gao. 2022. Execution-based evaluation for data science code generation models. arXiv preprint\\narXiv:2211.09374 (2022).\\n[108] Qiuyuan Huang, Naoki Wake, Bidipta Sarkar, Zane Durante, Ran Gong, Rohan Taori, Yusuke Noda, Demetri Ter-\\nzopoulos, Noboru Kuno, Ade Famoti, et al. 2024. Position Paper: Agent AI Towards a Holistic Intelligence. arXiv\\npreprint arXiv:2403.00833 (2024).\\n[109] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai\\nDang, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186 (2024).\\n[110] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[110] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet\\nchallenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).\\n[111] Yoichi Ishibashi and Yoshimasa Nishimura. 2024. Self-Organized Agents: A LLM Multi-Agent Framework toward\\nUltra Large-Scale Code Generation and Optimization. arXiv preprint arXiv:2404.02183 (2024).\\n[112] Shu Ishida, Gianluca Corrado, George Fedoseev, Hudson Yeo, Lloyd Russell, Jamie Shotton, Jo√£o F Henriques, and\\nAnthony Hu. 2024. LangProp: A code optimization framework using Language Models applied to driving. arXiv\\npreprint arXiv:2401.10314 (2024).\\n[113] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Mapping Language to Code in Pro-\\ngrammatic Context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.\\n1643‚Äì1652.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='grammatic Context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.\\n1643‚Äì1652.\\n[114] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu\\nWang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: Scaling language model instruction meta learning through\\nthe lens of generalization. arXiv preprint arXiv:2212.12017 (2022).\\n[115] Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Jungkyu Choi, and Minjoon\\nSeo. 2022. Towards Continual Knowledge Learning of Language Models. In 10th International Conference on Learning\\nRepresentations, ICLR 2022. International Conference on Learning Representations.\\n[116] Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. 1977. Perplexity‚Äîa measure of the difficulty of speech\\nrecognition tasks. The Journal of the Acoustical Society of America 62, S1 (1977), S63‚ÄìS63.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='recognition tasks. The Journal of the Acoustical Society of America 62, S1 (1977), S63‚ÄìS63.\\n[117] Susmit Jha, Sumit Gulwani, Sanjit A Seshia, and Ashish Tiwari. 2010. Oracle-guided component-based program\\nsynthesis. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 1. 215‚Äì224.\\n[118] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi\\nZhou, Zhaowei Zhang, et al. 2023. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852 (2023).\\n[119] Ruyi Ji, Jingjing Liang, Yingfei Xiong, Lu Zhang, and Zhenjiang Hu. 2020. Question selection for interactive program\\nsynthesis. In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation.\\n1143‚Äì1158.\\n[120] Zhenlan Ji, Pingchuan Ma, Zongjie Li, and Shuai Wang. 2023. Benchmarking and explaining large language model-\\nbased code generation: A causality-centric approach. arXiv preprint arXiv:2310.06680 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='based code generation: A causality-centric approach. arXiv preprint arXiv:2310.06680 (2023).\\n[121] Juyong Jiang and Sunghun Kim. 2023. CodeUp: A Multilingual Code Generation Llama2 Model with Parameter-\\nEfficient Instruction-Tuning. https://github.com/juyongjiang/CodeUp.\\n[122] Shuyang Jiang, Yuhao Wang, and Yu Wang. 2023. Selfevolve: A code evolution framework via large language models.\\narXiv preprint arXiv:2306.02907 (2023).\\n[123] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. 2023.\\nSWE-bench: Can Language Models Resolve Real-world Github Issues?. In The Twelfth International Conference on\\nLearning Representations.\\n[124] Alexander Wettig Kilian Lieret Shunyu Yao Karthik Narasimhan Ofir Press John Yang, Carlos E. Jimenez. 2024.\\nSWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING. (2024).\\nhttps://swe-agent.com/'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='SWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING. (2024).\\nhttps://swe-agent.com/\\n[125] Aravind Joshi and Owen Rambow. 2003. A formalism for dependency grammar based on tree adjoining grammar. In\\nProceedings of the Conference on Meaning-text Theory. MTT Paris, France, 207‚Äì216.\\n[126] Harshit Joshi, Jos√© Cambronero Sanchez, Sumit Gulwani, Vu Le, Gust Verbruggen, and Ivan Radiƒçek. 2023. Repair\\nis nearly generation: Multilingual program repair with llms. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, Vol. 37. 5131‚Äì5140.\\n[127] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford,\\nJeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\\n[128] Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 59, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[128] Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty.\\n2023. xcodeeval: A large scale multilingual multitask benchmark for code understanding, generation, translation and\\nretrieval. arXiv preprint arXiv:2303.03004 (2023).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:61\\n[129] Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, and Chanjun Park. 2024. sDPO:\\nDon‚Äôt Use Your Data All at Once. arXiv preprint arXiv:2403.19270 (2024).\\n[130] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim,\\nHyeonju Lee, Jihoo Kim, et al. 2023. Solar 10.7 b: Scaling large language models with simple yet effective depth\\nup-scaling. arXiv preprint arXiv:2312.15166 (2023).\\n[131] Barbara Kitchenham, O Pearl Brereton, David Budgen, Mark Turner, John Bailey, and Stephen Linkman. 2009.\\nSystematic literature reviews in software engineering‚Äìa systematic literature review. Information and software\\ntechnology 51, 1 (2009), 7‚Äì15.\\n[132] Denis Kocetkov, Raymond Li, LI Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Mu√±oz Ferrandis,\\nSean Hughes, Thomas Wolf, Dzmitry Bahdanau, et al. 2022. The Stack: 3 TB of permissively licensed source code.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, et al. 2022. The Stack: 3 TB of permissively licensed source code.\\nTransactions on Machine Learning Research (2022).\\n[133] Andreas K√∂pf, Yannic Kilcher, Dimitri von R√ºtte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum,\\nDuc Nguyen, Oliver Stanley, Rich√°rd Nagyfi, et al. 2024. Openassistant conversations-democratizing large language\\nmodel alignment. Advances in Neural Information Processing Systems 36 (2024).\\n[134] Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang. 2023. Is model attention aligned with human\\nattention? an empirical study on large language models for code generation. arXiv preprint arXiv:2306.01220 (2023).\\n[135] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised translation of\\nprogramming languages. arXiv preprint arXiv:2006.03511 (2020).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[135] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised translation of\\nprogramming languages. arXiv preprint arXiv:2006.03511 (2020).\\n[136] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida\\nWang, and Tao Yu. 2023. DS-1000: A natural and reliable benchmark for data science code generation. In International\\nConference on Machine Learning. PMLR, 18319‚Äì18345.\\n[137] Hugo Lauren√ßon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao,\\nLeandro Von Werra, Chenghao Mou, Eduardo Gonz√°lez Ponferrada, Huu Nguyen, et al. 2022. The bigscience\\nroots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems 35 (2022),\\n31809‚Äì31826.\\n[138] Moritz Laurer. 2024. Synthetic data: save money, time and carbon with open source. https://huggingface.co/blog/\\nsynthetic-data-save-costs.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='31809‚Äì31826.\\n[138] Moritz Laurer. 2024. Synthetic data: save money, time and carbon with open source. https://huggingface.co/blog/\\nsynthetic-data-save-costs.\\n[139] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering\\ncode generation through pretrained models and deep reinforcement learning. Advances in Neural Information\\nProcessing Systems 35 (2022), 21314‚Äì21328.\\n[140] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Iliƒá, Daniel Hesslow, Roman Castagn√©, Alexan-\\ndra Sasha Luccioni, Fran√ßois Yvon, Matthias Gall√©, et al. 2023. Bloom: A 176b-parameter open-access multilingual\\nlanguage model. (2023).\\n[141] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and\\nAbhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint\\narXiv:2309.00267 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Abhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint\\narXiv:2309.00267 (2023).\\n[142] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning.\\narXiv preprint arXiv:2104.08691 (2021).\\n[143] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler,\\nMike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp\\ntasks. Advances in Neural Information Processing Systems 33 (2020), 9459‚Äì9474.\\n[144] Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. 2024. EvoCodeBench: An Evolving Code Generation\\nBenchmark Aligned with Real-World Code Repositories. arXiv preprint arXiv:2404.00599 (2024).\\n[145] Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. 2023. Towards enhancing in-context learning for code generation.\\narXiv preprint arXiv:2303.17780 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[145] Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. 2023. Towards enhancing in-context learning for code generation.\\narXiv preprint arXiv:2303.17780 (2023).\\n[146] Li Li, Tegawend√© F Bissyand√©, Mike Papadakis, Siegfried Rasthofer, Alexandre Bartel, Damien Octeau, Jacques Klein,\\nand Le Traon. 2017. Static analysis of android apps: A systematic literature review. Information and Software\\nTechnology 88 (2017), 67‚Äì95.\\n[147] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\\nChristopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161\\n(2023).\\n[148] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B.\\nHashimoto. 2023. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-\\nlab/alpaca_eval.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 60, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Hashimoto. 2023. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-\\nlab/alpaca_eval.\\n[149] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint\\narXiv:2101.00190 (2021).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 61, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:62\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[150] Yuanzhi Li, S√©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks are\\nall you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 (2023).\\n[151] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R√©mi Leblond, Tom Eccles, James Keeling,\\nFelix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science 378, 6624\\n(2022), 1092‚Äì1097.\\n[152] Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang, Qiyi Tang, Sen Nie, and Shi Wu. 2022. Unleashing the power of\\ncompiler intermediate representation to enhance neural program embeddings. In Proceedings of the 44th International\\nConference on Software Engineering. 2253‚Äì2265.\\n[153] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameter-efficient\\nfine-tuning. arXiv preprint arXiv:2303.15647 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 61, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[153] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameter-efficient\\nfine-tuning. arXiv preprint arXiv:2303.15647 (2023).\\n[154] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak\\nNarayanan, Yuhuai Wu, Ananya Kumar, et al. 2023. Holistic Evaluation of Language Models. Transactions on Machine\\nLearning Research (2023).\\n[155] Andreas Liesenfeld, Alianda Lopez, and Mark Dingemanse. 2023. Opening up ChatGPT: Tracking openness, trans-\\nparency, and accountability in instruction-tuned text generators. In Proceedings of the 5th international conference on\\nconversational user interfaces. 1‚Äì6.\\n[156] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.\\n74‚Äì81.\\n[157] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022. A survey of transformers. AI open 3 (2022),\\n111‚Äì132.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 61, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='74‚Äì81.\\n[157] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022. A survey of transformers. AI open 3 (2022),\\n111‚Äì132.\\n[158] Erik Linstead, Paul Rigor, Sushil Bajracharya, Cristina Lopes, and Pierre Baldi. 2007. Mining internet-scale software\\nrepositories. Advances in neural information processing systems 20 (2007).\\n[159] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai,\\nDaya Guo, et al. 2024. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv\\npreprint arXiv:2405.04434 (2024).\\n[160] Bingchang Liu, Chaoyu Chen, Cong Liao, Zi Gong, Huan Wang, Zhichao Lei, Ming Liang, Dajun Chen, Min Shen,\\nHailian Zhou, et al. 2023. Mftcoder: Boosting code llms with multitask fine-tuning. arXiv preprint arXiv:2311.02303\\n(2023).\\n[161] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 61, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='(2023).\\n[161] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel.\\n2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural\\nInformation Processing Systems 35 (2022), 1950‚Äì1965.\\n[162] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is your code generated by chatgpt really\\ncorrect? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing\\nSystems 36 (2024).\\n[163] Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. 2023. Rltf: Reinforcement learning\\nfrom unit test feedback. arXiv preprint arXiv:2307.04349 (2023).\\n[164] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt,\\nand predict: A systematic survey of prompting methods in natural language processing. Comput. Surveys 55, 9 (2023),\\n1‚Äì35.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 61, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='and predict: A systematic survey of prompting methods in natural language processing. Comput. Surveys 55, 9 (2023),\\n1‚Äì35.\\n[165] Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang,\\nDenny Zhou, et al. 2024. Best Practices and Lessons Learned on Synthetic Data for Language Models. arXiv preprint\\narXiv:2404.07503 (2024).\\n[166] Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, and Yang Liu. 2020. Retrieval-Augmented Generation for Code\\nSummarization via Hybrid GNN. In International Conference on Learning Representations.\\n[167] Tianyang Liu, Canwen Xu, and Julian McAuley. 2023.\\nRepobench: Benchmarking repository-level code auto-\\ncompletion systems. arXiv preprint arXiv:2306.03091 (2023).\\n[168] Yan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji Zhang, Daoguang Zan, Jian-Guang Lou, Pin-Yu Chen, and Tsung-Yi\\nHo. 2023. Uncovering and quantifying social biases in code generation. Advances in Neural Information Processing'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 61, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Ho. 2023. Uncovering and quantifying social biases in code generation. Advances in Neural Information Processing\\nSystems 36 (2023), 2368‚Äì2380.\\n[169] Yue Liu, Chakkrit Tantithamthavorn, Li Li, and Yepang Liu. 2022. Deep learning for android malware defenses: a\\nsystematic literature review. Comput. Surveys 55, 8 (2022), 1‚Äì36.\\n[170] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang,\\nDmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. 2024. StarCoder 2 and The Stack v2: The Next Generation. arXiv\\npreprint arXiv:2402.19173 (2024).\\n[171] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy. 2022. ReACC: A Retrieval-\\nAugmented Code Completion Framework. In Proceedings of the 60th Annual Meeting of the Association for Computa-\\ntional Linguistics (Volume 1: Long Papers). 6227‚Äì6240.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:63\\n[172] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain,\\nDaxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and\\ngeneration. arXiv preprint arXiv:2102.04664 (2021).\\n[173] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin,\\nand Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. In The Twelfth\\nInternational Conference on Learning Representations.\\n[174] Michael R Lyu, Baishakhi Ray, Abhik Roychoudhury, Shin Hwei Tan, and Patanamon Thongtanunam. 2024. Automatic\\nProgramming: Large Language Models and Beyond. arXiv preprint arXiv:2405.02213 (2024).\\n[175] Wei Ma, Mengjie Zhao, Xiaofei Xie, Qiang Hu, Shangqing Liu, Jie Zhang, Wenhan Wang, and Yang Liu. 2022. Are'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[175] Wei Ma, Mengjie Zhao, Xiaofei Xie, Qiang Hu, Shangqing Liu, Jie Zhang, Wenhan Wang, and Yang Liu. 2022. Are\\nCode Pre-trained Models Powerful to Learn Code Syntax and Semantics? arXiv preprint arXiv:2212.10017 (2022).\\n[176] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\\nShrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. Advances in\\nNeural Information Processing Systems 36 (2024).\\n[177] James Manyika and Sissie Hsiao. 2023. An overview of Bard: an early experiment with generative AI. AI. Google\\nStatic Documents 2 (2023).\\n[178] Vadim Markovtsev, Waren Long, Hugo Mougard, Konstantin Slavnov, and Egor Bulychev. 2019. STYLE-ANALYZER:\\nfixing code style inconsistencies with interpretable unsupervised algorithms. In 2019 IEEE/ACM 16th International\\nConference on Mining Software Repositories (MSR). IEEE, 468‚Äì478.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='fixing code style inconsistencies with interpretable unsupervised algorithms. In 2019 IEEE/ACM 16th International\\nConference on Mining Software Repositories (MSR). IEEE, 468‚Äì478.\\n[179] Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. 2022. Generating training data with language models: Towards\\nzero-shot language understanding. Advances in Neural Information Processing Systems 35 (2022), 462‚Äì477.\\n[180] Meta. 2024. Introducing Meta Llama 3: The most capable openly available LLM to date. https://ai.meta.com/blog/meta-\\nllama-3/.\\n[181] MistralAI. 2024. Codestral. https://mistral.ai/news/codestral/.\\n[182] S√©bastien Bubeck Mojan Javaheripi. 2023. Phi-2: The surprising power of small language models. https://www.\\nmicrosoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models.\\n[183] Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang. 2014. TBCNN: A tree-based convolutional neural network for\\nprogramming language processing. arXiv preprint arXiv:1409.5718 (2014).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[183] Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang. 2014. TBCNN: A tree-based convolutional neural network for\\nprogramming language processing. arXiv preprint arXiv:1409.5718 (2014).\\n[184] Zahra Mousavi, Chadni Islam, Kristen Moore, Alsharif Abuadbba, and M Ali Babar. 2024. An investigation into\\nmisuse of java security apis by large language models. In Proceedings of the 19th ACM Asia Conference on Computer\\nand Communications Security. 1299‚Äì1315.\\n[185] Spyridon Mouselinos, Mateusz Malinowski, and Henryk Michalewski. 2022. A simple, yet effective approach to\\nfinding biases in code generation. arXiv preprint arXiv:2211.00609 (2022).\\n[186] Hussein Mozannar, Gagan Bansal, Adam Fourney, and Eric Horvitz. 2022. Reading between the lines: Modeling user\\nbehavior and costs in AI-assisted programming. arXiv preprint arXiv:2210.14306 (2022).\\n[187] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[187] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru\\nTang, Leandro Von Werra, and Shayne Longpre. 2023. Octopack: Instruction tuning code large language models.\\narXiv preprint arXiv:2308.07124 (2023).\\n[188] King Han Naman Jain, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik\\nSen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for\\ncode. arXiv preprint arXiv:2403.07974 (2024).\\n[189] Antonio Nappa, Richard Johnson, Leyla Bilge, Juan Caballero, and Tudor Dumitras. 2015. The attack of the clones: A\\nstudy of the impact of shared code on vulnerability patching. In 2015 IEEE symposium on security and privacy. IEEE,\\n692‚Äì708.\\n[190] Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. 2023. Lever:'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='692‚Äì708.\\n[190] Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. 2023. Lever:\\nLearning to verify language-to-code generation with execution. In International Conference on Machine Learning.\\nPMLR, 26106‚Äì26128.\\n[191] Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz,\\nCaiming Xiong, et al. 2023. L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language\\nModels. arXiv preprint arXiv:2309.17446 (2023).\\n[192] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. 2023. Codegen2: Lessons for\\ntraining llms on programming and natural languages. arXiv preprint arXiv:2305.02309 (2023).\\n[193] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022.\\nCodegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474\\n(2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 62, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474\\n(2022).\\n[194] Changan Niu, Chuanyi Li, Bin Luo, and Vincent Ng. 2022. Deep learning meets software engineering: A survey on\\npre-trained models of source code. arXiv preprint arXiv:2205.11739 (2022).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:64\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[195] Theo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. 2023. Is Self-Repair\\na Silver Bullet for Code Generation?. In The Twelfth International Conference on Learning Representations.\\n[196] OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/chatgpt.\\n[197] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/.\\n[198] OpenAI. 2024. New models and developer products announced at DevDay. https://openai.com/index/new-models-\\nand-developer-products-announced-at-devday/.\\n[199] OpenDevin. 2024. OpenDevin: Code Less, Make More. https://github.com/OpenDevin/OpenDevin.\\n[200] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.\\nAdvances in neural information processing systems 35 (2022), 27730‚Äì27744.\\n[201] Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023. Fine-tuning or retrieval? comparing\\nknowledge injection in llms. arXiv preprint arXiv:2312.05934 (2023).\\n[202] David N Palacio, Alejandro Velasco, Daniel Rodriguez-Cardenas, Kevin Moran, and Denys Poshyvanyk. 2023. Eval-\\nuating and explaining large language models for code using syntactic structures. arXiv preprint arXiv:2308.03873\\n(2023).\\n[203] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation\\nof machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics.\\n311‚Äì318.\\n[204] Nikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr, and Sergey Mechtaev.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='311‚Äì318.\\n[204] Nikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr, and Sergey Mechtaev.\\n2024. The Fact Selection Problem in LLM-Based Program Repair. arXiv preprint arXiv:2404.05520 (2024).\\n[205] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval Augmented\\nCode Generation and Summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021.\\n2719‚Äì2734.\\n[206] Arkil Patel, Siva Reddy, Dzmitry Bahdanau, and Pradeep Dasigi. 2023. Evaluating In-Context Learning of Libraries\\nfor Code Generation. arXiv preprint arXiv:2311.09635 (2023).\\n[207] Indraneil Paul, Jun Luo, Goran Glava≈°, and Iryna Gurevych. 2024. IRCoder: Intermediate Representations Make\\nLanguage Models Robust Multilingual Code Generators. arXiv preprint arXiv:2403.03894 (2024).\\n[208] Norman Peitek, Sven Apel, Chris Parnin, Andr√© Brechmann, and Janet Siegmund. 2021. Program comprehension and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[208] Norman Peitek, Sven Apel, Chris Parnin, Andr√© Brechmann, and Janet Siegmund. 2021. Program comprehension and\\ncode complexity metrics: An fmri study. In 2021 IEEE/ACM 43rd International Conference on Software Engineering\\n(ICSE). IEEE, 524‚Äì536.\\n[209] Huy N Phan, Hoang N Phan, Tien N Nguyen, and Nghi DQ Bui. 2024. RepoHyper: Better Context Retrieval Is All You\\nNeed for Repository-Level Code Completion. arXiv preprint arXiv:2403.06095 (2024).\\n[210] Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, Ashish Datta, Maksym Zhu-\\nravinskyi, Dakota Mahan, Marco Bellagente, Carlos Riquelme, et al. 2024. Stable Code Technical Report. arXiv\\npreprint arXiv:2404.01226 (2024).\\n[211] Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input\\nlength extrapolation. arXiv preprint arXiv:2108.12409 (2021).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[211] Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input\\nlength extrapolation. arXiv preprint arXiv:2108.12409 (2021).\\n[212] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023. Fine-tuning\\naligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693\\n(2023).\\n[213] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by\\ngenerative pre-training. (2018).\\n[214] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are\\nunsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.\\n[215] Steven Raemaekers, Arie Van Deursen, and Joost Visser. 2012. Measuring software library stability through historical\\nversion analysis. In 2012 28th IEEE international conference on software maintenance (ICSM). IEEE, 378‚Äì387.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='version analysis. In 2012 28th IEEE international conference on software maintenance (ICSM). IEEE, 378‚Äì387.\\n[216] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct\\npreference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing\\nSystems 36 (2024).\\n[217] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\nPeter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine\\nlearning research 21, 140 (2020), 1‚Äì67.\\n[218] Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. 2022. Evaluating the text-to-sql capabilities of large\\nlanguage models. arXiv preprint arXiv:2204.00498 (2022).\\n[219] Aurora Ramirez, Jose Raul Romero, and Christopher L Simons. 2018. A systematic review of interaction in search-based'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 63, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='language models. arXiv preprint arXiv:2204.00498 (2022).\\n[219] Aurora Ramirez, Jose Raul Romero, and Christopher L Simons. 2018. A systematic review of interaction in search-based\\nsoftware engineering. IEEE Transactions on Software Engineering 45, 8 (2018), 760‚Äì781.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:65\\n[220] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Sori-\\ncut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding\\nacross millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024).\\n[221] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco,\\nand Shuai Ma. 2020. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297\\n(2020).\\n[222] Replit. 2016. Idea to software, fast. https://replit.com.\\n[223] Replit. 2023. replit-code-v1-3b. https://huggingface.co/replit/replit-code-v1-3b.\\n[224] Tal Ridnik, Dedy Kredo, and Itamar Friedman. 2024. Code Generation with AlphaCodium: From Prompt Engineering\\nto Flow Engineering. arXiv preprint arXiv:2401.08500 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[224] Tal Ridnik, Dedy Kredo, and Itamar Friedman. 2024. Code Generation with AlphaCodium: From Prompt Engineering\\nto Flow Engineering. arXiv preprint arXiv:2401.08500 (2024).\\n[225] Nick Roshdieh. 2023. Evol-Instruct-Code-80k. https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1.\\n[226] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D Weisz. 2023. The programmer‚Äôs\\nassistant: Conversational interaction with a large language model for software development. In Proceedings of the\\n28th International Conference on Intelligent User Interfaces. 491‚Äì514.\\n[227] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal\\nRemez, J√©r√©my Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950\\n(2023).\\n[228] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='(2023).\\n[228] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy\\nKepner, Devesh Tiwari, and Vijay Gadepally. 2023. From words to watts: Benchmarking the energy costs of large\\nlanguage model inference. In 2023 IEEE High Performance Extreme Computing Conference (HPEC). IEEE, 1‚Äì9.\\n[229] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud\\nStiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask Prompted Training Enables Zero-Shot Task Generalization.\\nIn ICLR 2022-Tenth International Conference on Learning Representations.\\n[230] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization\\nalgorithms. arXiv preprint arXiv:1707.06347 (2017).\\n[231] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. 2021. You autocomplete me: Poisoning vulnera-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='algorithms. arXiv preprint arXiv:1707.06347 (2017).\\n[231] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. 2021. You autocomplete me: Poisoning vulnera-\\nbilities in neural code completion. In 30th USENIX Security Symposium (USENIX Security 21). 1559‚Äì1575.\\n[232] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv\\npreprint arXiv:1803.02155 (2018).\\n[233] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017.\\nOutrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538\\n(2017).\\n[234] Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang\\nZhao, et al. 2023. Pangu-coder2: Boosting large language models for code with ranking feedback. arXiv preprint\\narXiv:2307.14936 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Zhao, et al. 2023. Pangu-coder2: Boosting large language models for code with ranking feedback. arXiv preprint\\narXiv:2307.14936 (2023).\\n[235] Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, and David Lo. 2024. Greening large language models\\nof code. In Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society.\\n142‚Äì153.\\n[236] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language\\nagents with verbal reinforcement learning. Advances in Neural Information Processing Systems 36 (2024).\\n[237] Atsushi Shirafuji, Yusuke Oda, Jun Suzuki, Makoto Morishita, and Yutaka Watanobe. 2023. Refactoring Programs\\nUsing Large Language Models with Few-Shot Examples. arXiv preprint arXiv:2311.11690 (2023).\\n[238] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based code generation using\\ndeep reinforcement learning. arXiv preprint arXiv:2301.13816 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[238] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based code generation using\\ndeep reinforcement learning. arXiv preprint arXiv:2301.13816 (2023).\\n[239] Disha Shrivastava, Denis Kocetkov, Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. 2023. RepoFusion:\\nTraining Code Models to Understand Your Repository. arXiv preprint arXiv:2306.10998 (2023).\\n[240] Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. 2023. Repository-level prompt generation for large language\\nmodels of code. In International Conference on Machine Learning. PMLR, 31693‚Äì31715.\\n[241] Mukul Singh, Jos√© Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, and Gust Verbruggen. 2023. Codefusion: A\\npre-trained diffusion model for code generation. arXiv preprint arXiv:2310.17680 (2023).\\n[242] Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, and Tao Yu. 2024. ARKS: Active'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 64, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[242] Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, and Tao Yu. 2024. ARKS: Active\\nRetrieval in Knowledge Soup for Code Generation. arXiv preprint arXiv:2402.12317 (2024).\\n[243] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer\\nwith rotary position embedding. Neurocomputing 568 (2024), 127063.\\n[244] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: Code generation\\nusing transformer. In Proceedings of the 28th ACM joint meeting on European software engineering conference and\\nsymposium on the foundations of software engineering. 1433‚Äì1443.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:66\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[245] Marc Szafraniec, Baptiste Roziere, Hugh Leather, Francois Charton, Patrick Labatut, and Gabriel Synnaeve. 2022.\\nCode translation with compiler representations. In Proceedings of the Eleventh International Conference on Learning\\nRepresentations: ICLR.\\n[246] TabNine. 2018. AI Code Completions. https://github.com/codota/TabNine.\\n[247] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.\\nHashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_\\nalpaca.\\n[248] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\\nMorgane Rivi√®re, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and\\ntechnology. arXiv preprint arXiv:2403.08295 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Morgane Rivi√®re, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and\\ntechnology. arXiv preprint arXiv:2403.08295 (2024).\\n[249] Qwen Team. 2024. Code with CodeQwen1.5. https://qwenlm.github.io/blog/codeqwen1.5.\\n[250] Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt,\\nand Siddharth Garg. 2023. Benchmarking large language models for automated verilog rtl code generation. In 2023\\nDesign, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 1‚Äì6.\\n[251] theblackcat102. 2023.\\nThe evolved code alpaca dataset.\\nhttps://huggingface.co/datasets/theblackcat102/evol-\\ncodealpaca-v1.\\n[252] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste\\nRozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.\\narXiv preprint arXiv:2302.13971 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.\\narXiv preprint arXiv:2302.13971 (2023).\\n[253] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.\\narXiv preprint arXiv:2307.09288 (2023).\\n[254] Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. 2022. Natural language processing with transformers. \" O‚ÄôReilly\\nMedia, Inc.\".\\n[255] Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022. Expectation vs. experience: Evaluating the usability\\nof code generation tools powered by large language models. In Chi conference on human factors in computing systems\\nextended abstracts. 1‚Äì7.\\n[256] Boris Van Breugel, Zhaozhi Qian, and Mihaela Van Der Schaar. 2023. Synthetic data, real errors: how (not) to publish'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='extended abstracts. 1‚Äì7.\\n[256] Boris Van Breugel, Zhaozhi Qian, and Mihaela Van Der Schaar. 2023. Synthetic data, real errors: how (not) to publish\\nand use synthetic data. In International Conference on Machine Learning. PMLR, 34793‚Äì34808.\\n[257] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\\nPolosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).\\n[258] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https:\\n//github.com/kingoflolz/mesh-transformer-jax.\\n[259] Chong Wang, Jian Zhang, Yebo Feng, Tianlin Li, Weisong Sun, Yang Liu, and Xin Peng. 2024. Teaching Code LLMs to\\nUse Autocompletion Tools in Repository-Level Code Generation. arXiv preprint arXiv:2401.06391 (2024).\\n[260] Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. 2024. Software testing with'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[260] Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. 2024. Software testing with\\nlarge language models: Survey, landscape, and vision. IEEE Transactions on Software Engineering (2024).\\n[261] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen,\\nYankai Lin, et al. 2024. A survey on large language model based autonomous agents. Frontiers of Computer Science 18,\\n6 (2024), 1‚Äì26.\\n[262] Simin Wang, Liguo Huang, Amiao Gao, Jidong Ge, Tengfei Zhang, Haitao Feng, Ishna Satyarth, Ming Li, He Zhang, and\\nVincent Ng. 2022. Machine/deep learning for software engineering: A systematic literature review. IEEE Transactions\\non Software Engineering 49, 3 (2022), 1188‚Äì1231.\\n[263] Shiqi Wang, Li Zheng, Haifeng Qian, Chenghao Yang, Zijian Wang, Varun Kumar, Mingyue Shang, Samson Tan,\\nBaishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, and Bing Xiang. 2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, and Bing Xiang. 2022.\\nReCode: Robustness Evaluation of Code Generation Models. (2022). https://doi.org/10.48550/arXiv.2212.10264\\n[264] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. 2023. Knowledge editing for large language\\nmodels: A survey. arXiv preprint arXiv:2310.16218 (2023).\\n[265] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024. Executable code\\nactions elicit better llm agents. arXiv preprint arXiv:2402.01030 (2024).\\n[266] Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, and Qun Liu.\\n2022. Compilable Neural Code Generation with Compiler Feedback. In Findings of the Association for Computational\\nLinguistics: ACL 2022. 9‚Äì19.\\n[267] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 65, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Linguistics: ACL 2022. 9‚Äì19.\\n[267] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny\\nZhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171\\n(2022).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:67\\n[268] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.\\n2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. In The 61st Annual Meeting Of The\\nAssociation For Computational Linguistics.\\n[269] Yue Wang, Hung Le, Akhilesh Gotmare, Nghi Bui, Junnan Li, and Steven Hoi. 2023. CodeT5+: Open Code Large\\nLanguage Models for Code Understanding and Generation. In Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing. 1069‚Äì1088.\\n[270] Yanlin Wang and Hui Li. 2021. Code completion by modeling flattened abstract syntax trees as graphs. In Proceedings\\nof the AAAI conference on artificial intelligence, Vol. 35. 14015‚Äì14023.\\n[271] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='of the AAAI conference on artificial intelligence, Vol. 35. 14015‚Äì14023.\\n[271] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-\\nDecoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing. 8696‚Äì8708.\\n[272] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and\\nQun Liu. 2023. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966 (2023).\\n[273] Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. 2022. Execution-based evaluation for open-domain\\ncode generation. arXiv preprint arXiv:2212.10481 (2022).\\n[274] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\\nQuoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).\\n[275] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\\nDenny Zhou, Donald Metzler, et al. 2022. Emergent Abilities of Large Language Models. Transactions on Machine\\nLearning Research (2022).\\n[276] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.\\nChain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing\\nsystems 35 (2022), 24824‚Äì24837.\\n[277] Xiaokai Wei, Sujan Kumar Gonugondla, Shiqi Wang, Wasi Ahmad, Baishakhi Ray, Haifeng Qian, Xiaopeng Li, Varun\\nKumar, Zijian Wang, Yuchen Tian, et al. 2023. Towards greener yet powerful code generation via quantization: An\\nempirical study. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='empirical study. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the\\nFoundations of Software Engineering. 224‚Äì236.\\n[278] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need.\\narXiv preprint arXiv:2312.02120 (2023).\\n[279] Lilian Weng. 2023. LLM-powered Autonomous Agents. lilianweng.github.io (Jun 2023). https://lilianweng.github.io/\\nposts/2023-06-23-agent/\\n[280] Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024. QuRating: Selecting High-Quality Data for\\nTraining Language Models. arXiv preprint arXiv:2402.09739 (2024).\\n[281] Erroll Wood, Tadas Baltru≈°aitis, Charlie Hewitt, Sebastian Dziadzio, Thomas J Cashman, and Jamie Shotton. 2021.\\nFake it till you make it: face analysis in the wild using synthetic data alone. In Proceedings of the IEEE/CVF international\\nconference on computer vision. 3681‚Äì3691.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Fake it till you make it: face analysis in the wild using synthetic data alone. In Proceedings of the IEEE/CVF international\\nconference on computer vision. 3681‚Äì3691.\\n[282] Di Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Krishna Ramanathan, and Xiaofei Ma. 2024. Repoformer: Selective\\nRetrieval for Repository-Level Code Completion. arXiv preprint arXiv:2403.10059 (2024).\\n[283] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang,\\nand Chi Wang. 2023. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv\\npreprint arXiv:2308.08155 (2023).\\n[284] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin,\\nEnyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint\\narXiv:2309.07864 (2023).\\n[285] Rui Xie, Zhengran Zeng, Zhuohao Yu, Chang Gao, Shikun Zhang, and Wei Ye. 2024. CodeShell Technical Report.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='arXiv:2309.07864 (2023).\\n[285] Rui Xie, Zhengran Zeng, Zhuohao Yu, Chang Gao, Shikun Zhang, and Wei Ye. 2024. CodeShell Technical Report.\\narXiv preprint arXiv:2403.15747 (2024).\\n[286] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S Liang. 2023. Data selection for language models via\\nimportance resampling. Advances in Neural Information Processing Systems 36 (2023), 34201‚Äì34227.\\n[287] Bowen Li Xingyao Wang and Graham Neubig. 2024. Introducing OpenDevin CodeAct 1.0, a new State-of-the-art in\\nCoding Agents. https://www.cognition.ai/introducing-devin.\\n[288] Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang Huang, and Lu Zhang. 2017. Precise condition\\nsynthesis for program repair. In 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE,\\n416‚Äì426.\\n[289] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 66, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='416‚Äì426.\\n[289] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.\\nWizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 (2023).\\n[290] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large\\nlanguage models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:68\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n1‚Äì10.\\n[291] Junjielong Xu, Ying Fu, Shin Hwei Tan, and Pinjia He. 2024. Aligning LLMs for FL-free Program Repair. arXiv preprint\\narXiv:2404.08877 (2024).\\n[292] Weiwei Xu, Kai Gao, Hao He, and Minghui Zhou. 2024. A First Look at License Compliance Capability of LLMs in\\nCode Generation. arXiv preprint arXiv:2408.02487 (2024).\\n[293] Zhou Yang, Zhensu Sun, Terry Zhuo Yue, Premkumar Devanbu, and David Lo. 2024. Robustness, security, privacy,\\nexplainability, efficiency, and usability of large language models for code. arXiv preprint arXiv:2403.07506 (2024).\\n[294] Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsun Kim, Donggyun Han, and David Lo. 2024. Unveiling\\nmemorization in code models. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering.\\n1‚Äì13.\\n[295] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1‚Äì13.\\n[295] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of\\nthoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems\\n36 (2024).\\n[296] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct:\\nSynergizing Reasoning and Acting in Language Models. In International Conference on Learning Representations\\n(ICLR).\\n[297] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned\\ncode and natural language pairs from stack overflow. In Proceedings of the 15th international conference on mining\\nsoftware repositories. 476‚Äì486.\\n[298] Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim,\\nMunhyong Kim, Sungju Kim, et al. 2024. HyperCLOVA X Technical Report. arXiv preprint arXiv:2404.01954 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Munhyong Kim, Sungju Kim, et al. 2024. HyperCLOVA X Technical Report. arXiv preprint arXiv:2404.01954 (2024).\\n[299] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao\\nXie. 2024. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings\\nof the 46th IEEE/ACM International Conference on Software Engineering. 1‚Äì12.\\n[300] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle\\nRoman, et al. 2018. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing\\nand Text-to-SQL Task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.\\n3911‚Äì3921.\\n[301] Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng Yin. 2023.\\nWavecoder: Widespread and versatile enhanced instruction tuning with refined data generation. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation. arXiv preprint\\narXiv:2312.14187 (2023).\\n[302] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2023.\\nGpt-4 is too smart to be safe: Stealthy chat with llms via cipher. arXiv preprint arXiv:2308.06463 (2023).\\n[303] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. Rrhf: Rank responses to\\nalign language models with human feedback without tears. arXiv preprint arXiv:2304.05302 (2023).\\n[304] Jiawei Liu Yifeng Ding Naman Jain Harm de Vries Leandro von Werra Arjun Guha Lingming Zhang Yuxiang Wei,\\nFederico Cassano. 2024. StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation.\\nhttps://github.com/bigcode-project/starcoder2-self-align.\\n[305] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021.\\nBitfit: Simple parameter-efficient fine-tuning for'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='https://github.com/bigcode-project/starcoder2-self-align.\\n[305] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021.\\nBitfit: Simple parameter-efficient fine-tuning for\\ntransformer-based masked language-models. arXiv preprint arXiv:2106.10199 (2021).\\n[306] Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-\\nGuang Lou. 2022. CERT: continual pre-training on sketches for library-oriented code generation. arXiv preprint\\narXiv:2206.06888 (2022).\\n[307] Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, and Jian-Guang Lou. 2023.\\nLarge Language Models Meet NL2Code: A Survey. In Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers). 7443‚Äì7464.\\n[308] Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Computational Linguistics (Volume 1: Long Papers). 7443‚Äì7464.\\n[308] Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan,\\net al. 2024. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. arXiv preprint arXiv:2403.16443\\n(2024).\\n[309] Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\\n2023. RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. In Proceedings of\\nthe 2023 Conference on Empirical Methods in Natural Language Processing. 2471‚Äì2484.\\n[310] Jialu Zhang, Jos√© Pablo Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, and Gust Verbruggen.\\n2024. Pydex: Repairing bugs in introductory python assignments using llms. Proceedings of the ACM on Programming\\nLanguages 8, OOPSLA1 (2024), 1100‚Äì1124.\\n[311] Jialu Zhang, De Li, John Charles Kolesar, Hanyuan Shi, and Ruzica Piskac. 2022. Automated feedback generation'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 67, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Languages 8, OOPSLA1 (2024), 1100‚Äì1124.\\n[311] Jialu Zhang, De Li, John Charles Kolesar, Hanyuan Shi, and Ruzica Piskac. 2022. Automated feedback generation\\nfor competition-level code. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='A Survey on Large Language Models for Code Generation\\n1:69\\nEngineering. 1‚Äì13.\\n[312] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. 2023.\\nAdaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning\\nRepresentations.\\n[313] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang,\\nFei Wu, et al. 2023. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792 (2023).\\n[314] Shudan Zhang, Hanlin Zhao, Xiao Liu, Qinkai Zheng, Zehan Qi, Xiaotao Gu, Xiaohan Zhang, Yuxiao Dong, and Jie\\nTang. 2024. NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts.\\narXiv preprint arXiv:2405.04520 (2024).\\n[315] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='arXiv preprint arXiv:2405.04520 (2024).\\n[315] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong\\nChen, et al. 2023. Siren‚Äôs song in the AI ocean: a survey on hallucination in large language models. arXiv preprint\\narXiv:2309.01219 (2023).\\n[316] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. 2024. AutoCodeRover: Autonomous Program\\nImprovement. arXiv preprint arXiv:2404.05427 (2024).\\n[317] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023. Unifying the\\nperspectives of nlp and software engineering: A survey on language models for code. arXiv preprint arXiv:2311.07989\\n(2023).\\n[318] Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bin Qin, and Ting Liu. 2023. Length Extrapolation of Transformers: A\\nSurvey from the Perspective of Position Encoding. arXiv preprint arXiv:2312.17044 (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Survey from the Perspective of Position Encoding. arXiv preprint arXiv:2312.17044 (2023).\\n[319] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\\nZhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).\\n[320] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,\\nDacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\\nInformation Processing Systems 36 (2024).\\n[321] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li,\\net al. 2023. Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x. In\\nProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 5673‚Äì5684.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 5673‚Äì5684.\\n[322] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. 2024.\\nOpenCodeInterpreter: Integrating Code Generation with Execution and Refinement. arXiv preprint arXiv:2402.14658\\n(2024).\\n[323] Wenqing Zheng, SP Sharan, Ajay Kumar Jaiswal, Kevin Wang, Yihan Xi, Dejia Xu, and Zhangyang Wang. 2023.\\nOutline, then details: Syntactically guided coarse-to-fine code generation. In International Conference on Machine\\nLearning. PMLR, 42403‚Äì42419.\\n[324] Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen. 2023. A survey of\\nlarge language models for code: Evolution, benchmarking, and future trends. arXiv preprint arXiv:2311.10372 (2023).\\n[325] Li Zhong, Zilong Wang, and Jingbo Shang. 2024. LDB: A Large Language Model Debugger via Verifying Runtime\\nExecution Step-by-step. arXiv preprint arXiv:2402.16906 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='[325] Li Zhong, Zilong Wang, and Jingbo Shang. 2024. LDB: A Large Language Model Debugger via Verifying Runtime\\nExecution Step-by-step. arXiv preprint arXiv:2402.16906 (2024).\\n[326] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie\\nZhan, et al. 2023. Solving challenging math word problems using gpt-4 code interpreter with code-based self-\\nverification. arXiv preprint arXiv:2308.07921 (2023).\\n[327] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023. Language agent tree\\nsearch unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406 (2023).\\n[328] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili\\nYu, et al. 2024. Lima: Less is more for alignment. Advances in Neural Information Processing Systems 36 (2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='Yu, et al. 2024. Lima: Less is more for alignment. Advances in Neural Information Processing Systems 36 (2024).\\n[329] Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui,\\nOlivier Bousquet, Quoc V Le, et al. 2022. Least-to-Most Prompting Enables Complex Reasoning in Large Language\\nModels. In The Eleventh International Conference on Learning Representations.\\n[330] Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang, and Graham Neubig. 2022. DocPrompting: Generating Code by\\nRetrieving the Docs. In The Eleventh International Conference on Learning Representations.\\n[331] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong\\nMa, et al. 2024. DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence. arXiv\\npreprint arXiv:2406.11931 (2024).\\n[332] Terry Yue Zhuo. 2024. ICE-Score: Instructing Large Language Models to Evaluate Code. In Findings of the Association'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 68, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2406.11931 (2024).\\n[332] Terry Yue Zhuo. 2024. ICE-Score: Instructing Large Language Models to Evaluate Code. In Findings of the Association\\nfor Computational Linguistics: EACL 2024. 2232‚Äì2242.\\n[333] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf,\\nHaolan Zhan, Junda He, Indraneil Paul, et al. 2024. Bigcodebench: Benchmarking code generation with diverse\\nfunction calls and complex instructions. arXiv preprint arXiv:2406.15877 (2024).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2021/05/01 v1.78 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-11-12T01:58:59+00:00', 'source': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'file_path': '..\\\\data\\\\pdf\\\\code generation using LLMs (compressed).pdf', 'total_pages': 70, 'format': 'PDF 1.5', 'title': 'A Survey on Large Language Models for Code Generation', 'author': '', 'subject': '-  General and reference  ->  Surveys and overviews.-  Software and its engineering  ->  Software development techniques.-  Computing methodologies  ->  Artificial intelligence.', 'keywords': '', 'moddate': '2024-11-12T01:58:59+00:00', 'trapped': '', 'modDate': 'D:20241112015859Z', 'creationDate': 'D:20241112015859Z', 'page': 69, 'source_file': 'code generation using LLMs (compressed).pdf', 'file_type': 'pdf'}, page_content='1:70\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[334] Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian Liu, and Niklas\\nMuennighoff. 2024. Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models. arXiv preprint\\narXiv:2401.00788 (2024).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 0, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33\\nFlashFill++: Scaling Programming by Example by Cutting to\\nthe Chase\\nJOS√â CAMBRONERO‚àó, Microsoft, USA\\nSUMIT GULWANI‚àó, Microsoft, USA\\nVU LE‚àó, Microsoft, USA\\nDANIEL PERELMAN‚àó, Microsoft, USA\\nARJUN RADHAKRISHNA‚àó, Microsoft, USA\\nCLINT SIMON‚àó, Microsoft, USA\\nASHISH TIWARI‚àó, Microsoft, USA\\nProgramming-by-Examples (PBE) involves synthesizing an intended program from a small set of user-provided\\ninput-output examples. A key PBE strategy has been to restrict the search to a carefully designed small\\ndomain-specific language (DSL) with effectively-invertible (EI) operators at the top and effectively-enumerable\\n(EE) operators at the bottom. This facilitates an effective combination of top-down synthesis strategy (which\\nbackpropagates outputs over various paths in the DSL using inverse functions) with a bottom-up synthesis\\nstrategy (which propagates inputs over various paths in the DSL). We address the problem of scaling synthesis'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 0, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='strategy (which propagates inputs over various paths in the DSL). We address the problem of scaling synthesis\\nto large DSLs with several non-EI/EE operators. This is motivated by the need to support a richer class of\\ntransformations and the need for readable code generation. We propose a novel solution strategy that relies\\non propagating fewer values and over fewer paths.\\nOur first key idea is that of cut functions that prune the set of values being propagated by using knowledge\\nof the sub-DSL on the other side. Cuts can be designed to preserve completeness of synthesis; however, DSL\\ndesigners may use incomplete cuts to have finer control over the kind of programs synthesized. In either case,\\ncuts make search feasible for non-EI/EE operators and efficient for deep DSLs. Our second key idea is that of\\nguarded DSLs that allow a precedence on DSL operators, which dynamically controls exploration of various'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 0, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='guarded DSLs that allow a precedence on DSL operators, which dynamically controls exploration of various\\npaths in the DSL. This makes search efficient over grammars with large fanouts without losing recall. It also\\nmakes ranking simpler yet more effective in learning an intended program from very few examples. Both\\ncuts and precedence provide a mechanism to the DSL designer to restrict search to a reasonable, and possibly\\nincomplete, space of programs.\\nUsing cuts and gDSLs, we have built FlashFill++, an industrial-strength PBE engine for performing rich\\nstring transformations, including datetime and number manipulations. The FlashFill++ gDSL is designed to\\nenable readable code generation in different target languages including Excel‚Äôs formula language, PowerFx,\\nand Python. We show FlashFill++ is more expressive, more performant, and generates better quality code than\\ncomparable existing PBE systems. FlashFill++ is being deployed in several mass-market products ranging'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 0, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='comparable existing PBE systems. FlashFill++ is being deployed in several mass-market products ranging\\nfrom spreadsheet software to notebooks and business intelligence applications, each with millions of users.\\nCCS Concepts: ‚Ä¢ Software and its engineering ‚ÜíProgramming by example; Domain specific languages.\\n‚àóAuthors in alphabetic order\\nAuthors‚Äô addresses: Jos√© Cambronero, Microsoft, USA, jcambronero@microsoft.com; Sumit Gulwani, Microsoft, USA,\\nsumitg@microsoft.com; Vu Le, Microsoft, USA, levu@microsoft.com; Daniel Perelman, Microsoft, USA, danpere@microsoft.\\ncom; Arjun Radhakrishna, Microsoft, USA, arradha@microsoft.com; Clint Simon, Microsoft, USA, clint.simon@microsoft.\\ncom; Ashish Tiwari, Microsoft, USA, astiwar@microsoft.com.\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 0, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,\\ncontact the owner/author(s).\\n¬© 2023 Copyright held by the owner/author(s).\\n2475-1421/2023/1-ART33\\nhttps://doi.org/10.1145/3571226\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 1, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:2\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nAdditional Key Words and Phrases: programming by example, domain-specific languages, string transforma-\\ntions\\nACM Reference Format:\\nJos√© Cambronero, Sumit Gulwani, Vu Le, Daniel Perelman, Arjun Radhakrishna, Clint Simon, and Ashish\\nTiwari. 2023. FlashFill++: Scaling Programming by Example by Cutting to the Chase. Proc. ACM Program.\\nLang. 7, POPL, Article 33 (January 2023), 30 pages. https://doi.org/10.1145/3571226\\n1\\nINTRODUCTION\\nProgramming-by-examples (PBE) has seen tremendous interest and progress in the last decade [Gul-\\nwani et al. 2017]. A variety of approaches have been proposed targeting various applications. Starting\\nfrom purely symbolic techniques, the field has explored neural [Devlin et al. 2017] and neurosym-\\nbolic approaches [Chaudhuri et al. 2021; Kalyan et al. 2018; Rahmani et al. 2021; Verbruggen et al.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 1, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='from purely symbolic techniques, the field has explored neural [Devlin et al. 2017] and neurosym-\\nbolic approaches [Chaudhuri et al. 2021; Kalyan et al. 2018; Rahmani et al. 2021; Verbruggen et al.\\n2021]. In this paper, we present novel symbolic techniques to improve the scalability of PBE systems.\\nPBE applications range from enabling non-experts to author programs for spreadsheet data\\nmanipulation [Gulwani et al. 2012] or application creation in a low-code/no-code setting [Lukes\\net al. 2021], to improving productivity of data scientists for data wrangling tasks [Le and Gulwani\\n2014; Miltner et al. 2018] and even automating professional developers‚Äô repeated edits [Pan et al.\\n2021; Rolim et al. 2017]. A flagship application for PBE is that of string transformations, for\\ninstance, converting ‚ÄòAlan Turing‚Äô to ‚Äòturing, alan‚Äô‚Äîsuch tasks are very common and are\\nwell described by examples [Gulwani 2011]. We focus on such string transformations, though our'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 1, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='instance, converting ‚ÄòAlan Turing‚Äô to ‚Äòturing, alan‚Äô‚Äîsuch tasks are very common and are\\nwell described by examples [Gulwani 2011]. We focus on such string transformations, though our\\ntechnical contributions are more generally applicable to any grammar-based synthesis setting.\\nMost PBE engines work by defining a program search space, and then employing some strategy\\nto search over it. The program space is often defined by a domain-specific language (DSL), which\\nfixes a finite set of operators and all the different ways they can be composed to create programs. A\\nkey challenge in PBE is scaling the search to very large program spaces. DSL designers have to\\nbuild DSLs that are expressive enough to be useful, yet small enough to keep the program search\\nspace small. This tension in DSL design has hindered broader applications of program synthesis.\\nUsers implicitly advocate for larger DSLs as they want synthesizers to produce programs that are'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 1, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='space small. This tension in DSL design has hindered broader applications of program synthesis.\\nUsers implicitly advocate for larger DSLs as they want synthesizers to produce programs that are\\ncloser to the ones they would manually write, i.e., ones that use a large variety of functions that\\nare available in general purpose programming languages. On the other hand, these larger DSLs\\n(a) make the search space large and the synthesis slow, and (b) more importantly, allow the large\\nnumber of functions to be combined in unintuitive ways to produce undesirable programs. Program\\nsynthesis research has mainly focused on completeness, i.e., ensuring that we find a program when\\none exists, and insisting on completeness for large DSLs exacerbates these problems. We introduce\\ntwo new mechanisms, cuts and precedence, by which DSL designers can control the program search\\nspace even as the DSL itself grows in size. This not only eases the job of the DSL designer, but also'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 1, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='two new mechanisms, cuts and precedence, by which DSL designers can control the program search\\nspace even as the DSL itself grows in size. This not only eases the job of the DSL designer, but also\\nenables them to build synthesizers based on very expressive DSLs.\\nChallenges. Let us say we are given an input-output example: a tuple of input values and one\\noutput value. There are two commonly used search strategies to find programs that would generate\\nthe output value using the input values: bottom-up and top-down.\\nBottom-up (BU) search starts with the inputs and generates all possible values that can be computed\\nfrom the inputs using all possible (partial) programs in the search space. It does so by applying the\\nexecutable semantics functions of the DSL operators. Thus, information flows from the inputs, and all\\ncomputed intermediate results are completely oblivious to the output. The BU strategy is effective'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 1, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='computed intermediate results are completely oblivious to the output. The BU strategy is effective\\nonly when the sets of values generated in each intermediate stage remains small. This happens\\nwhen there are only a small number of leaf constants and each operator is effectively-enumerable\\n(EE) ‚Äì namely, it has a small arity and many-to-one semantics, thus having a small dynamic fan-out.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 2, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:3\\nTop-down (TD) search starts with the output and applies the inverse semantics of the operators,\\nso-called witness functions, to generate intermediate values that can generate the output value. Thus,\\ninformation flows from the output value, and at every stage, the intermediate values are computed\\nsolely based on the output and are completely oblivious to the input values. The TD strategy is\\neffective only when these intermediate sets are small. This happens only when every operator is\\neffectively invertible (EI) ‚Äì namely, it allows for effective (inverse) computation of various inputs\\nthat can yield a given output.\\nIf the DSL has both non-EE and non-EI operators, then neither bottom-up nor top-down strategies\\nare effective. Recently, it was observed that one could scale synthesis to larger DSLs by combining\\nthe two strategies [Lee 2021]. If there is a partition of the DSL such that the sub-DSL closer to'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 2, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='are effective. Recently, it was observed that one could scale synthesis to larger DSLs by combining\\nthe two strategies [Lee 2021]. If there is a partition of the DSL such that the sub-DSL closer to\\nthe start symbol has EI operators, and the rest contains EE operators, then the two strategies\\ncan be combined to yield a meet-in-the-middle strategy at that cut [Lee 2021]. However, this new\\nstrategy still has only a limited form of information flow between the inputs and the output. In\\nfact, the DSLs used in Duet [Lee 2021] are relatively small, albeit larger than those in FlashFill and\\nFlashMeta [Gulwani 2011; Polozov and Gulwani 2015]. These latter systems are based on a TD\\nstrategy over very small DSLs.\\nAn alternate way to scale PBE synthesis is based on abstraction and refinement types [Feng et al.\\n2017; Guo et al. 2020; Polikarpova et al. 2016; Wang et al. 2017]. The idea behind abstraction is that'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 2, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='2017; Guo et al. 2020; Polikarpova et al. 2016; Wang et al. 2017]. The idea behind abstraction is that\\ninstead of computing the exact set of values that can be generated (in either top-down or bottom-up\\nstrategy), we compute overapproximations of the sets of values. This approach works when high\\nquality abstractions can be quickly generated. Typically, it is still ‚Äúone-sided‚Äù ‚Äì either the inputs flow\\nto intermediate values or the output value flows backwards to intermediate values. Furthermore,\\nabstractions of compositions of operators are computed by composing the abstractions of operators,\\nwhich loses accuracy as the composition depth grows. We overcome some of these shortcomings\\nin our work; however, abstraction-based approaches are inherently complementary.1\\nOur Contribution. In this paper, we present two novel techniques - cuts and precedence - to\\neffectively address the scalability challenges of PBE synthesis. The executable semantics functions'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 2, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Our Contribution. In this paper, we present two novel techniques - cuts and precedence - to\\neffectively address the scalability challenges of PBE synthesis. The executable semantics functions\\n(that are the basis of BU) and the witness functions (that are basis of TD) are defined to allow\\ninformation to flow in one direction. We introduce cuts that prune values generated by witness\\nfunctions guided by the values that sub-DSLs could possibly compute on the inputs. This concept\\ninherently builds in bi-directional information flow in its definition, and in fact, generalizes the\\nsemantics functions and witness functions. A DSL designer can author a cut function based on their\\nintuition of the form of values that can be computed at a nonterminal using the inputs, and then\\nrestricting them to those that would be relevant for the output. Unlike abstractions, cuts are not\\ncomputed compositionally. They are provided for whole sub-DSLs; thus, they avoid information loss'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 2, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='restricting them to those that would be relevant for the output. Unlike abstractions, cuts are not\\ncomputed compositionally. They are provided for whole sub-DSLs; thus, they avoid information loss\\naccumulated by composing lossy abstractions. This is similar to how accelerations avoid information\\nloss in program analysis by capturing the effect (composition or transitive closure) of multiple state\\ntransitions by a single ‚Äúmeta transition‚Äù [Finkel 1987; Karp and Miller 1969].\\nA top-down strategy would get stuck at a non-EI operator. However, a cut function for an\\nargument of that non-EI operator can help unblock TD synthesis. As a special case, a cut for that\\nargument can be generated using bottom-up enumeration, in which case we get the meet-in-the-\\nmiddle strategy [Lee 2021]. However, cuts may be generated by other means based on the DSL\\ndesigner‚Äôs insight. In general, we get a novel search strategy, middle-out synthesis, which uses cuts'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 2, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='middle strategy [Lee 2021]. However, cuts may be generated by other means based on the DSL\\ndesigner‚Äôs insight. In general, we get a novel search strategy, middle-out synthesis, which uses cuts\\n1In this paper, we focus exclusively on synthesis approaches based on concrete values: the specification is a concrete IO\\nexample, and the semantics functions (and the inverse semantics) are given on concrete values (and not abstract values or\\nrefinement types). More specifically, we are in the context of version-space algebra (VSA) driven synthesis, and hence the\\nterms top-down and bottom-up are always used in that context.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 3, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:4\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nto reduce the original PBE problem over a large DSL (with potentially non-EI and non-EE operators)\\ninto simpler PBE problems over (smaller depth) sub-DSLs with only EI or only EE operators.\\nOur second key idea is to introduce precedence over operators in the grammar of domain-specific\\nlanguages. Precedence is a natural concept in grammars and arises naturally when the DSL designer\\nwants to prefer certain operators over others. We show that if the precedence is a series-parallel\\npartial order, then it can be encoded as an ordering on grammar rules to create a guarded DSL (gDSL),\\nand program search can be performed directly on the gDSL without compromising soundness or\\ncompleteness, while gaining efficiency. The ordering on rules in a gDSL is interpreted as a mandate\\nto explore a certain branch only when higher-ordered branches have failed to return a result. This'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 3, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='completeness, while gaining efficiency. The ordering on rules in a gDSL is interpreted as a mandate\\nto explore a certain branch only when higher-ordered branches have failed to return a result. This\\nhas two major advantages. First, it makes the search more scalable by dynamically using different\\nunderapproximations of the DSL to be explored. Second, it makes ranking simpler to write for DSL\\ndesigners because the precedence already builds in a default ranking over programs.\\nCuts and precedence provide DSL designers two new mechanisms to control the program search\\nspace, beyond what they get through designing DSLs. Our contributions include:\\n‚Ä¢ A new algorithmic approach for PBE (middle-out synthesis) that leverages a novel cut rule to\\nspeed up synthesis over large DSLs and to handle non-EI and non-EE operators.\\n‚Ä¢ A new formalism of guarded DSLs that supports operator precedence, and an extension of'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 3, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='speed up synthesis over large DSLs and to handle non-EI and non-EE operators.\\n‚Ä¢ A new formalism of guarded DSLs that supports operator precedence, and an extension of\\nour synthesis approach to gDSLs that scales synthesis to large DSLs and gives ranking based\\non path orderings [Dershowitz and Jouannaud 1990] for free.\\n‚Ä¢ A new and expressive system FlashFill++ for string transformations that supports datetime &\\nnumber manipulations and is designed for readable code generation.\\n‚Ä¢ An extensive comparison of FlashFill++ with existing state-of-the-art PBE systems for string\\ntransformations (FlashFill [Gulwani 2011], SmartFill [Chen et al. 2021a], and Duet [Lee 2021])\\nthat shows improvements in expressiveness, learning performance, and code readability.\\n2\\nOVERVIEW\\n2.1\\nNew Challenges in PBE for String Transformations\\nWe first discuss some challenges faced by the current generation of PBE tools for string trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 3, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='2\\nOVERVIEW\\n2.1\\nNew Challenges in PBE for String Transformations\\nWe first discuss some challenges faced by the current generation of PBE tools for string trans-\\nformations. These challenges were compiled in collaboration with two key industrial deployers\\nof PBE: the Microsoft Excel and the Microsoft PowerBI teams. To compile these challenges, we\\ninterviewed several product managers in these teams, interacted with both expert and novice users\\nof the FlashFill feature, and analyzed online help posts.\\nGenerating Readable Code. Consider the task of transforming the input pair (\"David Walker\",\\n\"623179\") to the output string \"D-6231#walker\". Any string processing library would contain\\nmany redundant methods for extracting \"Walker\" from \"David Walker\". For example, in Python,\\nwe could use the split method to accomplish the task. Alternatively, we could use the find method\\nalong with string slicing, or use regular expressions.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 3, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='we could use the split method to accomplish the task. Alternatively, we could use the find method\\nalong with string slicing, or use regular expressions.\\nIn contrast to the design of string processing libraries, the prevailing wisdom in DSL design for\\nsynthesis has been to work with a minimal number of operators [Gulwani 2016]. For example,\\nFlashFill [Gulwani 2011] and the more recent Duet [Lee 2021] DSLs contain only 3 and 5 functions\\nthat directly operate on strings, respectively. Smaller DSLs lead to smaller program search spaces,\\nyielding better synthesis performance and effective ranking [Polozov and Gulwani 2015]. Following\\nthis minimalism to an extreme can lead to a DSL whose programs translate to very unnatural and\\nunreadable programs in target languages like Python (see Figure 1) or PowerFx (see Figure 2).\\nOne straightforward approach to readability is writing a good translator from the synthesis DSL'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 3, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='unreadable programs in target languages like Python (see Figure 1) or PowerFx (see Figure 2).\\nOne straightforward approach to readability is writing a good translator from the synthesis DSL\\nto the target language. However, if the semantic gap between the DSL and the target languages‚Äô\\noperators is large, then ‚Äúreadable translation‚Äù itself becomes a new and nontrivial synthesis problem.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 4, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:5\\n# Python program generated by FlashFill\\nimport regex\\ndef transformation_text_program(input1 , input2 ):\\ncomputed_value_83 = regex.search(r\"\\\\p{Lu}+\", input1 ). group (0)\\nindex1_83 = regex.search(r\"[-.\\\\p{Lu}\\\\p{Ll}0 -9]+\", input2 ). start ()\\ncomputed_value_0_83 = input2[index1_83 :( len(input2) + -2)]\\nkth_match_83 =\\nl i s t (regex.finditer(r\"[-.\\\\p{Lu}\\\\p{Ll}0 -9]+\", input1 ))[ -1]\\ncomputed_value_1_83 = kth_match_83.group (0). lower ()\\nreturn computed_value_83+\"-\"+computed_value_0_83+\"#\"+computed_value_1_83\\n# Python program generated by FlashFill ++\\ndef formula(i1 , i2):\\ns1 = i1[:1]\\ns2 = i2[:4]\\ns3 = i1.split(\" \")[1]. lower()\\nreturn s1 + \"-\" + s2 + \"#\" + s3\\n# Python program generated by FlashFill ++ and renamed by Codex\\ndef formula(name , number ):\\nfirst_initial = name [:1]\\nnumber_prefix = number [:4]\\nlast_name = name.split(\" \")[1]. lower()\\nreturn first_initial + \"-\" + number_prefix + \"#\" + last_name'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 4, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='def formula(name , number ):\\nfirst_initial = name [:1]\\nnumber_prefix = number [:4]\\nlast_name = name.split(\" \")[1]. lower()\\nreturn first_initial + \"-\" + number_prefix + \"#\" + last_name\\nFig. 1. The python programs generated by FlashFill and FlashFill++ for the task of transforming (\"David\\nWalker\", \"623179\") to \"D-6231#walker\". FlashFill++‚Äôs program is much more readable and its readability\\nis further improved by renaming variables using a pretrained large language model, as shown.\\nOur insight is that to effectively generate readable code the DSLs should not be designed with the\\nsingle-minded goal of efficient learning, but also pay heed to the target languages.\\nWhile generating readable code is challenging, the need is sorely felt in industrial PBE tools‚Äî\\nusers are more likely to trust and use PBE tools if they produce idiomatic, readable code. Quoting\\none study participant in [Drosos et al. 2020]: ‚Äúdon‚Äôt know what is going on there, so I don‚Äôt know if I'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 4, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='one study participant in [Drosos et al. 2020]: ‚Äúdon‚Äôt know what is going on there, so I don‚Äôt know if I\\ncan trust it if I want to extend it to other tasks. I saw my examples were correctly transformed, but\\nbecause the code is hard to read, I would not be able to trust what it is doing‚Äù. The lack of readable\\ncode is one of the primary challenges preventing a broader adoption of PBE technologies.\\nMultiple Target Languages. The need for readable code generation is compounded by the\\nproliferation of different target languages, each with their own set of operations; see Figure 3.\\nThese target languages range across standard programming languages (e.g., Python, R), individual\\nlibraries (e.g., Pandas, PySpark), data query languages (e.g., SQL), and custom application-specific\\nlanguages (e.g., Google Sheets & Excel formula languages, PowerBI‚Äôs M language). Apart from the\\nobvious benefit, multiple target support can also help with learning: seeing the same program in'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 4, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='languages (e.g., Google Sheets & Excel formula languages, PowerBI‚Äôs M language). Apart from the\\nobvious benefit, multiple target support can also help with learning: seeing the same program in\\nmultiple languages helps with cross-language knowledge transfer [Shrestha et al. 2018].\\nDate-Time and Numeric Transformations. Most string PBE technologies cannot natively\\nhandle date-time and numeric operations efficiently, leading to situations like transforming ‚Äòjan‚Äô\\nto ‚ÄòJanember‚Äô given the input-output example ‚Äònov‚Äô ‚Ü¶‚Üí‚ÄòNovember‚Äô. Duet [Lee 2021] does allow\\nfor limited numeric operators, but still lacks support for important data-processing operations\\nsuch as rounding and bucketing. According to the Microsoft Excel team, date-time and numeric\\noperations (of the kind shown in Figure 3) are among the most requested FlashFill features. However,\\nas illustrated in Section 2.2, these operations are not amenable to standard synthesis techniques.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 4, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='operations (of the kind shown in Figure 3) are among the most requested FlashFill features. However,\\nas illustrated in Section 2.2, these operations are not amenable to standard synthesis techniques.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 5, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:6\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\n# PowerFx formula generated by FlashFill\\nConcatenate(\\nMid(Left(input1 , Match(input1 , \"\\\\p{Lu}+\"). StartMatch\\n+ Len(Match(input1 , \"\\\\p{Lu}+\"). FullMatch) - 1),\\nMatch(input1 , \"\\\\p{Lu}+\"). StartMatch),\\nConcatenate(\"-\",\\nConcatenate(Mid(Left(input2 ,Len(input2)-2), Match(input2 ,\"[0 -9]+\"). StartMatch),\\nConcatenate(\"#\",\\nLower(Mid(\\nLeft(input1 ,\\nFirst(LastN(MatchAll(input1 , \"[\\\\p{Lu}\\\\p{Ll}]+\"), 1)). StartMatch\\n+ Len(First(LastN(MatchAll(input1 , \"[\\\\p{Lu}\\\\p{Ll}]+\"), 1)). FullMatch )-1),\\nLast(MatchAll(input1 , \"[\\\\p{Lu}\\\\p{Ll}]+\")). StartMatch ))))))\\n# PowerFx formula generated by FlashFill ++\\nLeft(input1 , 1) & \"-\" & Left(input2 , 4) & \"#\"\\n& Lower(Last(FirstN(Split(input1 , \" \"), 2)). Result)\\nFig. 2. PowerFx formulas generated by FlashFill and FlashFill++ to transform (\"David Walker\", \"623179\") into\\n\"D-6231#walker\". The latter is much more readable than the former.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 5, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Fig. 2. PowerFx formulas generated by FlashFill and FlashFill++ to transform (\"David Walker\", \"623179\") into\\n\"D-6231#walker\". The latter is much more readable than the former.\\nPerforming operations over datetimes and numbers allows our system to handle use cases like\\nthe one detailed in Fig. 4(a), which presents 911 call records that need to be transformed. Each call\\nlog, shown in the Input column, contains an (optional) address, the township, the call date and time,\\nfollowed by possible annotations indicating the specific 911 station that addressed the call. Let us\\nsuppose that a data scientist wants to extract the date (2015-12-11) and time (13:34:52) from each log,\\nand map it to the corresponding weekday (Fri) and the 3-hour window (12PM - 3PM), as shown in\\nthe Output column. Performing this transformation requires string processing to extract candidate\\ndates and times, parsing these substrings into appropriate datatypes, performing type-specific'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 5, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='the Output column. Performing this transformation requires string processing to extract candidate\\ndates and times, parsing these substrings into appropriate datatypes, performing type-specific\\ntransformations on the extracted values, and then formatting them into an appropriate output\\nstring value. This is beyond the capabilities of current synthesizers. Our system can synthesize the\\nintended program (shown in Fig. 4(b)) from just the first example. This program is readable and\\nalso serves educational value (e.g. teaching the API of the popular datetime Python library).\\n2.2\\nOverview of FlashFill++\\nWe now show how our novel techniques address the various challenges from subsection 2.1.\\nExtended Domain-Specific Language. The main strength of FlashFill++ compared to previous\\nsystems is its expanded DSL containing over 40 operators, including 25 for just strings and the rest\\nfor datetime and numbers, such as for rounding and bucketing; see Figure 7. Contrast this with the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 5, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='systems is its expanded DSL containing over 40 operators, including 25 for just strings and the rest\\nfor datetime and numbers, such as for rounding and bucketing; see Figure 7. Contrast this with the\\nnumbers 3 and 5 mentioned previously for FlashFill and Duet.\\nThis extended DSL supports more expressive and more readable programs. Contrast the code\\ngenerated by FlashFill++ in Fig. 1 and 2 to that generated by FlashFill to see the clear difference an\\nextended DSL makes. However, expanding the DSL comes with its own set of challenges: (a) Given\\nthe larger search space, standard synthesis techniques fall short on efficiency. (b) The larger search\\nspace also complicates ranking‚Äîthe problem of picking the best (or intended) program among all\\nthe ones consistent with the examples. (c) Handling numeric and date-time operators requires new\\nsynthesis techniques. Next, we discuss some novel strategies to address these challenges.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 5, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='the ones consistent with the examples. (c) Handling numeric and date-time operators requires new\\nsynthesis techniques. Next, we discuss some novel strategies to address these challenges.\\nCuts and Middle-Out Synthesis. Our new DSL contains several non-EI operators (required for\\nnumber and datetime operations) that inhibit use of a top-down synthesis strategy across those\\noperators. Furthermore, bottom-up synthesis is not feasible for the sub-DSLs below those operators\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 6, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:7\\nRound to last day of month: 2/5/2020 =‚áí2/29/2020\\nfrom datetime import datetime\\nfrom dateutil.relativedelta import *\\ndef formula(i1):\\nmonth_start = datetime(i1.year ,i1.month ,1)\\nmonth_end = month_start\\n+ relativedelta(months =1)\\nreturn month_end - relativedelta(days =1)\\nEOMONTH(A1)\\nWith({ monthStart:\\nDate(Year(i1), Month(i1), 1)\\n},\\nDateAdd(\\nDateAdd(\\nmonthStart , 1, \"\" Months \"\"),\\n-1, \"\"Days\"\"\\n))\\nRound to start of quarter: 2/5/2020 =‚áí1/1/2020\\nfrom datetime import datetime\\ndef formula(i1):\\nquarter = (i1.month - 1) // 3 + 1\\nreturn datetime(i1.year ,3* quarter -2,1)\\nEOMONTH(\\nDATE(YEAR(A1),\\nROUNDUP(MONTH(A1)/3,\\n0)*3,1),\\n0)\\nWith({ quarter:\\nRoundUp(Month(i1) / 3, 0)\\n},\\nDate(Year(i1),quarter *3-2,1)\\n+ Time(0, 0, 0))\\nDays since start of year: 2/5/2029 =‚áí36\\ndef formula(i1):\\nreturn i1.timetuple (). tm_yday\\nA1 - DATE(YEAR(A1), 1, 1) + 1\\nDateDiff(\\nDate(Year(i1),1,1),i1) + 1\\nCreate year-quarter string: 4/5/1983 =‚áí‚Äò1983-Q2‚Äô'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 6, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='def formula(i1):\\nreturn i1.timetuple (). tm_yday\\nA1 - DATE(YEAR(A1), 1, 1) + 1\\nDateDiff(\\nDate(Year(i1),1,1),i1) + 1\\nCreate year-quarter string: 4/5/1983 =‚áí‚Äò1983-Q2‚Äô\\nfrom datetime import datetime\\ndef formula(i1):\\nquarter = (i1.month -1)//3 + 1\\nreturn i1.strftime(\"%Y\") +\\n\"-Q\"+f\"{quarter :01.0f}\"\\nYEAR(A1) & \"-Q\" &\\n& ROUNDUP(MONTH(A1)/3, 0)\\nText(i1 , \"yyyy\", \"en -US\")\\n& \"-Q\" &\\nText(\\nRoundUp(Month(i1) /3, 0),\\n\"0\",\\n\"en -US\")\\nExtract number, convert and round: ‚ÄòYour Total: $1,2564.45‚Äô =‚áí12564.5ùëë\\nfrom decimal import *\\ndef formula(i1):\\nsource = Decimal( str ( float (\\ni1.split(\"$\")[-1]. replace(\",\", \"\"))))\\ndelta = Decimal(\"0.5\")\\nreturn\\nfloat (( source / delta)\\n.quantize(0, ROUND_CEILING) * delta)\\nROUNDUP(\\nNUMBERVALUE(\\nRIGHT(A1 ,\\nLEN(A1)-FIND(\"$\", A1))\\n) / 0.5,\\n0\\n) * 0.5\\nRoundUp(Value(\\nLast(\\nSplit(i1 , \"$\")\\n).Result ,\\n\"en -US\"\\n) * 2, 0) / 2\\nFig. 3. Code produced by FlashFill++ for various date-time and rounding scenarios in Python (left), Excel\\n(center), and PowerFx (right) respectively.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 6, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Split(i1 , \"$\")\\n).Result ,\\n\"en -US\"\\n) * 2, 0) / 2\\nFig. 3. Code produced by FlashFill++ for various date-time and rounding scenarios in Python (left), Excel\\n(center), and PowerFx (right) respectively.\\ndue to enumeration blowup, rendering a meet-in-the-middle strategy infeasible. We propose a\\nnovel middle-out synthesis strategy that uses cuts to deal with such non-EI operators.\\nConsider the number parsing, rounding, and formatting subset of the FlashFill++ DSL below\\ndecimal roundNumber := RoundNumber(parseNumber, roundNumDesc)\\ndecimal parseNumber := ParseNumber(substr, locale) | ...\\nstring substr\\n:= ...\\nFix the input-output example ‚ü®‚ÄúThe price is $24.58 and 46 units are available.‚Äù ‚Ü¶‚Üí\\n24.00‚ü©for the non-terminal roundNumber. We first discuss the short-comings of both bottom-\\nup and top-down synthesis in this case.\\nTop-Down Synthesis using Witness Functions. In FlashMeta-style programming-by-example, the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 6, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='up and top-down synthesis in this case.\\nTop-Down Synthesis using Witness Functions. In FlashMeta-style programming-by-example, the\\nprimary deductive tools are witness functions. Given a specification in the form of an input-output\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 7, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:8\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\n(a)\\nInput\\nOutput\\nCEDAR AVE & COTTAGE AVE; HORSHAM; 2015-12-11 @ 13:34:52;\\nFri, 12PM - 3PM\\nRT202 PKWY; MONTGOMERY; 2016-01-13 @ 09:05:41-Station:STA18;\\nWed, 9AM - 12PM\\n; UPPER GWYNEDD; 2015-12-11 @ 21:11:18;\\nFri, 9PM - 12AM\\n(b)\\ndef derive_value(_input ):\\ntext = _input.split(\";\")[2]\\npart_0 = text.split(\" \")[0]; part_1 = text.split(\" \")[2][:8]\\ndate = datetime.datetime.strptime(part_0 , \"%Y-%m-%d\")\\ntime = datetime.datetime.strptime(part_1 , \"%H:%M:%S\")\\nbase_value = datetime.timedelta(hours=time.hour , minutes=time.minute ,\\nseconds=time.second , microseconds=time.microsecond)\\ndelta_value = datetime.timedelta(hours =3)\\ntime_str = (time - base_value % delta_value ). strftime(\"%#I%p\")\\nrounded_up_next = (time - base_value % delta_value) + delta_value\\ncomputed_value = time_str + \"-\" + rounded_up_next.strftime(\"%#I%p\")\\nreturn date.strftime(\"%a\") + \", \" + computed_value'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 7, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='rounded_up_next = (time - base_value % delta_value) + delta_value\\ncomputed_value = time_str + \"-\" + rounded_up_next.strftime(\"%#I%p\")\\nreturn date.strftime(\"%a\") + \", \" + computed_value\\nFig. 4. (a) A task to map the 911-call logs in the Input column to weekday/time buckets in the Output column.\\n(b) Python function synthesized by our approach for this task from just one example.\\nexample ùëñ‚Ü¶‚Üíùëúand a top-level operator ùêπ, a witness function for position ùëògenerates a sub-\\nspecification for the ùëòùë°‚Ñéparameter for ùêπ. For example, if the top-level operator is concat(ùëÅ1, ùëÅ2)\\nand the input-output example is ùëñ‚Ü¶‚Üí‚Äúabc‚Äù, the witness function for the 1ùë†ùë°position will return\\n{‚Äúa‚Äù, ‚Äúab‚Äù} (assuming we do not consider the trivial case of appending an empty string). In the\\nexample we are considering, writing a witness functions for the RoundNumber operator is not as\\nstraight-forward‚Äîthere are an infinite set of numbers that can round to 24.00. A standard top-down'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 7, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='example we are considering, writing a witness functions for the RoundNumber operator is not as\\nstraight-forward‚Äîthere are an infinite set of numbers that can round to 24.00. A standard top-down\\nprocedure cannot handle this infinite-width witness function.\\nBottom-Up Synthesis. The other major paradigm for programming-by-example is bottom-up syn-\\nthesis: here, the synthesizer starts enumerating programs ‚Äì starting from constants and iteratively\\napplying operators from the grammar on the previously generated programs ‚Äì and checks if any\\nof the enumerated programs satisfies the given input-output example. An efficient bottom-up\\nsynthesizer will avoid enumerating all programs using observational equivalence‚Äîthat is, it only\\ngenerate programs that produce different outputs for the given input. In our running example, the\\nsynthesizer will begin by generating substr sub-programs and concrete values for locale and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 7, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='generate programs that produce different outputs for the given input. In our running example, the\\nsynthesizer will begin by generating substr sub-programs and concrete values for locale and\\nroundNumberDesc. However, enumerating such sub-programs is expensive‚Äîthe fragment of the\\nDSL reachable from the non-terminal substr is large. In fact, string operations like substr are\\nbest handled using witness functions.\\nMiddle-Out Synthesis using Cuts. Examining our input-output example by hand, it is easy to see that\\nin any valid program the output of ParseNumber should be derived from a numerical substring\\nin the input. Intuitively, it does not matter what or how complex the substr sub-program is; we\\ncan be confident that the output of the ParseNumber will be either 24.58 or 46. Cuts capture this\\nsimple intuition‚Äîfor a given input-output example ùëñ‚Ü¶‚Üíùëúand a non-terminal ùëÅthat expands to\\nùëì(ùëÅ1, ùëÅ2), the cut for ùëÅ1 (say) in the context of ùëÅwill be a set of values {ùëú1,ùëú2, . . . ,ùëúùëõ} such that'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 7, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='simple intuition‚Äîfor a given input-output example ùëñ‚Ü¶‚Üíùëúand a non-terminal ùëÅthat expands to\\nùëì(ùëÅ1, ùëÅ2), the cut for ùëÅ1 (say) in the context of ùëÅwill be a set of values {ùëú1,ùëú2, . . . ,ùëúùëõ} such that\\nin any desired program ùëÉgenerated by ùëÅ, the output of the sub-program corresponding to ùëÅ1 will\\nbe one of the ùëúùëòfor ùëò‚àà{1, 2, . . . ,ùëõ}.\\nGiven that the output of ParseNumber will be either 24.58 or 46, the synthesizer has two sub-\\ntasks for the 24.58 case (the 46 case will be similar): (a) synthesizing a program for ùëùùëéùëüùë†ùëíùëÅùë¢ùëöùëèùëíùëü\\nfor the example ùëñ‚Ü¶‚Üí24.58, and (b) synthesizing a program for ùëüùëúùë¢ùëõùëëùëÅùë¢ùëöùëèùëíùëüfor the example\\nùëñ‚Ü¶‚Üí24.00 using a modified DSL, which is generated dynamically in middle-out synthesis, where\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 8, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:9\\nparseNumber ‚Üí24.58 is the only rule whose head is parseNumber. Both sub-tasks can now be\\nrecursively solved, possibly using either the top-down strategy or the bottom-up strategy.\\nGuarded Context-Free Grammars. The larger program space resulting from an extended DSL\\nwith a wide range of operators poses both efficiency and ranking challenges. However, human\\nprogrammers often encounter the same challenge when writing their own implementations and\\ndecide between these operators and programs using simple rules of thumb, which can be leveraged\\nboth for improving search efficiency and ranking. For example, ‚Äúif a task can be done using a date-\\ntime function, do not use string transformation functions‚Äù or ‚Äúif a task can be done using string\\nindexing, do not use regular expressions‚Äù. To mimic this kind of coarse reasoning, we introduce\\nthe notion of gDSLs. In a gDSL, the production rules for each non-terminal are ordered (with a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 8, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='indexing, do not use regular expressions‚Äù. To mimic this kind of coarse reasoning, we introduce\\nthe notion of gDSLs. In a gDSL, the production rules for each non-terminal are ordered (with a\\npartial order |‚ä≤), with production rules earlier in the order preferred to ones later in the order. For\\nexample, the rule concat := segment |‚ä≤Concat(segment, concat) expresses that we always\\nprefer programs that do not use the Concat operation to ones that do. During synthesis for a gDSL\\nrule ùëÅ‚Üíùõº|‚ä≤ùõΩthe branch ùõΩis explored only if the branch ùõºfails to produce a program. This\\ngreatly improves the performance of synthesis and the FlashFill++ synthesis times are competitive\\nwith other synthesis techniques that work with significantly smaller DSLs.\\nApart from improving the efficiency of search, gDSLs also simplify the task of writing ranking\\nfunctions. Intuitively, the precedence in the guarded rules induce a ranking on programs, and any'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 8, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Apart from improving the efficiency of search, gDSLs also simplify the task of writing ranking\\nfunctions. Intuitively, the precedence in the guarded rules induce a ranking on programs, and any\\nadditional ranking function only needs to order the remaining incomparable programs. Precedences\\nrank programs by a lexicographic path ordering (LPO) [Dershowitz and Jouannaud 1990], and our\\nfinal ranker will be a lexicographic combination of LPO and base arithmetic ranker ‚Äì such program\\nrankers have not been used in program synthesis before.\\n3\\nBACKGROUND: PROGRAMMING BY EXAMPLE\\nWe now define the problem of programming-by-example and discuss common solutions.\\n3.1\\nDomain-Specific Languages\\nWe use domain-specific languages (DSLs) to specify the set of target programs for a synthesizer.\\nFormally, a DSL D is given by ‚ü®N, T, F, R, Vin, ùë£out‚ü©where:\\n‚Ä¢ N is a finite set of non-terminal symbols (or non-terminals). The symbol ùë£out is a special start'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 8, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Formally, a DSL D is given by ‚ü®N, T, F, R, Vin, ùë£out‚ü©where:\\n‚Ä¢ N is a finite set of non-terminal symbols (or non-terminals). The symbol ùë£out is a special start\\nnon-terminal in N that represents the output of a program in the DSL.\\n‚Ä¢ T is a set of terminal symbols (or terminals) that is partitioned as Vin ‚à™O into inputs Vin and\\nvalues O. The set Vin contains special terminals, in1, in2, . . ., that represent the input symbols\\nin a program of the DSL. The set O contains constant values.\\n‚Ä¢ F is a finite set of function symbols (or operations). Each operation f ‚ààF has a fixed arity\\nArity(f). The semantics of f, denoted by JfK, is a mapping from OArity(f) to O.\\n‚Ä¢ R is a set of rules of the form ùëÅ‚Üíf(ùë£1, . . . , ùë£ùëò) or ùëÅ‚Üíùë£0 where ùëÅ‚ààN, f ‚ààF , ùë£0 ‚ààT,\\nand ùë£1, . . . , ùë£ùëò‚ààN ‚à™T.\\nThe formalism above is untyped for ease of reading. In practice, the FlashFill++ DSL is typed‚Äî values\\ncan be integers, floats, strings, Booleans, and date-time objects, and each non-terminal, terminal,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 8, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='can be integers, floats, strings, Booleans, and date-time objects, and each non-terminal, terminal,\\nand operator have specific type signatures.\\nEvery terminal or nonterminal ùë£generates a set L(ùë£) of programs defined recursively as follows:\\n(a) L(inùëò) = {inùëò} for all input symbols inùëò‚ààVin, (b) L(ùëú) = {ùëú} for all values ùëú‚ààO, and\\n(c) L(ùëÅ) =\\n\\x08f(ùëÉ1, . . . , ùëÉùëõ) | ùëÅ‚Üíf(ùë£1, . . . , ùë£ùëõ) ‚ààR, ‚àÄùëñ.ùëÉùëñ‚ààL(ùë£ùëñ)\\n\\t\\n‚à™\\n\\x08\\nùëÉ| ùëÅ‚Üíùë£‚ààR, ùë£‚ààT, ùëÉ‚àà\\nL(ùë£)\\n\\t\\n. The set of programs defined by the whole DSL L(D) is L(ùë£out).\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 9, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:10\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nExample 3.1. The following is a simple DSL Dùê¥for affine arithmetic expressions over naturals\\nN: (a) Here, the terminals T are {input1, input2, 0, 1, 2, . . .}, with input1, input2 being the spe-\\ncial input terminals in Vin. (b) The non-terminals N are {output, addend, const}, with output\\nbeing the output symbol ùë£out. (c) The operators F are Plus and Times. (d) The rules R are given\\nby {output ‚ÜíPlus(addend, output), output ‚Üíconst, addend ‚ÜíTimes(const, input1),\\naddend ‚ÜíTimes(const, input2), const ‚Üí0 | 1 | 2 | . . .}. Note that the declaration uint\\nconst; in the listing is shorthand for a set of rules for the form const ‚Üíùëòfor each ùëò‚ààN.\\nPlus(Times(5, input1), 3) is a sample program in L(Dùê¥).\\n‚ñ°\\n@input uint input1, input2;\\n@start uint output := Plus(addend, output) | const;\\nuint addend := Times(const, input1) | Times(const, input2);\\nuint const;\\n3.2\\nSynthesis Tasks and Solutions'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 9, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='‚ñ°\\n@input uint input1, input2;\\n@start uint output := Plus(addend, output) | const;\\nuint addend := Times(const, input1) | Times(const, input2);\\nuint const;\\n3.2\\nSynthesis Tasks and Solutions\\nA state ùëÜis a valuation of all input symbols in a DSL, i.e., ùëÜ= {in1 ‚Ü¶‚Üíùëú1, . . . , inùëò‚Ü¶‚Üíùëúùëò} where inùëñ\\nare input symbols and ùëúùëñare values. An example Ex is a pair ùëÜ‚Ü¶‚Üíùëúof a state ùëÜand value ùëú.\\nA synthesis task ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©is given by: (a) a term ùõº, which is either a nonterminal, terminal, or\\nright-hand side of a rule, (b) a set R of rules, and (c) an example ùëÜ‚Ü¶‚Üíùëú. A solution of the synthesis\\ntask ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©is a program ùëÉsuch that: (a) JùëÉK(ùëÜ) = ùëú, and (b) ùëÉ‚ààL(ùõº). Here, JùëÉK : OVin ‚Ü¶‚ÜíO\\nrepresents the standard semantics of a program. Formally, JùëÉK(ùëÜ) is recursively defined as: (1)\\nJinùëñK(ùëÜ) = ùëÜ(inùëñ), (2) JùëúK(ùëÜ) = ùëúfor every value ùëú, (3) Jf(ùë£1, ùë£2)K(ùëÜ) = JfK(Jùë£1K(ùëÜ), Jùë£2K(ùëÜ)) for every\\noperator f. To keep the presentation simple, the synthesis task is defined to contain one input-output'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 9, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='operator f. To keep the presentation simple, the synthesis task is defined to contain one input-output\\nexample. In practice, a synthesis task often involves multiple examples. It is straight-forward to\\nextend our technique for this, as done in our implementation.\\nExample 3.2. A sample synthesis task for the affine expression DSL Dùê¥is ‚ü®output, R,ùëÜ‚Ü¶‚Üí7‚ü©\\nwhere ùëÜ= {input1 ‚Ü¶‚Üí2, input2 ‚Ü¶‚Üí0}. The program Plus(Times(3, input1), 1) is a solution of\\nthis task. Here Times and Plus have their usual arithmetic semantics.\\n‚ñ°\\nGiven a synthesis task ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, a synthesizer generates a program set PS such that every\\nprogram in the set PS is a solution of the synthesis task, which we denote by the assertion PS |=\\n‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. Note that it is vacuously true that ‚àÖ|= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, and so practical synthesizers\\nstrive to establish the above assertion for nonempty sets PS. The notation Ã∏|= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©denotes\\nthat there is no nonempty set PS that is a solution for the synthesis task.\\n3.3'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 9, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='strive to establish the above assertion for nonempty sets PS. The notation Ã∏|= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©denotes\\nthat there is no nonempty set PS that is a solution for the synthesis task.\\n3.3\\nBottom-Up and Top-Down Synthesis\\nThere are two main approaches for solving the synthesis task: bottom-up and top-down.\\nThe bottom-up (BU) approach enumerates programs generated by different nonterminals of the\\ngrammar and collects the values that those programs compute on the input state ùëÜ. More precisely,\\nfor each nonterminal ùëÅ‚Ä≤, the BU approach computes the bottom-up value set buùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) given by\\n{JùëÉ‚Ä≤K(ùëÜ) | ùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤)}. These sets are computed starting from the leaf (terminals) of the grammar\\nand moving up to the root (start symbol ùë£out). Success is declared if the output value ùëúis found to\\nbe in the set buùë£out (ùëÜ‚Ü¶‚Üíùëú). Note that the BU procedure is not guided by the output value ùëú.\\nThe top-down (TD) approach starts with the output value ùëúthat needs to be generated at start'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 9, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='be in the set buùë£out (ùëÜ‚Ü¶‚Üíùëú). Note that the BU procedure is not guided by the output value ùëú.\\nThe top-down (TD) approach starts with the output value ùëúthat needs to be generated at start\\nsymbol ùë£out, and for every nonterminal ùëÅ‚Ä≤, it computes the set of values that flow to ùëúat ùë£out.\\nMore formally, we say a value ùëú‚Ä≤ at ùëÅ‚Ä≤ flows to value ùëúat ùëÅif either (a) ùëÅ‚Üíùëì(. . . , ùëÅ‚Ä≤, . . .) is a\\ngrammar rule and JùëìK(ùëú1, . . . ,ùëú‚Ä≤, . . . ,ùëúùëò) = ùëúfor some values ùëú1, . . . ,ùëúùëò, or (b) there exist ùëú‚Ä≤‚Ä≤ and\\nùëÅ‚Ä≤‚Ä≤ s.t. ùëú‚Ä≤ at ùëÅ‚Ä≤ flows to ùëú‚Ä≤‚Ä≤ at ùëÅ‚Ä≤‚Ä≤ and ùëú‚Ä≤‚Ä≤ at ùëÅ‚Ä≤‚Ä≤ flows to ùëúat ùëÅ. For each nonterminal ùëÅ‚Ä≤, the\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 10, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:11\\nTD approach computes a top-down value set tdùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) that contains the values ùëú‚Ä≤ at ùëÅ‚Ä≤ that\\nflow into the value ùëúat ùë£out. The top-down value sets are computed using witness functions. An\\noperator f with arity ùëõis associated with ùëõwitness functions - one for each argument position. The\\nùëò-th witness function, WFf,ùëò: Oùëò‚Ü¶‚Üí2O, for operator f maps the desired output and ùëò‚àí1 values\\nfor previous arguments to possible values for the ùëò-th argument. Such a parameterized collection\\nof witness functions is sound (complete) if ùëúùëò‚ààWFf,ùëò(ùëú,ùëú1, . . . ,ùëúùëò‚àí1) for ùëò= 1, . . . ,ùëõimplies (is\\nimplied by) JfK(ùëú1, . . . ,ùëúùëõ) = ùëú. For example, if the top-level operator is Plus and the output is 7, the\\npossible arguments for Plus would be (0, 7), (1, 6), (2, 5), . . ., and hence, WFPlus,1(7) = {0, 1, 2, . . .}\\nand WFPlus,2(7,ùë•) = {7 ‚àíùë•}.\\nTop-down and bottom-up synthesis are both efficient and practical in different scenarios. TD'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 10, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='and WFPlus,2(7,ùë•) = {7 ‚àíùë•}.\\nTop-down and bottom-up synthesis are both efficient and practical in different scenarios. TD\\nsynthesis performs well when operators are effectively invertible (EI), i.e., the witness functions\\nWFf,ùëòfor each operator f produces sets that are finite and manageable in size. BU synthesis performs\\nwell when the grammar is effectively enumerable (EE), i.e., both the number of constants in the\\ngrammar is bounded and the number of different intermediate values produced is manageable.\\nNote that the focus in this paper is exclusively on synthesis approaches based on concrete values:\\nthe semantics and witness functions are given on concrete values, and not abstract values or types.\\nAbstractions and types can make top-down strategies work on grammars with non-EI operators,\\nfor example [Feng et al. 2017; Polikarpova et al. 2016], but require additional machinery, such as\\ntype systems, abstract domains, and constraint solvers.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 10, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='for example [Feng et al. 2017; Polikarpova et al. 2016], but require additional machinery, such as\\ntype systems, abstract domains, and constraint solvers.\\nExample 3.3. The affine expression DSL from Example 3.1 is neither effectively invertible nor\\neffectively enumerable. The operator Plus has a witness function WFPlus,1 which produces a set\\nof size ùëõ+ 1 for an example ùëÜ‚Ü¶‚Üíùëõ. Further, it contains an infinite number of constants (i.e., the\\nnon-negative integers) which make bottom-up approach infeasible. In the FlashFill++ DSL, many\\nstring operators are not effectively invertible. E.g., the LowerCase operator‚Äôs witness function that\\ncan produce a set that is exponential in the input‚Äôs length: WFLowerCase,1(‚Äòabc‚Äô) produces a set of\\nsize 8, i.e., {‚ÄòABC‚Äô, ‚ÄòABc‚Äô, ‚ÄòAbC‚Äô, ‚ÄòaBC‚Äô, ‚ÄòabC‚Äô, ‚ÄòaBc‚Äô, ‚ÄòAbc‚Äô, ‚Äòabc‚Äô}.\\n‚ñ°\\nIn Section 4, we introduce cuts that can be used to decompose the synthesis problem to enable\\nmiddle-out synthesis, which can learn over deep DSLs ‚Äì DSLs that can generate programs with'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 10, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='‚ñ°\\nIn Section 4, we introduce cuts that can be used to decompose the synthesis problem to enable\\nmiddle-out synthesis, which can learn over deep DSLs ‚Äì DSLs that can generate programs with\\nlarge depth ‚Äì where neither bottom-up nor top-down is feasible. In Section 5, we introduce gDSLs,\\nwhich provide a precedence-based mechanism to help learning scale to broad DSLs ‚Äì DSLs with\\nseveral options for a single nonterminal.\\n4\\nCUTS AND MIDDLE-OUT SYNTHESIS\\nTop-down and bottom-up approaches, as well as their combination, struggle to scale to large DSLs.\\nCuts can help scale synthesis. If we want to generate a value ùëúat nonterminal ùëÅ, and another\\nnonterminal ùëÅ‚Ä≤ is on the path from ùëÅto the terminals, then a cut at ùëÅ‚Ä≤ returns values to generate\\nat ùëÅ‚Ä≤ that can help with generating ùëúat ùëÅ.\\nDefinition 4.1 (Cuts). Given a synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and a non-terminal ùëÅ‚Ä≤ ‚ààN, a cut\\nCutùëÅ‚Ä≤,ùëÅfor ùëÅ‚Ä≤ in the context ùëÅ, maps an example, ùëÜ‚Ü¶‚Üíùëú, to a set of values. Such a function is'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 10, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Definition 4.1 (Cuts). Given a synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and a non-terminal ùëÅ‚Ä≤ ‚ààN, a cut\\nCutùëÅ‚Ä≤,ùëÅfor ùëÅ‚Ä≤ in the context ùëÅ, maps an example, ùëÜ‚Ü¶‚Üíùëú, to a set of values. Such a function is\\ncomplete if for every solution ùëÉfor the task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, whenever ùëÉcontains a sub-program\\nùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤), then JùëÉ‚Ä≤K(ùëÜ) ‚ààCutùëÅ‚Ä≤,ùëÅ(ùëÜ‚Ü¶‚Üíùëú).\\nNote that ùëÅneed not be the start symbol of the grammar and ùëúneed not be the original output\\nvalue in the input-output example. Typically, we define cuts CutùëÅ‚Ä≤,ùëÅwhen ùëÅ‚Üíùëì(ùëÅ‚Ä≤, ùëÅ‚Ä≤‚Ä≤) is a\\ngrammar rule. Such a cut can be used in place of the witness function for the first argument of ùëì.\\nLet us illustrate cuts through an example.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 11, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:12\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nMO.Cut\\nPS1 |= ‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©\\nPS2 |= ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©\\n√ò\\nùëú1\\nPS2[ùëú1 ‚Ü¶‚ÜíPS1] |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nif ùëú1 ‚ààCutùëÅ1,ùëÅ(ùëÜ‚Ü¶‚Üíùëú)\\nFig. 5. The cut inference rule that enables middle-out program synthesis.\\nExample 4.2. Consider again the synthesis task from Section 2.2, where the input-output example\\nwas ùëÜ‚Ü¶‚Üí24.00 and the input state ùëÜwas ‚ü®in1 ‚Ü¶‚Üí‚ÄúThe price is $24.58 and 46 units are\\navailable.‚Äù ‚ü©. Suppose we want to synthesize a program from this example starting from the\\nnonterminal roundNumber of the FlashFill++ DSL (Figure 7). One potential cut for parseNumber in\\nthe context roundNumber could work by scanning the input for any maximal substrings that are\\nnumerical constants and returning them (as a number). Here, it would return {24.58, 46}. A more\\nsophisticated cut could additionally look at the output 24.00 and only return the set {24.58}, as it is'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 11, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='numerical constants and returning them (as a number). Here, it would return {24.58, 46}. A more\\nsophisticated cut could additionally look at the output 24.00 and only return the set {24.58}, as it is\\nthe only value in the string that can be rounded down to 24.00. These cuts are not complete as they\\ndo not include 24.5, which can also be extracted from the input and rounded to 24.\\n‚ñ°\\nRecall that we model synthesizers as generating nonempty program sets PS and asserting\\nPS |= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. Figure 5 presents a new inference rule, called the cut rule, that can be used\\nto generate such assertions. This rule can be used in conjunction with any synthesizer (such\\nas those based on top-down or bottom-up approach). The cut rule uses a cut for a nonterminal\\nùëÅ1 in the context of ùëÅto decompose the overall synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©into two subtasks\\n‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©and ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©. The first, or inner, subtask tries to find a program'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 11, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='ùëÅ1 in the context of ùëÅto decompose the overall synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©into two subtasks\\n‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©and ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©. The first, or inner, subtask tries to find a program\\nin L(ùëÅ1) that maps ùëÜto ùëú1, whereas the second, or outer, subtask tries to find a program in L(ùëÅ)\\nthat maps ùëÜto ùëúassuming that we have a program to maps ùëÜto ùëú1. The notation R with ùëÅ1 ‚Üíùëú1\\nsimply means we remove all old rules in R of the form ùëÅ1 ‚Üíùõºand only have one rule ùëÅ1 ‚Üíùëú1.\\nThe cut rule also shows how the solutions to the two subtasks are combined to generate a solution\\nfor the original synthesis task.\\nTheorem 4.3. [Soundness] If program sets PS1 and PS2 are such that PS1 |= ‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©and\\nPS2 |= ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©, then PS2[ùëú1 ‚Ü¶‚ÜíPS1] |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. Furthermore, [complete-\\nness] if a program ùëÉis a solution for the synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and the program ùëÉcontains a\\nsubprogram ùëÉ1 ‚ààL(ùëÅ‚Ä≤) that maps the input ùëÜto a value ùëú1 (i.e., JùëÉ1K(ùëÜ) = ùëú1), then ùëú1 will be in the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 11, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='ness] if a program ùëÉis a solution for the synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and the program ùëÉcontains a\\nsubprogram ùëÉ1 ‚ààL(ùëÅ‚Ä≤) that maps the input ùëÜto a value ùëú1 (i.e., JùëÉ1K(ùëÜ) = ùëú1), then ùëú1 will be in the\\ncut CutùëÅ1,ùëÅ(ùëÜ‚Ü¶‚Üíùëú) assuming that the cut is complete, and moreover, the program ùëÉ[ùëÉ1 ‚Ü¶‚Üíùëú1] is a\\nsolution for the task ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©.\\nWe use the term middle-out synthesis to describe the synthesis approach that uses the Rule MO.Cut\\nto perform synthesis. Note that the subproblems created by Rule MO.Cut can be solved using either\\nthe top-down approach, or the bottom-up approach, or the middle-out approach, or a hybrid\\ncombination of the approaches. One common strategy is: after applying Rule MO.Cut, we solve\\nthe outer subtask using bottom-up or hybrid approach, and for each ùëú1 for which the outer has a\\nsolution, we solve the inner subtask using the top-down or hybrid approach. Note that the cut rule\\ncan be used multiple times to solve a synthesis task.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 11, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='solution, we solve the inner subtask using the top-down or hybrid approach. Note that the cut rule\\ncan be used multiple times to solve a synthesis task.\\nExample 4.4. Consider the synthesis task ‚ü®roundNumber, R,ùëÜ‚Ü¶‚Üí24.00‚ü©from Example 4.2. Us-\\ning the fact that 24.58 was in the cut for parseNumber, we can use MO.Cut rule from Figure 5\\nand get the subtasks ‚ü®parseNumber, R,ùëÜ‚Ü¶‚Üí24.58‚ü©and ‚ü®roundNumber, R‚Ä≤,ùëÜ‚Ü¶‚Üí24.00‚ü©, where R‚Ä≤\\nis R with parseNumber ‚Üí24.58. The second subproblem now has only one rule for parseNumber,\\nwhich directly generates 24.58. The first subproblem now has to generate 24.58 from the input, and\\nthe second subproblem has to round 24.58 to 24.00.\\n‚ñ°\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 12, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:13\\n4.1\\nGeneralizing Top-Down and Bottom-Up Synthesis\\nExamining the top-down and bottom-up synthesis approaches closely, it can be seen that top-\\ndown synthesis is purely output driven and bottom-up synthesis is purely input driven. Cuts neatly\\ngeneralize both these approaches, allowing us the possibility to use a set of values influenced by\\nboth: (a) the set ùêµof values reachable through forward semantics from the input values (bottom-up\\nsearch), and (b) the set ùëáof values reachable through inverse semantics from the output values\\n(top-down search).\\nRecall that the set buùëÅ‚Ä≤ is the set of values that bottom-up enumeration generates corresponding\\nto nonterminal ùëÅ‚Ä≤ and the set tdùëÅ,ùëÅ‚Ä≤ is the set of values that arise at ùëÅ‚Ä≤ by repeatedly applying\\n(precise) witness functions starting from ùëÅ. Let real value set rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) be the set of values\\nJùëÉ‚Ä≤K(ùëÜ) where ùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤) is a sub-program of a program ùëÉ‚ààL(ùëÅ) such that JùëÉK(ùëÜ) = ùëú. Clearly,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 12, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='(precise) witness functions starting from ùëÅ. Let real value set rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) be the set of values\\nJùëÉ‚Ä≤K(ùëÜ) where ùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤) is a sub-program of a program ùëÉ‚ààL(ùëÅ) such that JùëÉK(ùëÜ) = ùëú. Clearly,\\nit can be seen that rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) ‚äÜtdùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) ‚à©buùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú). Restating the definition of\\ncomplete cuts, a cut is complete if and only if the set it returns is a superset of rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú). Both\\ntop-down and bottom-up search for synthesis are special cases of cut-based middle-out synthesis.\\nTheorem 4.5. [Cuts generalize top-down and bottom-up value sets.] Given a synthesis problem\\n‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and a non-terminal ùëÅ‚Ä≤, both the functions buùëÅ‚Ä≤ and tdùëÅ,ùëÅ‚Ä≤ are complete cuts for the\\nnon-terminal ùëÅ‚Ä≤ in the context of ùëÅ.\\nIn the light of this theorem, restricting the cut in the middle-out synthesis rule from Figure 5 to\\nonly being buùëÅ‚Ä≤ produces the state-of-the-art combination of top-down and bottom-up synthesis\\nDuet [Lee 2021]. While the above theorem states that TD and BU analyses produce complete cuts,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 12, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='only being buùëÅ‚Ä≤ produces the state-of-the-art combination of top-down and bottom-up synthesis\\nDuet [Lee 2021]. While the above theorem states that TD and BU analyses produce complete cuts,\\nnot all complete cuts are (overapproximations of) top-down value set or bottom-up value set.\\nExample 4.6. Building on Example 4.2, consider now the example ùëÜ‚Ü¶‚Üí24.58, but we want\\nto synthesize a program from this example starting from the nonterminal parseNumber, which\\nhas a rule parseNumber ‚ÜíParseNumber(substr, locale). One potential cut for substr in the\\ncontext parseNumber could be obtained by scanning the input string for any substrings that are\\nnumerical constants and returning them (as a string). Here it returns a set containing ‚Äú24.58‚Äù and\\n‚Äú46‚Äù and all substrings of these two strings that are valid numbers. This complete cut is not an\\noverapproximation of the bottom-up values that substr can generate since there are many more'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 12, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='‚Äú46‚Äù and all substrings of these two strings that are valid numbers. This complete cut is not an\\noverapproximation of the bottom-up values that substr can generate since there are many more\\nsubstrings in an input. However, it is complete in the context parseNumber because these are the\\nonly strings that can be parsed as numbers.\\n‚ñ°\\n4.2\\nComputing Cuts\\nWe can use top-down value sets or bottom-up value sets as the cuts, as shown in Theorem 4.5;\\nhowever, if we do that, we only replicate top-down, bottom-up, and Duet‚Äôs meet-in-the-middle\\nsynthesis [Lee 2021]. DSL designers can provide more refined cuts that would enable going beyond\\ncurrent methods. How can designers author more refined cuts? For most operators, using witness\\nfunctions to perform top-down synthesis might suffice. Specialized cuts are only required when we\\nhave not-effectively-invertible operators that have either no witness function or very inefficient\\nwitness functions.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 12, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='have not-effectively-invertible operators that have either no witness function or very inefficient\\nwitness functions.\\nIn practice, DSL designers do not necessarily have to use complete cuts. Looking back at Exam-\\nple 4.6, the set {‚Äú24.58‚Äù, ‚Äú46‚Äù} is a reasonable, but incomplete cut, because any program that extracts\\na number from a strict substring, say ‚Äú4.5‚Äù, of these two strings would be a contrived program. Cuts\\nare reminiscent of interpolants or invariants from program analysis: a cut at ùëÅ‚Ä≤ in the context of ùëÅ\\nis the denotation of an invariant that holds true at ùëÅ‚Ä≤ for all programs that compute the desired\\noutput at ùëÅ. We next describe a few cut functions used in FlashFill++ along with the DSL designer‚Äôs\\nintuition about the DSL that helped construct that cut function.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 13, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:14\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nCut for LowerCase. When computing a cut for a nonterminal ùëÅ‚Ä≤ in context of nonterminal ùëÅ, we\\nneed to consider all paths from ùëÅ‚Ä≤ to ùëÅin the grammar. We focus on one path at a time, and take a\\nunion if there are multiple paths. Consider the grammar rule single := LowerCase(concat) in the\\nFlashFill++ DSL (Figure 7), and ignore the other paths between nonterminals single and concat\\nfor now. Clearly, the witness function is given as:\\nWFLowerCase,1(ùë¶) = {ùë•| lowercase(ùë•) = ùë¶}\\nThe returned set contains 2len(ùë¶) strings. In contrast, the cut could be much smaller. Ignoring other\\npaths between the two nonterminals, one possible cut is:\\nCutconcat,single(ùëÜ‚Ü¶‚Üíùë¶) = {ùë•| lowercase(ùë•) = ùë¶and each char of ùë•is in some input in ùëÜ}.\\nHere the DSL designer uses their knowledge that the non-terminal concat can only generate strings'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 13, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Cutconcat,single(ùëÜ‚Ü¶‚Üíùë¶) = {ùë•| lowercase(ùë•) = ùë¶and each char of ùë•is in some input in ùëÜ}.\\nHere the DSL designer uses their knowledge that the non-terminal concat can only generate strings\\nwhose characters are in some input. One exception to this invariance rule are strings that are\\ngenerated as ‚Äúconstant strings\", and hence this cut is not complete, but it may be a reasonable\\ncompromise between completeness and efficiency. An alternative cut could be:\\n{ùë•| lowercase(ùë•) = ùë¶and for each char ùë•[ùëñ] of ùë•, ùë•[ùëñ] is in some input or ùë•[ùëñ] == ùë¶[ùëñ]}.\\nFinally, the designer can further refine the cut to only include those strings that contain large\\nchunks of substrings of some input. Consider input ‚ÄòAmal Ahmed <AMAL@CCS.NEU.EDU>‚Äô and\\noutput ‚Äòamal‚Äô. The witness function would return 16 values, all variations of the string ‚Äòamal‚Äô\\nwith each letter optionally capitalized. However, a cut would only return 2 values ‚ÄòAmal‚Äô and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 13, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='output ‚Äòamal‚Äô. The witness function would return 16 values, all variations of the string ‚Äòamal‚Äô\\nwith each letter optionally capitalized. However, a cut would only return 2 values ‚ÄòAmal‚Äô and\\n‚ÄòAMAL‚Äô, i.e., those variations that occur contiguously in the input. The same kind of reasoning\\ncan be used on all other paths from concat to single that go via the operators UpperCase and\\nProperCase, and thus we can get a cut for concat in the context of single.\\nCut for Concat. Consider the grammar rule concat := segment|Concat(segment, concat) in the\\nFlashFill++ DSL (Figure 7). The witness function for (the first argument of) Concat is given by\\nWFConcat,1(ùë¶) = {ùë•| ùë•is a prefix of ùë¶}\\nThe DSL designer knows the invariant that every string generated by segment is either a substring\\nof an input, or a string representation of date or number, or a constant string. So, a possible cut,\\nCutsegment,concat(ùëÜ‚Ü¶‚Üíùë¶), could be:\\n{ùë•| ùë•is maximal prefix of ùë¶either contained in some input or a number or a date}.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 13, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Cutsegment,concat(ùëÜ‚Ü¶‚Üíùë¶), could be:\\n{ùë•| ùë•is maximal prefix of ùë¶either contained in some input or a number or a date}.\\nThis cut is not complete since segment could generate a constant string, but the DSL designer may\\nprefer to use this cut and fallback on the witness function only if the above choices fail to work.\\nCut for RoundNumber and FormatDateTime. Example 4.2 presented the cut for parseNumber\\nin the context roundNumber. The witness function for roundNumber on the input ùëÜ‚Ü¶‚Üí24.00 will\\nhave to return an infinite set of floating point values that can all round to 24.00. However, the DSL\\ndesigner knows that parseNumber can only generate a number that occurs in the input, i.e., 24.58 or\\n46, and furthermore, numbers such as 4.5 are not reasonable choices. Hence, the designer can pick the\\ncut CutparseNumber,roundNumber(ùëÜ‚Ü¶‚Üíùë¶):\\n{ùë•| ùë•is a maximally long number extracted from a substring of an input}.\\nHere the DSL designer used their knowledge about the form of values that a nonterminal can'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 13, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='{ùë•| ùë•is a maximally long number extracted from a substring of an input}.\\nHere the DSL designer used their knowledge about the form of values that a nonterminal can\\ngenerate to construct a cut. Another such example is the cut, CutasDate,formatDate(ùëÜ‚Ü¶‚Üíùë¶), which\\ncan be computed as:\\n{ùë•| ùë•is a maximally long datetime value extracted from a substring of an input}.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 14, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:15\\nCut for Const in the Affine Grammar. The DSL designer of arithmetic expressions given in\\nExample 3.1 can provide a cut for const in the context output by noting the monotonicity invariant:\\nvalues computed by subexpressions are smaller than values computed by the whole expression. Consider\\nthe input stateùëÜ= ‚ü®in1 ‚Ü¶‚Üí2, in2 ‚Ü¶‚Üí0‚ü©and the input-output exampleùëÜ‚Ü¶‚Üí7. Since the output is 7, we\\ncan restrict the potential values for the coefficient of in1 (which is 2) to at most 3 values, i.e., {0, 1, 2, 3}\\nas any larger value will make the product exceed the value 7. Thus, the cut, Cutconst,output(ùëÜ‚Ü¶‚Üíùë¶),\\nis computed as:\\n{ùë•‚ààN | 0 ‚â§ùë•‚â§‚åä\\nùë¶\\nùëÜ[in1] ‚åã}.\\nUsing this cut we can now perform bottom-up synthesis, whereas it wasn‚Äôt possible before since\\nthere are an infinite set of possible values for the non-terminal const and the grammar is not\\neffectively enumerable.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 14, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='there are an infinite set of possible values for the non-terminal const and the grammar is not\\neffectively enumerable.\\nWhenever there is a nontrivial invariant that holds for all values that can be generated at a\\ncertain nonterminal, we can usually exploit that invariant to design a cut for that nonterminal.\\nWhile we have focused mainly on the FlashFill++ DSL to illustrate this process of designing good\\ncuts, the ideas extend to DSLs for any other domain.\\n5\\nPRECEDENCE IN DOMAIN-SPECIFIC LANGUAGES\\nWe first define the problem of synthesis in presence of precedence over operators. We then introduce\\ngDSLs, which extend the notion of DSL (Section 3.1) with precedence. We then present the gDSL\\nfor FlashFill++ and inference rules that solve the PBE synthesis problem over gDSLs.\\n5.1\\nSynthesis with Preference\\nDSL designers who want to translate programs generated in a DSL into a popular target language,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 14, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='5.1\\nSynthesis with Preference\\nDSL designers who want to translate programs generated in a DSL into a popular target language,\\nsuch as Python, typically want the DSL to contain all operators from the target language libraries.\\nThese operators are often redundant. For example, substrings of a string can be extracted using\\nabsolute index positions, or regular expressions, or finding locations of constant substrings in the\\nstring, or splitting a string by certain delimiters. In such cases, whenever a task is achievable in\\nmany different ways, we get a so-called broad DSL. For broad DSLs, DSL designers often have a\\npreference for which operators to use. For example, they may prefer split over find, which they\\nmay prefer in turn over using regular expressions or absolute indices. As another example, DSL\\ndesigners may prefer transforming a string containing ‚ÄúJan‚Äù to ‚ÄúJanuary‚Äù by treating the substring'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 14, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='may prefer in turn over using regular expressions or absolute indices. As another example, DSL\\ndesigners may prefer transforming a string containing ‚ÄúJan‚Äù to ‚ÄúJanuary‚Äù by treating the substring\\naround ‚ÄúJan‚Äù in the input as a datetime object and working with them, rather than generating\\n‚ÄúJanuary‚Äù by concatenating ‚ÄúJan‚Äù with a constant string ‚Äúuary‚Äù.\\nExample 5.1. Consider the task of extracting the second column from a line in a comma-separated\\nvalues (CSV) file, specified by the input-output example ‚ÄòWA, Olympia, UTC-8‚Äô ‚Ü¶‚Üí‚ÄòOlympia‚Äô.\\nIn the FlashFill++ DSL, this can be done using the program Split(x, ‚Äò,‚Äô, 2), where ùë•is the input,\\nor using the program Slice(x, Find(x, ‚Äò,‚Äô, 1, 0), Find(x, ‚Äò,‚Äô, 2, 0)). A traditional VSA-based syn-\\nthesizer would (possibly implicitly) produce both programs, assign scores to both using a ranking\\nfunction, and return the better ranked one. However, typically we strictly prefer programs that use'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 14, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='thesizer would (possibly implicitly) produce both programs, assign scores to both using a ranking\\nfunction, and return the better ranked one. However, typically we strictly prefer programs that use\\nthe Split operator over the Slice operator. Ideally, a synthesizer should not even examine Slice\\nprograms when an equivalent Split program exists. Similar preference is also seen in Python\\nprogrammers who often prefer to use str.split over regex.find or str.find.\\n‚ñ°\\nThis motivates the need to perform synthesis over a broad DSL where there is preference over\\noperators. Suppose a DSL designer has a DSL and a preference over operators and terminals. Let\\nŒ£ := F ‚à™T be the collection of all operators and terminal symbols in the DSL and let ‚âªŒ£ be a\\nprecedence relation on the symbols f ‚ààŒ£. We make the assumption that (A1) the DSL designer\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 15, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:16\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nonly provides precedence over operators that occur as alternates for the same nonterminal; that is,\\nif the designer sets f1 ‚âªŒ£ f2, then ùëÅ‚Üíf1(. . .) ‚ààR and ùëÅ‚Üíf2(. . .) ‚ààR are two rules with the\\nsame nonterminal ùëÅin the grammar. We also assume that (A2) the relation ‚âªŒ£ is a strict partial\\norder (irreflexive, asymmetric, transitive) to ensure that the DSL designers preference is consistent.\\nWe first need to formalize what it means for a synthesizer to satisfy the operator precedence ‚âªŒ£\\nprovided by the DSL designer. For this, we need to lift ‚âªŒ£ to a precedence on the set of programs ùëÉ\\ngenerated by the DSL. However, this is not easy since it is not clear which of f1(f2(in)) or f‚Ä≤\\n1(f‚Ä≤\\n2(in))\\nto prefer if the DSL designer says f1 ‚âªŒ£ f‚Ä≤\\n1 and f‚Ä≤\\n2 ‚âªŒ£ f2. We resolve this issue by saying that the\\npreference for the operator occurring ‚Äúabove‚Äù in the program is more important than anything'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 15, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='to prefer if the DSL designer says f1 ‚âªŒ£ f‚Ä≤\\n1 and f‚Ä≤\\n2 ‚âªŒ£ f2. We resolve this issue by saying that the\\npreference for the operator occurring ‚Äúabove‚Äù in the program is more important than anything\\nbelow. In the above example, we give more weight to f1 ‚âªŒ£ f‚Ä≤\\n1 and hence we want f1(f2(in)) to be\\npreferred over f‚Ä≤\\n1(f‚Ä≤\\n2(in)). This follows the intuition that operators at the top in, say, the FlashFill++\\nDSL, such as Concat, FormatNumber, or FormatDateTime, are more influential in determining the\\nhigh-level strategy for solving a task than operators at the bottom, such as Split or Slice. Hence,\\nwe extend the user provided ‚âªŒ£ to ‚âªùëí\\nŒ£‚äá‚âªŒ£ so that whenever ùëÅ0 ‚Üíf1(. . . , ùëÅ1, . . .) and ùëÅ1 ‚Üíf2(. . .)\\nare rules in the DSL, then f1 ‚âªùëí\\nŒ£ f2.\\nExample 5.2. Consider the synthesis task from Example 4.2 of generating ‚Äò24.00‚Äô from the input\\nstring. One possible program is Concat(ùëù1, ‚Äò.00‚Äô), where ùëù1 is a subprogram that extracts ‚Äò24‚Äô'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 15, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Œ£ f2.\\nExample 5.2. Consider the synthesis task from Example 4.2 of generating ‚Äò24.00‚Äô from the input\\nstring. One possible program is Concat(ùëù1, ‚Äò.00‚Äô), where ùëù1 is a subprogram that extracts ‚Äò24‚Äô\\nfrom the input string. A second possible program is Segment(FormatNumber(ùëù2, fmt_desc) that\\ngenerates ‚Äò24.00‚Äô by formatting a number computed by program ùëù2. Here, Segment is a dummy\\nidentity operator having higher preference than Concat in the FlashFill++ gDSL (Figure 7). In the\\nFlashFill++ DSL, the second program is preferred since it does not use concatenation, irrespective of\\nhow ùëù1 and ùëù2 work‚Äîat the top-level concatenation is strictly less preferred. Note that here ùëù1 is\\nlikely to be significantly smaller and simpler than ùëù2 as it is just extracting the string ‚Äò24‚Äô, while\\nùëù2 is extracting a number and then rounding it. A traditional arithmetic ranking function (as used\\nin FlashFill and FlashMeta) intuitively computes the score of programs as a weighted sum of the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 15, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='ùëù2 is extracting a number and then rounding it. A traditional arithmetic ranking function (as used\\nin FlashFill and FlashMeta) intuitively computes the score of programs as a weighted sum of the\\nscore of its sub-programs, and hence, will need to be tuned carefully to ensure that the smaller\\nconcat program is scored worse than the larger segment program.\\n‚ñ°\\nWe want the relation ‚âªùëí\\nŒ£ to be a strict partial order. However, in general, it may not be a strict\\npartial order due to cycles in the grammar (where some ùëÅ0 generates a term containing ùëÅ0), which\\nagain makes ‚âªùëí\\nŒ£ violate irreflexivity or transitivity. We make the reasonable assumption that there\\nare no cycles since we often limit the depth of terms being synthesized and then the assumption\\ncan be satisfied by renaming the nonterminals. Thus, without loss of much generality, we can\\nassume that the extended precedence ‚âªùëí\\nŒ£ on Œ£ is a strict partial order.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 15, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='can be satisfied by renaming the nonterminals. Thus, without loss of much generality, we can\\nassume that the extended precedence ‚âªùëí\\nŒ£ on Œ£ is a strict partial order.\\nNow we formalize synthesizing in presence of precedence ‚âªŒ£ by lifting the precedence ‚âªŒ£ on\\nŒ£ to ‚âªùëí\\nŒ£, and then to a preference on programs (trees) over Œ£ using the well-known lexicographic\\npath ordering (LPO), ‚âªùëôùëùùëú, which is defined as follows [Dershowitz and Jouannaud 1990]: Given\\nprograms ùëÉ1 := f1(ùëÉ11, . . . , ùëÉ1ùëò) and ùëÉ2 := f2(ùëÉ21, . . . , ùëÉ2ùëô), we have ùëÉ1 ‚âªùëôùëùùëúùëÉ2 if either (a) f1 ‚âªùëí\\nŒ£ f2\\nand ùëÉ1 ‚âªùëôùëùùëúùëÉ2ùëñfor all ùëñ, or (b) f1 = f2 and there exists a ùëös.t. ùëÉ1ùëñ= ùëÉ2ùëñfor ùëñ< ùëöand ùëÉ1ùëö‚âªùëôùëùùëúùëÉ2ùëö,\\nor (c) ùëÉ1ùëñ‚âªùëôùëùùëúùëÉ2 for some ùëñ.\\nDefinition 5.3. Let ‚™∞base be the base ordering to rank programs that are unordered by ‚âªùëôùëùùëú. Given\\na DSL D with precedence ‚âªŒ£ on the set Œ£ := F ‚à™T of all operators and terminal symbols in D, and\\n‚™∞base, the PBE synthesis with precedence problem is to find the maximally ranked program by the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 15, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='a DSL D with precedence ‚âªŒ£ on the set Œ£ := F ‚à™T of all operators and terminal symbols in D, and\\n‚™∞base, the PBE synthesis with precedence problem is to find the maximally ranked program by the\\nlexicographic combination of ‚âªùëôùëùùëúand ‚™∞base that satisfies a given example, where ‚âªùëôùëùùëúis the LPO\\ninduced by the extended precedence ‚âªùëí\\nŒ£.2\\n2Given orderings ‚âª1 and ‚âª2, the lexicographic combination ‚âª‚âª1,‚âª2 is defined as follows: ùë†‚âª‚âª1,‚âª2 ùë°if either (a) ùë†‚âª1 ùë°, or\\n(b) ùë†‚äÅ1 ùë°and ùë°‚äÅ1 ùë†and ùë†‚âª2 ùë°.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 16, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:17\\nWe solve the PBE synthesis with precedence problem by extending DSLs to gDSLs and modifying\\nthe inference rules to correctly handle the precedence.\\n5.2\\nGuarded Domain-Specific Languages\\nA guarded domain-specific language (gDSL) is a DSL D = ‚ü®N, T, F, R, Vin, ùë£out‚ü©where the set R of\\nrules can additionally contain guarded rules of the form ùëÅ‚Üíùõº1 |‚ä≤ùõº2 |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤ùõºùëòwhere each ùõºùëñis\\neither f(ùë£1, . . . , ùë£ùëõ) or ùë£0, where f ‚ààF , ùë£0 ‚ààT, and ùë£1, . . . , ùë£ùëõ‚ààN ‚à™T. A non-terminal ùëÅcan have\\nany number of guarded rules associated with it, each with possibly different values of ùëò‚â•1.\\nThe rules in a regular DSL can be viewed as a special case of guarded rules where ùëò= 1 (in the\\ndefinition of guarded production rules above.) When ùëò> 1, a guarded rule ùëÅ‚Üíùõº1 |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤ùõºùëò\\nassociated with the nonterminal ùëÅhas ùëòalternates on the right-hand side that are ordered. The ùëñ-th'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 16, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='definition of guarded production rules above.) When ùëò> 1, a guarded rule ùëÅ‚Üíùõº1 |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤ùõºùëò\\nassociated with the nonterminal ùëÅhas ùëòalternates on the right-hand side that are ordered. The ùëñ-th\\nalternate ùõºùëñyields a (regular) rule ùëÅ‚Üíùõºùëñ. We call ùëÅ‚Üíùõºùëñthe ùëñ-th constituent rule of the original\\nguarded rule. Define Rùëêas the collection of all constituent rules of rules in R; that is, Rùëê:= {ùëÅ‚Üí\\nùõºùëñ| ùëÅ‚Üí(. . . |‚ä≤ùõºùëñ|‚ä≤. . .) ‚ààR}. We call the (regular) DSL Dùëê:= ‚ü®N, T, F, Rùëê, Vin, ùë£out‚ü©obtained\\nfrom a gDSL D := ‚ü®N, T, F, R, Vin, ùë£out‚ü©a constituent DSL of the gDSL D.\\nGiven an instance of the PBE synthesis with precedence problem, we can annotate the given DSL\\nwith precedence to get a gDSL. We assume that the precedence relation ‚âªŒ£ satisfies Assumptions (A1)\\nand (A2). Furthermore, we also assume that (A3) the precedence is a series-parallel partial order\\n(SPPO) [B√©chet et al. 1997]. Under Assumption (A3), the precedence can be encoded in gDSL'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 16, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='and (A2). Furthermore, we also assume that (A3) the precedence is a series-parallel partial order\\n(SPPO) [B√©chet et al. 1997]. Under Assumption (A3), the precedence can be encoded in gDSL\\nby introducing new nonterminals where necessary. Specifically, if we have a maximal chain\\nf1 ‚âªŒ£ f2 ‚âªŒ£ . . . ‚âªŒ£ fùëòover alternate operators ùëÅ‚Üíf1(. . .)| ¬∑ ¬∑ ¬∑ |fùëò(. . .) in the DSL, then we\\nadd a guarded rule ùëÅ‚Üíf1(. . .) |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤fùëò(. . .). However, if certain alternate operators are left\\nincomparable by the DSL designers, then we introduce a new nonterminal. For example, if f1 ‚âªŒ£ f3\\nand f2 ‚âªŒ£ f3, but there is no preference between f1 and f2, then we introduce a new nonterminal\\nùëÅ‚Ä≤ and have ùëÅ‚ÜíùëÅ‚Ä≤ |‚ä≤f3(. . .) and ùëÅ‚Ä≤ ‚Üíf1(. . .)|f2(. . .) in the gDSL. Any SPPO ‚âªŒ£ can thus be\\nencoded in the gDSL.\\nExample 5.4. Consider the DSL for affine arithmetic from Example 3.1. Suppose the DSL designer\\nwants the precedence input1 ‚âªŒ£ input2. Then, we can replace the two rules for nonterminal'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 16, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Example 5.4. Consider the DSL for affine arithmetic from Example 3.1. Suppose the DSL designer\\nwants the precedence input1 ‚âªŒ£ input2. Then, we can replace the two rules for nonterminal\\naddend by a single guarded rule: addend ‚ÜíTimes(const, input1) |‚ä≤Times(const, input2) to\\nget a gDSL, Dùëî\\nùê¥. In the LPO induced by the extension ‚âªùëí\\nŒ£ of this preference, the program ùëÉ1 :=\\nPlus(Times(3, input1), 1) is preferred over ùëÉ2 := Plus(Times(3, input2), 7).\\n‚ñ°\\nLet ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©be a synthesis task, where R is the rules of a gDSL D. We say a program ùëÉis\\na solution for this task if\\n(a) ùëÉis a solution for the task ‚ü®ùë£out, Rùëê,ùëÜ‚Ü¶‚Üíùëú‚ü©(in the constituent DSL Dùëê), and\\n(b) for any other ùëÉ‚Ä≤ that is a solution in the constituent DSL, ùëÉ‚Ä≤ ‚äÅùëôùëùùëúùëÉ, where ‚âªùëôùëùùëúis the LPO\\ninduced by the extended precedence ‚âªùëí\\nŒ£ coming from the guarded rules.\\nCondition (a) says that ùëÉshould be a solution for the synthesis problem ignoring the precedence.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 16, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='induced by the extended precedence ‚âªùëí\\nŒ£ coming from the guarded rules.\\nCondition (a) says that ùëÉshould be a solution for the synthesis problem ignoring the precedence.\\nCondition (b) says that the ordering in the rules should be interpreted as an ordering on programs\\nusing the induced LPO, and we should ignore programs smaller in this ordering.\\nExample 5.5. Consider the gDSL Dùëî\\nùê¥and programs ùëÉ1, ùëÉ2 from Example 5.4. Consider the synthesis\\ntask ‚ü®ùë£out, R, ‚ü®input1 ‚Ü¶‚Üí2, input2 ‚Ü¶‚Üí0‚ü©‚Ü¶‚Üí7‚ü©from Example 3.2, but with R now coming from the\\ngDSL Dùëî\\nùê¥. Both programs, ùëÉ1 and ùëÉ2, map the input state to 7. However, ùëÉ2 is now not a solution in\\nDùëî\\nùê¥because there exists ùëÉ1 that is preferred. The program ùëÉ3 := 7 also maps the input state to 7.\\nThe (derivations of) ùëÉ1 and ùëÉ3 are incomparable; and in fact, both are solutions in Dùëî\\nùê¥.\\n‚ñ°\\nIf a solution for the unguarded DSL exists, then there will be a solution that is maximal and\\nhence a solution for the gDSL will exist.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 16, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='ùê¥.\\n‚ñ°\\nIf a solution for the unguarded DSL exists, then there will be a solution that is maximal and\\nhence a solution for the gDSL will exist.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 17, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:18\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nGuarded.If\\nPS1 |= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nùëÅ‚Üíùõº1 |‚ä≤ùõº2 ‚ààR\\nPS1 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nGuarded.Else\\nÃ∏|= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nùëÅ‚Üíùõº1 |‚ä≤ùõº2 ‚ààR\\nPS2 |= ‚ü®ùõº2, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nPS2 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nFig. 6. Extending top-down synthesis for guarded rules.\\nTheorem 5.6 (Precedence preserves solvability.). Let D be a gDSL based on precedence ‚âªŒ£\\nthat is a strict partial order. Let ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùë£‚ü©be a synthesis task. This task has a solution in D iff\\nthere is a solution in Dùëê.\\nIf we can compute all solutions for a synthesis task over a gDSL, we can order them by any ‚™∞base\\nordering to solve the PBE synthesis with precedence problem.\\n5.3\\nPBE Synthesis Rules for Guarded DSLs\\nFigure 6 contains two inference rules that describe how guarded rules are handled in top-down,\\nbottom-up, or middle-out synthesis. If ùëÅ‚Üíùõº1 |‚ä≤ùõº2 is a guarded rule in R and we can (recursively)\\nprove ùëÉùëÜ1 |= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, then Rule Guarded.If can be used to assert ùëÉùëÜ1 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. (This'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 17, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='bottom-up, or middle-out synthesis. If ùëÅ‚Üíùõº1 |‚ä≤ùõº2 is a guarded rule in R and we can (recursively)\\nprove ùëÉùëÜ1 |= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, then Rule Guarded.If can be used to assert ùëÉùëÜ1 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. (This\\nassertion will be nontrivial only if ùëÉùëÜ1 ‚â†‚àÖ.) In case there is no program that solves ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©,\\nand we can (recursively) prove ùëÉùëÜ2 |= ‚ü®ùõº2, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, then Rule Guarded.Else can be used to assert\\nùëÉùëÜ2 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©.\\nRecall that the notation ùëÉùëÜ|= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©simply asserts that every program in ùëÉùëÜsolves the\\ngiven synthesis task, and does not require ùëÉùëÜto contain all solutions. The Rule Guarded.Else has a\\ncondition that a certain synthesis task be unsolvable. If we have the ability to compute all solutions\\n(such as, using version-space algebras, or VSAs), then that can help in determining when a problem\\nis unsolvable, but other synthesis approaches that can establish infeasibility can also be used.\\nExample 5.7. Continuing from Example 5.5, consider the task of generating 7 from inputs 2'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 17, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='is unsolvable, but other synthesis approaches that can establish infeasibility can also be used.\\nExample 5.7. Continuing from Example 5.5, consider the task of generating 7 from inputs 2\\nand 0. Say we reduce the original task to the proof obligation ùëã|= ‚ü®addend, R,ùëÜ‚Ü¶‚Üíùë•‚ü©, where\\nwe pick say 6 for ùë•. Now, we have a guarded rule for addend and we can use Rule Guarded.If\\nto get the proof obligation ùëã|= ‚ü®Times(const, input1), R,ùëÜ‚Ü¶‚Üí6‚ü©. This subtask has a solution\\nùëã= {Times(3, input1)}, and hence we will not consider the option Times(const, input2).\\n‚ñ°\\nPerforming synthesis over the gDSL is equivalent to solving PBE synthesis with precedence.\\nTheorem 5.8. Let ‚âªŒ£ be a precedence on Œ£ := F ‚à™T that satisfies Assumptions (A1), (A2), and (A3).\\nLet D := ‚ü®N, T, F, R, Vin, ùë£out‚ü©be a gDSL that encodes ‚âªŒ£. Let Dùëê:= ‚ü®N, T, F, Rùëê, Vin, ùë£out‚ü©be\\nits (unguarded) constituent DSL. Let ‚™∞base be an ordering on programs and let ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©be a\\nsynthesis task. Then, the following are equivalent:'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 17, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='its (unguarded) constituent DSL. Let ‚™∞base be an ordering on programs and let ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©be a\\nsynthesis task. Then, the following are equivalent:\\n(1) {ùëÉ} |= ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and ùëÉis maximal w.r.t ‚™∞base among all such solutions.\\n(2) The program ùëÉis a solution in Dùëêfor the task ‚ü®ùë£out, Rùëê,ùëÜ‚Ü¶‚Üíùëú‚ü©that is maximal w.r.t a lexicographic\\ncombination of ‚âªùëôùëùùëú(induced by ‚âªŒ£) and ‚™∞base.\\nTheorem 5.8 shows that using a ranker ‚™∞base with a gDSL D has the same effect as using a\\ncomplex ranker (lexicographic combination of ‚âªùëôùëùùëúand ‚™∞base) with a regular DSL Dùëê. This shows\\nthat our gDSL-based approach solves the PBE synthesis with precedence problem (under some\\nassumptions). Theorem 5.8 also explains why the ranker ‚™∞base used with a gDSL D can be very\\nsimple compared to what is needed with a regular DSL Dùëê. Designing a good complex ranking\\nfunction has traditionally been very challenging in PBE, taking many developer-months to converge'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 17, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='simple compared to what is needed with a regular DSL Dùëê. Designing a good complex ranking\\nfunction has traditionally been very challenging in PBE, taking many developer-months to converge\\non a good ranking function [Kalyan et al. 2018; Natarajan et al. 2019; Singh and Gulwani 2015]. In\\ncontrast, FlashFill++ uses the power of gDSLs (Theorem 5.8) to reduce the requirements on its base\\nranker, which was developed significantly faster.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 18, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:19\\nlanguage FlashFillPP;\\n@start string output := single |> If(cond, single, output)\\n// conditions\\nbool cond := pred |> And(pred, cond);\\nbool pred := StartsWith(x, matchPattern) |> EndsWith(x, matchPattern)\\n|> Contains(x, matchPattern) |> ...;\\n// single branch\\nstring single := concat | LowerCase(concat) | UpperCase(concat) | ProperCase(concat);\\n// optional concatenation of substrings\\nstring concat := segment |> Concat(segment, concat)\\n// substring logic\\nstring segment := substr | formatNumber | formatDate | constStr;\\n// format substring as a number\\nstring formatNumber := FormatNumber(roundNumber, numFmtDesc);\\ndecimal roundNumber := parseNumber |> RoundNumber(parseNumber, roundNumDesc);\\ndecimal parseNumber := AsNumber(row, columnName) |> ParseNumber(substr, locale);\\n// format substring as a date\\nstring formatDate := FormatDateTime(asDate, dateFmtDesc);'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 18, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='decimal parseNumber := AsNumber(row, columnName) |> ParseNumber(substr, locale);\\n// format substring as a date\\nstring formatDate := FormatDateTime(asDate, dateFmtDesc);\\nDateTime asDate := AsDateTime(row, columnName) |> ParseDateTime(substr, parseDateFmtDesc);\\n// find a substring within the input x\\nstring substr := Split(x, splitDelimiter, splitInstance)\\n|> Slice(x, pos, pos)\\n|> MatchFull(x, matchPattern, matchInstance);\\n// find a position within the input x\\nint pos := End(x) |> Abs(x, absPos)\\n|> Find(x, findDelimiter, findInstance, findOffset)\\n|> Match(x, matchPattern, matchInstance)\\n|> MatchEnd(x, matchPattern, matchInstance);\\n// literal terminals\\nFmtNumDescriptor numFmtDesc; RndNumDescriptor roundNumDesc;\\nFmtDateTimeDescriptor dateFmtDes, parseDateFmtDesc;\\nstring constStr, splitDelimiter, findDelimiter;\\nint splitInstance, findInstance, matchInstance, findOffset;\\nRegex matchPattern; int absPos;'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 18, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FmtDateTimeDescriptor dateFmtDes, parseDateFmtDesc;\\nstring constStr, splitDelimiter, findDelimiter;\\nint splitInstance, findInstance, matchInstance, findOffset;\\nRegex matchPattern; int absPos;\\nFig. 7. A fragment of the gDSL for FlashFill++. | choices are unguarded, |> choices are guarded.\\n5.4\\nGuarded DSL for FlashFill++\\nWe now describe the FlashFill++ gDSL and compare it to FlashFill. Figure 7 shows a major part of the\\nDSL. FlashFill++ shares the top level rules that perform conditional statements, case conversion, and\\nstring concatenation with FlashFill. Conditionals enable if-then-else logic. The condition is one or\\nmore conjunctive predicates based on properties of the input string. Case conversion transforms a\\nstring into lower-, upper-, or proper-case. Concatenation concatenates two strings.\\nAlthough FlashFill can perform some datetime and number operations using text manipulation\\n(such as \"01/01/2020\" ‚Üí\"2020\" or \"10.01\" ‚Üí\"10\"), it is unable to express other sophisticated'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 18, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Although FlashFill can perform some datetime and number operations using text manipulation\\n(such as \"01/01/2020\" ‚Üí\"2020\" or \"10.01\" ‚Üí\"10\"), it is unable to express other sophisticated\\ndatetime and number operations as it does not incorporate operations over those datatypes, but\\nrather treats them as standard strings. For instance, FlashFill cannot get the day of week from a date\\n(such as \"01/01/2020\" ‚Üí\"Wednesday\"), or round up a number (e.g., \"10.49\" ‚Üí\"10.5\"). This\\nmotivates us to add new rules to support richer datetime (rules parseDate and formatDate) and\\nnumber (rules parseNumber and formatNumber) transformations.\\nThe next major differences are in the substr and pos rules. FlashFill has a single Slice operator\\nwhich selects a substring defined by its start and end positions. These positions can be defined\\neither as absolute positions or with the complex RegPos operator which finds the ùëòth place in the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 18, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='which selects a substring defined by its start and end positions. These positions can be defined\\neither as absolute positions or with the complex RegPos operator which finds the ùëòth place in the\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 19, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:20\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nstring where the left- and right substrings match the two given regular expressions. While this is\\nexpressive enough to cover any desired substring selection and all of the operators in FlashFill++\\ncan technically be expressed in terms of it, this introduces a challenge for an industrial synthesizer\\nthat targets different languages for code generation: not all platforms of interest support regular\\nexpressions natively (e.g. Microsoft Excel‚Äôs formula language does not support regular expressions).\\nIn contrast, when designing FlashFill++ we chose a wider collection of more natural operators that\\nare closer to what developers use in practice when working with target languages ‚Äì this removes\\nthe mismatch between synthesis DSL and code generation target language.\\nIn particular, instead of only allowing substrings to be defined as a Slice with their start and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 19, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='the mismatch between synthesis DSL and code generation target language.\\nIn particular, instead of only allowing substrings to be defined as a Slice with their start and\\nend positions, FlashFill++ adds a Split operator to select the ùëòth element in a sequence generated\\nby splitting the input string by some delimiters. We also add a MatchFull operator to find the ùëòth\\nmatch of a regular expression. Additionally, in FlashFill++ the pos rule replaces the operator RegPos\\n(which relies on a pair of regular expressions to identify a position) with a Find of a constant string\\nin the input and a Match/MatchEnd of a regular expression.\\nExample Guarded Rules in the FlashFill++ DSL. We go over a few cases of guarded rules in FlashFill++\\nto show they capture natural intuitions and rules of thumb in the string transformation domain.\\n‚Ä¢ Single segments over concats. The guarded rule segment |‚ä≤Concat(segment, concat) ensures'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 19, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='to show they capture natural intuitions and rules of thumb in the string transformation domain.\\n‚Ä¢ Single segments over concats. The guarded rule segment |‚ä≤Concat(segment, concat) ensures\\nthat we try to synthesize programs that generate the whole output at once, before generating\\nprograms that produce a prefix and a suffix of the output and then combine them. Whenever\\nsuch a program exists, this guarded rule potentially saves the exploration of a huge portion\\nof the program space. This single guarded rule plays a crucial role in keeping FlashFill++\\nperformant since witness function of the concat operator produces 2(ùëõ‚àí1) subtasks, one\\neach for the prefix and suffix at each point where the output string can be split.\\n‚Ä¢ Splits over slices. As illustrated Example 5.1, FlashFill++ strictly prefers programs that use\\nthe Split operator over programs that use the Slice operator. Program in data wrangling\\nscenarios very commonly begin by extracting the appropriate part of the input from a'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 19, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='the Split operator over programs that use the Slice operator. Program in data wrangling\\nscenarios very commonly begin by extracting the appropriate part of the input from a\\ndelimited file record (CSVs, TSVs, etc). In all such cases, a split program more closely follows\\nthe human intuition of ‚Äúextract the ùëõùë°‚Ñécolumn delimited by‚Äù as compared to a slice program.\\nNote that the split operator is a ‚Äúhigher level‚Äù construct preferred over the ‚Äúlow-level‚Äù slice.\\n‚Ä¢ Input numbers over rounded numbers. The RoundNumber operator in Figure 7 is guarded by\\nparseNumber, meaning that we can only generate a program that rounds a number if no\\nnumber in the input can be used directly to produce the same output.\\n6\\nEXPERIMENTAL EVALUATION\\nWe now present our experimental results, including an ablation study and survey-based user study.\\n6.1\\nMethodology\\nWe used 3 publicly available benchmark sets ‚Äì Duet string benchmarks [Lee 2021], Playgol [Cropper'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 19, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='6.1\\nMethodology\\nWe used 3 publicly available benchmark sets ‚Äì Duet string benchmarks [Lee 2021], Playgol [Cropper\\n2019], and Prose [PROSE 2022] ‚Äì covering a range of string transformations, including datetime\\nand number formatting.\\nWe compare FlashFill++ with three systems: two publicly available state-of-the-art synthesis\\nsystems that are deployed in productions, namely FlashFill [Gulwani 2011] and SmartFill [Chen\\net al. 2021a; Google 2021], and one, Duet, from a recent publication [Lee 2021]. To experiment with\\nFlashFill, we implemented it on top of the FlashMeta framework [Polozov and Gulwani 2015]. We\\ncarry out our FlashFill, FlashFill++, and Duet experiments on a machine with 2 CPUs & 8GB RAM. To\\nexperiment with SmartFill, we employ Google Sheets in Chrome and use it to solve the subset of\\ntasks that are suitable for its spreadsheet environment.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 20, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:21\\nTable 1. Comparing different tools (rows) on the 3 public benchmarks (columns) based on (1) number of\\nbenchmarks correctly solved (Columns 2‚Äì5) and (2) average number of examples required on the successful\\nbenchmarks (Columns 6‚Äì8). The total number of benchmarks attempted are shown in brackets.\\nNumber benchmarks solved\\nAverage #examples required\\nDuet (205)\\nPlaygol (327)\\nProse (354)\\nTotal (886)\\nDuet\\nPlaygol\\nProse\\nFlashFill\\n139\\n264\\n172\\n575\\n1.41\\n1.26\\n1.54\\nFlashFill++\\n159\\n307\\n353\\n819\\n1.69\\n1.46\\n1.52\\nDuet\\n102\\n211\\n166\\n479\\n1.97\\n2.11\\n2.01\\nSmartFill\\n37 (158)\\n34 (327)\\n18 (296)\\n89 (781)\\n2.75\\n2.85\\n2.83\\nSince SmartFill is not exposed in the Google Sheets API, we rely on browser-automation using\\nSelenium [Selenium 2022] for SmartFill evaluation. Moreover, we remove problematic benchmark\\ntasks and use 158, 327, and 296 tasks from the Duet, Playgol, and Prose benchmarks, respectively,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 20, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Selenium [Selenium 2022] for SmartFill evaluation. Moreover, we remove problematic benchmark\\ntasks and use 158, 327, and 296 tasks from the Duet, Playgol, and Prose benchmarks, respectively,\\nfor SmartFill evaluation. The tasks removed were unsuitable for browser automation (e.g. too many\\nrows or new line characters).\\nThe Duet tool [Lee 2021] accepts a DSL as part of its input. Different Duet benchmarks used\\nslightly different DSLs. For fair comparison, we set a single fixed DSL. We obtained the fixed DSL\\nby taking all commonly-used rules in the string transformation benchmarks of Duet. Second, Duet\\nhas some hyperparameters, for which we picked the best setting after some experimentation.\\n6.2\\nExpressiveness and Performance\\nA key feature of FlashFill++ is improved expressiveness. Table 1 (Columns 2‚Äì5) shows the number\\nof benchmark tasks that the various tools (rows) can correctly solve. FlashFill++ produces a correct'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 20, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='A key feature of FlashFill++ is improved expressiveness. Table 1 (Columns 2‚Äì5) shows the number\\nof benchmark tasks that the various tools (rows) can correctly solve. FlashFill++ produces a correct\\nprogram for most number of benchmarks. FlashFill is limited due to a lack of datetime and number\\nformatting. The DSL used in the Duet tool has no datetime support, and only limited support for\\nnumber and string formatting. The SmartFill tool is a neural-based general purpose tool and has\\nthe weakest numbers here. Since our SmartFill experiments rely on browser-based interaction, it is\\npossible that the underlying synthesizer can solve more benchmarks but these are not exposed to\\nthe UI. However, our setup reflects the experience that a user would face.\\nOur DSL is expressive, covering a large class of string, datetime and number transformations;\\nthus showing the added value from using cuts and guarded rules.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 20, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Our DSL is expressive, covering a large class of string, datetime and number transformations;\\nthus showing the added value from using cuts and guarded rules.\\nWhile increasing expressiveness enables users to accomplish more tasks, there is a risk of reducing\\nperformance on existing tasks. To this end, we consider the minimum number of examples required\\nfor a synthesizer to learn a correct program. To find that number, we use a counter-example guided\\n(CEGIS) loop which provides the next failing I/O example in every iteration to the synthesizer.\\nWe use a time out of 20 seconds. Table 1 Columns 6‚Äì8 present the average number of examples\\nthe various tools used across the benchmarks where they were successful. Here FlashFill has the\\nbest numbers, which indicates that when it works (for string benchmarks), it learns with very\\nfew examples. Our tool FlashFill++ takes only slightly more examples on average, partly because'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 20, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='best numbers, which indicates that when it works (for string benchmarks), it learns with very\\nfew examples. Our tool FlashFill++ takes only slightly more examples on average, partly because\\ndatetime and number formatting typically requires more examples for intent disambiguation. For\\nexample, the string ‚Äò2/3/2020‚Äô can either be the 2ùëõùëëof March or the 3ùëõùëëof February. The Duet and\\nSmartFill tools take more examples in general. We emphasize that the performance of FlashFill++ is\\ngood here because it solves more problems (presumably much harder instances) and yet it doesn‚Äôt\\nuse too many more examples (the harder instances did not make the averages much worse).\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 21, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:22\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\n0\\n2500\\n5000\\n7500\\n10000 12500\\nBest Duet or FF Synthesis Time (ms)\\n‚àí2\\n‚àí1\\n0\\n1\\n2\\nlog10(Best / FlashFill++)\\nOutcome\\nFlashFill++ better\\nFlashFill++ worse\\n(a) log10( min(FlashFill,Duet)\\nFlashFill++\\n) vs min(FlashFill, Duet).\\nBenchmark classes\\nDuet\\nPlaygol\\nProse\\nOverall\\nFlashFill\\n689.7\\n1757.0\\n734.1\\n1116.4\\nFlashFill++\\n203.0\\n217.1\\n246.4\\n228.5\\nDuet\\n264.0\\n558.4\\n1351.7\\n766.74\\n(b) Average time (in ms) taken by the tools\\n(rows) over successful benchmarks from the three\\nsources (columns), and the average for each tool\\nover the entire suite (last column).\\nFig. 8. FlashFill++ is generally faster ‚Äì up to two orders of magnitude ‚Äì than the best of FlashFill and Duet,\\nand the slowdowns are mostly on fast benchmarks.\\nDespite solving more tasks, FlashFill++ continues to require a reasonable number of examples\\ncompared to baselines, showing that it generalizes to unseen examples and does not overfit.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 21, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Despite solving more tasks, FlashFill++ continues to require a reasonable number of examples\\ncompared to baselines, showing that it generalizes to unseen examples and does not overfit.\\nWe next compare synthesis times (averaged over 5 runs). In particular, we compute the synthesis\\ntime when the tools are given the minimum number of examples they required to produce the\\ncorrect program. Figure 8 shows the results. We restrict this experiment to FlashFill, Duet, and\\nFlashFill++, as synthesis time for SmartFill is unobservable through the browser.\\nFigure 8a compares FlashFill++ with the faster of FlashFill and Duet. We focus on benchmarks that\\nare solved by FlashFill++, and by either FlashFill or Duet. The y-axis displays the log base 10 of the\\nratio of the best of FlashFill and Duet synthesis time to the FlashFill++ synthesis time. A higher value\\nrepresents a larger reduction in synthesis time. On the x-axis we display the best of FlashFill and'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 21, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='represents a larger reduction in synthesis time. On the x-axis we display the best of FlashFill and\\nDuet synthesis time (in milliseconds) for that benchmark task. FlashFill++ reduces synthesis time\\nin 63% of the benchmarks, and the remaining 37% happen to be benchmarks where synthesis is\\nfast (< 500ùëöùë†in most cases) and slowdown is likely not observable in a user-facing application. In\\n37.3% cases, FlashFill++ is at least one order of magnitude faster and in 18% cases it is at least two\\norders of magnitude faster. Table 8b shows the average synthesis time for various tools across the\\nbenchmark classes. We averaged over the benchmarks that the tool solved. We see that FlashFill++\\nhas better averages despite solving more (presumably harder) benchmarks.\\nFlashFill++ is faster on average despite solving more (presumably harder) benchmarks and better\\nthan the best of the baselines on most hard instances.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 21, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++ is faster on average despite solving more (presumably harder) benchmarks and better\\nthan the best of the baselines on most hard instances.\\nFinally, we evaluate the gain from using gDSLs. We create FlashFill++‚àíby replacing gDSL used\\nin FlashFill++ by a regular DSL ( |‚ä≤operator is treated as the usual |). We compare FlashFill++ and\\nFlashFill++‚àíon synthesis time and minimum examples required metrics. Figure 9b summarize the\\nresults. We note that gDSLs reduce synthesis time in 91% of the benchmark tasks, give more than\\n3x speedup in 20% of cases, and generate better performance across the benchmarks and metrics.\\nPrecedences in gDSLs consistently help FlashFill++ in improving both search (synthesis time)\\nand ranking (minimum number of examples).\\n6.3\\nCode Readability\\nTraditionally DSL design has focused on efficacy of the learning and ranking process, and not on\\nreadable code generation. We evaluated the extent to which FlashFill++ enables such readable code.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 21, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Traditionally DSL design has focused on efficacy of the learning and ranking process, and not on\\nreadable code generation. We evaluated the extent to which FlashFill++ enables such readable code.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 22, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:23\\nDuet\\nPlaygol\\nProse\\nOverall\\nAverage Synthesis Time\\nFlashFill++\\n203.0\\n217.1\\n246.4\\n228.5\\nFlashFill++ ‚àí\\n255.0\\n350.6\\n410.3\\n361.1\\nAverage #examples required\\nFlashFill++\\n1.69\\n1.46\\n1.52\\n1.53\\nFlashFill++ ‚àí\\n1.76\\n1.54\\n1.56\\n1.59\\n(a) Synthesis time and no. of examples required to learn.\\n0\\n2000\\n4000\\n6000\\n8000\\nSynthesis Time FF++ (Ablated) (ms)\\n‚àí0.50\\n‚àí0.25\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\nlog10(FF++ (Ablated) / FlashFill++)\\nOutcome\\nFlashFill++ worse\\nFlashFill++ better\\n(b) Synthesis Time Ratios.\\nFig. 9. FlashFill++ improves over an ablation that removes the use of gDSLs, denoted FlashFill++‚àí.\\nFirst, we compared the code complexity for programs synthesized by FlashFill++ and FlashFill for\\ntwo target languages: Python & PowerFx, the low/no-code language of the Power platform [PowerFx\\n2021]. On average, FlashFill++‚Äôs Python is 76% shorter and uses 83% fewer functions, whereas'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 22, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='two target languages: Python & PowerFx, the low/no-code language of the Power platform [PowerFx\\n2021]. On average, FlashFill++‚Äôs Python is 76% shorter and uses 83% fewer functions, whereas\\nFlashFill++‚Äôs PowerFx is 57% shorter and uses 55% fewer functions. We next compared the Google\\nSheets formula language code generated by SmartFill with Excel code generated by FlashFill++. We\\nfound that FlashFill++ does better in ‚âà60% of the formulas generated and is at parity for ‚âà20% of\\nthe formulas. We did not compare with Duet since it generates programs only in its DSL and not in\\nany target language.\\nNext, we carried out a survey-based user study where we asked users to read and compare\\ndifferent Python functions synthesized by alternative approaches. In our study, we further augment\\nFlashFill++ with a procedure to rename variables. This part of the system is optional, and is only\\nadded to further underscore the benefits of readable code generation. To perform variable renaming'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 22, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++ with a procedure to rename variables. This part of the system is optional, and is only\\nadded to further underscore the benefits of readable code generation. To perform variable renaming\\nwe use the few-shot learning capability of Codex [Chen et al. 2021b], a pretrained large language\\nmodel, and iteratively provide the following prompt [Gao et al. 2021]: (1) two samples of the\\nrenaming task, where each sample contains I/O examples, FlashFill++ program, and the renamed\\nprogram, (2) the current renaming task, which contains the examples and the FlashFill++ program\\nto be renamed, and (3) partially renamed program up to the next non-renamed variable.\\nWe sampled 10 tasks from our benchmarks, with probability proportional to the number of\\nidentifier renaming calls made to Codex. For each task, we displayed the Python code generated by\\nFlashFill, FlashFill++, and FlashFill++ with Codex (anonymized as A, B, and C). For the first 5 tasks,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 22, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill, FlashFill++, and FlashFill++ with Codex (anonymized as A, B, and C). For the first 5 tasks,\\nthe participants were asked (on a 7-point Likert scale) the extent to which they agreed with the\\nstatements ‚ÄúFlashFill++ is more readable than FlashFill‚Äù and ‚ÄúFlashFill++ with Codex is more readable\\nthan FlashFill++‚Äù. For the last 5 tasks, the participants answered (on a 7-point Likert scale) the extent\\nto which they agreed with the statement ‚ÄúX is similar to the code I write‚Äù, where X was replaced\\nwith the corresponding (anonymized) system name.\\nFigure 10a shows that participants found code generated by FlashFill++ (without identifier renam-\\ning) was more readable than code generated by FlashFill for the same task. Adding Codex-based\\nrenaming further improved readability with most participants at least somewhat agreeing.\\nFigure 10b shows that participants strongly disagreed that FlashFill code is similar to the code'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 22, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='renaming further improved readability with most participants at least somewhat agreeing.\\nFigure 10b shows that participants strongly disagreed that FlashFill code is similar to the code\\nthey write. In contrast, most participants at least somewhat agreed that FlashFill++ code is similar\\nto the code they write. Adding identifier renaming resulted in improvements, across all five tasks.\\nWe also provided an open-ended text box for additional feedback with each task. Here are some\\nillustrative excerpts, where we have replaced the anonymized system names (A,B,C) with meaningful\\ncounterparts: FlashFill: ‚Äúis a mess‚Äù, FlashFill++: ‚Äúvery readable‚Äù, FlashFill++ +Codex: \"parameter name is\\nmore self describing‚Äù; ‚ÄúFlashFill is just confusing while FlashFill++ and/or FlashFill++ with Codex are\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 23, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:24\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nD3 D2 D1 N S1 S2 S3\\nResponse\\n0\\n50\\n100\\n150\\n200\\n# of Participants\\ncomparison\\nFF++ > FF\\nFF++ w/Codex > FF++\\n(a) Statement: X is more readable than Y (denoted\\nas ùëã> ùëå). Most participants found FlashFill++\\nbetter than FlashFill. Adding Codex further im-\\nproved it.\\nD3 D2 D1 N S1 S2 S3\\nResponse\\n0\\n50\\n100\\n150\\n# of Participants\\nApproach\\nFF\\nFF++\\nFF++ w/Codex\\n(b) Statement: X is similar to the code I write. Par-\\nticipants did not find FlashFill similar. FlashFill++\\nwas closer, but FlashFill++ +Codex most similar.\\nFig. 10. Survey: D3-S3 Strongly disagree/agree. FF=FlashFill, FF++=FlashFill++, FF++ w/Codex=FlashFill++\\nwith Codex.\\nquite simple and direct‚Äù; and ‚ÄúFlashFill is very badly written, and FlashFill++ with Codex‚Äôs parameter\\nname tells a much better story‚Äù.\\n7\\nDISCUSSION\\n7.1\\nRelated Work\\nCuts are closely related to the widely-studied concept of accelerations in program analysis. Accel-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 23, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='name tells a much better story‚Äù.\\n7\\nDISCUSSION\\n7.1\\nRelated Work\\nCuts are closely related to the widely-studied concept of accelerations in program analysis. Accel-\\nerations are used to capture the effect (transitive closure) of multiple state transitions by a single\\n‚Äúmeta transition‚Äù [Finkel 1987; Karp and Miller 1969]. It has often been used to handle numerical\\nupdates in programs, especially when more classical abstract interpretation techniques either do\\nnot converge or become very imprecise [Bardin et al. 2008; Boigelot 2003; Jeannet et al. 2014].\\nOur use of cuts inherits its motivation and purpose from these works, but applies them to PBE.\\nWhereas in program analysis, accelerations had success mostly on numerical domains, in PBE we\\nfind cuts helpful more generally. Currently, we assume cuts are provided by the DSL designer, but\\nautomatically generating them remains an interesting topic for future research.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 23, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='find cuts helpful more generally. Currently, we assume cuts are provided by the DSL designer, but\\nautomatically generating them remains an interesting topic for future research.\\nIn PBE, cuts help speed-up search (by guiding top-down propagation across non-EI operators\\nbased on abstracting behavior of inner sub-DSLs). Other ways to speed-up search include using\\ntypes and other forms of abstractions [Guo et al. 2020; Osera and Zdancewic 2015; Wang et al.\\n2018], or combining search strategies [Lee 2021]. The difference between cuts and abstraction-based\\nmethods in synthesis is the same as the difference between accelerations and abstraction in program\\nanalysis. We need cuts for only some operators, whereas abstract transformers are required for\\nall operators. Moreover, the methods are not incompatible ‚Äì a promising direction would be to\\ncombine them.\\nMiddle-out synthesis, enabled by cuts, is a new way to combine bottom-up [Alur et al. 2013,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 23, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='all operators. Moreover, the methods are not incompatible ‚Äì a promising direction would be to\\ncombine them.\\nMiddle-out synthesis, enabled by cuts, is a new way to combine bottom-up [Alur et al. 2013,\\n2017] and top-down [Gulwani 2011; Polozov and Gulwani 2015] synthesis. It is very different from\\nthe meet-in-the-middle way of combining them where search starts simultaneously from the bottom\\n(enumerating subprograms that generate new values) and from the top (back propagating the\\noutput) until we find values that connect the two [Gulwani et al. 2011; Lee 2021]. Helped by the\\njump provided by cuts, middle-out synthesis starts at the middle creating two subproblems that can\\nbe solved using either approach. Meet-in-the-middle approach in [Lee 2021] guides the top-down\\nsearch based on the values propagated by bottom-up, similar to middle-out synthesis; however, our\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 24, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:25\\ncuts are more general and can handle more scenarios because they overcome issues (not effectively\\nenumerable operators and large number of constants) that may make partial bottom-up fail.\\nMiddle-out synthesis can be viewed as a divide-and-conquer strategy for synthesis. The coopera-\\ntive synthesis framework in [Huang et al. 2020] proposes 3 such strategies that are used when the\\noriginal synthesis problem remains unsolved. However, [Huang et al. 2020] works on complete\\nlogical specifications, and not on input-output examples.\\nAt a very abstract level, top-down synthesis and middle-out synthesis can both be viewed as\\nan approach that synthesizes a sketch and then fills the sketch in a PBE setting [Feng et al. 2017;\\nPolikarpova et al. 2016; Wang et al. 2017, 2020]. In this context, Scythe [Wang et al. 2017] uses\\noverapproximation of the set of values that are computed by partial programs to synthesize a sketch.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 24, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Polikarpova et al. 2016; Wang et al. 2017, 2020]. In this context, Scythe [Wang et al. 2017] uses\\noverapproximation of the set of values that are computed by partial programs to synthesize a sketch.\\nUnlike our work, [Wang et al. 2017] is exclusively focused on bottom-up synthesis. Since [Wang\\net al. 2017] is bottom-up, its approximations get coarser as the program gets deeper. Scythe, tends\\nto do well for shallow programs. FlashFill++‚Äôs use of cuts is more ‚Äúon-demand‚Äù and thus not affected\\nby depth of programs. In fact, FlashFill++ can synthesize deep programs, with the largest solution in\\nour benchmark suite having a depth of 24, with over 10% of our benchmarks requiring programs\\nwith a depth of at least 10. Furthermore, [Wang et al. 2017] is exclusively focused on SQL ‚Äì its\\nmain contribution is an approach to overapproximate SQL queries that abstracts carefully selected\\nnonterminals. In contrast, our formalization is not fixed to any particular DSL, but relies on the'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 24, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='main contribution is an approach to overapproximate SQL queries that abstracts carefully selected\\nnonterminals. In contrast, our formalization is not fixed to any particular DSL, but relies on the\\nDSL designer to provide the cuts.\\nMorpheus [Feng et al. 2017] and Synquid [Polikarpova et al. 2016] overapproximate each com-\\nponent to prune partial programs to synthesize sketches that are subsequently filled. Morpheus\\nis specialized to tables and uses the distinction between value and table transformations. In con-\\ntrast, our framework is more general as it allows the use of approximations (cuts) for only certain\\nfunctions (wherever they are provided); however, we cannot (and do not) prune partial programs.\\nWe always work on concrete values ‚Äì there is no abstract domain involved. We do not use SMT\\nsolvers, whereas SMT solvers are a key component of [Feng et al. 2017; Polikarpova et al. 2016].\\nPrecedence and gDSLs. Precedences or priorities have been used in many grammar formalisms,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 24, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='solvers, whereas SMT solvers are a key component of [Feng et al. 2017; Polikarpova et al. 2016].\\nPrecedence and gDSLs. Precedences or priorities have been used in many grammar formalisms,\\nbut mainly for achieving disambiguation while parsing in ambiguous grammars [Aasa 1995; Aho\\net al. 1973; Earley 1974; Heering et al. 1989]. Disambiguation here refers to preferring the parse\\nùëé+(ùëè‚àóùëê) over (ùëé+ùëè)‚àóùëêfor the same string ùëé+ùëè‚àóùëê. In contrast, we use gDSLs to compare derivations\\nof different strings (programs). Furthermore, in the work on filters and SDF [Heering et al. 1989],\\nthe semantics of the precedence relation ‚âªon rules is different: there ùëÜ1 ‚Üíùë§1 ‚âªùëÜ2 ‚Üíùë§2 means\\nthat one can not use ùëÜ2 ‚Üíùë§2 as a child of ùëÜ1 ‚Üíùë§1 in a parse tree [van den Brand et al. 2002]. In\\nour case, we disallow precedence on rules with different left-hand nonterminals. Nevertheless, our\\nprecedence can be viewed as a specialized filter in the terminology of [van den Brand et al. 2002].'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 24, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='our case, we disallow precedence on rules with different left-hand nonterminals. Nevertheless, our\\nprecedence can be viewed as a specialized filter in the terminology of [van den Brand et al. 2002].\\nFarzan and Nicolet [Farzan and Nicolet 2021] use a sub-grammar to optimize search. This can be\\nmodeled using our precedence operator. They use constraint-based synthesis (using SMT solvers)\\nwhere the interest is in any one correct program. Ranking is not of interest there, whereas we use\\nprecedence in the context of top-down synthesis where we synthesize program sets and rank them.\\nThe interaction of precedence in the grammar and the program ranking is one of our contributions.\\nCasper [Ahmad and Cheung 2018] uses a hierarchy of grammars growing in size for synthesis,\\nmaking search efficient. This hierarchy is dynamically generated - guided by the input-output\\nexample. Our precedence-based approach provides a different mechanism to constrain search. The'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 24, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='making search efficient. This hierarchy is dynamically generated - guided by the input-output\\nexample. Our precedence-based approach provides a different mechanism to constrain search. The\\nvalue of our approach is that it is easily integrated with the underlying synthesis framework, giving\\nsynthesis-engine-builders more flexibility in controlling search and ranking. Precedence is also\\nintuitive for DSL designers because they must think only locally to decide if the operators need a\\nprecedence relation. Technically speaking, our notion of precedence implicitly represents a lattice\\nof grammars rather than a strict linear hierarchy.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 25, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:26\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nEarlier work on ‚ÄúParsing expression grammars‚Äù [Ford 2004] introduces grammars that are like\\nCFGs, but contain features such as prioritized choice (precedence), greedy repetitions, etc., but\\nit does so for parsing, whereas our focus is on top-down, bottom-up, and combination synthesis\\ntechniques. Our novelty is in supporting precedence in our synthesis framework, and formally\\nworking out how it impacts ranking and search.\\nUsing precedence is one way to handle potentially redundant operators in the DSL and prune\\nsearch space. The other way is to explicitly write the equivalence relation on programs and\\nonly consider programs that are canonical representatives of each equivalence class [Osera and\\nZdancewic 2015; Smith and Albarghouthi 2019; Udupa et al. 2013]. The gDSL approach is low\\noverhead, but may consider equivalent programs during search. However, this is by design as our'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 25, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Zdancewic 2015; Smith and Albarghouthi 2019; Udupa et al. 2013]. The gDSL approach is low\\noverhead, but may consider equivalent programs during search. However, this is by design as our\\ngoal is to generate whatever program leads to most naturally readable code.\\nPrecedence of grammar rules can be viewed as a specialized case of probabilistic context-free\\ngrammars (pCFGs) that have been used to bias the enumeration of programs through their gram-\\nmar [Lee et al. 2018; Liang et al. 2010; Menon et al. 2013]. While specialized, precedence is actually\\npreferable in many scenarios where we want to synthesize not just any program that works on the\\ninput-output examples, but one that has other desirable properties, such as, the program generalizes\\nto unseen inputs and has a readable translation in target language. As such, precedence in gDSLs\\ngive designers of synthesizers a clean way to state their ranking preference. Weights in a pCFG'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 25, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='to unseen inputs and has a readable translation in target language. As such, precedence in gDSLs\\ngive designers of synthesizers a clean way to state their ranking preference. Weights in a pCFG\\nhave to be learned from data or manually set ‚Äì both are daunting tasks compared to writing a gDSL.\\nThe contrast between pCFGs and gDSLs is akin to that between neural and symbolic approaches.\\nFlashFill [Gulwani 2011] demonstrated the effectiveness of inductive synthesis at tackling\\ncomplex string transformation. FlashMeta [Polozov and Gulwani 2015] recognized that many\\npopular inductive synthesizers [Gulwani 2011; Le and Gulwani 2014] could be decomposed into\\ndomain-specific features, such as the DSL operators and their semantics, and general (shareable)\\ndeductive steps. We used the FlashMeta framework to implement FlashFill, based on the original\\npaper [Gulwani 2011], for our experiments. We also built FlashFill++ the same way, which extends'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 25, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='deductive steps. We used the FlashMeta framework to implement FlashFill, based on the original\\npaper [Gulwani 2011], for our experiments. We also built FlashFill++ the same way, which extends\\nthe capabilities of FlashFill to include new operations like date and number formatting, and also\\nfocuses on generating readable code.\\nIn recent work [Verbruggen et al. 2021], FlashFill has been combined with a pre-trained language\\nmodel (LM), GPT3, to facilitate semantic transformations, such as converting the city \"San Francisco\"\\nto the state \"CA\". Complementing FlashFill‚Äôs syntactic effectiveness with GPT3‚Äôs ability to do\\nsemantic transformation is interesting, but orthogonal to the problem here. However, we do exploit\\na LM to (optionally) generate meaningful variable names for our user study on code readability.\\nTrust and readability. Wrex [Drosos et al. 2020] argues that readable code is indispensable'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 25, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='a LM to (optionally) generate meaningful variable names for our user study on code readability.\\nTrust and readability. Wrex [Drosos et al. 2020] argues that readable code is indispensable\\nfor users of synthesis technology. Wrex employed hand-written rewrite rules to produce readable\\ncode for their user study. However, this is not an approach that easily extends to all scenarios and\\nlarger languages. FlashFill++ is inspired by Wrex [Drosos et al. 2020] to address the readable code\\ngeneration challenge in a more general and scalable way: redesigning the DSL used with a focus\\non enabling readable code generation, rather than post-processing.\\nZhang et al [Zhang et al. 2021, 2020] introduced a system for interpretable synthesis, where the\\nuser interacts with the synthesizer. This approach is complementary to FlashFill++.\\n7.2\\nLimitations\\nThe concepts of cuts and precedence have been developed exclusively for synthesis approaches'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 25, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='user interacts with the synthesizer. This approach is complementary to FlashFill++.\\n7.2\\nLimitations\\nThe concepts of cuts and precedence have been developed exclusively for synthesis approaches\\nbased on concrete values, so-called version space algebra (VSA) based methods, in this paper. The\\nterm top-down, respectively bottom-up, has been used for techniques that are based on propagation\\nof concrete output (respectively, input) values through partial sketches (respectively, programs)\\ntypically represented using VSAs. In systems that represent approximations of the sets of values\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 26, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:27\\n(at each component) using abstractions or refinement types (e.g., Synquid, Morpheus, etc), cuts\\ncan potentially address the issue of abstractions becoming too coarse as they are composed from\\nabstractions of sub-components. This is similar to how interpolants can be used to compute tight\\ninvariants in program verification when successive application of abstract transformers lead to\\noverly general invariants. Studying broader implications of these ideas is left for future work.\\nNontrivial cuts that go beyond bottom-up or top-down approaches can only be computed if there\\nare nontrivial invariants that hold (about the values that are generated) at certain intermediate\\nnonterminals in the grammar. Moreover, the DSL designer must be aware of these invariants. The\\nDSL designer can choose to write cuts that are incomplete in theory, but reasonable in practice,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 26, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='nonterminals in the grammar. Moreover, the DSL designer must be aware of these invariants. The\\nDSL designer can choose to write cuts that are incomplete in theory, but reasonable in practice,\\nguided by their understanding of the domain. Nontrivial cuts are likely to exist in large DSLs where\\ndifferent forms of values flow on different paths. We have not done a formal study of how difficult\\nit is for a DSL designer to write useful cut functions‚Äîthis is beyond the scope of the current paper\\nwhich just lays down the foundations for cuts.\\nDesigning a guarded DSL requires the DSL designer to have clear preference for certain operators\\nover other alternatives, and moreover, any precedence on operators closer to the start symbol (in the\\ngrammar) should override any precedence on operators closer to the leaves of the program tree. The\\n‚Äúhigher-up‚Äù operators in any DSL typically establish the high-level tactic of the program, and hence'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 26, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='grammar) should override any precedence on operators closer to the leaves of the program tree. The\\n‚Äúhigher-up‚Äù operators in any DSL typically establish the high-level tactic of the program, and hence\\nthis requirement often holds. Precedences in a grammar are likely to exist if it contains redundant\\noperators that are some preferred compositions of other low-level operators in the grammar. Further,\\nthere are certain technical assumptions we make about precedences. The assumption that the\\nprecedence is a series-parallel partial order is not required if we start with the gDSL (rather than\\nstart with precedences), which is what we do in practice. The assumption that the reachability\\nrelation on the grammar nonterminals be acyclic is required only to provide a clean mathematical\\ninterpretation of the ranking induced by gDSL on programs (terms) as a path ordering. In the\\npresence of cycles, the synthesis rule and the full system can still be used without problems, but'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 26, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='interpretation of the ranking induced by gDSL on programs (terms) as a path ordering. In the\\npresence of cycles, the synthesis rule and the full system can still be used without problems, but\\nranking cannot be described in a simple way.\\n8\\nCONCLUSION\\nWe introduced two techniques, cuts and precedence through guarded DSLs, that DSL designers can\\nuse to prune search in programming by example. Cuts enable a novel synthesis strategy: middle-\\nout synthesis. This strategy allows FlashFill++ to support synthesis tasks that require non-EI/EE\\noperators, such as datetime and numeric transformations. The use of precedence through gDSLs\\nallows us to increase the size of our DSL, by incorporating redundant operators, which facilitate\\nreadable code generation in different target languages. We compare our tool to existing state-of-\\nthe-art PBE systems, FlashFill, Duet, and SmartFill, on three public benchmark datasets and show'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 26, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='readable code generation in different target languages. We compare our tool to existing state-of-\\nthe-art PBE systems, FlashFill, Duet, and SmartFill, on three public benchmark datasets and show\\nthat FlashFill++ can solve more tasks, in less time, and without substantially increasing the number\\nof examples required. We also perform a survey-based study on code readability, confirming that\\nthe programs synthesized by FlashFill++ are more readable than those generated by FlashFill.\\nREFERENCES\\nAnnika Aasa. 1995. Precedences in specifications and implementations of programming languages. Theoretical Computer\\nScience 142, 1 (1995), 3‚Äì26.\\nMaaz Bin Safeer Ahmad and Alvin Cheung. 2018. Automatically Leveraging MapReduce Frameworks for Data-Intensive\\nApplications. In Proc. 2018 International Conference on Management of Data, SIGMOD Conference. ACM, 1205‚Äì1220.\\nhttps://doi.org/10.1145/3183713.3196891'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 26, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Applications. In Proc. 2018 International Conference on Management of Data, SIGMOD Conference. ACM, 1205‚Äì1220.\\nhttps://doi.org/10.1145/3183713.3196891\\nAlfred Aho, S. Johnson, and Jeffrey Ullman. 1973. Deterministic parsing of ambiguous grammars. Commun. ACM 18 (01\\n1973), 441‚Äì452. https://doi.org/10.1145/360933.360969\\nRajeev Alur, Rastislav Bod√≠k, Garvit Juniwal, Milo M. K. Martin, Mukund Raghothaman, Sanjit A. Seshia, Rishabh Singh,\\nArmando Solar-Lezama, Emina Torlak, and Abhishek Udupa. 2013. Syntax-Guided Synthesis. In Formal Methods in\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:28\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nComputer-Aided Design, FMCAD 2013. 1‚Äì8.\\nRajeev Alur, Arjun Radhakrishna, and Abhishek Udupa. 2017. Scaling Enumerative Program Synthesis via Divide and\\nConquer. In TACAS. 319‚Äì336.\\nS√©bastien Bardin, Alain Finkel, J√©r√¥me Leroux, and Laure Petrucci. 2008. FAST: acceleration from theory to practice. Int. J.\\nSoftw. Tools Technol. Transf. 10, 5 (2008), 401‚Äì424. https://doi.org/10.1007/s10009-008-0064-3\\nDenis B√©chet, Philippe de Groote, and Christian Retor√©. 1997. A Complete Axiomatisation for the Inclusion of Series-Parallel\\nPartial Orders. In Rewriting Techniques and Applications, 8th Int. Conf., RTA-97 (Lecture Notes in Computer Science,\\nVol. 1232). Springer, 230‚Äì240. https://doi.org/10.1007/3-540-62950-5_74\\nBernard Boigelot. 2003. On iterating linear transformations over recognizable sets of integers. Theor. Comput. Sci. 309, 1-3\\n(2003), 413‚Äì468. https://doi.org/10.1016/S0304-3975(03)00314-1'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Bernard Boigelot. 2003. On iterating linear transformations over recognizable sets of integers. Theor. Comput. Sci. 309, 1-3\\n(2003), 413‚Äì468. https://doi.org/10.1016/S0304-3975(03)00314-1\\nSwarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, and Yisong Yue. 2021. Neu-\\nrosymbolic Programming. Found. Trends Program. Lang. 7, 3 (2021), 158‚Äì243.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards,\\nYuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser,\\nMohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,\\nIgor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua\\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter\\nWelinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021b. Evaluating Large\\nLanguage Models Trained on Code. CoRR abs/2107.03374 (2021). arXiv:2107.03374 https://arxiv.org/abs/2107.03374\\nXinyun Chen, Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, and Denny Zhou. 2021a. Spreadsheet-\\nCoder: Formula Prediction from Semi-structured Context. In Proceedings of the 38th International Conference on Machine\\nLearning (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 1661‚Äì1672.\\nhttps://proceedings.mlr.press/v139/chen21m.html'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Learning (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 1661‚Äì1672.\\nhttps://proceedings.mlr.press/v139/chen21m.html\\nAndrew Cropper. 2019. Playgol: Learning Programs Through Play. In Proceedings of the Twenty-Eighth International Joint\\nConference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, Sarit Kraus (Ed.). ijcai.org, 6074‚Äì6080.\\nhttps://doi.org/10.24963/ijcai.2019/841 https://github.com/andrewcropper/ijcai19-playgol.\\nNachum Dershowitz and Jean-Pierre Jouannaud. 1990. Rewrite Systems. In Handbook of Theoretical Computer Science,\\nVolume B: Formal Models and Semantics. Elsevier and MIT Press, 243‚Äì320.\\nJacob Devlin, Rudy Bunel, Rishabh Singh, Matthew J. Hausknecht, and Pushmeet Kohli. 2017. Neural Program Meta-Induction.\\nIn NIPS, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and\\nRoman Garnett (Eds.). 2080‚Äì2088.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='In NIPS, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and\\nRoman Garnett (Eds.). 2080‚Äì2088.\\nIan Drosos, Titus Barik, Philip J. Guo, Robert DeLine, and Sumit Gulwani. 2020. Wrex: A Unified Programming-by-Example\\nInteraction for Synthesizing Readable Code for Data Scientists. In Proceedings of the 2020 CHI Conference on Human\\nFactors in Computing Systems (Honolulu, HI, USA) (CHI ‚Äô20). Association for Computing Machinery, New York, NY, USA,\\n1‚Äì12. https://doi.org/10.1145/3313831.3376442\\nJay Earley. 1974. Ambiguity and Precedence in Syntax Description. Acta Informatica 4 (1974), 183‚Äì192.\\nAzadeh Farzan and Victor Nicolet. 2021. Phased synthesis of divide and conquer programs. In PLDI ‚Äô21: 42nd ACM SIGPLAN\\nInternational Conference on Programming Language Design and Implementation. ACM, 974‚Äì986. https://doi.org/10.1145/\\n3453483.3454089'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='International Conference on Programming Language Design and Implementation. ACM, 974‚Äì986. https://doi.org/10.1145/\\n3453483.3454089\\nYu Feng, Ruben Martins, Jacob Van Geffen, Isil Dillig, and Swarat Chaudhuri. 2017. Component-based synthesis of table\\nconsolidation and transformation tasks from examples. In Proc. 38th ACM SIGPLAN Conference on Programming Language\\nDesign and Implementation, PLDI. ACM, 422‚Äì436. https://doi.org/10.1145/3062341.3062351\\nAlain Finkel. 1987. A Generalization of the Procedure of Karp and Miller to Well Structured Transition Systems. In Proc.\\n14th Intl. Colloquium on Automata, Languages and Programming, ICALP87 (Lecture Notes in Computer Science, Vol. 267),\\nThomas Ottmann (Ed.). Springer, 499‚Äì508. https://doi.org/10.1007/3-540-18088-5_43\\nBryan Ford. 2004. Parsing expression grammars: a recognition-based syntactic foundation. In Proc. 31st ACM SIGPLAN-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Thomas Ottmann (Ed.). Springer, 499‚Äì508. https://doi.org/10.1007/3-540-18088-5_43\\nBryan Ford. 2004. Parsing expression grammars: a recognition-based syntactic foundation. In Proc. 31st ACM SIGPLAN-\\nSIGACT Symposium on Principles of Programming Languages, POPL. ACM, 111‚Äì122. https://doi.org/10.1145/964001.964011\\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained Language Models Better Few-shot Learners. In\\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\\nConference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, Online,\\n3816‚Äì3830. https://doi.org/10.18653/v1/2021.acl-long.295\\nGoogle. 2021. SpreadSheetCoder. https://github.com/google-research/google-research/tree/master/spreadsheet_coder\\nSumit Gulwani. 2011. Automating string processing in spreadsheets using input-output examples. In POPL. 317‚Äì330.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 27, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Sumit Gulwani. 2011. Automating string processing in spreadsheets using input-output examples. In POPL. 317‚Äì330.\\nSumit Gulwani. 2016. Programming by Examples - and its applications in Data Wrangling. In Dependable Software Systems\\nEngineering. 137‚Äì158.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='FlashFill++: Scaling PBE by Cutting to the Chase\\n33:29\\nSumit Gulwani, William R. Harris, and Rishabh Singh. 2012. Spreadsheet data manipulation using examples. Commun.\\nACM 55, 8 (2012), 97‚Äì105.\\nS. Gulwani, V. Korthikanti, and A. Tiwari. 2011. Synthesizing geometry constructions. In Proc. ACM Conf. on Prgm. Lang.\\nDesgn. and Impl. PLDI. 50‚Äì61.\\nSumit Gulwani, Oleksandr Polozov, and Rishabh Singh. 2017. Program Synthesis. Foundations and Trends in Programming\\nLanguages 4, 1-2 (2017), 1‚Äì119.\\nZheng Guo, Michael James, David Justo, Jiaxiao Zhou, Ziteng Wang, Ranjit Jhala, and Nadia Polikarpova. 2020. Program\\nsynthesis by type-guided abstraction refinement. Proc. ACM Program. Lang. 4, POPL (2020), 12:1‚Äì12:28. https://doi.org/\\n10.1145/3371080\\nJan Heering, P. R. H. Hendriks, Paul Klint, and J. Rekers. 1989. The syntax definition formalism SDF - reference manual.\\nACM SIGPLAN Notices 24, 11 (1989), 43‚Äì75.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='10.1145/3371080\\nJan Heering, P. R. H. Hendriks, Paul Klint, and J. Rekers. 1989. The syntax definition formalism SDF - reference manual.\\nACM SIGPLAN Notices 24, 11 (1989), 43‚Äì75.\\nKangjing Huang, Xiaokang Qiu, Peiyuan Shen, and Yanjun Wang. 2020. Reconciling enumerative and deductive program\\nsynthesis. In Proc. 41st ACM SIGPLAN Intl. Conf. on Programming Language Design and Implementation, PLDI, Alastair F.\\nDonaldson and Emina Torlak (Eds.). ACM, 1159‚Äì1174. https://doi.org/10.1145/3385412.3386027\\nBertrand Jeannet, Peter Schrammel, and Sriram Sankaranarayanan. 2014. Abstract acceleration of general linear loops.\\nIn The 41st Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL. ACM, 529‚Äì540.\\nhttps://doi.org/10.1145/2535838.2535843\\nAshwin Kalyan, Abhishek Mohta, Alex Polozov, Dhruv Batra, Prateek Jain, and Sumit Gulwani. 2018. Neural-Guided\\nDeductive Search for Real-Time Program Synthesis from Examples. In 6th International Conference on Learning Repre-'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Deductive Search for Real-Time Program Synthesis from Examples. In 6th International Conference on Learning Repre-\\nsentations (ICLR) (6th international conference on learning representations (iclr) ed.). https://www.microsoft.com/en-\\nus/research/publication/neural-guided-deductive-search-real-time-program-synthesis-examples/\\nRichard M. Karp and Raymond E. Miller. 1969. Parallel Program Schemata. J. Comput. Syst. Sci. 3, 2 (1969), 147‚Äì195.\\nhttps://doi.org/10.1016/S0022-0000(69)80011-5\\nVu Le and Sumit Gulwani. 2014. FlashExtract: A Framework for Data Extraction by Examples. In PLDI. 542‚Äì553.\\nWoosuk Lee. 2021. Combining the top-down propagation and bottom-up enumeration for inductive program synthesis.\\nProc. ACM Program. Lang. 5, POPL (2021), 1‚Äì28. https://doi.org/10.1145/3434335 https://github.com/wslee/duet.\\nWoosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. 2018. Accelerating search-based program synthesis using'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. 2018. Accelerating search-based program synthesis using\\nlearned probabilistic models. In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and\\nImplementation, PLDI, Jeffrey S. Foster and Dan Grossman (Eds.). ACM, 436‚Äì449. https://doi.org/10.1145/3192366.3192410\\nPercy Liang, Michael I. Jordan, and Dan Klein. 2010. Learning Programs: A Hierarchical Bayesian Approach. In Proceedings\\nof the 27th International Conference on Machine Learning (ICML-10), Johannes F√ºrnkranz and Thorsten Joachims (Eds.).\\nOmnipress, 639‚Äì646.\\nDylan Lukes, John Sarracino, Cora Coleman, Hila Peleg, Sorin Lerner, and Nadia Polikarpova. 2021. Synthesis of web\\nlayouts from examples. In ESEC/FSE ‚Äô21: 29th ACM Joint European Software Engineering Conference and Symposium on the\\nFoundations of Software Engineering, Athens, Greece, August 23-28, 2021, Diomidis Spinellis, Georgios Gousios, Marsha'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Foundations of Software Engineering, Athens, Greece, August 23-28, 2021, Diomidis Spinellis, Georgios Gousios, Marsha\\nChechik, and Massimiliano Di Penta (Eds.). ACM, 651‚Äì663.\\nAditya Krishna Menon, Omer Tamuz, Sumit Gulwani, Butler W. Lampson, and Adam Kalai. 2013. A Machine Learning\\nFramework for Programming by Example. In Proceedings of the 30th International Conference on Machine Learning, ICML\\n(JMLR Workshop and Conference Proceedings, Vol. 28). JMLR.org, 187‚Äì195. http://proceedings.mlr.press/v28/menon13.html\\nAnders Miltner, Kathleen Fisher, Benjamin C. Pierce, David Walker, and Steve Zdancewic. 2018. Synthesizing bijective\\nlenses. Proc. ACM Program. Lang. 2, POPL (2018), 1:1‚Äì1:30.\\nNagarajan Natarajan, Danny Simmons, Naren Datha, Prateek Jain, and Sumit Gulwani. 2019. Learning Natural Programs\\nfrom a Few Examples in Real-Time. In AIStats. https://www.microsoft.com/en-us/research/publication/learning-natural-\\nprograms-from-a-few-examples-in-real-time/'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='from a Few Examples in Real-Time. In AIStats. https://www.microsoft.com/en-us/research/publication/learning-natural-\\nprograms-from-a-few-examples-in-real-time/\\nPeter-Michael Osera and Steve Zdancewic. 2015. Type-and-example-directed program synthesis. In Proc. 36th ACM SIGPLAN\\nConf. on Programming Language Design and Implementation. ACM, 619‚Äì630. https://doi.org/10.1145/2737924.2738007\\nRangeet Pan, Vu Le, Nachiappan Nagappan, Sumit Gulwani, Shuvendu K. Lahiri, and Mike Kaufman. 2021. Can Program\\nSynthesis be Used to Learn Merge Conflict Resolutions? An Empirical Analysis. In 43rd IEEE/ACM International Conference\\non Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021. IEEE, 785‚Äì796.\\nNadia Polikarpova, Ivan Kuraj, and Armando Solar-Lezama. 2016. Program synthesis from polymorphic refinement\\ntypes. In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation,'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 28, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='types. In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation,\\nPLDI 2016, Santa Barbara, CA, USA, June 13-17, 2016, Chandra Krintz and Emery D. Berger (Eds.). ACM, 522‚Äì538.\\nhttps://doi.org/10.1145/2908080.2908093\\nOleksandr Polozov and Sumit Gulwani. 2015. FlashMeta: A Framework for Inductive Program synthesis. In OOPSLA/SPLASH.\\n107‚Äì126.\\nPowerFx 2021. PowerFx: The low code programming language. https://powerapps.microsoft.com/en-us/blog/introducing-\\nmicrosoft-power-fx-the-low-code-programming-language-for-everyone/. Accessed: 2021-11-19.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 29, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='33:30\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nMicrosoft PROSE. 2022. PROSE public benchmark suite. https://github.com/microsoft/prose-benchmarks.\\nKia Rahmani, Mohammad Raza, Sumit Gulwani, Vu Le, Daniel Morris, Arjun Radhakrishna, Gustavo Soares, and Ashish\\nTiwari. 2021. Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis.\\nProc. ACM Program. Lang. 5, OOPSLA (2021), 1‚Äì29.\\nReudismam Rolim, Gustavo Soares, Loris D‚ÄôAntoni, Oleksandr Polozov, Sumit Gulwani, Rohit Gheyi, Ryo Suzuki, and Bj√∂rn\\nHartmann. 2017. Learning syntactic program transformations from examples. In ICSE. IEEE / ACM, 404‚Äì415.\\nSelenium. 2022. Selenium. https://github.com/SeleniumHQ/selenium\\nNischal Shrestha, Titus Barik, and Chris Parnin. 2018. It‚Äôs Like Python But: Towards Supporting Transfer of Programming\\nLanguage Knowledge. In 2018 IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC, J√°come'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 29, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Language Knowledge. In 2018 IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC, J√°come\\nCunha, Jo√£o Paulo Fernandes, Caitlin Kelleher, Gregor Engels, and Jorge Mendes (Eds.). IEEE Computer Society, 177‚Äì185.\\nhttps://doi.org/10.1109/VLHCC.2018.8506508\\nRishabh Singh and Sumit Gulwani. 2015. Predicting a Correct Program in Programming by Example. In CAV. 398‚Äì414.\\nCalvin Smith and Aws Albarghouthi. 2019. Program Synthesis with Equivalence Reduction. In VMCAI, Constantin Enea\\nand Ruzica Piskac (Eds.).\\nAbhishek Udupa, Arun Raghavan, Jyotirmoy V. Deshmukh, Sela Mador-Haim, Milo M. K. Martin, and Rajeev Alur. 2013.\\nTRANSIT: specifying protocols with concolic snippets. In ACM SIGPLAN Conference on Programming Language Design\\nand Implementation, PLDI, Hans-Juergen Boehm and Cormac Flanagan (Eds.). ACM, 287‚Äì296. https://doi.org/10.1145/\\n2491956.2462174\\nMark van den Brand, Jeroen Scheerder, Jurgen J. Vinju, and Eelco Visser. 2002. Disambiguation Filters for Scannerless'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 29, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='2491956.2462174\\nMark van den Brand, Jeroen Scheerder, Jurgen J. Vinju, and Eelco Visser. 2002. Disambiguation Filters for Scannerless\\nGeneralized LR Parsers. In Compiler Construction, 11th Intl. Conf, CC 2002, Part of ETAPS, Proceedings (Lecture Notes in\\nComputer Science, Vol. 2304). Springer, 143‚Äì158.\\nGust Verbruggen, Vu Le, and Sumit Gulwani. 2021. Semantic programming by example with pre-trained models. Proc. ACM\\nProgram. Lang. 5, OOPSLA (2021), 1‚Äì25. https://doi.org/10.1145/3485477\\nChenglong Wang, Alvin Cheung, and Rastislav Bod√≠k. 2017. Synthesizing highly expressive SQL queries from input-output\\nexamples. In Proc. 38th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI. ACM,\\n452‚Äì466. https://doi.org/10.1145/3062341.3062365\\nXinyu Wang, Isil Dillig, and Rishabh Singh. 2018. Program synthesis using abstraction refinement. Proc. ACM Program.\\nLang. 2, POPL (2018), 63:1‚Äì63:30. https://doi.org/10.1145/3158151'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 29, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='Xinyu Wang, Isil Dillig, and Rishabh Singh. 2018. Program synthesis using abstraction refinement. Proc. ACM Program.\\nLang. 2, POPL (2018), 63:1‚Äì63:30. https://doi.org/10.1145/3158151\\nYuepeng Wang, Rushi Shah, Abby Criswell, Rong Pan, and Isil Dillig. 2020. Data Migration using Datalog Program Synthesis.\\nProc. VLDB Endow. 13, 7 (2020), 1006‚Äì1019. https://doi.org/10.14778/3384345.3384350\\nTianyi Zhang, Zhiyang Chen, Yuanli Zhu, Priyan Vaithilingam, Xinyu Wang, and Elena L. Glassman. 2021. Interpretable\\nProgram Synthesis. Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3411764.3445646\\nTianyi Zhang, London Lowmanstone, Xinyu Wang, and Elena L. Glassman. 2020. Interactive Program Synthesis by\\nAugmented Examples. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology\\n(Virtual Event, USA) (UIST ‚Äô20). Association for Computing Machinery, New York, NY, USA, 627‚Äì648. https://doi.org/10.\\n1145/3379337.3415900'),\n",
       " Document(metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.23 (TeX Live 2021) kpathsea version 6.3.3', 'creator': 'LaTeX with acmart 2022/08/27 v1.87 Typesetting articles for the Association for Computing Machinery and hyperref 2021-06-07 v7.00m Hypertext links for LaTeX', 'creationdate': '2022-12-05T20:35:42+00:00', 'source': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'file_path': '..\\\\data\\\\pdf\\\\flashfill.pdf', 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'FlashFill++: Scaling Programming by Example by Cutting to the Chase', 'author': '', 'subject': '-  Software and its engineering  ->  Programming by example.Domain specific languages.', 'keywords': '', 'moddate': '2022-12-05T20:35:42+00:00', 'trapped': '', 'modDate': 'D:20221205203542Z', 'creationDate': 'D:20221205203542Z', 'page': 29, 'source_file': 'flashfill.pdf', 'file_type': 'pdf'}, page_content='(Virtual Event, USA) (UIST ‚Äô20). Association for Computing Machinery, New York, NY, USA, 627‚Äì648. https://doi.org/10.\\n1145/3379337.3415900\\nReceived 2022-07-07; accepted 2022-11-07\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 0, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nContents lists available at ScienceDirect \\nComputer Languages, Systems & Structures \\njournal homepage: www.elsevier.com/locate/cl \\nSystematic mapping study of template-based code generation \\nEugene Syriani ‚àó, Lechanceux Luhunu , Houari Sahraoui \\nDepartment of computer science and operations research (DIRO), University of Montreal, Montreal, Quebec, Canada \\na r t i c l e \\ni n f o \\nArticle history: \\nReceived 18 August 2017 \\nRevised 24 November 2017 \\nAccepted 30 November 2017 \\nAvailable online 7 December 2017 \\nKeywords: \\nCode generation \\nSystematic mapping study \\nModel-driven engineering \\na b s t r a c t \\nContext: Template-based code generation (TBCG) is a synthesis technique that produces \\ncode from high-level speciÔ¨Åcations, called templates. TBCG is a popular technique in \\nmodel-driven engineering (MDE) given that they both emphasize abstraction and automa-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 0, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='code from high-level speciÔ¨Åcations, called templates. TBCG is a popular technique in \\nmodel-driven engineering (MDE) given that they both emphasize abstraction and automa- \\ntion. Given the diversity of tools and approaches, it is necessary to classify existing TBCG \\ntechniques to better guide developers in their choices. \\nObjective: The goal of this article is to better understand the characteristics of TBCG tech- \\nniques and associated tools, identify research trends, and assess the importance of the role \\nof MDE in this code synthesis approach. \\nMethod: We survey the literature to paint an interesting picture about the trends and uses \\nof TBCG in research. To this end, we follow a systematic mapping study process. \\nResults: Our study shows, among other observations, that the research community has \\nbeen diversely using TBCG over the past 16 years. An important observation is that TBCG \\nhas greatly beneÔ¨Åted from MDE. It has favored a template style that is output-based and'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 0, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='been diversely using TBCG over the past 16 years. An important observation is that TBCG \\nhas greatly beneÔ¨Åted from MDE. It has favored a template style that is output-based and \\nhigh-level modeling languages as input. TBCG is mainly used to generate source code and \\nhas been applied to many domains. \\nConclusion: TBCG is now a mature technique and much research work is still conducted \\nin this area. However, some issues remain to be addressed, such as support for template \\ndeÔ¨Ånition and assessment of the correctness and quality of the generated code. \\n¬© 2017 Elsevier Ltd. All rights reserved. \\n1. Introduction \\nCode generation has been around since the 1950s, taking its origin in early compilers [1] . Since then, software organi- \\nzations have been relying on code synthesis techniques in order to reduce development time and increase productivity [2] . \\nAutomatically generating code is an approach where the same generator can be reused to produce many different artifacts'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 0, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Automatically generating code is an approach where the same generator can be reused to produce many different artifacts \\naccording to the varying inputs it receives. It also helps detecting errors in the input artifact early on before the generated \\ncode is compiled, when the output is source code. \\nThere are many techniques to generate code, such as programmatically [3] , using a meta-object protocol [4] , or aspect- \\noriented programming [5] . Since the mid-1990‚Äôs, template-based code generation (TBCG) emerged as an approach requiring \\nless effort for the programmers to develop code generators. Templates favor reuse following the principle of write once, \\nproduce many . The concept was heavily used in web designer software (such as Dreamweaver) to generate web pages \\nand Computer Aided Software Engineering (CASE) tools to generate source code from UML diagrams. Many development \\n‚àóCorresponding author.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 0, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='and Computer Aided Software Engineering (CASE) tools to generate source code from UML diagrams. Many development \\n‚àóCorresponding author. \\nE-mail addresses: syriani@iro.umontreal.ca (E. Syriani), luhunukl@iro.umontreal.ca (L. Luhunu), sahraoui@iro.umontreal.ca (H. Sahraoui). \\nhttps://doi.org/10.1016/j.cl.2017.11.003 \\n1477-8424/¬© 2017 Elsevier Ltd. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='44 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nenvironments started to include a template mechanism in their framework such as Microsoft Text Template Transformation \\nToolkit (T4) 1 for .NET and Velocity 2 for Apache. \\nModel-driven engineering (MDE) has advocated the use of model-to-text transformations as a core component of its \\nparadigm [6] . TBCG is a popular technique in MDE given that they both emphasize abstraction and automation. MDE tools, \\nsuch as Acceleo 3 and Xpand 4 , allow developers to generate code from high-level models without worrying on how to parse \\nand traverse input models. We can Ô¨Ånd today TBCG applied in a plethora of computer science and engineering research. \\nThe software engineering research community has focused essentially on primary studies proposing new TBCG tech- \\nniques, tools and applications. However, to the best of our knowledge, there is no classiÔ¨Åcation, characterization, or'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='niques, tools and applications. However, to the best of our knowledge, there is no classiÔ¨Åcation, characterization, or \\nassessment of these studies available yet. Therefore, in this paper, we conducted a systematic mapping study (SMS) of \\nthe literature in order to understand the trends, identify the characteristics of TBCG, assess the popularity of existing tools \\nwithin the research community, and determine the inÔ¨Çuence that MDE has had on TBCG. We are interested in various \\nfacets of TBCG, such as characterizing the templates, the inputs, and outputs, along with the evolution of the amount of \\npublications using TBCG over the past 16 years. \\nThe remainder of this paper is organized as follows. In Section 2 , we introduce the necessary background on TBCG \\nand discuss related work. In Section 3 , we elaborate on the methodology we followed for this SMS, and on the results of'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='and discuss related work. In Section 3 , we elaborate on the methodology we followed for this SMS, and on the results of \\nthe paper selection phase. The following sections report the results: the trends and evolution of TBCG in Section 4 , the \\ncharacteristics of TBCG according to our classiÔ¨Åcation scheme in Section 5 , the relationships between the different facets \\nin Section 6 , how TBCG tools have been used in primary studies in Section 7 , and the relation between MDE and TBCG \\nin Section 8 . In Section 9 , we answer our research questions and discuss limitations of the study. Finally, we conclude in \\nSection 10 . \\n2. Background and related work \\nWe brieÔ¨Çy explain TBCG in the context of MDE and discuss related work on secondary studies about code generation. \\n2.1. Code generation \\nIn this paper, we view code generation as in automatic programming [1] rather than compilers. The underlying principle'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='2.1. Code generation \\nIn this paper, we view code generation as in automatic programming [1] rather than compilers. The underlying principle \\nof automatic programming is that a user deÔ¨Ånes what he expects from the program and the program should be automat- \\nically generated by a software without any assistance by the user. This generative approach is different from a compiler \\napproach. Compilers produce code executable by a computer from a speciÔ¨Åcation conforming to a programming language, \\nwhereas automatic programming transforms user speciÔ¨Åcations into code which often conforms to a programming language. \\nCompilers have a phase called code generation that retrieves an abstract syntax tree produced by a parser and translates it \\ninto machine code or bytecode executable by a virtual machine. Compared to code generation as in automatic programming, \\ncompilers can be regarded as tasks or services that are incorporated in or post-positioned to code generators [7] .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='compilers can be regarded as tasks or services that are incorporated in or post-positioned to code generators [7] . \\nCode generation is an important model-to-text transformation that ensures the automatic transformation of a model into \\ncode. Organization are adopting the use of code generation since it reduces the development process time and increases \\nthe productivity. Generating the code using the most appropriate technique is even more crucial since it is the key to \\nbeneÔ¨Åt from all the advantages code synthesis offers to an organization. Nowadays, TBCG has raised to be the most popular \\nsynthesis technique available. Using templates can quickly become a complex task especially when the model should satisfy \\na certain condition before a template fragment is executed. \\nAs Balzer [8] states, there are many advantages to code generation. The effort of the user is reduced as he has fewer'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='a certain condition before a template fragment is executed. \\nAs Balzer [8] states, there are many advantages to code generation. The effort of the user is reduced as he has fewer \\nlines to write: speciÔ¨Åcations are shorter than the program that implements them. SpeciÔ¨Åcations are easier to write and \\nto understand for a user, given that they are closer to the application and domain concepts. Writing speciÔ¨Åcations is less \\nerror-prone than writing the program directly, since the expert is the one who writes the speciÔ¨Åcation rather than another \\nprogrammer. \\nThese advantages are in fact the pillar principles of MDE and domain-speciÔ¨Åc modeling. Floch et al. [9] observed many \\nsimilarities between MDE and compilers research and principles. Thus, it is not surprising to see that many, though not \\nexclusively, code generation tools came out of the MDE community. The advantages of code generation should be contrasted'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 1, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='exclusively, code generation tools came out of the MDE community. The advantages of code generation should be contrasted \\nwith some of its limitations. For example, there are issues related to integration of generated code with manually written \\ncode and to evolving speciÔ¨Åcations that require to re-generate the code [10] . Sometimes, relying too much on code genera- \\ntors may produce an overly general solution that may not necessarily be optimal for a speciÔ¨Åc problem. \\n1 https://msdn.microsoft.com/en-us/library/bb126445.aspx \\n2 http://velocity.apache.org/ \\n3 http://www.eclipse.org/acceleo/ \\n4 http://wiki.eclipse.org/Xpand'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 2, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n45 \\nconforms to\\nuses\\ninput\\ninput\\ngenerates\\nTemplate\\n<%context class%>\\npublic class <%name%> { String id; }\\nDesign-time input\\nClass\\nname:string\\nRuntime input\\nPerson\\nOutput\\npublic class Person { String id; }\\nTemplate engine\\nFig. 1. Components of TBCG. \\n2.2. Model-to-text transformations \\nIn MDE, model transformations can have different purposes [11] , such as translating, simulating, or reÔ¨Åning models. One \\nparticular kind of model transformation is devoted to code generation with model-to-text transformations (M2T) [12] . In \\ngeneral, M2T transforms a model (often in the form of a graph structure) into a linearized textual representation. M2T is \\nused to produce various text artifacts, e.g., to generate code, to serialize models, to generate documentation and reports, or \\nto visualize and explore models. As such, code generation is a special case of M2T, where the output artifacts are executable'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 2, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='to visualize and explore models. As such, code generation is a special case of M2T, where the output artifacts are executable \\nsource code. There are commonly Ô¨Åve different code generation approaches present in the literature [7,12] : \\nVisitor based approaches consist of programmatically traversing the internal representation of the input, while relying \\non an API dedicated to manipulate the input, to write the output to a text stream. This requires to program directly \\nthe creation of the output Ô¨Åles and storing the strings while navigating through the data structure of the input. The \\nimplementation typically makes use of the visitor design pattern [13] . This approach is used in [3] to generate Java \\ncode from a software architecture description language. \\nMeta-programming is a language extension approach, such as using a meta-object protocol, that relies on introspection'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 2, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='code from a software architecture description language. \\nMeta-programming is a language extension approach, such as using a meta-object protocol, that relies on introspection \\nin order to access the abstract syntax tree of the source program. For example, in OpenJava [4] , a Java meta-program \\ncreates a Java Ô¨Åle, compiles it on the Ô¨Çy, and loads the generated program in its own run-time. \\nIn-line generation relies on a preprocessor that generates additional code to expand the existing one, such as with the \\nC++ standard template library or C macro preprocessor instructions. The generation instructions can be deÔ¨Åned at a \\nhigher-level of abstraction, either using a dedicated language distinct from the base language (e.g., for macros) or as \\na dedicated library as in [14] . \\nCode annotations are in-line descriptions that added to statement declarations (e.g., class deÔ¨Ånition) that can either be'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 2, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='a dedicated library as in [14] . \\nCode annotations are in-line descriptions that added to statement declarations (e.g., class deÔ¨Ånition) that can either be \\ninternally transformed into more expanded code (e.g., attributes in C#) or that are processed by a tool other than the \\ncompiler of the language (e.g., the speciÔ¨Åcation of documentation comments processed by Javadoc). This approach is \\nused in [15] . \\nTemplate based is described below. \\n2.3. Template-based code generation \\nThe literature agrees on a general deÔ¨Ånition of M2T code generation [12] and on templates. J√∂rges [7] identiÔ¨Åes three \\ncomponents in TBCG: the data, the template, and the output. However, there is another component that is not mentioned, \\nwhich is the meta-information the generation logic of the template relies on. Therefore, we conducted this study according \\nto the following notion of TBCG. \\nFigure 1 summarizes the main concepts of TBCG. We consider TBCG as a synthesis technique that uses templates in order'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 2, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='to the following notion of TBCG. \\nFigure 1 summarizes the main concepts of TBCG. We consider TBCG as a synthesis technique that uses templates in order \\nto produce a textual artifact, such as source code, called the output . A template is an abstract and generalized representation \\nof the textual output it describes. It has a static part , text fragments that appear in the output ‚Äúas is‚Äù. It also has a dynamic \\npart embedded with splices of meta-code that encode the generation logic. Templates are executed by the template engine \\n(sometimes refereed to as template processor ) to compute the dynamic part and replace meta-codes by static text according \\nto run-time input . The design-time input deÔ¨Ånes the meta-information which the run-time input conforms to. The dynamic \\npart of a template relies on the design-time input to query the run-time input by Ô¨Åltering the information retrieved and'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 2, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='part of a template relies on the design-time input to query the run-time input by Ô¨Åltering the information retrieved and \\nperforming iterative expansions on it. Therefore, TBCG relies on a design-time input that is used to deÔ¨Åne the template \\nand a run-time input on which the template is applied to produce the output. For example, a TBCG engine that takes as \\nrun-time input an XML document relies on an XML schema as design-time input. DeÔ¨Ånition 1 summarizes our deÔ¨Ånition of \\nTBCG.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='46 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nDeÔ¨Ånition 1. A synthesis technique is a TBCG if it consists of a set of templates speciÔ¨Åed in a formalism, where the speciÔ¨Å- \\ncation of a template is based on a design-time input such that, when executed, it reads a run-time input to produce a textual \\noutput . \\nFor example, the work in [16] generates a C# API from Ecore models using Xpand. According to DeÔ¨Ånition 1 , the tem- \\nplates of this TBCG example are Xpand templates, the design-time input is the metamodel of Ecore, the run-time input is \\nan Ecore model, and the output is a C# project Ô¨Åle and C# classes. \\n2.4. Literature reviews on code generation \\nIn evidence-based software engineering [17] , a systematic literature review is a secondary study that reviews primary \\nstudies with the aim of synthesizing evidence related to a speciÔ¨Åc research question. Several forms of systematic reviews'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='studies with the aim of synthesizing evidence related to a speciÔ¨Åc research question. Several forms of systematic reviews \\nexist depending on the depth of reviewing primary studies and on the speciÔ¨Åcities of research questions. Unlike conven- \\ntional systematic literature reviews that attempt to answer a speciÔ¨Åc question, a SMS aim at classifying and performing a \\nthematic analysis on a topic [18] . SMS is a secondary study method that has been adapted from other disciplines to soft- \\nware engineering in [19] and later evolved by Petersen et al. in [20] . A SMS is designed to provide a wide overview of \\na research area, establish if research evidence exists on a speciÔ¨Åc topic, and provide an indication of the quantity of the \\nevidence speciÔ¨Åc to the domain. \\nOver the years, there have been many primary studies on code generation. However, we could not Ô¨Ånd any secondary \\nstudy on TBCG explicitly. Still, the following are closely related secondary studies.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='study on TBCG explicitly. Still, the following are closely related secondary studies. \\nMehmood et al. [21] performed a SMS regarding the use of aspect-oriented modeling for code generation, which is not \\nbased on templates. They analyzed 65 papers mainly based on three main categories: the focus area, the type of research, \\nand the type of contribution. The authors concluded that this synthesis technique is still immature. The study shows that \\nno work has been reported to use or evaluate any of the techniques proposed. \\nGurunule et al. [22] presented a comparison of aspect orientation and MDE techniques to investigate how they can each \\nbe used for code generation. The authors found that further research in these areas can lead to signiÔ¨Åcant advancements in \\nthe development of software systems. Unlike Mehmood et al. [21] , they did not follow a systematic and repeatable process.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='the development of software systems. Unlike Mehmood et al. [21] , they did not follow a systematic and repeatable process. \\nDominguez et al. [23] performed a systematic literature review of studies that focus on code generation from state ma- \\nchine speciÔ¨Åcations. The study is based on a set of 53 papers, which have been classiÔ¨Åed into two groups: pattern-based \\nand not pattern-based. The authors do not take template-based approaches into consideration. \\nBatot et al. [24] performed a systematic mapping study on model transformations solving a concrete problem that have \\nbeen published in the literature. They analyzed 82 papers based on a classiÔ¨Åcation scheme that is general to any model \\ntransformation approach, which includes M2T. They conclude that concrete model transformations have been pulling out \\nfrom the research literature since 2009 and are being considered as development tasks. They also found that 22% of their'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='from the research literature since 2009 and are being considered as development tasks. They also found that 22% of their \\ncorpus solve concrete problems using reÔ¨Ånement and code synthesis techniques. Finally, they found that research in model \\ntransformations is heading for a more stable and grounded validation. \\nThere are other studies that attempted to classify code generation techniques. However, they did not follow a systematic \\nand repeatable process. For example, Czarnecki et al. [12] proposed a feature model providing a terminology to characterize \\nmodel transformation approaches. They distinguished two categories for M2T approaches: those that are visitor-based and \\nthose that are template-based; the latter being in line with DeÔ¨Ånition 1 . The authors found that many new approaches \\nto model-to-model transformation have been proposed recently, but relatively little experience is available to assess their \\neffectiveness in practical applications.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='to model-to-model transformation have been proposed recently, but relatively little experience is available to assess their \\neffectiveness in practical applications. \\nRose et al. [25] extended the feature model of Czarnecki et al. to focus on template-based M2T tools. Their classiÔ¨Åcation \\nis centered exclusively on tool-dependent features. Their goal is to help developers when they are faced to choose between \\ndifferent tools. This study is close to the work of Czarnecki in [12] but focuses only on a feature model for M2T. The \\ndifference with our study is that it focuses on a feature diagram and deals with tool-dependent features only. \\nThere are also other systematic reviews performed on other related topics. For example, the study in [26] performed a \\nSMS on domain-speciÔ¨Åc modeling languages (DSL). They analyzed 390 papers to portray the published literature on DSLs,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='SMS on domain-speciÔ¨Åc modeling languages (DSL). They analyzed 390 papers to portray the published literature on DSLs, \\nwhich is comparable to the size of our corpus. Also, the study in [27] presents a systematic literature review on software \\nproduct lines engineering in the context of DSLs. Similar to our study, they propose a deÔ¨Ånition for the life-cycle of language \\nproduct lines and use it to analyze how the literature has an impact this life-cycle. Another SMS recently published in \\n[28] presents a taxonomy of source code labeling. \\n3. Research methods \\nIn order to analyze the topic of TBCG, we conducted a SMS following the process deÔ¨Åned by Petersen et al. in [20] and \\nsummarized in Fig. 2 . The deÔ¨Ånition of research question is discussed in Section 3.1 . The search conduction is described \\nin Section 3.2 . We present the screening of papers in Section 3.3 . The relevant papers are obtained based on the criteria'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 3, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='in Section 3.2 . We present the screening of papers in Section 3.3 . The relevant papers are obtained based on the criteria \\npresented in Section 3.3.1 and Section 3.3.2 . The elaboration of the classiÔ¨Åcation scheme is described in Section 3.4 . Finally, \\nwe detail the selection of the papers in Section 3.5 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 4, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n47 \\nProcess Steps\\nOutcomes\\nDefinition of\\nResearch Question\\nConduct Search\\nScreening of Papers\\nKeywording using\\nAbstracts\\nData Extraction and\\nMapping Process\\nSystematic Map\\nClassification\\nScheme\\nRelevant Papers\\nAll Papers\\nReview Scope\\nFig. 2. The systematic mapping study process we followed from Peterson et al. \\n3.1. Objectives \\nThe objective of this study is to obtain an overview of the current research in the area of TBCG and to characterize the \\ndifferent approaches that have been developed. We deÔ¨Åned four research questions to set the scope of this study: \\n1. What are the trends in template-based code generation? We are interested to know how this technique has evolved over \\nthe years through research publications. \\n2. What are the characteristics of template-based code generation approaches? We want to identify major characteristics of \\nthese techniques and their tendencies.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 4, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='2. What are the characteristics of template-based code generation approaches? We want to identify major characteristics of \\nthese techniques and their tendencies. \\n3. To what extent are template-based code generation tools being used in research? We are interested in identifying popular \\ntools and their uses. \\n4. What is the place of MDE in template-based code generation? We seek to determine whether and how MDE has inÔ¨Çuenced \\nTBCG. \\n3.2. Selection of source \\nWe delimited the scope of the search to be regular publications that mention TBCG as at least one of the approaches \\nused for code generation and published between 20 0 0‚Äì2016. Therefore, this includes publications where code generation is \\nnot necessarily the main contribution. For example, Buchmann et al. [29] used TBCG to obtain ATL code while their main \\nfocus was implementing a higher-order transformation. Given that not all publications have the term ‚Äúcode generation‚Äù in'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 4, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='focus was implementing a higher-order transformation. Given that not all publications have the term ‚Äúcode generation‚Äù in \\ntheir title, we retrieved publications based on their title, abstract, or full text (when available) mentioning the keywords \\n‚Äútemplate‚Äù and its variations, ‚Äúcode‚Äù, and ‚Äúgeneration‚Äù and synonyms with their variations (e.g., ‚Äúsynthesis‚Äù). We used the \\nfollowing search query for all digital libraries, using their respective syntax: \\ntemplat ‚àóAND ‚Äúcode generat ‚àó‚Äù OR ‚Äúcode synthesi ‚àó‚Äù\\nWe validated our search with a sample of 100 pilot papers we preselected. These papers were chosen from papers we \\napriori knew should be included and resulting from a preliminary pilot search online. We iterated over different versions of \\nthe search strings until all pilot papers could be retrieved from the digital libraries. \\n3.3. Screening procedure \\nScreening is the most crucial phase in a SMS [20] . We followed a two-stage screening procedure: automatic Ô¨Åltering,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 4, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='3.3. Screening procedure \\nScreening is the most crucial phase in a SMS [20] . We followed a two-stage screening procedure: automatic Ô¨Åltering, \\nthen title and abstract screening. In order to avoid the exclusion of papers that should be part of the Ô¨Ånal corpus, we \\nfollowed a strict screening procedure. With four reviewers at our disposal, each article is screened by at least two reviewers \\nindependently. When both reviewers of a paper disagree upon the inclusion or exclusion of the paper, a physical discussion \\nis required. If the conÔ¨Çict is still unresolved, an additional senior reviewer is involved in the discussion until a consensus \\nis reached. To determine a fair exclusion process, a senior reviewer reviews a sample of no less than 20% of the excluded \\npapers at the end of the screening phase, to make sure that no potential paper is missed. \\n3.3.1. Inclusion criteria'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 4, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='papers at the end of the screening phase, to make sure that no potential paper is missed. \\n3.3.1. Inclusion criteria \\nA paper is included if it explicitly indicates the use of TBCG or if it proposes a TBCG technique. We also include papers \\nif the name of a TBCG tool appears in the title, abstract, or content. ‚ÄúUse‚Äù is taken in a large sense when it is explicit that \\na TBCG contributes to the core of the paper (not in the related work). \\n3.3.2. Exclusion criteria \\nResults from the search were Ô¨Årst Ô¨Åltered automatically to discard records that were outside the scope of this study: \\npapers not in computer science, not in the software engineering domain, with less than two pages of length (e.g., proceed-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='48 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nings preface), not peer-reviewed (e.g., white papers), not written in English, or not published between the years 20 0 0 and \\n2016. Then, papers were excluded through manual inspection based on the following criteria: \\n‚Ä¢ No code generation. There is no code generation technique used. \\n‚Ä¢ Not template-based code generation. Code generation is mentioned, but the considered technique is not template-based \\naccording to DeÔ¨Ånition 1 . \\n‚Ä¢ Not a paper. This exclusion criterion spans papers that were not caught by the automatic Ô¨Åltering. For example, some \\npapers had only the abstract written in English and the content of the paper in another language. Additionally, there \\nwere 24 papers where the full text was not accessible online. \\nFor the Ô¨Årst two criteria, when the abstract did not give enough details about the code generation approach, a quick'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='were 24 papers where the full text was not accessible online. \\nFor the Ô¨Årst two criteria, when the abstract did not give enough details about the code generation approach, a quick \\nlook at the full text helped clear any doubts on whether to exclude the paper or not. Reviewers were conservative on that \\nmatter. \\n3.4. ClassiÔ¨Åcation scheme \\nWe elaborated a classiÔ¨Åcation scheme by combining our general knowledge with the information extracted from the \\nabstracts during the screening phase. We classify all papers along different categories that are of interest in order to answer \\nour research questions. This helps analyzing the overall results and gives an overview of the trends and characteristics of \\nTBCG. The categories we classiÔ¨Åed the corpus with are the following: \\n‚Ä¢ Template style : We characterize the level of customization and expressiveness of the templates used in the code gen-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='TBCG. The categories we classiÔ¨Åed the corpus with are the following: \\n‚Ä¢ Template style : We characterize the level of customization and expressiveness of the templates used in the code gen- \\neration approach. PredeÔ¨Åned style is reserved for approaches where the template used for code generation is deÔ¨Åned \\ninternally to the tool. However, a subset of the static part of the template is customizable to vary slightly the gener- \\nated output. This is, for example, the case for common CASE tools where there is a predeÔ¨Åned template to synthesize \\na class diagram into a number of programming languages. Nevertheless, the user can specify what construct to use for \\nmany-cardinality associations, e.g., Array or ArrayList for Java templates. Output-based style covers templates that \\nare syntactically based on the actual target output. In contrast with the previous style, output-based templates offer full'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='are syntactically based on the actual target output. In contrast with the previous style, output-based templates offer full \\ncontrol on how the code is generated, both on the static and dynamic parts. The generation logic is typically encoded in \\nmeta-code as in the example of Fig. 1 . Rule-based style puts the focus of the template on computing the dynamic part \\nwith the static part being implicit. The template lists declarative production rules that are applied on-demand by the \\ntemplate engine to obtain the Ô¨Ånal target output. For example, this is used to render the concrete textual syntax from \\nthe abstract syntax of a model using a grammar. \\n‚Ä¢ Input type : We characterize the language of the design-time input that is necessary to develop templates. The run-time \\ninput is an instance that conforms to it. General-purpose modeling language is for generic languages reusable across dif-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='input is an instance that conforms to it. General-purpose modeling language is for generic languages reusable across dif- \\nferent domains that are not programming languages, such as UML. Domain-speciÔ¨Åc modeling language is for languages \\ntargeted for a particular domain, for example, where the run-time input is a Simulink model. Schema is for structured \\ndata deÔ¨Ånitions, such as XML or database schema. Programming language is for well-deÔ¨Åned programming languages, \\nwhere the run-time input is source code. \\n‚Ä¢ Output type : We characterize the output of the code generator (more than one category can be selected when multiple \\nartifacts are generated). Source code is executable code conforming to a speciÔ¨Åc programming language. Structured data \\nis for code that is not executable, such as HTML. \\n‚Ä¢ Application scale : We characterize the scale of the artifact on which the TBCG approach is applied with respect to the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='is for code that is not executable, such as HTML. \\n‚Ä¢ Application scale : We characterize the scale of the artifact on which the TBCG approach is applied with respect to the \\nevaluation or illustration cases in the paper. The presence of a formal case study that either relied on multiple data \\nsources or subjects qualiÔ¨Åed as large scale application. Small scale was used mainly when there was only one example or \\ntoy examples in the paper that illustrated the TBCG. No application if for when the code generation was not applied on \\nany example. \\n‚Ä¢ Application domain : We classify the general domain TBCG has been applied on. For example, this includes Software engi- \\nneering, Embedded systems, Compilers, Bio-medicine , etc. \\n‚Ä¢ Orientation : We distinguish industrial papers, where at least one author is aÔ¨Éliated to industry, from academic papers \\notherwise. \\n‚Ä¢ Tool : We capture the tool used for TBCG. If a tool is not clearly identiÔ¨Åed in a paper or the TBCG is programmed directly,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 5, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='otherwise. \\n‚Ä¢ Tool : We capture the tool used for TBCG. If a tool is not clearly identiÔ¨Åed in a paper or the TBCG is programmed directly, \\nwe classify the tool as unspeciÔ¨Åed . We consider a tool to be popular within the research community when it is used in \\nat least 1% of the papers. Otherwise, we classify it as other . \\n‚Ä¢ MDE : We determine whether the part of the solution where TBCG is applied in the paper follows MDE techniques and \\nprinciples. A good indication is if the design-time input is a metamodel. \\n3.5. Paper selection \\nTable 1 summarizes the Ô¨Çow of information through the selection process of this study. This section explains how we \\nobtained the Ô¨Ånal corpus of papers to classify and analyze.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 6, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n49 \\nTable 1 \\nEvolution of paper corpus during the study process. \\nPhase \\nNumber of papers \\nCollection \\nEngineering Village \\n4043 \\nScopus \\n932 \\nSpringerLink \\n2671 \\nInitial corpus \\n5131 \\nScreening \\nExcluded during screening \\n4553 \\nIncluded \\n578 \\nClassiÔ¨Åcation \\nExcluded during classiÔ¨Åcation \\n99 \\nFinal corpus \\n481 \\n3.5.1. Paper collection \\nThe paper collection step was done in two phases: querying and automatic duplicates removal. There are several \\nonline databases that index software engineering literature. For this study, we considered three main databases to maxi- \\nmize coverage: Engineering Village 5 , Scopus 6 , and SpringerLink 7 . The Ô¨Årst two cover typical software engineering editors \\n( IEEE Xplore , ACM Digital Library , Elsevier ). However, from past experiences [24] , they do not include all of Springer pub-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 6, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='( IEEE Xplore , ACM Digital Library , Elsevier ). However, from past experiences [24] , they do not include all of Springer pub- \\nlications. We used the search string from Section 3.2 to retrieve all papers from these three databases. We obtained 7 646 \\ncandidate papers that satisfy the query and the options of the search stated in Section 3.3.2 . We then removed automatically \\nall duplicates using EndNote software. This resulted in 5 131 candidate papers for the screening phase. \\n3.5.2. Screening \\nBased on the exclusion criteria stated in Section 3.3.2 , each candidate paper was screened by at least two reviewers to \\ndecide on its inclusion. To make the screening phase more eÔ¨Écient, we used a home-made tool [30] . After all the reviewers \\ncompleted screening the papers they were assigned, the tool calculates an inter-rater agreement coeÔ¨Écient. In our case, the \\nCohens Kappa coeÔ¨Écient was 0.813. This high value shows that the reviewers were in almost perfect agreement.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 6, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Cohens Kappa coeÔ¨Écient was 0.813. This high value shows that the reviewers were in almost perfect agreement. \\nAmong the initial corpus of candidate papers, 4556 were excluded, 551 were included and 24 received conÔ¨Çicting ratings. \\nDuring the screening, the senior reviewer systematically veriÔ¨Åed each set of 100 rejected papers for sanity check. A total \\nof 7 more papers were included back hence the rejected papers were reduced to 4549. Almost all cases of conÔ¨Çicts were \\nabout a disagreement on whether the code generation technique of a paper was using templates or not. These conÔ¨Çicts were \\nresolved in physical meetings and 20 of them were Ô¨Ånally included for a total of 578 papers and 4553 excluded. \\nAmong the excluded papers, 52% were rejected because no code generation was used. We were expecting such a high \\nrate because terms such as ‚Äútemplates‚Äù are used in many other Ô¨Åelds, like biometrics. Also, many of these papers were'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 6, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='rate because terms such as ‚Äútemplates‚Äù are used in many other Ô¨Åelds, like biometrics. Also, many of these papers were \\nreferring to the C++ standard template library [31] , which is not about code generation. We counted 34% papers excluded \\nbecause they were not using templates . Examples of such papers are cited in Section 2.2 . Also, more than a quarter of the \\npapers were in the compilers or embedded system domains, where code generation is programmed imperatively rather than \\ndeclaratively speciÔ¨Åed using a template mechanism. Finally, 5% of the papers were considered as not a paper . In fact, this \\ncriterion was in place to catch papers that escaped the automatic Ô¨Åltering from the databases. \\n3.5.3. Eligibility during classiÔ¨Åcation \\nOnce the screening phase over, we thoroughly analyzed the full text of the remaining 578 papers to classify them \\naccording to our classiÔ¨Åcation scheme. Doing so allowed us to conÔ¨Årm that the code generation approach was effectively'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 6, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='according to our classiÔ¨Åcation scheme. Doing so allowed us to conÔ¨Årm that the code generation approach was effectively \\ntemplate-based according to DeÔ¨Ånition 1 . We encountered papers that used multiple TBCG tools: they either compared tools \\nor adopted different tools for different tasks. We classiÔ¨Åed each of these papers as a single publication, but incremented the \\noccurrence corresponding to the tools referred to in the paper. This is the case of [32] where the authors use Velocity and \\nXSLT for code generation. Velocity generates Java and SQL code, while XSLT generates the control code. \\nWe excluded 99 additional papers. During screening, we detected situations where the abstract suggested the imple- \\nmentation of TBCG, whereas the full text proved otherwise. In most of the cases, the meaning of TBCG differed from the \\ndescription presented in Section 2.3 . As shown in [33] the terms template-based and generation are used in the context of'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 6, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='description presented in Section 2.3 . As shown in [33] the terms template-based and generation are used in the context of \\nnetworking and distributed systems. We also encountered circumstances where the tool mentioned in the abstract requires \\nthe explicit use of another component to be considered as TBCG, such as Simulink TLC, as in [34] . \\nThe Ô¨Ånal corpus 8 considered for this study contains 481 papers. \\n5 https://www.engineeringvillage.com/ \\n6 https://www.scopus.com/ \\n7 http://link.springer.com/ \\n8 The complete list of papers is available online at http://www-ens.iro.umontreal.ca/ ‚àºluhunukl/survey/classiÔ¨Åcation.html'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 7, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='50 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\n# of papers\\nFig. 3. Evolution of papers in the corpus. \\nTable 2 \\nMost popular venues. \\nVenue \\nVenue & publication type \\n# Papers \\nModel Driven Engineering Languages and Systems ( Models ) \\nMDE \\nConference \\n26 \\nSoftware and Systems Modeling ( Sosym ) \\nMDE \\nJournal \\n26 \\nEuropean Conference on Modeling Foundations and Applications ( Ecmfa ) \\nMDE \\nConference \\n19 \\nGenerative and Transformational Techniques in Software Engineering ( Gttse ) \\nSoft. eng. \\nConference \\n11 \\nGenerative Programming: Concepts & Experience ( Gpce ) \\nSoft. eng. \\nConference \\n8 \\nInternational Conference on Computational Science and Applications ( Iccsa ) \\nOther \\nConference \\n8 \\nSoftware Language Engineering ( Sle ) \\nMDE \\nConference \\n7 \\nLeveraging Applications of Formal Methods, VeriÔ¨Åcation and Validation ( Isola ) \\nOther \\nConference \\n7 \\nAutomated Software Engineering ( Ase )'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 7, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='MDE \\nConference \\n7 \\nLeveraging Applications of Formal Methods, VeriÔ¨Åcation and Validation ( Isola ) \\nOther \\nConference \\n7 \\nAutomated Software Engineering ( Ase ) \\nSoft. eng. \\nJournal + Conference \\n5 + 2 \\nInternational Conference on Web Engineering ( Icwe ) \\nOther \\nConference \\n6 \\nEvaluation of Novel Approaches to Software Engineering ( Enase ) \\nSoft. eng. \\nConference \\n5 \\n4. Evolution of TBCG \\nWe start with a thorough analysis of the trends in TBCG in order to answer the Ô¨Årst research question. \\n4.1. General trend \\nFigure 3 reports the number of papers per year, averaging around 28. The general trend indicates that the number of \\npublications with at least one TBCG method started increasing in 2002 to reach a Ô¨Årst local maximum in 2005 and then \\nremained relatively constant until 2012. This increase coincides with the early stages of MDE and the Ô¨Årst edition of the \\nModels conference, previous called Uml conference. This is a typical trend where a research community gets carried away'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 7, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Models conference, previous called Uml conference. This is a typical trend where a research community gets carried away \\nby the enthusiasm of a new potentially interesting domain, which leads to more publications. However, the most proliÔ¨Åc \\nperiod was in 2013, where we notice a signiÔ¨Åcant peak with 2.4 times the average numbers of publications observed in the \\nprevious years. Fig. 3 then shows sudden a decrease in 2015. \\nResorting to statistical methods, the high coeÔ¨Écient of variability and modiÔ¨Åed Thompson Tau test indicate that 2013 \\nand 2015 are outliers in the range 2005‚Äì2016, where the average is 37 papers per year. The sudden isolated peak in 2013 \\nis the result of a special event or popularity of TBCG. The following decrease in the amount of papers published should not \\nbe interpreted as a decline in interest in TBCG, but that some event happened around 2013 which boosted publications,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 7, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='be interpreted as a decline in interest in TBCG, but that some event happened around 2013 which boosted publications, \\nand then it went back to the steady rate of publication as previous years. In fact, 2016 is one standard deviation above the \\naverage. \\n4.2. Publications and venues \\nWe analyzed the papers based on the type of publication and the venue of their publication. MDE venues account for \\nonly 22% of the publications, so are software engineering venues, while the majority (56%) were published in other venues. \\nTable 2 shows the most popular venues that have at least Ô¨Åve papers. These top venues account for just more than a quarter \\nof the total number of publications. Among them, MDE venues account for 60% of the papers. Models , Sosym , and Ecmfa \\nare the three most popular venues 9 with a total of 66 publications between them. This is very signiÔ¨Åcant given that the \\n9 We grouped Uml conference with Models and Ecmda-fa with Ecmfa .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 8, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n51 \\nTable 3 \\nDistribution of the template style facet. \\nOutput-based \\nPredeÔ¨Åned \\nRule-based \\n72% \\n24% \\n4% \\n0\\n10\\n20\\n30\\n40\\n50\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nOutput-based\\nPredefined\\nRule-based\\n# of papers\\nFig. 4. Template style evolution. \\naverage is only 1.67 paper per venue with a standard deviation of 2.63. Also, 43% of venues had only one paper using TBCG, \\nwhich is the case for most of the other venues. \\nThe peak in 2013 was mainly inÔ¨Çuenced by MDE and software engineering venues. However the drop in 2015 is the result \\nof an accumulation of the small variations among the other venues. Since 2014, MDE venues account for 10‚Äì12 papers per \\nyear, while only 6‚Äì7 in software engineering. \\nAs for the publication type, conference publications have been dominating at 64%. Journal article account for 24% of all'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 8, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='year, while only 6‚Äì7 in software engineering. \\nAs for the publication type, conference publications have been dominating at 64%. Journal article account for 24% of all \\npapers. The remaining papers were published as book chapters, workshops or other publication type. Interestingly, we notice \\na steady increase in journal articles, reaching a maximum of 15 in 2016. \\n5. Characteristics of template-based code generation \\nWe examine the characteristics of TBCG using the classiÔ¨Åcation scheme presented in Section 3.4 . \\n5.1. Template style \\nAs Table 3 illustrates, the vast majority of the publications follow the output-based style. This consists of papers like [35] , \\nwhere Xpand is used to generate workÔ¨Çow code used to automate modeling tools. There, it is the Ô¨Ånal output target text \\nthat drives the development of the template. This high score is expected since output-based style is the original template'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 8, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='that drives the development of the template. This high score is expected since output-based style is the original template \\nstyle for TBCG as depicted in Fig. 4 . This style has always been the most popular style since 20 0 0. \\nThe predeÔ¨Åned style is the second most popular. Most of these papers generate code using a CASE tool, such as [36] that \\nuses Rhapsody to generate code to map UML2 semantics to Java code with respect to association ends. Apart from CASE \\ntools, we also classiÔ¨Åed papers like [37] as predeÔ¨Åned style since the output code is already Ô¨Åxed as HTML and the pro- \\ngrammer uses the tags to change some values based on the model. Each year, around 28% of the papers were using the \\npredeÔ¨Åned style, except for a peak of 39% in 2005, given the popularity of CASE tools then. \\nWe found 19 publications that used rule-based style templates. This includes papers like [38] which generates Java code'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 8, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='We found 19 publications that used rule-based style templates. This includes papers like [38] which generates Java code \\nwith Stratego from a DSL. A possible explanation of such a low score is that this is the most diÔ¨Écult template style to \\nimplement. It had a maximum of two papers per year throughout the study period. \\n5.2. Input type \\nGeneral purpose languages account for almost half of the design-time input of the publications, as depicted in Table 4 . \\nUML (class) diagrams, which are used as metamodels for code generation, are the most used for 87% of these papers as in \\n[35] . Other popular general-purpose languages that were used are, for example, the architecture analysis and design lan- \\nguage [39] and feature diagrams [40] . The schema category comes second with 21% of the papers. For example, a database \\nTable 4 \\nDistribution of the input type facet. \\nGeneral purpose \\nSchema \\nDomain speciÔ¨Åc \\nProgramming language \\n48% \\n22% \\n20% \\n10%'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 9, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='52 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nGeneral purpose\\nSchema\\nDomain Specific\\nProgramming Language\\n# of papers\\nFig. 5. Evolution of the design-time input type. \\nschema is used as input at design-time in [41] to generate Java for a system that demonstrates that template can im- \\nprove software development. Also, an XML schema is used in [42] as design-time input to produce C programs in order to \\nimplement an approach that can eÔ¨Éciently support all the conÔ¨Åguration options of an application in embedded systems. \\nDSLs are almost at par with schema. They have been gaining popularity and gradually reducing the gap with general-purpose \\nlanguages. For example in [43] , a custom language is given as the design input in order to generate C and C++ to develop \\na TBCG approach dedicated to real-time systems. The least popular design-time input type is programming language . This'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 9, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='a TBCG approach dedicated to real-time systems. The least popular design-time input type is programming language . This \\nincludes papers like [44] where T4 is used to generate hardware description (VHDL) code for conÔ¨Ågurable hardware. In this \\ncase, the input is another program on which the template depends. \\nOver the years, the general-purpose category has dominated the input type facet, as depicted in Fig. 5 . 2003 and 2006 \\nwere the only exceptions where schema obtained slightly more publications. We also notice a shift from schema to domain- \\nspeciÔ¨Åc design-time input types. Domain-speciÔ¨Åc input started increasing in 2009 but never reached the same level as \\ngeneral purpose. Programming language input maintained a constant level, with an average of 1% per year. Interestingly, in \\n2011, there were more programming languages used than DSLs. \\n5.3. Output type \\nAn overwhelming majority of the papers use TBCG to generate source code (81%), in contrast with 19% was structured'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 9, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='2011, there were more programming languages used than DSLs. \\n5.3. Output type \\nAn overwhelming majority of the papers use TBCG to generate source code (81%), in contrast with 19% was structured \\ndata (though some papers output both types). Table 5 shows the distribution of the output languages that appeared in more \\nthan Ô¨Åve papers, representing 74% of the corpus. This includes papers like [45] where Java code is generated an adaptable \\naccess control tool for electronic medical records. Java and C are the most targeted programming languages. Writing a \\nprogram manually often requires proved abilities, especially with system and hardware languages, such as VHDL [46] . This \\nis why 8% of all papers generate low level source codes. Generation of structured data includes TBCG of mainly XML and \\nHTML Ô¨Åles. For example [47] produces both HTML and XML as parts of the web component to ease regression testing. In'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 9, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='HTML Ô¨Åles. For example [47] produces both HTML and XML as parts of the web component to ease regression testing. In \\naddition, we found that around 4% of the papers generate combinations of at least two output types. This includes papers \\nsuch as [48] that generate both C# and HTML from a domain-speciÔ¨Åc model. \\nStructured data output remained constant over the years, unlike source code which follows the general trend. \\n5.4. Application scale \\nAs depicted in Table 6 , most papers applied TBCG on large scale examples. This result indicates that TBCG is a technique \\nwhich scales with larger amounts of data. This includes papers like [49] that uses Acceleo to generate hundreds of lines \\nTable 5 \\nDistribution of the most popular output languages. \\nJava \\nC \\nHTML \\nC ++ \\nC# \\nXML \\nVHDL \\nSQL \\nAspectJ \\nSystemC \\n33% \\n11% \\n7% \\n7% \\n4% \\n3% \\n3% \\n2% \\n2% \\n2% \\nTable 6 \\nDistribution of application scale facet \\nLarge scale \\nSmall scale \\nNo application \\n63% \\n32% \\n5%'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 10, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n53 \\n0\\n10\\n20\\n30\\n40\\n50\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nLarge scale\\nSmall scale\\nNo application\\n# of papers\\nFig. 6. Application scale evolution. \\nFig. 7. Distribution of application domain facet. \\nof aspect-oriented programming code. Small scale obtains 32% of the papers. This is commonly found in research papers \\nthat only need a small and simple example to illustrate their solution. This is the case in [50] in which a small concocted \\nexample shows the generation process with the Epsilon Generation Language (EGL) 10 . No application was used in 5% of the \\npublications. This includes papers like [51] where authors just mention that code synthesis is performed using a tool named \\nMako-template. Even though the number of publications without an actual application is very low, this demonstrates that'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 10, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Mako-template. Even though the number of publications without an actual application is very low, this demonstrates that \\nsome authors have still not adopted good practice to show an example of the implementation. This is important, especially \\nwhen the TBCG approach is performed with a newly developed tool. \\nAs depicted in Fig. 6 , most papers illustrated their TBCG using large scale applications up to 2015. In this period, while \\nit follows the general trend of papers, the other two categories remained constant over the years. However, in 2016, we \\nobserve a major increase of small scale for TBCG. \\n5.5. Application domain \\nThe tree map in Fig. 7 highlights the fact that TBCG is used in many different areas. Software engineering obtains more \\nthan half of the papers with 55% of the publications. We have grouped in this category other related areas like ontolo- \\ngies, information systems or software product lines. This is expected given that the goal of TBCG is to synthesize software'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 10, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='gies, information systems or software product lines. This is expected given that the goal of TBCG is to synthesize software \\napplications. For example, the work in [52] uses the Rational CASE tool to generate Java programs in order to implement an \\napproach that transforms UML state machine to behavioral code. The next category is embedded systems which obtains 13% \\nof papers. Embedded systems often require low level hardware code diÔ¨Écult to write. Some even consider code generation \\nto VHDL as a compilation rather than automatic programming. In this category, we found papers like [53] in which Velocity \\nis used to produce Verilog code to increase the speed of simulation. Web technology related application domains account for \\n8% of the papers. It consists of papers like [54] where the authors worked to enhance the development dynamic web sites. \\nNetworking obtains 4% of the papers, such as [55] where code is generated for a telephony service network. Compiler obtains'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 10, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Networking obtains 4% of the papers, such as [55] where code is generated for a telephony service network. Compiler obtains \\n1% of the papers, such as [56] where a C code is generated and optimized for an Intel C compiler. It is interesting to note \\nthat several papers were applied in domains such as bio-medicine [57] , artiÔ¨Åcial intelligence [58] , and graphics [59] . \\n10 http://www.eclipse.org/epsilon/doc/egl/'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 11, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='54 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nWe combined application domains with a single paper into the other category. This regroups domains such as agronomy, \\neducation, and Ô¨Ånance. It is important to mention that the domain discussed in this category corresponds to the domain of \\napplication of TBCG employed, which differs from the publication venue. \\n5.6. Orientation \\nA quarter (24%) of the papers in the corpus are authored by a researcher from industry . The remaining 76% are written \\nonly by academics . This is a typical distribution since industrials tend to not publish their work. This result shows that TBCG \\nis used in industry as in [16] . Industry oriented papers have gradually increased since 2003 until they reached a peak in \\n2013. \\n6. Relations between characteristics \\nTo further characterize the trends observed in Section 5 , we identiÔ¨Åed signiÔ¨Åcant and interesting relations between the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 11, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='2013. \\n6. Relations between characteristics \\nTo further characterize the trends observed in Section 5 , we identiÔ¨Åed signiÔ¨Åcant and interesting relations between the \\ndifferent facets of the classiÔ¨Åcation scheme, using SPSS. \\n6.1. Statistical correlations \\nA Shapiro‚ÄìWilk test of each category determined that the none of them are normally distributed. Therefore, we opted \\nfor the Spearman two-tailed test of non-parametric correlations with a signiÔ¨Åcance value of 0.05 to identify correlations \\nbetween the trends of each category. The only signiÔ¨Åcantly strong correlations we found statistically are between the two \\ninput types, and between MDE and input type. \\nWith no surprise, the correlation between run-time and design time input is the strongest among all, with a correlation \\ncoeÔ¨Écient of 0.944 and a p- value of less than 0.001. This concurs with the results found in Section 5.2 . An example is'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 11, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='coeÔ¨Écient of 0.944 and a p- value of less than 0.001. This concurs with the results found in Section 5.2 . An example is \\nwhen the design-time input is UML, the run-time input is always a UML diagram as in [57] . Such a strong relationship is \\nalso noticeable in [60] with programming languages and source code, as well as in [58] when a schema design is used for \\nstructured data. As a result, all run-time input categories are correlated to the same categories as for design-time input. We \\nwill therefore treat these two facets together as input type . \\nThere is a strong correlation of coeÔ¨Écient of 0.738 and a p-value of less than 0.001 between input type and MDE . As \\nexpected, more than 90% of the papers using general purpose and domain speciÔ¨Åc inputs are follow the MDE approach. \\n6.2. Other interesting relations \\nWe also found weak but statistically signiÔ¨Åcant correlations between the remaining facets. We discuss the result here. \\n6.2.1. Template style'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 11, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='6.2. Other interesting relations \\nWe also found weak but statistically signiÔ¨Åcant correlations between the remaining facets. We discuss the result here. \\n6.2.1. Template style \\nFigure 8 shows the relationship between template style, design-time input, and output types. We found that for \\nthe predeÔ¨Åned templates, there are twice as many papers that use schema input than domain speciÔ¨Åc. However, for \\n168\\n72\\n79\\n26\\n62\\n13\\n24\\n18\\n13\\n3\\n3\\nOutput-based\\nPredefined\\nRule-based\\nGeneral \\npurpose\\nDomain \\nspecific\\nSchema\\nProg. \\nlanguage\\nSource \\ncode\\nStructured \\ndata\\nDesign-time input type\\nOutput type\\nFig. 8. Relation between template style (vertical) and input/output types (horizontal).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 12, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n55 \\nSource \\ncode\\nStructured \\ndata\\nGeneral \\npurpose\\nDomain \\nspecific\\nSchema\\nProg. \\nlanguage\\nFig. 9. Relation between output (vertical) and design-time input (horizontal) types showing the number of papers in each intersection. \\nTable 7 \\nDistribution of the tool facet. \\nNamed MDE \\nUnspeciÔ¨Åed \\nOther \\nNamed not MDE \\n41% \\n28% \\n23% \\n8% \\noutput-based, domain speciÔ¨Åc inputs are used slightly more often. We also notice that general purpose input is never used \\nwith rule-based templates. The output type follows the same general distribution regardless of the template style. \\nAll rule-based style approaches have included a sample application. Meanwhile, the proportion of small scale was twice \\nmore important for predeÔ¨Åned templates (51%) then for output-based (27%). \\nWe found that popular tools were used twice as often on output-based templates (58%) than on predeÔ¨Åned templates'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 12, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='more important for predeÔ¨Åned templates (51%) then for output-based (27%). \\nWe found that popular tools were used twice as often on output-based templates (58%) than on predeÔ¨Åned templates \\n(23%). Rule-based templates never employed a tool that satisÔ¨Åed our popularity threshold, but used other tools such as \\nStratego. \\nWe found that all papers using a rule-based style template do not follow an MDE approach. On the contrary, 70% of the \\noutput-based style papers and 56% of the predeÔ¨Åned ones follow an MDE approach. \\nFinally, we found that for each template style, the number of papers authored by an industry researcher Ô¨Çuctuated \\nbetween 22‚Äì30%. \\n6.2.2. Input type \\nThe bubble chart in Fig. 9 illustrates the tendencies between input and output types. It is clear that source code is \\nthe dominant generated artifact regardless of the input type. Source code is more often generated from general purpose'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 12, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='the dominant generated artifact regardless of the input type. Source code is more often generated from general purpose \\nand domain speciÔ¨Åc inputs than from schema and programming languages. Also, the largest portion of structured data is \\ngenerated from a schema input. \\nMoving on to input type and application scale, we found that small scales are used 40% of the time when the input is a \\nprogramming language. The number of papers with no sample application is very low (5%) regardless of the template style. \\nFinally, 74% of papers using large scale applications use a domain speciÔ¨Åc input, which is slightly higher than those using a \\ngeneral purpose input with 71%. \\n6.2.3. Output type, application scale, and orientation \\nAs we compared output type to orientation, we found that industrials generate slightly more source code than academics: \\n89% vs. 80%. However, academics generate more structured data than industrials: 18% vs. 6% and 3% vs. 1%., respectively. We'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 12, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='89% vs. 80%. However, academics generate more structured data than industrials: 18% vs. 6% and 3% vs. 1%., respectively. We \\nfound that 65% of the papers without application are from the academy. \\n7. Template-based code generation tools \\nTable 7 shows that half of the papers used a popular TBCG tool ( named ). The other half used less popular tools (the other \\ncategory), did not mention any TBCG tool, or implemented the code generation directly for the purpose of the paper. \\n7.1. Popular tools in research \\nFigure 10 shows the distribution of popular tools used in at least 1% of the papers, i.e., Ô¨Åve papers. We see that only \\n6/14 popular tools follow MDE approaches. Acceleo and Xpand are the most popular with respectively 16% and 15% of the \\npapers using them. Their popularity is probably due to their simple syntax and ease of use [61] and the fact that they are \\nMDE tools [16] . They both have an OCL-like language for the dynamic part and rely on a metamodel speciÔ¨Åed in Ecore as'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 12, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='MDE tools [16] . They both have an OCL-like language for the dynamic part and rely on a metamodel speciÔ¨Åed in Ecore as \\ndesign-time input.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 13, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='56 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nFig. 10. Popular tools. \\nEGL also has a structure similar to the other model-based tools. It is natively integrated with languages from the Epsilon \\nfamily, thus relies on the Epsilon Object Language for its dynamic part. MOFScript is another popular model-based tool that \\nonly differs in syntax from the others. Xtend2 is the least used popular model-based tool. It is both an advanced form of \\nXpand and a simpliÔ¨Åed syntactical version of Java. \\nXSLT is the third most popular tool used. It is suitable for XML documents only. Some use it for models represented in \\ntheir XMI format, as it is the case in [62] . XSLT follows the template and Ô¨Åltering strategy. It matches each tag of the input \\ndocument and applies the corresponding template. \\nJET [63] and Velocity [53] are used as often as each other on top of being quite similar. The main difference is that JET'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 13, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='document and applies the corresponding template. \\nJET [63] and Velocity [53] are used as often as each other on top of being quite similar. The main difference is that JET \\nuses an underlying programming language (Java) for the dynamic part. In JET, templates are used to developers generate the \\nJava code speciÔ¨Åc for the synthesis of code to help developers implement the code generation. \\nStringTemplate [64] has its own template structure. It can be embedded into a Java code where strings to be output are \\ndeÔ¨Åned using templates. Note that all the tools mentioned above use an output-based template style. \\nThe most popular CASE tools for TBCG are Fujaba [65] , Rational [66] , and Rhapsody [67] . One of the features they offer is \\nto generate different target languages from individual UML elements. All CASE tools (even counting the other category) have \\nbeen used in a total of 39 papers, which puts them at par with Xpand. CASE tools are mostly popular for design activities;'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 13, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='been used in a total of 39 papers, which puts them at par with Xpand. CASE tools are mostly popular for design activities; \\ncode generation is only one of their many features. CASE tools have a predeÔ¨Åned template style. \\nSimulink TLC is the only rule-based tool among the most popular ones. As a rule-based approach, it has a different \\nstructure compared to the above mentioned tools. Its main difference is that the developer writes the directives to be \\nfollowed by Simulink in order to render the Ô¨Ånal C code from S-functions. \\nWe notice that the most popular tools are evenly distributed between model-based tools (Acceleo, Xpand) and code- \\nbased tools (JET, XSLT). Surprisingly, XSLT, which has been around the longest, is less popular than Xpand. This is undoubt- \\nedly explained by the advantages that MDE has to offer [7,8] . \\n7.2. UnspeciÔ¨Åed and other tools \\nAs depicted in Table 7 , 28% of the papers did not specify the tool that was used, as in [68] where the authors introduce'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 13, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='7.2. UnspeciÔ¨Åed and other tools \\nAs depicted in Table 7 , 28% of the papers did not specify the tool that was used, as in [68] where the authors introduce \\nthe concept of a meta-framework to resolve issues involved in extending the life of applications. Furthermore, 23% of the \\npapers used less popular tools, present in less than Ô¨Åve papers, such as T4 [44] and Cheetah [56] , a python powered template \\nmainly used for web developing. Like JET, Cheetah templates generate Python classes, while T4 is integrated with .NET \\ntechnology. Some CASE tools were also in this category, such as AndroMDA [69] . Other examples of less popular tools are \\nGroovy template [47] , Meta-Aspect-J [70] , and Jinja2 [71] . The fact that new or less popular tools are still abundantly used \\nsuggests that research in TBCG is still active with new tools being developed or evolved. \\n7.3. Trends of tools used'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 13, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='suggests that research in TBCG is still active with new tools being developed or evolved. \\n7.3. Trends of tools used \\nEach one of these tools had a different evolution over the years. UnspeciÔ¨Åed tools were prevalent before 2004 and then \\nkept a constant rate of usage until a drop since 2014. We notice a similar trend for CASE tools that were the most popular \\nin 2005 before decreasing until 2009. They only appear in at most three papers per year after 2010. The use of the most \\npopular tool, Xpand, gradually increased since 2005 to reach the peak in 2013 before decreasing. The other category main- \\ntained an increasing trend until 2014. Yet, a few other popular tools appeared later on. For example, EGL started appearing \\nin 2008 and had its peak in 2013. Acceleo appeared a year later and was the most popular TBCG tool in 2013‚Äì2014. Finally,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n57 \\nMOFScript had no more than a paper per year since 2005. StringTemplate and T4 were used scarcely since 2006 and 2009, \\nrespectively. \\n7.4. Characteristics of tools \\nWe have also analyzed each popular tool with respect to the characteristics presented in Section 5 . As mentioned earlier, \\nmost of the popular tools implement output-based template technique except the CASE tools which are designed following \\nthe predeÔ¨Åned style. \\nTools such as Acceleo, Xpand, EGL, MOFScript and 97% of the CASE tools papers are only used based on an MDE ap- \\nproach, given that they were created by this community. Nevertheless, there are tools that were never used with MDE \\nprinciples, like JET and Cheetah. Such tools can handle a program code or a schema as metamodel but have no internal \\nsupport for modeling languages. Moreover, the programmer has to write his own stream reader to parse the input, but they'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='support for modeling languages. Moreover, the programmer has to write his own stream reader to parse the input, but they \\nallow for a broader range of artifacts as inputs that do not have to be modeled explicitly. A few code-based tools provide \\ninternal support for model-based approaches. For instance, Velocity, XSLT, and StringTemplate can handle both UML and pro- \\ngrammed metamodel as design-time input. T4 can be integrated with MDE artifacts (e.g., generate code based on a domain- \\nspeciÔ¨Åc language in Visual studio). However, all four papers in the corpus that implemented TBCG in T4 did not use an MDE \\napproach. \\nA surprising result we found is that EGL is the only MDE tool that has its papers mostly published in MDE venues like \\nSosym , Models , and Ecmfa . All the other tools are mostly published in other venues like Icssa , whereas software engineering \\nvenues, like Ase or Icse , and MDE venues account for 26‚Äì33% of the papers for each of the rest of the MDE tools.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='venues, like Ase or Icse , and MDE venues account for 26‚Äì33% of the papers for each of the rest of the MDE tools. \\nCASE tools, MOFScript, Velocity, and Simulink TLC mostly generate program code. The latter is always used in the domain \\nof embedded systems. Papers that use StringTemplate do not include any validation process, so is Velocity in 93% of the \\npapers using it. XSLT has been only used to generate structured data as anticipated. \\nOther tools are the most used TBCG in the industry. This is because the tool is often internal to the company [72] . Among \\nthe most popular tools, Xpand is the most in the industry. \\n7.5. Application scale \\nBetween application scale and tools, we found that 74% of the papers that make use of a popular tool used large scale \\napplication to illustrate their approach. Also, 62% of the papers using unpopular tools 11 use large scale applications. Small \\nscale is likely to be used in unpopular tools rather than popular tools. \\n7.6. Tool support'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='scale is likely to be used in unpopular tools rather than popular tools. \\n7.6. Tool support \\nIn total, we found 82 different tools named in the corpus. Among them 54% are no longer supported (i.e., no release, \\nupdate or commit since the past two years). Interestingly, 55% of the tools have been developed by the industry. 70% of \\nthese industry tools are still supported, in contrast with 51% for the academic tools. As one would expect, the tendency \\nshows that tools that are frequently mentioned in research papers are still supported and are developed by industry. \\n8. MDE and template-based code generation \\nOverall, 64% of the publications followed MDE techniques and principles. For example in [73] , the authors propose a sim- \\nulation environment with an architecture that aims at integrating tools for modeling, simulation, analysis, and collaboration. \\nAs expected, most of the publications using output-based and predeÔ¨Åned techniques are classiÔ¨Åed as model-based papers.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='As expected, most of the publications using output-based and predeÔ¨Åned techniques are classiÔ¨Åed as model-based papers. \\nThe remaining 36% of the publications did not use MDE. This includes all papers that use a rule-based template style as \\nreported in Section 6 . For example, the authors in [40] developed a system that handles the implementation of dependable \\napplications and offers a better certiÔ¨Åcation process for the fault-tolerance mechanisms. \\nAs Fig. 11 shows, the evolution of the MDE category reveals that MDE-based approach started over passing non MDE- \\nbased techniques in 2005, except for 2006. It increased to reach a peak in 2013 and then started decreasing as the general \\ntrend of the corpus. Overall, MDE-based techniques for TBCG have been dominating other techniques in the past 12 years. \\nWe also analyzed the classiÔ¨Åcation of only MDE papers with respect to the characteristics presented in Section 3 . We'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='We also analyzed the classiÔ¨Åcation of only MDE papers with respect to the characteristics presented in Section 3 . We \\nonly focus here on facets with different results compared to the general trend of papers. We found that only half of the \\ntotal number of papers using unspeciÔ¨Åed and other tools are MDE-based papers. We only found one paper that uses a \\nprogramming language as design-time input with MDE [74] . This analysis also shows that the year 2005 clearly marked the \\nshift from schema to domain-speciÔ¨Åc design-time inputs, as witnessed in Section 5.2 . Thus after general purpose, which \\nobtains 69% of the publications, domain speciÔ¨Åc accounts for a better score of 26%, while schema obtains only 4%. With \\nrespect to the run-time category, the use of domain-speciÔ¨Åc models increased to reach a peak in 2013. As expected, no \\nprogram code is used for MDE papers, because MDE typically does not consider them as models, unless a metamodel of the \\nprogramming language is used.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 14, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='program code is used for MDE papers, because MDE typically does not consider them as models, unless a metamodel of the \\nprogramming language is used. \\n11 Refers to the union of other and unspeciÔ¨Åed categories of the tool facet.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 15, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='58 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nUsing MDE\\nNot using MDE\\n# of papers\\nFig. 11. Evolution of the MDE facet. \\nInterestingly, MDE venues are only the second most popular after other venues for MDE approaches. Finally, MDE journal \\npapers maintained a linear increase over the years, while MDE conference papers had a heterogeneous evolution similar to \\nthe general trend of papers. \\n9. Discussion \\n9.1. RQ1: What are the trends in TBCG? \\nThe statistical results from this signiÔ¨Åcantly large sample of papers clearly suggest that TBCG has received suÔ¨Écient \\nattention from the research community. The community has maintained a production rate in-line with the last 11 years \\naverage, especially with a constant rate of appearance in journal articles. The only exceptions were a signiÔ¨Åcant boost in'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 15, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='average, especially with a constant rate of appearance in journal articles. The only exceptions were a signiÔ¨Åcant boost in \\n2013 and a dip in 2015. The lack of retention of papers appearing in non MDE may indicate that TBCG is now applied \\nin development projects rather than being a critical research problem to solve. Also, conference papers as well as venues \\noutside MDE and software engineering had a signiÔ¨Åcant impact on the evolution of TBCG. Given that TBCG seems to have \\nreached a steady publication rate since 2005, we can expect contributions from the research community to continue in that \\ntrend. \\n9.2. RQ2: What are the characteristics of TBCG approaches? \\nOur classiÔ¨Åcation scheme constitutes the main source to answer this question. The results clearly indicate the preferences \\nthe research community has regarding TBCG. Output-based templates have always been the most popular style from the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 15, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='the research community has regarding TBCG. Output-based templates have always been the most popular style from the \\nbeginning. Nevertheless, there have been some attempts to propose other template styles, like the rule-based style, but they \\ndid not catch on. Because of its simplicity to use, the predeÔ¨Åned style is probably still popular in practice, but it is less \\nmentioned in research papers. TBCG has been used to synthesize a variety of application code or documents. As expected, \\nthe study shows that modeling language inputs have prevailed over any other type. SpeciÔ¨Åcally for MDE approaches to TBCG, \\nthe input to transform is moving from general purpose to domain-speciÔ¨Åc models. Academic researchers have contributed \\nmost, as expected with a literature review, but we found that industry is actively and continuously using TBCG as well. The \\nstudy also shows that the community is moving from large-scale applications to smaller-sized examples in research papers.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 15, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='study also shows that the community is moving from large-scale applications to smaller-sized examples in research papers. \\nThis concurs with the level of maturity of this synthesis approach. The study conÔ¨Årms that the community uses TBCG to \\ngenerate mainly source code. This trend is set to continue since the automation of computerized tasks is continuing to gain \\nground in all Ô¨Åelds. Finally, TBCG has been implemented in many domains, software engineering and embedded systems \\nbeing the most popular, but also unexpectedly in unrelated domains, such as bio-medicine and Ô¨Ånance. \\n9.3. RQ3: To what extent are TBCG tools being used in research? \\nIn this study, we discovered a total of 82 different tools for TBCG that are mentioned in research papers. Many studies \\nimplemented code generation with a custom-made tool that was never or seldom reused. This indicates that the develop-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 15, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='implemented code generation with a custom-made tool that was never or seldom reused. This indicates that the develop- \\nment of new tools is still very active. MDE tools are the most popular. Since the research community has favored output- \\nbased template style, this has particularly inÔ¨Çuenced the tools implementation. This template style allows for more Ô¨Åne- \\ngrained customization of the synthesis logic which seems to be what users have favored. This particular aspect is also \\ninÔ¨Çuencing the expansion of TBCG into industry. Most popular tool are actively supported by industry. Well-known tools \\nlike Acceleo, Xpand and Velocity are moving from being simple research material to effective development resources in in- \\ndustry. Finally, the study There are many TBCG tools that are popular in industry that fall under the ‚ÄúOther tools‚Äù category \\nbecause they are rarely reported in the scientiÔ¨Åc literature (under 1% of the papers in our corpus). Since this study is a lit-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 15, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='because they are rarely reported in the scientiÔ¨Åc literature (under 1% of the papers in our corpus). Since this study is a lit- \\nerature review, the presence of a tool in this study is biased towards the what is published, and may not reÔ¨Çect the reality \\nin industry. This is a common threat of SMS.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n59 \\n9.4. RQ4: What is the place of MDE in TBCG? \\nAll this analysis clearly concludes that the advent of MDE has been driving TBCG research. In fact, MDE has led to \\nincrease the average number of publications by a factor of four. There are many advantages to code generation, such as \\nreduced development effort, easier to write and understand domain/application concepts and less error-prone [8] . These are, \\nin fact, the pillar principles of MDE and domain-speciÔ¨Åc modeling [2] . Thus, it is not surprising to see that many, though \\nnot exclusively, code generation tools came out from the MDE community. As TBCG became a commonplace in general, \\nthe research in this area is now mostly conducted by the MDE community. Furthermore, MDE has brought very popular \\ntools that have encountered a great success, and they are also contributing to the expansion of TBCG across industry. It'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='tools that have encountered a great success, and they are also contributing to the expansion of TBCG across industry. It \\nis important to mention that the MDE community publishes in speciÔ¨Åc venues like Models , Sosym , or Ecmfa unlike other \\nresearch communities where the venues are very diversiÔ¨Åed. This resulted in three MDE venues at the top of the ranking. \\n9.5. IdentiÔ¨Åed challenges \\nAfter thoroughly analyzing each paper in the corpus, we noted several problems that the research community has ne- \\nglected in TBCG. First, we found 66% of the papers did not provide any assessment of the code generation process or the \\ngenerated output. We only found one paper with a formal veriÔ¨Åcation of the generated code using non-functional require- \\nment analysis [75] . Furthermore, the TBCG can be veriÔ¨Åed through benchmarks as in [55] . Second, we found no paper that \\ninvestigates eÔ¨Éciency of code generation. Researchers may be inspired from other related communities, such as compiler'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='investigates eÔ¨Éciency of code generation. Researchers may be inspired from other related communities, such as compiler \\noptimization [60] . Third, designing templates requires skillful engineering. Good practices, design patterns and other forms \\nof good reusable idioms would be of great value to developers [76] . \\n9.6. Threats to validity \\nThe results presented in this survey have depended on many factors that could potentially threaten its validity. \\n9.6.1. Construction validity \\nIn a strict sense, our Ô¨Åndings are valid only for our sample that we collected from 20 0 0‚Äì2016. This leads to determine \\nwhether the primary studies used in our survey are a good representation of the whole population. From Fig. 3 , we can \\nobserve that our sample can be attributed as a representative sample of the whole population. In particular, the average \\nnumber of identiÔ¨Åed primary studies per year is 28 with standard deviation 15.76. A more systematic selection process'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='number of identiÔ¨Åed primary studies per year is 28 with standard deviation 15.76. A more systematic selection process \\nwould have been diÔ¨Écult to be exhaustive about TBCG. We chose to obtain the best possible coverage at the cost of du- \\nplications. Nevertheless, the size of the corpus we classiÔ¨Åed is about ten times larger than other systematic reviews related \\nto code generation (see Section 2.4 ). We are, therefore, conÔ¨Ådent that this sample is a representative subset of all relevant \\npublications on TBCG. \\nAnother potential limitation is the query formulation for the keyword search. It is diÔ¨Écult to encode a query that is \\nrestrictive enough to discard unrelated publications, but at the same time retrieves all the relevant ones. In order to obtain \\na satisfactory balance, we included synonyms and captured possible declinations. In this study, we are only interested in'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='a satisfactory balance, we included synonyms and captured possible declinations. In this study, we are only interested in \\ncode generation. Therefore we discarded articles where TBCG was used for reporting or mass mailing, for example. We \\nbelieve that our sample is large enough that additional papers will not signiÔ¨Åcantly affect the general trends and results. \\nWe are fully aware that, since TBCG is widely used in practice, industries have their own tools and many have not been \\npublished in academic venues. Our goal was not to be exhaustive, but to get a representative sample. \\n9.6.2. Internal validity \\nA potential limitation is related to data extraction. It is diÔ¨Écult to extract data from relevant publications, especially \\nwhen the quality of the paper is low, when code generation is not the primary contribution of the paper, or when critical \\ninformation for the classiÔ¨Åcation is not directly available in the paper. For example in [77] , the authors only mention the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='information for the classiÔ¨Åcation is not directly available in the paper. For example in [77] , the authors only mention the \\nname of the tool used to generate the code. In order to mitigate this threat, we had to resort to searching for additional \\ninformation about the tool: reading other publications that use the tool, traversing the website of the tool, installing the \\ntool, or discussing with the tools experts. \\nAnother possible threat is the screening of papers based on inclusion and exclusion criteria that we deÔ¨Åned before the \\nstudy was conducted. During this process, we examined only the title, the abstract. Therefore, there is a probability that we \\nexcluded relevant publications such as [55] , that do not include any TBCG terms. In order to mitigate this threat, whenever \\nwe were unsure whether a publication should be excluded or not we conservatively opted to include it. However, during'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 16, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='we were unsure whether a publication should be excluded or not we conservatively opted to include it. However, during \\nclassiÔ¨Åcation when reading the whole content of the paper, we may still have excluded it. \\n9.6.3. External validity \\nThe results we obtained are based on TBCG only. Even though our classiÔ¨Åcation scheme includes facets like orientation, \\napplication domain, that are not related to the area, we followed a topic based classiÔ¨Åcation. The core characteristics of our \\nstudy are strictly related to this particular code synthesis technique. We have deÔ¨Åned characteristics like template style and \\nthe two levels of inputs that we believe are exclusive to TBCG. Therefore, the results cannot be generalized to other code \\ngeneration techniques mentioned in Section 2.2 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='60 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n9.6.4. Conclusion validity \\nOur study is based on a large number of primary studies. This helps us mitigate the potential threats related to the \\nconclusions of our study. A missing paper or a wrongly classiÔ¨Åed paper would have a very low impact on the statistics \\ncompared to a smaller number of primary studies. In addition, as a senior reviewer did a sanity check on the rejected \\npapers, we are conÔ¨Ådent that we did not miss a signiÔ¨Åcant number of papers. Hence, the chances for wrong conclusions are \\nsmall. Replication of this study can be achieved as we provided all the details of our research method in Section 3 . Also, our \\nstudy follows the methodology described in [20] . \\n10. Conclusion \\nThis paper reports the results of a large survey we conducted on the topic of TBCG, which has been missing in the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='study follows the methodology described in [20] . \\n10. Conclusion \\nThis paper reports the results of a large survey we conducted on the topic of TBCG, which has been missing in the \\nliterature. The objectives of this study are to better understand the characteristics of TBCG techniques and associated tools, \\nidentify research trends, and assess the importance of the role that MDE plays. The analysis of this corpus is organized into \\nfacets of a novel classiÔ¨Åcation scheme, which is of great value to modeling and software engineering researchers who are \\ninterested in painting an overview of the literature on TBCG. \\nOur study shows that the community has been diversely using TBCG over the past 16 years, and that research and \\ndevelopment is still very active. TBCG has greatly beneÔ¨Åted from MDE in 2005 and 2013 which mark the two peaks of \\nthe evolution of this area, tripling the average number of publications. In addition, TBCG has favored a template style that'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='the evolution of this area, tripling the average number of publications. In addition, TBCG has favored a template style that \\nis output-based and modeling languages as input. It has been applied in a variety of domains. The community has been \\nfavoring the use of custom tools for code generation over popular ones. Most research using TBCG follows an MDE approach. \\nFurthermore, both MDE and non-MDE tools are becoming effective development resources in industry. \\nThe study also revealed that, although the research in TBCG is mature enough, there are many open issues that can \\nbe addressed in the future, upstream and downstream the generation itself. Upstream, the deÔ¨Ånition of templates is not \\na trivial task. Supporting the developers in such a deÔ¨Ånition is a must. Downstream, methods and techniques need to be \\ndeÔ¨Åned to assess the correctness and quality of the generated code.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='a trivial task. Supporting the developers in such a deÔ¨Ånition is a must. Downstream, methods and techniques need to be \\ndeÔ¨Åned to assess the correctness and quality of the generated code. \\nWe believe this survey will be of beneÔ¨Åt to someone familiar with code generation by knowing how their favorite tool \\nranks in popularity within the research community, the relevance and importance of the use of templates, and in which \\ncontext TBCG has been applied (application domain). The relations across categories in Section VI show non-intuitive results \\nas well. The paper also promotes MDE in Ô¨Åelds that have not been traditionally exposed to it. \\nSupplementary material \\nSupplementary material associated with this article can be found, in the online version, at 10.1016/j.cl.2017.11.003 . \\nReferences \\n[1] Rich C , Waters RC . Automatic programming: myths and prospects. Computer 1988;21(8):40‚Äì51 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='References \\n[1] Rich C , Waters RC . Automatic programming: myths and prospects. Computer 1988;21(8):40‚Äì51 . \\n[2] Kelly S , Tolvanen J-P . Domain-speciÔ¨Åc modeling: enabling full code generation. John Wiley & Sons; 2008 . \\n[3] Bonta E , Bernardo M . Padl2java: a Java code generator for process algebraic architectural descriptions. In: Proceedings of the european conference on \\nsoftware architecture. IEEE; 2009. p. 161‚Äì70 . \\n[4] Tatsubori M , Chiba S , Killijian M-O , Itano K . OpenJava: a class-based macro system for Java. In: ReÔ¨Çection and software engineering. In: LNCS, 1826. \\nSpringer; 20 0 0. p. 117‚Äì33 . \\n[5] Lohmann D , Blaschke G , Spinczyk O . Generic advice: on the combination of AOP with generative programming in AspectC++. In: Proceedings of \\ninternational conference on generative programming and component engineering. In: LNCS, 3286. Berlin Heidelberg: Springer; 2004. p. 55‚Äì74 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='international conference on generative programming and component engineering. In: LNCS, 3286. Berlin Heidelberg: Springer; 2004. p. 55‚Äì74 . \\n[6] Kleppe AG , Warmer J , Bast W . MDA explained. The model driven architecture: practice and promise. Addison-Wesley; 2003 . \\n[7] J√∂rges S . Construction and evolution of code generators 7747. Ch 2 The state of the art in code generation. Berlin Heidelberg: Springer; 2013. p. 11‚Äì38 . \\n[8] Balzer R . A 15 year perspective on automatic programming. Trans Softw Eng 1985;11(11):1257‚Äì68 . \\n[9] Floch A , Yuki T , Guy C , Derrien S , Combemale B , Rajopadhye S , et al. Model-driven engineering and optimizing compilers: a bridge too far?. In: Model \\nDriven Engineering Languages and Systems. In: LNCS, 6981. Springer Berlin Heidelberg; 2011. p. 608‚Äì22 . \\n[10] Stahl T , Voelter M , Czarnecki K . Model-driven software development ‚Äì technology, engineering, management. John Wiley & Sons; 2006 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='[10] Stahl T , Voelter M , Czarnecki K . Model-driven software development ‚Äì technology, engineering, management. John Wiley & Sons; 2006 . \\n[11] L√∫cio L , Amrani M , Dingel J , Lambers L , Salay R , Selim GM , et al. Model transformation intents and their properties. Softw Syst Model \\n2014;15(3):685‚Äì705 . \\n[12] Czarnecki K , Helsen S . Feature-based survey of model transformation approaches. IBM Syst J 2006;45(3):621‚Äì45 . \\n[13] Gamma E , Helm R , Johnson R , Vlissides J . Design patterns: elements of reusable object-oriented software. Addison Wesley Professional; 1994 . \\n[14] Beckmann O , Houghton A , Mellor M , Kelly PH . Runtime code generation in C++ as a foundation for domain-speciÔ¨Åc optimisation. In: Domain-SpeciÔ¨Åc \\nProgram Generation. In: LNCS, 3016. Berlin Heidelberg: Springer; 2004. p. 291‚Äì306 . \\n[15] C√≥rdoba I , de Lara J . ANN: a domain-speciÔ¨Åc language for the effective design and validation of Java annotations. Comput Lang Syst Struct \\n2016;45:164‚Äì90 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='[15] C√≥rdoba I , de Lara J . ANN: a domain-speciÔ¨Åc language for the effective design and validation of Java annotations. Comput Lang Syst Struct \\n2016;45:164‚Äì90 . \\n[16] Jugel U , Preu√üner A . A case study on API generation. In: System analysis and modeling: about models. In: LNCS, 6598. Springer; 2011. p. 156‚Äì72 . \\n[17] Kitchenham BA , Dyba T , Jorgensen M . Evidence-based software engineering. In: Proceedings of international conference on software engineering. \\nWashington, DC, USA: IEEE Computer Society; 2004. p. 273‚Äì81 . \\n[18] Kitchenham BA , Budgen D , Brereton OP . Using mapping studies as the basis for further research - a participant-observer case study. Inf Softw Technol \\n2011;53(6):638‚Äì51 . \\n[19] Brereton P , Kitchenham BA , Budgen D , Turner M , Khalil M . Lessons from applying the systematic literature review process within the software engi- \\nneering domain. J Syst Softw 2007;80(4):571‚Äì83 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 17, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='neering domain. J Syst Softw 2007;80(4):571‚Äì83 . \\n[20] Petersen K , Feldt R , Mujtaba S , Mattsson M . Systematic mapping studies in software engineering. In: Proceedings of the 12th international conference \\non evaluation and assessment in software engineering, EASE‚Äô08, 17. British Computer Society; 2008. p. 68‚Äì77 . \\n[21] Mehmood A , Jawawi DN . Aspect-oriented model-driven code generation: a systematic mapping study. Inf Softw Technol 2013;55(2):395‚Äì411 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n61 \\n[22] Gurunule D , Nashipudimath M . A review: analysis of aspect orientation and model driven engineering for code generation. Procedia Comput Sci \\n2015;45:852‚Äì61 . \\n[23] Dom√≠guez E , P√©rez B , Rubio AL , Zapata MA . A systematic review of code generation proposals from state machine speciÔ¨Åcations. Inf Softw Technol \\n2012;54(10):1045‚Äì66 . \\n[24] Batot E , Sahraoui H , Syriani E , Molins P , Sboui W . Systematic mapping study of model transformations for concrete problems. In: Model-driven \\nengineering and software development. IEEE; 2016. p. 176‚Äì83 . \\n[25] Rose LM , Matragkas N , Kolovos DS , Paige RF . A feature model for model-to-text transformation languages. In: Modeling in software engineering. IEEE; \\n2012. p. 57‚Äì63 . \\n[26] Kosar T , Bohra S , Mernik M . Domain-speciÔ¨Åc languages: a systematic mapping study. Inf Softw Technol 2016;71:77‚Äì91 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='2012. p. 57‚Äì63 . \\n[26] Kosar T , Bohra S , Mernik M . Domain-speciÔ¨Åc languages: a systematic mapping study. Inf Softw Technol 2016;71:77‚Äì91 . \\n[27] M√©ndez-Acu√±a D , Galindo JA , Degueule T , Combemale B , Baudry B . Leveraging software product lines engineering in the development of external \\nDSLs: a systematic literature review. Comput Lang Syst Struct 2016;46:206‚Äì35 . \\n[28] Mat√∫s Sul√≠r JP . Labeling source code with metadata: a survey and taxonomy. In: Proceedings of federated conference on computer science and infor- \\nmation systems. In: Workshop on Advances in Programming Languages (WAPL‚Äô17), CFP1785N-ART. IEEE; 2017. p. 721‚Äì9 . \\n[29] Buchmann T , Schw√§gerl F . Using meta-code generation to realize higher-order model transformations. In: Proceedings of international conference on \\nsoftware technologies; 2013. p. 536‚Äì41 . \\n[30] Seriai A , Benomar O , Cerat B , Sahraoui H . Validation of software visualization tools: a systematic mapping study. In: Proceedings of IEEE working'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='software technologies; 2013. p. 536‚Äì41 . \\n[30] Seriai A , Benomar O , Cerat B , Sahraoui H . Validation of software visualization tools: a systematic mapping study. In: Proceedings of IEEE working \\nconference on software visualization. VISSOFT; 2014. p. 60‚Äì9 . \\n[31] Liu Q . C++ techniques for high performance Ô¨Ånancial modelling. WIT Trans Model Simul 2006;43:1‚Äì8 . \\n[32] Fang M , Ying J , Wu M . A template engineering based framework for automated software development. In: Proceeding of the 10th international con- \\nference on computer supported cooperative work in design. IEEE; 2006. p. 1‚Äì6 . \\n[33] Singh A , Schaeffer J , Green M . A template-based approach to the generation of distributed applications using a network of workstations. IEEE Trans \\nParallel Distrib Syst 1991;2(1):52‚Äì67 . \\n[34] O‚ÄôHalloran C . Automated veriÔ¨Åcation of code automatically generated from simulink ¬Æ. Autom Softw Eng 2013;20(2):237‚Äì64 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Parallel Distrib Syst 1991;2(1):52‚Äì67 . \\n[34] O‚ÄôHalloran C . Automated veriÔ¨Åcation of code automatically generated from simulink ¬Æ. Autom Softw Eng 2013;20(2):237‚Äì64 . \\n[35] Dahman W , Grabowski J . UML-based speciÔ¨Åcation and generation of executable web services. In: System analysis and modeling. In: LNCS, 6598. \\nSpringer; 2011. p. 91‚Äì107 . \\n[36] Gessenharter D . Mapping the UML2 semantics of associations to a Java code generation model. In: Proceedings of international conference on model \\ndriven engineering languages and systems. In: LNCS, 5301. Springer; 2008. p. 813‚Äì27 . \\n[37] Valderas P , Pelechano V , Pastor O . Towards an end-user development approach for web engineering methods. In: Proceedings of international confer- \\nence on advanced information systems engineering, 4001. Springer; 2006. p. 528‚Äì43 . \\n[38] Hemel Z , Kats LC , Groenewegen DM , Visser E . Code generation by model transformation: a case study in transformation modularity. Softw Syst Model \\n2010;9(3):375‚Äì402 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='[38] Hemel Z , Kats LC , Groenewegen DM , Visser E . Code generation by model transformation: a case study in transformation modularity. Softw Syst Model \\n2010;9(3):375‚Äì402 . \\n[39] Brun M , Delatour J , Trinquet Y . Code generation from AADL to a real-time operating system: an experimentation feedback on the use of model \\ntransformation. In: Engineering of complex computer systems. IEEE; 2008. p. 257‚Äì62 . \\n[40] Buckl C , Knoll A , Schrott G . Development of dependable real-time systems with Zerberus. In: Proceedings of the 11th IEEE paciÔ¨Åc rim international \\nsymposium on dependable computing; 2005. p. 404‚Äì8 . \\n[41] Li J , Xiao H , Yi D . Designing universal template for database application system based on abstract factory. In: Computer science and information \\nprocessing. IEEE; 2012. p. 1167‚Äì70 . \\n[42] Gopinath VS , Sprinkle J , Lysecky R . Modeling of data adaptable reconÔ¨Ågurable embedded systems. In: International conference and workshops on'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='processing. IEEE; 2012. p. 1167‚Äì70 . \\n[42] Gopinath VS , Sprinkle J , Lysecky R . Modeling of data adaptable reconÔ¨Ågurable embedded systems. In: International conference and workshops on \\nengineering of computer based systems. IEEE; 2011. p. 276‚Äì83 . \\n[43] Buckl C , Regensburger M , Knoll A , Schrott G . Models for automatic generation of safety-critical real-time systems. In: Availability, reliability and \\nsecurity. IEEE; 2007. p. 580‚Äì7 . \\n[44] Fischer T , Kollner C , Hardle M , Muller Glaser KD . Product line development for modular FPGA-based embedded systems. In: Proceedings of symposium \\non rapid system prototyping. IEEE; 2014. p. 9‚Äì15 . \\n[45] Chen K , Chang Y-C , Wang D-W . Aspect-oriented design and implementation of adaptable access control for electronic medical records. Int J Med \\nInform 2010;79(3):181‚Äì203 . \\n[46] Brox M , S√°nchez-Solano S , del Toro E , Brox P , Moreno-Velo FJ . CAD tools for hardware implementation of embedded fuzzy systems on FPGAs. IEEE'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='Inform 2010;79(3):181‚Äì203 . \\n[46] Brox M , S√°nchez-Solano S , del Toro E , Brox P , Moreno-Velo FJ . CAD tools for hardware implementation of embedded fuzzy systems on FPGAs. IEEE \\nTrans Ind Inform 2013;9(3):1635‚Äì44 . \\n[47] Fraternali P , Tisi M . A higher order generative framework for weaving traceability links into a code generator for web application testing. In: Proceed- \\nings of international conference on web engineering. In: LNCS, 5648. Springer; 2009. p. 340‚Äì54 . \\n[48] Vok√°Àác M , Glattetre JM . Using a domain-speciÔ¨Åc language and custom tools to model a multi-tier service-oriented application experiences and chal- \\nlenges. In: Model Driven Engineering Languages and Systems, 3713. Springer; 2005. p. 492‚Äì506 . \\n[49] Kokar M , Baclawski K , Gao H . Category theory-based synthesis of a higher-level fusion algorithm: an example. In: Proceedings of international confer- \\nence on information fusion; 2006. p. 1‚Äì8 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='ence on information fusion; 2006. p. 1‚Äì8 . \\n[50] Hoisl B , Sobernig S , Strembeck M . Higher-order rewriting of model-to-text templates for integrating domain-speciÔ¨Åc modeling languages. In: Model‚Äì\\nDriven Engineering and Software Development. SCITEPRESS; 2013. p. 49‚Äì61 . \\n[51] Ecker W , Velten M , Zafari L , Goyal A . The metamodeling approach to system level synthesis. In: Proceedings of design, automation & test in Europe \\nconference & exhibition. IEEE; 2014. p. 1‚Äì2 . \\n[52] Behrens T , Richards S . Statelator-behavioral code generation as an instance of a model transformation. In: Proceedings of international conference on \\nadvanced information systems engineering. In: LNCS, 1789. Springer; 20 0 0. p. 401‚Äì16 . \\n[53] Durand SH , Bonato V . A tool to support Bluespec SystemVerilog coding based on UML diagrams. In: Proceedings of annual conference on IEEE indus- \\ntrial electronics society. IEEE; 2012. p. 4670‚Äì5 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='trial electronics society. IEEE; 2012. p. 4670‚Äì5 . \\n[54] Schattkowsky T , Lohmann M . Rapid development of modular dynamic web sites using UML. In: Proceedings of international conference on the uniÔ¨Åed \\nmodeling language. In: LNCS, 2460. Springer; 2002. p. 336‚Äì50 . \\n[55] Buezas N , Guerra E , de Lara J , Mart√≠n J , Monforte M , Mori F , et al. Umbra designer: graphical modelling for telephony services. In: Proceedings of \\neuropean conference on modelling foundations and applications. In: LNCS, 7949. Berlin Heidelberg: Springer; 2013. p. 179‚Äì91 . \\n[56] Manley R , Gregg D . A program generator for intel AES-NI instructions. In: Proceedings of international conference on cryptology. In: LNCS, 6498. \\nSpringer; 2010. p. 311‚Äì27 . \\n[57] Phillips J , Chilukuri R , Fragoso G , Warzel D , Covitz PA . The caCORE software development kit: streamlining construction of interoperable biomedical \\ninformation services. BMC Med Inform Decis Mak 2006;6(2):1‚Äì16 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='information services. BMC Med Inform Decis Mak 2006;6(2):1‚Äì16 . \\n[58] Fu J , Bastani FB , Yen I-L . Automated AI planning and code pattern based code synthesis. In: Proceedings of international conference on tools with \\nartiÔ¨Åcial intelligence. IEEE; 2006. p. 540‚Äì6 . \\n[59] Possatto MA , Lucr√©dio D . Automatically propagating changes from reference implementations to code generation templates. Inf Softw Technol \\n2015;67:65‚Äì78 . \\n[60] Ghodrat MA , Givargis T , Nicolau A . Control Ô¨Çow optimization in loops using interval analysis. In: Proceedings of international conference on compilers, \\narchitectures and synthesis for embedded systems. ACM; 2008. p. 157‚Äì66 . \\n[61] Guduvan A-R , Waeselynck H , Wiels V , Durrieu G , Fusero Y , Schieber M . A meta-model for tests of avionics embedded systems. In: Proceedings of \\ninternational conference on model-driven engineering and software development; 2013. p. 5‚Äì13 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 18, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='international conference on model-driven engineering and software development; 2013. p. 5‚Äì13 . \\n[62] Adamko A . Modeling data-oriented web applications using UML. In: Proceedings of the international conference on computer as a tool, EUROCON \\n2005, 1. IEEE; 2005. p. 752‚Äì5 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 19, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='62 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n[63] K√∂vi A , Varr√≥ D . An eclipse-based framework for AIS service conÔ¨Ågurations. In: Proceedings of the 4th international symposium on service availability, \\nISAS. In: LNCS, 4526. Springer; 2007. p. 110‚Äì26 . \\n[64] Anjorin A , Saller K , Rose S , Sch√ºrr A . A framework for bidirectional model-to-platform transformations. In: Proceedings of the 5th international con- \\nference on software language engineering, SLE 2012. In: LNCS, 7745. Berlin Heidelberg: Springer; 2013. p. 124‚Äì43 . \\n[65] Burmester S , Giese H , Sch√§fer W . Model-driven architecture for hard real-time systems: from platform independent models to code. In: Proceedings \\nof European conference on model driven architecture-foundations and applications. In: LNCS, 3748. Berlin Heidelberg: Springer; 2005. p. 25‚Äì40 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 19, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='of European conference on model driven architecture-foundations and applications. In: LNCS, 3748. Berlin Heidelberg: Springer; 2005. p. 25‚Äì40 . \\n[66] Brown AW , Conallen J , Tropeano D . Introduction: models, modeling, and model-driven architecture (MDA). In: Proceedings of international conference \\non model-driven software development. Berlin Heidelberg: Springer; 2005. p. 1‚Äì16 . \\n[67] Basu AS , Lajolo M , Prevostini M . A methodology for bridging the gap between UML and codesign. In: UML for SOC design. US: Springer; 2005. \\np. 119‚Äì46 . \\n[68] Furusawa T . Attempting to increase longevity of applications based on new SaaS/cloud technology. Fujitsu Sci Tech J 2010;46:223‚Äì8 . \\n[69] Muller P-A , Studer P , Fondement F , B√©zivin J . Platform independent web application modeling and development with Netsilon. Softw Syst Model \\n2005;4(4):424‚Äì42 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 19, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='[69] Muller P-A , Studer P , Fondement F , B√©zivin J . Platform independent web application modeling and development with Netsilon. Softw Syst Model \\n2005;4(4):424‚Äì42 . \\n[70] Antkiewicz M , Czarnecki K . Framework-speciÔ¨Åc modeling languages with round-trip engineering. In: Model driven engineering languages and systems. \\nIn: LNCS, 4199. Berlin Heidelberg: Springer; 2006. p. 692‚Äì706 . \\n[71] Hinkel G , Denninger O , Krach S , Groenda H . Experiences with model-driven engineering in neurorobotics. In: Proceedings of the 12th european con- \\nference on modelling foundations and applications, ECMFA 2016. Cham: Springer International Publishing; 2016. p. 217‚Äì28 . \\n[72] Kulkarni V , Barat S , Ramteerthkar U . Early experience with agile methodology in a model-driven approach. In: Model driven engineering languages \\nand systems. In: LNCS, 6981. Springer; 2011. p. 578‚Äì90 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 19, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='and systems. In: LNCS, 6981. Springer; 2011. p. 578‚Äì90 . \\n[73] Touraille L , Traor√© MK , Hill DR . A model-driven software environment for modeling, simulation and analysis of complex systems. In: Proceedings of \\nsymposium on theory of modeling & simulation. SCSC; 2011. p. 229‚Äì37 . \\n[74] Fertalj K , Kalpic D , Mornar V . Source code generator based on a proprietary speciÔ¨Åcation language. In: Proceedings of Hawaii international conference \\non system sciences, 9. IEEE; 2002. p. 3696‚Äì704 . \\n[75] Yen I-L, Goluguri J, Bastani F, Khan L, Linn J. A component-based approach for embedded software development. In: International symposium on \\nobject-oriented real-time distributed computing. ISORC 2002. IEEE Computer Society; 2002. p. 402‚Äì10. doi: 10.1109/ISORC.2002.1003805 . \\n[76] Luhunu L , Syriani E . Comparison of the expressiveness and performance of template-based code generation tools. In: Software Language Engineering. \\nACM; 2017 .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.10 (Windows)', 'creator': 'Elsevier', 'creationdate': '2017-12-13T18:12:10+05:30', 'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Systematic mapping study of template-based code generation', 'author': 'Eugene Syriani', 'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003', 'keywords': 'Code generation; Systematic mapping study; Model-driven engineering', 'moddate': '2017-12-13T18:12:45+05:30', 'trapped': '', 'modDate': \"D:20171213181245+05'30'\", 'creationDate': \"D:20171213181210+05'30'\", 'page': 19, 'source_file': 'Systematic mapping study of template based code generation.pdf', 'file_type': 'pdf'}, page_content='[76] Luhunu L , Syriani E . Comparison of the expressiveness and performance of template-based code generation tools. In: Software Language Engineering. \\nACM; 2017 . \\n[77] Ma M , Meissner M , Hedrich L . A case study: automatic topology synthesis for analog circuit from an ASDEX speciÔ¨Åcation. In: Synthesis, modeling, \\nanalysis and simulation methods and applications to circuit design. IEEE; 2012. p. 9‚Äì12 .')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94670a27",
   "metadata": {},
   "source": [
    "## convert text to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94669aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\nA Survey on Large Language Models for Code Generation\\nJUYONG JIANG‚àó, The Hong Kong University of Science and Technology (Guangzhou), China\\nFAN WANG‚àó, The Hong Kong University of Science and Technology (Guangzhou), China\\nJIASI SHEN‚Ä†, The Hong Kong University of Science and Technology, China\\nSUNGJU KIM‚Ä†, NAVER Cloud, South Korea\\nSUNGHUN KIM‚Ä†, The Hong Kong University of Science and Technology (Guangzhou), China\\nLarge Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks,\\nknown as Code LLMs, particularly in code generation that generates source code with LLM from natural\\nlanguage descriptions. This burgeoning field has captured significant interest from both academic researchers\\nand industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite\\nthe active exploration of LLMs for a variety of code tasks, either from the perspective of natural language',\n",
       " 'the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language\\nprocessing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive\\nand up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge\\nthis gap by providing a systematic literature review that serves as a valuable reference for researchers\\ninvestigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize\\nand discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest\\nadvances, performance evaluation, ethical implications, environmental impact, and real-world applications.\\nIn addition, we present a historical overview of the evolution of LLMs for code generation and offer an\\nempirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels',\n",
       " 'empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels\\nof difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities\\nfor code generation. We identify critical challenges and promising opportunities regarding the gap between\\nacademia and practical development. Furthermore, we have established a dedicated resource GitHub page\\n(https://github.com/juyongjiang/CodeLLMSurvey) to continuously document and disseminate the most recent\\nadvances in the field.\\nCCS Concepts: ‚Ä¢ General and reference ‚ÜíSurveys and overviews; ‚Ä¢ Software and its engineering ‚Üí\\nSoftware development techniques; ‚Ä¢ Computing methodologies ‚ÜíArtificial intelligence.\\nAdditional Key Words and Phrases: Large Language Models, Code Large Language Models, Code Generation\\nACM Reference Format:\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2018. A Survey on Large Language Models',\n",
       " 'ACM Reference Format:\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2018. A Survey on Large Language Models\\nfor Code Generation. J. ACM 37, 4, Article 1 (August 2018), 70 pages. https://doi.org/XXXXXXX.XXXXXXX\\n‚àóEqually major contributors.\\n‚Ä†Corresponding authors.\\nAuthors‚Äô addresses: Juyong Jiang, jjiang472@connect.hkust-gz.edu.cn, The Hong Kong University of Science and Technology\\n(Guangzhou), Guangzhou, China; Fan Wang, fwang380@connect.hkust-gz.edu.cn, The Hong Kong University of Science\\nand Technology (Guangzhou), Guangzhou, China; Jiasi Shen, sjs@cse.ust.hk, The Hong Kong University of Science and\\nTechnology, Hong Kong, China; Sungju Kim, sungju.kim@navercorp.com, NAVER Cloud, Seoul, South Korea; Sunghun\\nKim, hunkim@cse.ust.hk, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee',\n",
       " 'Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\\n¬© 2018 Association for Computing Machinery.\\n0004-5411/2018/8-ART1 $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.\\narXiv:2406.00515v2  [cs.CL]  10 Nov 2024',\n",
       " '1:2\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n1\\nINTRODUCTION\\nThe advent of Large Language Models (LLMs) such as ChatGPT1 [196] has profoundly transformed\\nthe landscape of automated code-related tasks [48], including code completion [87, 171, 270, 282],\\ncode translation [52, 135, 245], and code repair [75, 126, 195, 204, 291, 310]. A particularly intriguing\\napplication of LLMs is code generation, a task that involves producing source code from natural\\nlanguage descriptions. Despite varying definitions across studies [51, 221, 238, 269], for the main\\nscope of this survey, we focus on the code generation task and adopt a consistent definition of code\\ngeneration as the natural-language-to-code (NL2Code) task [16, 17, 307]. To enhance clarity, the\\ndifferentiation between code generation and other code-related tasks, along with a more nuanced\\ndefinition, is summarized in Table 1. This area has garnered substantial interest from both academia',\n",
       " 'differentiation between code generation and other code-related tasks, along with a more nuanced\\ndefinition, is summarized in Table 1. This area has garnered substantial interest from both academia\\nand industry, as evidenced by the development of tools like GitHub Copilot2 [48], CodeGeeX3 [321],\\nand Amazon CodeWhisperer4, which leverage groundbreaking code LLMs to facilitate software\\ndevelopment.\\nInitial investigations into code generation primarily utilized heuristic rules or expert systems,\\nsuch as probabilistic grammar-based frameworks [10, 62, 119, 125, 288] and specialized language\\nmodels [64, 83, 117]. These early techniques were typically rigid and difficult to scale. However,\\nthe introduction of Transformer-based LLMs has shifted the paradigm, establishing them as the\\npreferred method due to their superior proficiency and versatility. One remarkable aspect of LLMs is\\ntheir capability to follow instructions [56, 187, 200, 275, 289], enabling even novice programmers to',\n",
       " 'their capability to follow instructions [56, 187, 200, 275, 289], enabling even novice programmers to\\nwrite code by simply articulating their requirements. This emergent ability has democratized coding,\\nmaking it accessible to a broader audience [307]. The performance of LLMs on code generation\\ntasks has seen remarkable improvements, as illustrated by the HumanEval leaderboard5, which\\nshowcases the evolution from PaLM 8B [54] of 3.6% to LDB [325] of 95.1% on Pass@1 metrics.\\nAs can be seen, the HumanEval benchmark [48] has been established as a de facto standard for\\nevaluating the coding proficiency of LLMs [48].\\nTo offer a comprehensive chronological evolution, we present an overview of the development\\nof LLMs for code generation, as illustrated in Figure 1. The landscape of LLMs for code generation\\nis characterized by a spectrum of models, with certain models like ChatGPT [200], GPT4 [5],\\nLLaMA [252, 253], and Claude 3 [14] serving general-purpose applications, while others such',\n",
       " 'is characterized by a spectrum of models, with certain models like ChatGPT [200], GPT4 [5],\\nLLaMA [252, 253], and Claude 3 [14] serving general-purpose applications, while others such\\nas StarCoder [147, 170], Code LLaMA [227], DeepSeek-Coder [88], and Code Gemma [59] are\\ntailored specifically for code-centric tasks. The convergence of code generation with the latest LLM\\nadvancements is pivotal, especially when programming languages can be considered as distinct\\ndialects of multilingual natural language [16, 321]. These models are not only tested against software\\nengineering (SE) requirements but also propel the advancement of LLMs into practical production\\n[317].\\nWhile recent surveys have shed light on code LLMs from the lenses of Natural Language Pro-\\ncessing (NLP), Software Engineering (SE), or a combination of both disciplines [74, 101, 174, 307,\\n317, 324], they have often encompassed a broad range of code-related tasks. There remains a dearth',\n",
       " 'cessing (NLP), Software Engineering (SE), or a combination of both disciplines [74, 101, 174, 307,\\n317, 324], they have often encompassed a broad range of code-related tasks. There remains a dearth\\nof literature specifically reviewing advanced topics in code generation, such as meticulous data\\ncuration, instruction tuning, alignment with feedback, prompting techniques, the development of\\nautonomous coding agents, retrieval augmented code generation, LLM-as-a-Judge for code genera-\\ntion, among others. A notably pertinent study [16, 307] also concentrates on LLMs for text-to-code\\ngeneration (NL2Code), yet it primarily examines models released from 2020 to 2022. Consequently,\\n1https://chat.openai.com\\n2https://github.com/features/copilot\\n3https://codegeex.cn/en-US\\n4https://aws.amazon.com/codewhisperer\\n5https://paperswithcode.com/sota/code-generation-on-humaneval\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:3\\nTable 1. The applications of code LLMs in various code-related understanding and generation tasks. The I-O\\ncolumn indicates the type of input and output for each task, where C, NL, and K represent code, natural\\nlanguage, and label, respectively. Note that the detailed definitions of each task aligns with the descriptions\\nin [7, 16, 17, 194, 307]. The main scope of this survey focuses on code generation while it may involve code\\ncompletion in Section 5.7 and 5.8, aiming to illustrate the corresponding advancements.\\nType\\nI-O\\nTask\\nDefinition\\nUnderstanding\\nC-K\\nCode Classification\\nClassify code snippets based on functionality, purpose, or attributes\\nto aid in organization and analysis.\\nBug Detection\\nDetect and diagnose bugs or vulnerabilities in code to ensure\\nfunctionality and security.\\nClone Detection\\nIdentifying duplicate or similar code snippets in software to enhance\\nmaintainability, reduce redundancy, and check plagiarism.',\n",
       " 'functionality and security.\\nClone Detection\\nIdentifying duplicate or similar code snippets in software to enhance\\nmaintainability, reduce redundancy, and check plagiarism.\\nException Type Prediction\\nPredict different exception types in code to manage and handle\\nexceptions effectively.\\nC-C\\nCode-to-Code Retrieval\\nRetrieve relevant code snippets based on a given\\ncode query for reuse or analysis.\\nNL-C\\nCode Search\\nFind relevant code snippets based on natural language\\nqueries to facilitate coding and development tasks.\\nGeneration\\nC-C\\nCode Completion\\nPredict and suggest the next portion of code, given contextual\\ninformation from the prefix (and suffix), while typing to enhance\\ndevelopment speed and accuracy.\\nCode Translation\\nTranslate the code from one programming language to another\\nwhile preserving functionality and logic.\\nCode Repair\\nIdentify and fix bugs in code by generating the correct version to\\nimprove functionality and reliability.\\nMutant Generation',\n",
       " 'while preserving functionality and logic.\\nCode Repair\\nIdentify and fix bugs in code by generating the correct version to\\nimprove functionality and reliability.\\nMutant Generation\\nGenerate modified versions of code to test and evaluate the\\neffectiveness of testing strategies.\\nTest Generation\\nGenerate test cases to validate code functionality, performance,\\nand robustness.\\nC-NL\\nCode Summarization\\nGenerate concise textual descriptions or explanations of code to\\nenhance understanding and documentation.\\nNL-C\\nCode Generation\\nGenerate source code from natural language descriptions to\\nstreamline development and reduce manual coding efforts.\\nthis noticeable temporal gap has resulted in an absence of up-to-date literature reviews that con-\\ntemplate the latest advancements, including models like CodeQwen [249], WizardCoder [173],\\nCodeFusion [241], and PPOCoder [238], as well as the comprehensive exploration of the advanced\\ntopics previously mentioned.',\n",
       " 'CodeFusion [241], and PPOCoder [238], as well as the comprehensive exploration of the advanced\\ntopics previously mentioned.\\nRecognizing the need for a dedicated and up-to-date literature review, this survey endeavors to fill\\nthat void. We provide a systematic review that will serve as a foundational reference for researchers\\nquickly exploring the latest progress in LLMs for code generation. A taxonomy is introduced to\\ncategorize and examine recent advancements, encompassing data curation [173, 268, 278], advanced\\ntopics [45, 51, 104, 139, 163, 171, 187, 190, 205, 239, 309], evaluation methods [48, 95, 123, 332], and\\npractical applications [48, 321]. This category aligns with the comprehensive lifecycle of an LLM for\\ncode generation. Furthermore, we pinpoint critical challenges and identify promising opportunities\\nto bridge the research-practicality divide. Therefore, this survey allows NLP and SE researchers',\n",
       " 'code generation. Furthermore, we pinpoint critical challenges and identify promising opportunities\\nto bridge the research-practicality divide. Therefore, this survey allows NLP and SE researchers\\nto seamlessly equip with a thorough understanding of LLM for code generation, highlighting\\ncutting-edge directions and current hurdles and prospects.\\nThe remainder of the survey is organized following the structure outlined in our taxonomy in\\nFigure 6. In Section 2, we introduce the preliminaries of LLM with Transformer architecture and\\nformulate the task of LLM for code generation. Section 3, we detail the systematic methodologies\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:4\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nemployed in conducting literature reviews. Then, in Section 4, we propose a taxonomy, categorizing\\nthe complete process of LLMs in code generation. Section 5 delves into the specifics of LLMs\\nfor code generation within this taxonomy framework. In Section 6, we underscore the critical\\nchallenges and promising opportunities for bridging the research-practicality gap and conclude\\nthis work in Section 7.\\n2\\nBACKGROUND\\n2.1\\nLarge Language Models\\nThe effectiveness of large language models (LLMs) is fundamentally attributed to their substantial\\nquantity of model parameters, large-scale and diversified datasets, and the immense computational\\npower utilized during training [97, 127]. Generally, scaling up language models consistently results\\nin enhanced performance and sample efficiency across a broad array of downstream tasks [275, 319].',\n",
       " 'power utilized during training [97, 127]. Generally, scaling up language models consistently results\\nin enhanced performance and sample efficiency across a broad array of downstream tasks [275, 319].\\nHowever, with the expansion of the model size to a certain extent (e.g., GPT-3 [33] with 175B-\\nparameters and PaLM [54] with 540B), LLMs have exhibited an unpredictable phenomenon known\\nas emergent abilities6, including instruction following [200], in-context learning [70], and step-\\nby-step reasoning [105, 276], which are absent in smaller models but apparent in larger ones\\n[275].\\nAdhering to the same architectures of the Transformer [257] in LLMs, code LLMs are specifically\\npre-trained (or continually pre-trained on general LLMs) using large-scale unlabeled code corpora\\nwith a smaller portion of text (and math) data, whereas general-purpose LLMs are pre-trained\\nprimarily on large-scale text data, incorporating a smaller amount of code and math data to',\n",
       " 'with a smaller portion of text (and math) data, whereas general-purpose LLMs are pre-trained\\nprimarily on large-scale text data, incorporating a smaller amount of code and math data to\\nenhance logical reasoning capabilities. Additionally, some code LLMs, such as Qwen2.5-Coder\\n[109], incorporate synthetic data in their training processes, a practice that is attracting increasing\\nattention from both industry and academia. Analogous to LLMs, Code LLMs can also be classified\\ninto three architectural categories: encoder-only models, decoder-only models, and encoder-decoder\\nmodels. For encoder-only models, such as CodeBERT [76], they are typically suitable for code\\ncomprehension tasks including type prediction, code retrieval, and clone detection. For decoder-\\nonly models, such as StarCoder [33], they predominantly excel in generation tasks, such as code\\ngeneration, code translation, and code summarization. Encoder-decoder models, such as CodeT5',\n",
       " 'only models, such as StarCoder [33], they predominantly excel in generation tasks, such as code\\ngeneration, code translation, and code summarization. Encoder-decoder models, such as CodeT5\\n[271], can accommodate both code understanding and generation tasks but do not necessarily\\noutperform encoder-only or decoder-only models. The overall architectures of the different Code\\nLLMs for code generation are depicted in Figure 2.\\nIn the following subsection, we will delineate the key modules of the Transformer layers in Code\\nLLMs.\\n2.1.1\\nMulti-Head Self-Attention Modules. Each Transformer layer incorporates a multi-head self-\\nattention (MHSA) mechanism to discern the inherent semantic relationships within a sequence\\nof tokens across ‚Ñédistinct latent representation spaces. Formally, the MHSA employed by the\\nTransformer can be formulated as follows:\\nh(ùëô) = MultiHeadSelfAttn(Q, K, V) = Concat {Headùëñ}‚Ñé\\nùëñ=1 WO,\\n(1)\\nHeadùëñ= Attention(H(ùëô‚àí1)WQ\\nùëñ\\n|      {z      }\\nQ\\n, H(ùëô‚àí1)WK\\nùëñ\\n|      {z      }\\nK',\n",
       " 'Transformer can be formulated as follows:\\nh(ùëô) = MultiHeadSelfAttn(Q, K, V) = Concat {Headùëñ}‚Ñé\\nùëñ=1 WO,\\n(1)\\nHeadùëñ= Attention(H(ùëô‚àí1)WQ\\nùëñ\\n|      {z      }\\nQ\\n, H(ùëô‚àí1)WK\\nùëñ\\n|      {z      }\\nK\\n, H(ùëô‚àí1)WV\\nùëñ\\n|      {z      }\\nV\\n),\\n(2)\\n6It should be noted that an LLM is not necessarily superior to a smaller language model, and emergent abilities may not\\nmanifest in all LLMs [319].\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:5\\nphi-2\\n2021\\nMay\\nGPT-C\\nCodeGPT\\nFeb.\\nMar.\\nMay\\nGPT-Neo PLBART\\nGPT-J\\nJul.\\nCodex\\nSep.\\nCodeT5\\nNov.\\nCodeParrot\\nPolyCoder\\nAlphaCode\\nJuPyT5\\nCodeGen\\nGPT-NeoX\\nPaLM-Coder InCoder\\nCodeRL\\nPanGu-Coder\\nPyCodeGPT\\nCodeGeeX\\nBLOOM\\nERNIE-Code\\nSantaCoder\\nJan.\\nPPOCoder\\nLLaMA\\nFeb.\\nMay\\nCodeGen2\\nreplit-code\\nStarCoder\\nCodeT5+\\nCodeTF\\nJun.\\nWizardCoder\\nphi-1\\nJul.\\nChainCoder\\nCodeGeeX2\\nPanGu-Coder2\\nAug.\\nOctoPack\\nSep.\\nMFTCoder\\nOct.\\nCodeShell\\nphi-1.5\\nCodeFusion\\nNov.\\nDeepSeek-Coder\\nDec.\\nMagicoder\\nAlphaCode 2\\nWaveCoder\\nJan.\\nFeb.\\nAST-T5\\nToolGen\\nStableCode\\nAlphaCodium\\nStepCoder OpenCodeInterpreter StarCoder2\\nMar.\\nDevin\\nOpenDevin\\nCodeS\\nApr.\\nProCoder\\nCodeQwen1.5\\nCodeGemma\\nCode Llama\\nApr.\\nSelf-Debugging\\nJan.\\nFeb.\\nJun.\\nJul.\\nSep.\\nNov.\\nDec.\\nApr.\\nMar.\\n2020\\n2022\\n2023\\n2024\\nChatGPT\\nMar.\\nGPT4\\nLlama 2\\nLlama 3\\nClaude 3\\nCodeT\\nSelfEvolve\\nLEVER\\nRLTF\\nOct.\\nPyMT5\\nStarCoder2-Instruct\\nOpen Source Closed Source\\n9\\n3\\n6\\n6\\n5\\n3\\n1\\n4\\nMar.\\nCodestral',\n",
       " 'Nov.\\nDec.\\nApr.\\nMar.\\n2020\\n2022\\n2023\\n2024\\nChatGPT\\nMar.\\nGPT4\\nLlama 2\\nLlama 3\\nClaude 3\\nCodeT\\nSelfEvolve\\nLEVER\\nRLTF\\nOct.\\nPyMT5\\nStarCoder2-Instruct\\nOpen Source Closed Source\\n9\\n3\\n6\\n6\\n5\\n3\\n1\\n4\\nMar.\\nCodestral\\nFig. 1. A chronological overview of large language models (LLMs) for code generation in recent years. The\\ntimeline was established mainly according to the release date. The models with publicly available model\\ncheckpoints are highlighted in green color.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:6\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nAttention(Q, K, V) = softmax\\n \\nQKùëá\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñé\\n!\\nV,\\n(3)\\nwhere H(ùëô‚àí1) ‚ààRùëõ√óùëëùëöùëúùëëùëíùëôdenotes the input to the ùëô-th Transformer layer, while h(ùëô) ‚ààRùëõ√óùëëùëöùëúùëëùëíùëô\\nrepresents the output of MHSA sub-layer. The quantity of distinct attention heads is represented\\nby ‚Ñé, and ùëëùëöùëúùëëùëíùëôrefers to the model dimension. The set of projections\\nn\\nWQ\\nùëñ, WK\\nùëñ, WV\\nùëñ, WO\\nùëñ\\no\\n‚àà\\nRùëëùëöùëúùëëùëíùëô√óùëëùëöùëúùëëùëíùëô/‚Ñéencompasses the affine transformation parameters for each attention head Headùëñ,\\ntransforming the Query Q, Key K, Value V, and the output of the attention sub-layer. The softmax\\nfunction is applied in a row-wise manner. The dot-products of queries and keys are divided by\\na scaling factor\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñéto counteract the potential risk of excessive large inner products and\\ncorrespondingly diminished gradients in the softmax function, thus encouraging a more balanced\\nattention landscape.',\n",
       " '‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñéto counteract the potential risk of excessive large inner products and\\ncorrespondingly diminished gradients in the softmax function, thus encouraging a more balanced\\nattention landscape.\\nIn addition to multi-head self-attention, there are two other types of attention based on the\\nsource of queries and key-value pairs:\\n‚Ä¢ Masked Multi-Head Self-Attention. Within the decoder layers of the Transformer, the\\nself-attention mechanism is constrained by introducing an attention mask, ensuring that\\nqueries at each position can only attend to all key-value pairs up to and inclusive of that\\nposition. To facilitate parallel training, this is typically executed by assigning a value of 0\\nto the lower triangular part and setting the remaining elements to ‚àí‚àû. Consequently, each\\nitem attends only to its predecessors and itself. Formally, this modification in Equation 3 can\\nbe depicted as follows:\\nAttention(Q, K, V) = softmax\\n \\nQKùëá\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñé\\n+ Mùëöùëéùë†ùëò\\n!\\nV,\\n(4)\\nMùëöùëéùë†ùëò=\\n\\x10\\nùëöùëñùëó\\n\\x11\\nùëõ√óùëõ=\\n\\x10\\nI(ùëñ‚â•ùëó)\\n\\x11',\n",
       " 'be depicted as follows:\\nAttention(Q, K, V) = softmax\\n \\nQKùëá\\n‚àöÔ∏Å\\nùëëùëöùëúùëëùëíùëô/‚Ñé\\n+ Mùëöùëéùë†ùëò\\n!\\nV,\\n(4)\\nMùëöùëéùë†ùëò=\\n\\x10\\nùëöùëñùëó\\n\\x11\\nùëõ√óùëõ=\\n\\x10\\nI(ùëñ‚â•ùëó)\\n\\x11\\nùëõ√óùëõ=\\n(\\n0\\nfor ùëñ‚â•ùëó\\n‚àí‚àû\\notherwise ,\\n(5)\\nThis form of self-attention is commonly denoted as autoregressive or causal attention [157].\\n‚Ä¢ Cross-Layer Multi-Head Self-Attention. The queries are derived from the outputs of the\\npreceding (decoder) layer, while the keys and values are projected from the outputs of the\\nencoder.\\n2.1.2\\nPosition-wise Feed-Forward Networks. Within each Transformer layer, a Position-wise Feed-\\nForward Network (PFFN) is leveraged following the MHSA sub-layer to refine the sequence\\nembeddings at each position ùëñin a separate and identical manner, thereby encoding more intricate\\nfeature representations. The PFFN is composed of a pair of linear transformations, interspersed\\nwith a ReLU activation function. Formally,\\nPFFN(‚Ñé(ùëô)) =\\n\\x10\\nConcat\\nn\\nFFN(‚Ñé(ùëô)\\nùëñ)ùëáoùëõ\\nùëñ=1\\n\\x11ùëá\\n,\\n(6)\\nFFN(‚Ñé(ùëô)\\nùëñ) = ReLU(‚Ñé(ùëô)\\nùëñW(1) + ùëè(1))W(2) + ùëè(2),\\n(7)',\n",
       " 'with a ReLU activation function. Formally,\\nPFFN(‚Ñé(ùëô)) =\\n\\x10\\nConcat\\nn\\nFFN(‚Ñé(ùëô)\\nùëñ)ùëáoùëõ\\nùëñ=1\\n\\x11ùëá\\n,\\n(6)\\nFFN(‚Ñé(ùëô)\\nùëñ) = ReLU(‚Ñé(ùëô)\\nùëñW(1) + ùëè(1))W(2) + ùëè(2),\\n(7)\\nwhere ‚Ñé(ùëô) ‚ààRùëõ√óùëëùëöùëúùëëùëíùëôis the outputs of MHSA sub-layer in ùëô-th Transformer layer, and ‚Ñé(ùëô)\\nùëñ\\n‚àà\\nRùëëùëöùëúùëëùëíùëôdenotes the latent representation at each sequence position. The projection matrices\\n\\x08\\nW(1), (W(2))ùëá\\t\\n‚ààRùëëùëöùëúùëëùëíùëô√ó4ùëëùëöùëúùëëùëíùëôand bias vectors {b(1), b(2)} ‚ààRùëëùëöùëúùëëùëíùëôare parameters learned\\nduring training. These parameters remain consistent across all positions while are individually\\ninitialized from layer to layer. In this context, ùëárepresents the transpose operation on a matrix.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:7\\nMasked\\nMulti-Head\\nSelf-Attention\\nMulti-Head\\nSelf-Attention\\n+\\n+\\n+\\n+\\n+\\nLayer Norm\\nPosition-wise\\nFeed Forward\\nLinear & Softmax\\nToken & Position\\nEmbedding\\nInputs\\nOutputs (Shifted Right)\\nMulti-Head\\nSelf-Attention\\nLayer Norm\\nLayer Norm\\nLayer Norm\\nLayer Norm\\nPosition-wise\\nFeed Forward\\nToken & Position\\nEmbedding\\nOutput Probabilities\\nùëÅ√ó\\nùëÅ√ó\\n(a) Encoder-Decoder Models\\nMasked\\nMulti-Head\\nSelf-Attention\\nPosition-wise\\nFeed Forward\\n+\\n+\\nLinear & Softmax\\nToken & Position\\nEmbedding\\nInputs\\nLayer Norm\\nLayer Norm\\nOutput Probabilities\\nùëÅ√ó\\n(b) Decoder-only Models\\nFig. 2. The overview of large language models (LLMs) with encoder-decoder and decoder-only Transformer\\narchitecture for code generation, adapted from [257].\\n2.1.3\\nResidual Connection and Normalization. To alleviate the issue of vanishing or exploding\\ngradients resulting from network deepening, the Transformer model incorporates a residual con-',\n",
       " '2.1.3\\nResidual Connection and Normalization. To alleviate the issue of vanishing or exploding\\ngradients resulting from network deepening, the Transformer model incorporates a residual con-\\nnection [94] around each of the aforementioned modules, followed by Layer Normalization [18].\\nFor the placement of Layer Normalization operation, there are two widely used approaches: 1)\\nPost-Norm: Layer normalization is implemented subsequent to the element-wise residual addition,\\nin accordance with the vanilla Transformer [257]. 2) Pre-Norm: Layer normalization is applied to\\nthe input of each sub-layer, as seen in models like GPT-2 [214]. Formally, it can be formulated as:\\nPost-Norm : H(l) = LayerNorm(PFFN(h(l)) + h(l)),\\nh(l) = LayerNorm(MHSA(H(l‚àí1)) + H(l‚àí1))\\n(8)\\nPre-Norm : H(l) = PFFN(LayerNorm(h(l))) + h(l),\\nh(l) = MHSA(LayerNorm(H(l‚àí1))) + H(l‚àí1)\\n(9)\\n2.1.4\\nPositional Encoding. Given that self-attention alone cannot discern the positional information',\n",
       " '(8)\\nPre-Norm : H(l) = PFFN(LayerNorm(h(l))) + h(l),\\nh(l) = MHSA(LayerNorm(H(l‚àí1))) + H(l‚àí1)\\n(9)\\n2.1.4\\nPositional Encoding. Given that self-attention alone cannot discern the positional information\\nof each input token, the vanilla Transformer introduces an absolute positional encoding method to\\nsupplement this positional information, known as sinusoidal position embeddings [257]. Specifically,\\nfor a token at position ùëùùëúùë†, the position embedding is defined as:\\npùëùùëúùë†,2ùëñ= sin(\\nùëùùëúùë†\\n100002ùëñ/ùëëùëöùëúùëëùëíùëô),\\n(10)\\npùëùùëúùë†,2ùëñ+1 = cos(\\nùëùùëúùë†\\n100002ùëñ/ùëëùëöùëúùëëùëíùëô),\\n(11)\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:8\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nwhere 2ùëñ, 2ùëñ+1 represent the dimensions of the position embedding, while ùëëùëöùëúùëëùëíùëôdenotes the model\\ndimension. Subsequently, each position embedding is added to the corresponding token embedding,\\nand the sum is fed into the Transformer. Since the inception of this method, a variety of innovative\\npositional encoding approaches have emerged, such as learnable embeddings [66], relative position\\nembeddings [232], RoPE [243], and ALiBi [211]. For more detailed descriptions of each method,\\nplease consult [157, 318].\\n2.1.5\\nArchitecture. There are two types of Transformer architecture for code generation task,\\nincluding encoder-decoder and decoder-only. For the encoder-decoder architecture, it consists of\\nboth an encoder and a decoder, in which the encoder processes the input data and generates a\\nset of representations, which are then used by the decoder to produce the output. However, for',\n",
       " 'both an encoder and a decoder, in which the encoder processes the input data and generates a\\nset of representations, which are then used by the decoder to produce the output. However, for\\ndecoder-only architecture, it consists only of the decoder part of the transformer, where it uses\\na single stack of layers to both process input data and generate output. Therefore, the encoder-\\ndecoder architecture is suited for tasks requiring mapping between different input and output\\ndomains, while the decoder-only architecture is designed for tasks focused on sequence generation\\nand continuation. The overview of LLMs with these two architectures are illustrated in Figure 2.\\n2.2\\nCode Generation\\nLarge language models (LLMs) for code generation refer to the use of LLM to generate source\\ncode from natural language descriptions, a process also known as a natural-language-to-code\\ntask. Typically, these natural language descriptions encompass programming problem statements',\n",
       " 'code from natural language descriptions, a process also known as a natural-language-to-code\\ntask. Typically, these natural language descriptions encompass programming problem statements\\n(or docstrings) and may optionally include some programming context (e.g., function signatures,\\nassertions, etc.). Formally, these natural language (NL) descriptions can be represented as x. Given x,\\nthe use of an LLM with model parameters ùúÉto generate a code solution y can be denoted as ùëÉùúÉ(y | x).\\nThe advent of in-context learning abilities in LLM [275] has led to the appending of exemplars to\\nthe natural language description x as demonstrations to enhance code generation performance or\\nconstrain the generation format [145, 206]. A fixed set of ùëÄexemplars is denoted as {(xi, yi)}ùëÄ\\nùëñ=1.\\nConsequently, following [190], a more general formulation of LLMs for code generation with\\nfew-shot (or zero-shot) exemplars can be revised as:\\nùëÉùúÉ(y | x) ‚áíùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(12)',\n",
       " 'few-shot (or zero-shot) exemplars can be revised as:\\nùëÉùúÉ(y | x) ‚áíùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(12)\\nwhere prompt(x, {(xi, yi)}ùëò\\nùëñ=1)) is a string representation of the overall input, and {(xi, yi)}ùëò\\nùëñ=1\\ndenotes a set of ùëòexemplars randomly selected from {(xi, yi)}ùëÄ\\nùëñ=1. In particular, when ùëò= 0, this\\ndenotes zero-shot code generation, equivalent to vanilla ones without in-context learning. In the\\ndecoding process, a variety of decoding strategies can be performed for code generation, including\\ndeterministic-based strategies (e.g., greedy search and beam search) and sampling-based strategies\\n(e.g., temperature sampling, top-k sampling, and top-p (nucleus) sampling). For more detailed\\ndescriptions of each decoding strategy, please consult [99]. For example, the greedy search and\\nsampling-based decoding strategies can be formulated as follows:\\nGreedy Search : y‚àó= argmax\\ny\\nùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(13)',\n",
       " 'sampling-based decoding strategies can be formulated as follows:\\nGreedy Search : y‚àó= argmax\\ny\\nùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(13)\\nSampling : y ‚àºùëÉùúÉ(y | prompt(x, {(xi, yùëñ)}ùëò\\nùëñ=1)),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(14)\\nTo verify the functionality correctness of the generated code solution, y is subsequently executed\\nvia a compiler or interpreter, represented as Exe(¬∑), on a suit of unit tests T. The feedback from\\nthis execution can be denoted as Feedback(Exe(y, T)). If the generated code solution fails to pass\\nall test cases, the error feedback can be iteratively utilized to refine the code by leveraging the\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:9\\nResearch\\nQuestions\\nLarge Language\\nModel (LLM)\\nCode Generation\\nTop-tier LLMs\\nand SE Venues\\nTotal 235 Papers\\nSnowballing\\nSearch\\nQuality\\nAssessment\\nInclusion and\\nExclusion Criteria\\nAutomatic\\nFiltering\\nSearch Strings\\nAutomated Search\\nManual Search\\n235\\n247\\n351\\n294\\n73\\n36\\n261\\nFig. 3. Overview of the paper search and collection process.\\nprevious attempt (yùëùùëüùëí) and the associated feedback. Formally,\\ny ‚àºùëÉùúÉ(y | prompt(x, {(xi, yi)}ùëò\\nùëñ=1, yùëùùëüùëí, Feedback(Exe(y, T)))),ùëò‚àà{0, 1, . . . , ùëÄ}\\n(15)\\nFurther details and relevant studies on using feedback to improve code generation are comprehen-\\nsively discussed in Section 5.5 and 5.6.\\n3\\nMETHODOLOGY\\nIn this section, we detail the systematic methodologies employed in conducting literature reviews.\\nWe follow the systematic literature review methodology outlined by [131], which has been widely\\nadopted in numerous software engineering literature reviews [101, 146, 169, 219, 262]. The overall',\n",
       " 'We follow the systematic literature review methodology outlined by [131], which has been widely\\nadopted in numerous software engineering literature reviews [101, 146, 169, 219, 262]. The overall\\nprocess is illustrated in Figure 3, and the detailed steps in our methodology are documented below.\\n3.1\\nResearch Questions\\nTo deliver a comprehensive and up-to-date literature review on the latest advancements in large\\nlanguage models (LLMs) for code generation, this systematic literature review addresses the\\nfollowing research questions (RQs):\\nRQ1: How can we categorize and evaluate the latest advances in LLMs for code genera-\\ntion? The recent proliferation of LLMs has resulted in many of these models being adapted for code\\ngeneration task. While the adaptation of LLMs for code generation essentially follows the evolution\\nof LLMs, this evolution encompasses a broad spectrum of research directions and advancements.',\n",
       " 'generation task. While the adaptation of LLMs for code generation essentially follows the evolution\\nof LLMs, this evolution encompasses a broad spectrum of research directions and advancements.\\nFor software engineering (SE) researchers, it can be challenging and time-consuming to fully grasp\\nthe comprehensive research landscape of LLMs and their adaptation to code generation. RQ1 aims\\nto propose a taxonomy that serves as a comprehensive reference for researchers, enabling them to\\nquickly familiarize themselves with the state-of-the-art in this dynamic field and identify specific\\nresearch problems and directions of interest.\\nRQ2: What are the key insights into LLMs for code generation? RQ2 seeks to assist\\nresearchers in establishing a comprehensive, up-to-date, and advanced understanding of LLMs for\\ncode generation. This includes discussing various aspects of this rapidly evolving domain, such as',\n",
       " 'researchers in establishing a comprehensive, up-to-date, and advanced understanding of LLMs for\\ncode generation. This includes discussing various aspects of this rapidly evolving domain, such as\\ndata curation, latest advancements, performance evaluation, ethical and environmental implications,\\nand real-world applications. A historical overview of the evolution of LLMs for code generation is\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:10\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 2. Publication venues for conference proceedings and journals articles for manual search.\\nDomain\\nVenue\\nAcronym\\nLLMs\\nInternational Conference on Learning Representations\\nICLR\\nConference on Neural Information Processing Systems\\nNeurIPS\\nInternational Conference on Machine Learning\\nICML\\nAnnual Meeting of the Association for Computational Linguistics\\nACL\\nConference on Empirical Methods in Natural Language Processing\\nEMNLP\\nInternational Joint Conference on Artificial Intelligence\\nNAACL\\nAAAI Conference on Artificial Intelligence\\nAAAI\\nSE\\nInternational Conference on Software Engineering\\nICSE\\nJoint European Software Engineering Conference and Symposium on the Foundations of Software Engineering\\nESEC/FSE\\nInternational Conference on Automated Software Engineering\\nASE\\nTransactions on Software Engineering and Methodology\\nTOSEM\\nTransactions on Software Engineering\\nTSE\\nInternational Symposium on Software Testing and Analysis',\n",
       " 'ASE\\nTransactions on Software Engineering and Methodology\\nTOSEM\\nTransactions on Software Engineering\\nTSE\\nInternational Symposium on Software Testing and Analysis\\nISSTA\\nTable 3. Keywords related to LLMs and code generation task for automated search.\\nKeywords Related to LLMs\\nKeywords Related to Code Generation Task\\nCode Large Language Model‚àó, Code LLMs, Code Language Model,\\nCode LMs, Large Language Model‚àó, LLM, Language Model‚àó, LM,\\nPre-trained Language Model‚àó, PLM, Pre-trained model,\\nNatural Language Processing, NLP, GPT-3, ChatGPT, GPT-4, LLaMA,\\nCodeLlama, PaLM‚àó, CodeT5, Codex, CodeGen, InstructGPT\\nCode Generation, Program Synthesis, Code Intelligence,\\n‚àóCoder‚àó, natural-language-to-code, NL2Code, Programming\\nprovided, along with an empirical comparison using the widely recognized HumanEval and MBPP\\nbenchmarks, as well as the more practical and challenging BigCodeBench benchmark, to highlight\\nthe progressive enhancements in LLM capabilities for code generation. RQ2 offers an in-depth',\n",
       " 'benchmarks, as well as the more practical and challenging BigCodeBench benchmark, to highlight\\nthe progressive enhancements in LLM capabilities for code generation. RQ2 offers an in-depth\\nanalysis of critical insights related to LLMs for code generation.\\nRQ3: What are the critical challenges and promising research opportunities in LLMs for\\ncode generation? Despite the revolutionary impact of LLMs on the paradigm of code generation\\nand their remarkable performance, numerous challenges remain unaddressed. These challenges\\nprimarily stem from the gap between academic research and practical development. For instance,\\nwhile the HumanEval benchmark is established as a de facto standard for evaluating the coding\\nproficiency of LLMs in academia, it has been shown that this evaluation does not adequately reflect\\npractical development scenarios [68, 72, 123, 162]. RQ3 aims to identify critical challenges and',\n",
       " 'proficiency of LLMs in academia, it has been shown that this evaluation does not adequately reflect\\npractical development scenarios [68, 72, 123, 162]. RQ3 aims to identify critical challenges and\\nhighlight promising opportunities to bridge the gap between research and practical application.\\n3.2\\nSearch Process\\n3.2.1\\nSearch Strings. To address the aforementioned three research questions (RQs), we initiate a\\nmanual review of conference proceedings and journal articles from top-tier venues in the fields of\\nLLMs and SE, as detailed in Table 2. This process allowed us to identify relevant studies and derive\\nsearch strings, which are subsequently utilized for an automated search across various scientific\\ndatabases. The complete set of search keywords is presented in Table 3.\\n3.2.2\\nSearch Databases. Following the development of search strings, we executed an automated\\nsearch using four popular scientific databases: the ACM Digital Library, IEEE Xplore Digital Library,',\n",
       " '3.2.2\\nSearch Databases. Following the development of search strings, we executed an automated\\nsearch using four popular scientific databases: the ACM Digital Library, IEEE Xplore Digital Library,\\narXiv, and DBLP. Our search focus on identifying papers whose titles contain keywords pertinent\\nto LLMs and code generation. This approach enhances the likelihood of retrieving relevant papers\\nsince both sets of keywords must be present in the title. Although this title-based search strategy\\neffectively retrieves a large volume of papers, it is important to note that in some instances [238],\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:11\\nthe scope of code generation can be broader, encompassing areas such as code completion, code\\ntranslation, and program synthesis. As outlined in Section 1, this survey adopts a prevalent definition\\nof code generation as the natural-language-to-code (NL2Code) task [16, 17, 307].\\nConsequently, we conduct further automatic filtering based on the content of the papers. Papers\\nfocusing on ‚Äúcode completion‚Äù and ‚Äúcode translation‚Äù are excluded unless they pertain to the\\nspecific topics discussed in Section 5.7 and Section 5.8, where code completion is a primary focus.\\nAfter completing the automated search, the results from each database are merged and deduplicated\\nusing scripts. This process yields 294 papers from arXiv, 73 papers from the ACM Digital Library,\\n36 papers from IEEE Xplore, and 261 papers from DBLP.\\n3.3\\nInclusion and Exclusion Criteria',\n",
       " 'using scripts. This process yields 294 papers from arXiv, 73 papers from the ACM Digital Library,\\n36 papers from IEEE Xplore, and 261 papers from DBLP.\\n3.3\\nInclusion and Exclusion Criteria\\nThe search process conducted across various databases and venues is intentionally broad to gather\\na comprehensive pool of candidate papers. This approach maximizes the collection of potentially\\nrelevant studies. However, such inclusivity may lead to the inclusion of papers that do not align\\nwith the scope of this survey, as well as duplicate entries from multiple sources. To address this,\\nwe have established a clear set of inclusion and exclusion criteria, based on the guidelines from\\n[101, 260]. These criteria are applied to each paper to ensure alignment with our research scope\\nand questions, and to eliminate irrelevant studies.\\nInclusion Criteria. A paper will be included if it meets any of the following criteria:\\n‚Ä¢ It is available in full text.',\n",
       " 'and questions, and to eliminate irrelevant studies.\\nInclusion Criteria. A paper will be included if it meets any of the following criteria:\\n‚Ä¢ It is available in full text.\\n‚Ä¢ It presents a dataset or benchmark specifically designed for code generation with LLMs.\\n‚Ä¢ It explores specific LLM techniques, such as pre-training or instruction tuning, for code\\ngeneration.\\n‚Ä¢ It provides an empirical study or evaluation related to the use of LLMs for code generation.\\n‚Ä¢ It discusses the ethical considerations and environmental impact of deploying LLMs for code\\ngeneration.\\n‚Ä¢ It proposes tools or applications powered by LLMs for code generation.\\nExclusion Criteria. Conversely, papers will be excluded if they meet any of the following\\nconditions:\\n‚Ä¢ They are not written in English.\\n‚Ä¢ They are found in books, theses, monographs, keynotes, panels, or venues (excluding arXiv)\\nthat do not undergo a full peer-review process.',\n",
       " 'conditions:\\n‚Ä¢ They are not written in English.\\n‚Ä¢ They are found in books, theses, monographs, keynotes, panels, or venues (excluding arXiv)\\nthat do not undergo a full peer-review process.\\n‚Ä¢ They are duplicate papers or different versions of similar studies by the same authors.\\n‚Ä¢ They focus on text generation rather than source code generation, such as generating code\\ncomments, questions, test cases, or summarization.\\n‚Ä¢ They do not address the task of code generation, for instance, focusing on code translation\\ninstead.\\n‚Ä¢ They leverage software engineering methods to enhance code generation without emphasiz-\\ning LLMs.\\n‚Ä¢ They do not utilize LLMs, opting for other models like Long Short-Term Memory (LSTM)\\nnetworks.\\n‚Ä¢ They use encoder-only language models, such as BERT, which are not directly applicable to\\ncode generation task.\\n‚Ä¢ LLMs are mentioned only in future work or discussions without being central to the proposed\\napproach.',\n",
       " 'code generation task.\\n‚Ä¢ LLMs are mentioned only in future work or discussions without being central to the proposed\\napproach.\\nPapers identified through both manual and automated searches undergo a detailed manual review\\nto ensure they meet the inclusion criteria and do not fall under the exclusion criteria. Specifically,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:12\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nthe first two authors independently review each paper to determine its eligibility. In cases of\\ndisagreement, the third author makes the final inclusion decision.\\n3.4\\nQuality Assessment\\nTo ensure the inclusion of high-quality studies, we have developed a comprehensive set of ten\\nQuality Assessment Criteria (QAC) following [101]. These QAC are designed to evaluate the\\nrelevance, clarity, validity, and significance of the papers considered for our review.\\nIn accordance with [101], the first three QAC assess the study‚Äôs alignment with our objectives.\\nThese criteria are rated as ‚Äúirrelevant/unmet‚Äù, ‚Äúpartially relevant/met‚Äù, or ‚Äúrelevant/fully met‚Äù,\\ncorresponding to scores of -1, 0, and 1, respectively. If a study receive a score of -1 across these\\ninitial three criteria, it is deemed ineligible for further consideration and subsequently excluded\\nfrom our review process.',\n",
       " 'initial three criteria, it is deemed ineligible for further consideration and subsequently excluded\\nfrom our review process.\\nThe subsequent seven QAC focus on a more detailed content evaluation, employing a scoring\\nrange of -1 to 2, representing ‚Äúpoor‚Äù, ‚Äúfair‚Äù, ‚Äúgood‚Äù, and ‚Äúexcellent‚Äù. We compute a cumulative score\\nbased on the responses to QAC4 through QAC10 for each paper. For published works, the maximum\\nachievable score is 14 (2 points per question). We retain those with a score of 11.2 (80% of the total\\nscore) or higher. For unpublished papers available on arXiv, QAC4 defaults to a score of 0, making\\nthe maximum possible score for the remaining criteria 12. Accordingly, we retain papers scoring\\n9.6 (80% of the adjusted total score) or above .\\n‚Ä¢ QAC1: Is the research not classified as a secondary study, such as a systematic literature\\nreview or survey? (-1, 0, 1)\\n‚Ä¢ QAC2: Does the study incorporate the use of LLMs? (-1, 0, 1)',\n",
       " '‚Ä¢ QAC1: Is the research not classified as a secondary study, such as a systematic literature\\nreview or survey? (-1, 0, 1)\\n‚Ä¢ QAC2: Does the study incorporate the use of LLMs? (-1, 0, 1)\\n‚Ä¢ QAC3: Is the study relevant to the code generation task? (-1, 0, 1)\\n‚Ä¢ QAC4: Is the research published in a prestigious venue? (-1, 0, 1, 2)\\n‚Ä¢ QAC5: Does the study present a clear research motivation? (-1, 0, 1, 2)\\n‚Ä¢ QAC6: Are the key contributions and limitations of the study discussed? (-1, 0, 1, 2)\\n‚Ä¢ QAC7: Does the study contribute to the academic or industrial community? (-1, 0, 1, 2)\\n‚Ä¢ QAC8: Are the LLM techniques employed in the study clearly described? (-1, 0, 1, 2)\\n‚Ä¢ QAC9: Are the experimental setups, including experimental environments and dataset infor-\\nmation, thoroughly detailed? (-1, 0, 1, 2)\\n‚Ä¢ QAC10: Does the study clearly confirm its experimental findings? (-1, 0, 1, 2)\\n3.5\\nSnowballing Search',\n",
       " 'mation, thoroughly detailed? (-1, 0, 1, 2)\\n‚Ä¢ QAC10: Does the study clearly confirm its experimental findings? (-1, 0, 1, 2)\\n3.5\\nSnowballing Search\\nFollowing the quality assessment, we establish an initial set of papers for our study. To minimize\\nthe risk of excluding pertinent literature, we implement a snowballing search strategy. Snowballing\\nsearch involves utilizing a paper‚Äôs reference list or its citations to discover additional relevant\\nstudies, known as backward and forward snowballing, respectively. In this survey, we exclusively\\nemployed backward snowballing following [260]. Despite this effort, no additional studies are\\nidentified through this method. This could be attributed to the task-specific nature of the code\\ngeneration (natural-language-to-code), where reference studies are typically published earlier.\\nConsequently, our methodology, which encompassed an extensive manual and automated search,',\n",
       " 'generation (natural-language-to-code), where reference studies are typically published earlier.\\nConsequently, our methodology, which encompassed an extensive manual and automated search,\\nlikely covered the relevant literature comprehensively, explaining the lack of additional studies\\nthrough snowballing search.\\n3.6\\nData Collection and Analysis\\nThe data collection process for our study, illustrated in Figure 3, began with a manual search\\nthrough conference proceedings and journal articles from leading venues in LLMs and SE. This\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:13\\n2018\\n2019\\n2020\\n2021\\n2022\\n2023\\n2024\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n# Number of Papers\\n1\\n1\\n1\\n6\\n11\\n75\\n140\\nVenue\\nTrend\\nAAAI\\nACL\\nCAV\\nCCS\\nCHI\\nCVPR\\nEMNLP\\nFSE\\nICLR\\nICML\\nICSE\\nISSTA\\nKDD\\nNAACL\\nNeurIPS\\nOthers\\nTACL\\nTOSEM\\nTSE\\nUSENIX\\narXiv\\nPre-Training & \\nFoundation Model (21.5%)\\nFine-tuning (7.5%)\\nReinforcement Learning (4.8%)\\nPrompting (11.8%)\\nEvaluation &\\n Benchmark (24.1%)\\nData\\n Synthesis (1.8%)\\nRepository\\n Level (5.7%)\\nRetrieval Augmented (3.1%)\\nOthers (8.3%)\\nCode LLMs\\n Alignment (7.0%)\\nAutonomous Coding \\nAgents (4.4%)\\nTotal Papers:\\n235\\nResearch Topics\\nPre-Training & Foundation Model\\nFine-tuning\\nReinforcement Learning\\nPrompting\\nEvaluation & Benchmark\\nData Synthesis\\nRepository Level\\nRetrieval Augmented\\nOthers\\nCode LLMs Alignment\\nAutonomous Coding Agents\\nFig. 4. Data qualitative analysis. Top: Annual distribution of selected papers across various publication',\n",
       " 'Repository Level\\nRetrieval Augmented\\nOthers\\nCode LLMs Alignment\\nAutonomous Coding Agents\\nFig. 4. Data qualitative analysis. Top: Annual distribution of selected papers across various publication\\nvenues. Bottom: Distribution analysis of research topics covered in the included papers.\\ninitial step yielded 42 papers, from which we extracted relevant search strings. Following this,\\nwe performed an automated search across four academic databases using keyword-based queries,\\nresulting in the retrieval of 664 papers. After performing automatic filtering (351 papers), applying\\ninclusion and exclusion criteria (247 papers), conducting quality assessments (235 papers), and\\nutilizing snowballing search (235 papers), we finalize a collection of 235 papers focusing on LLMs\\nfor code generation.\\nTo provide insights from the selected papers, we begin by presenting an overview of their distribu-\\ntion across publication venues each year, as illustrated at the top of Figure 4. Our analysis indicates',\n",
       " 'tion across publication venues each year, as illustrated at the top of Figure 4. Our analysis indicates\\nthat 14% of the papers are published in LLM-specific venues and 7% in SE venues. Remarkably, 49%\\nof the papers remain unpublished in peer-reviewed venues and are available on arXiv. This trend is\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:14\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nunderstandable given the emerging nature of this field, with many works being recent and pending\\nformal submission. Despite the absence of peer review on arXiv, our quality assessment process\\nensures that only high-quality papers are included, thereby maintaining the integrity of this survey.\\nFurthermore, the annual trend in the number of collected papers indicates nearly exponential\\ngrowth in the field. From a single paper in the period 2018 to 2020, the numbers increased to 6 in\\n2021, 11 in 2022, 75 in 2023, and 140 in 2024. This trend reflects growing interest and attention\\nin this research area, with expectations for continued expansion in the future. Additionally, to\\ncapture the breadth of advancements in LLMs for code generation, we conducted a distribution\\nanalysis of the research topics covered in the included papers, as shown at the bottom of Figure 4.',\n",
       " 'capture the breadth of advancements in LLMs for code generation, we conducted a distribution\\nanalysis of the research topics covered in the included papers, as shown at the bottom of Figure 4.\\nWe observe that the development of LLMs for code generation closely aligns with broader trends\\nin general-purpose LLM research. Notably, the most prevalent research topics are Pre-training and\\nFoundation Models (21.5%), Prompting (11.8%), and Evaluation and Benchmarks (24.1%). These\\nareas hold significant promise for enhancing, refining, and evaluating LLM-driven code generation.\\n4\\nTAXONOMY\\nThe recent surge in the development of LLMs has led to a significant number of these models\\nbeing repurposed for code generation task through continual pre-training or fine-tuning. This\\ntrend is particularly observable in the realm of open-source models. For instance, Meta AI initially\\nmade the LLaMA [252] model publicly available, which was followed by the release of Code Llama',\n",
       " 'trend is particularly observable in the realm of open-source models. For instance, Meta AI initially\\nmade the LLaMA [252] model publicly available, which was followed by the release of Code Llama\\n[227], designed specifically for code generation. Similarly, DeepSeek LLM [26] developed and\\nreleased by DeepSeek has been extended to create DeepSeek Coder [88], a variant tailored for code\\ngeneration. The Qwen team has developed and released Code Qwen [249], building on their original\\nQwen [20] model. Microsoft, on the other hand, has unveiled WizardLM [289] and is exploring its\\ncoding-oriented counterpart, WizardCoder [173]. Google has joined the fray by releasing Gemma\\n[248], subsequently followed by Code Gemma [59]. Beyond simply adapting general-purpose LLMs\\nfor code-related tasks, there has been a proliferation of models specifically engineered for code\\ngeneration. Notable examples include StarCoder [147], OctoCoder [187], and CodeGen [193]. These',\n",
       " 'for code-related tasks, there has been a proliferation of models specifically engineered for code\\ngeneration. Notable examples include StarCoder [147], OctoCoder [187], and CodeGen [193]. These\\nmodels underscore the trend of LLMs being developed with a focus on code generation.\\nRecognizing the importance of these developments, we conduct a thorough analysis of selected\\npapers on LLMs for code generation, sourced from widely used scientific databases as mentioned\\nin Section 3. Based on this analysis, we propose a taxonomy that categorizes and evaluates the\\nlatest advancements in LLMs for code generation. This taxonomy, depicted in Figure 6, serves as a\\ncomprehensive reference for researchers seeking to quickly familiarize themselves with the state-\\nof-the-art in this dynamic field. It is important to highlight that the category of recent advances\\nemphasizes the core techniques used in the current state-of-the-art code LLMs.',\n",
       " 'of-the-art in this dynamic field. It is important to highlight that the category of recent advances\\nemphasizes the core techniques used in the current state-of-the-art code LLMs.\\nIn the subsequent sections, we will provide an in-depth analysis of each category related to code\\ngeneration. This will encompass a definition of the problem, the challenges to be addressed, and a\\ncomparison of the most prominent models and their performance evaluation.\\n5\\nLARGE LANGAUGE MODELS FOR CODE GENERATION\\nLLMs with Transformer architecture have revolutionized a multitude of fields, and their application\\nin code generation has been particularly impactful. These models follow a comprehensive process\\nthat starts with the curation and synthesis of code data, followed by a structured training approach\\nthat includes pre-training and fine-tuning (instruction tuning), reinforcement learning with various\\nfeedback, and the use of sophisticated prompt engineering techniques. Recent advancements have',\n",
       " 'that includes pre-training and fine-tuning (instruction tuning), reinforcement learning with various\\nfeedback, and the use of sophisticated prompt engineering techniques. Recent advancements have\\nseen the integration of repository-level and retrieval-augmented code generation, as well as the\\ndevelopment of autonomous coding agents. Furthermore, the evaluation of coding abilities of LLMs\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:15\\nPretrained\\n(Base) LLM\\nInstruct\\nCode LLM\\nSupervised\\nFine-tuning\\n(SFT)\\nHuman Preference\\nAlignment with RL\\n(e.g., RLHF)\\n(Optional)\\nPre-training Database\\nInstruction Database\\nPreference Database\\nStage ‚ë†\\nStage ‚ë¢\\nStage ‚ë£\\nPre-training Database\\nStage ‚ë°\\nContinual\\nPre-training\\n(Optional)\\nTask\\nDescription\\nGenerated\\nSource Code\\nInference\\nEvaluation\\nBenchmark\\nFig. 5. A diagram illustrating the general training, inference, and evaluation workflow for Code LLMs and\\ntheir associated databases. The training workflow is mainly divided into four distinct stages: Stage 1‚óãand 2‚óã\\nare the pre-training phase, whereas Stages 3‚óãand 4‚óãrepresent the post-training phases. It is important to\\nnote that Stage 2‚óãand 4‚óãare optional. For instance, StarCoder [147] incorporates only Stage 1‚óã. WizardCoder\\n[173], fine-tuned upon StarCoder, includes only Stage 3‚óã, while Code Llama [227], continually pre-trained on',\n",
       " '[173], fine-tuned upon StarCoder, includes only Stage 3‚óã, while Code Llama [227], continually pre-trained on\\nLlama 2, encompasses Stages 2‚óãand 3‚óã. DeepSeek-Coder-V2 [331], continually pre-trained on DeepSeek-V2,\\ncovers Stages 2‚óã, 3‚óã, and 4‚óã. Note that pre-trained model can be directly used for inference through prompt\\nengineering.\\nhas become a critical component of this research area. Figure 5 illustrates the general training,\\ninference, and evaluation workflow for Code LLMs and their associated databases.\\nIn the forthcoming sections, we will explore these dimensions of LLMs in the context of code\\ngeneration in detail. Section 5.1 will address the data curation and processing strategies employed\\nthroughout the various stages of LLM development. Section 5.2 will discuss data synthesis methods\\ndesigned to mitigate the scarcity of high-quality data. Section 5.3 will outline the prevalent model\\narchitectures used in LLMs for code generation. Moving to Section 5.4, we will examine the',\n",
       " 'designed to mitigate the scarcity of high-quality data. Section 5.3 will outline the prevalent model\\narchitectures used in LLMs for code generation. Moving to Section 5.4, we will examine the\\ntechniques for full parameter fine-tuning and parameter-efficient fine-tuning, which are essential\\nfor tailoring LLMs to code generation task. Section 5.5 will shed light on enhancing code quality\\nthrough reinforcement learning, utilizing the power of feedback. Section 5.6 will delve into the\\nstrategic use of prompts to maximize the coding capabilities of LLMs. The innovative approaches\\nof repository-level and retrieval-augmented code generation will be elaborated in Sections 5.7 and\\n5.8, respectively. Additionally, Section 5.9 will discuss the exciting field of autonomous coding\\nagents. Section 5.10 discusses various evaluation strategies and offer an empirical comparison using\\nthe widely recognized HumanEval, MBPP, and the more practical and challenging BigCodeBench',\n",
       " 'agents. Section 5.10 discusses various evaluation strategies and offer an empirical comparison using\\nthe widely recognized HumanEval, MBPP, and the more practical and challenging BigCodeBench\\nbenchmarks to highlight the progressive enhancements in LLM capabilities for code generation.\\nFurthermore, the ethical implications and the environmental impact of using LLMs for code\\ngeneration are discussed in Section 5.11, aiming to establish a trustworthiness, responsibility, safety,\\nefficiency, and green of LLM for code generation. Lastly, Section 5.12 will provide insights into some\\nof the practical applications that leverage LLMs for code generation, demonstrating the real-world\\nimpact of these sophisticated models. Through this comprehensive exploration, we aim to highlight\\nthe significance and potential of LLMs within the domain of automated code generation.\\n5.1\\nData Curation & Processing\\nThe exceptional performance of LLMs can be attributed to their training on large-scale and diverse',\n",
       " '5.1\\nData Curation & Processing\\nThe exceptional performance of LLMs can be attributed to their training on large-scale and diverse\\ndatasets [307]. Meanwhile, the extensive parameters of these models necessitate substantial data to\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:16\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nLLMs for Code Generation\\nData\\nCuration\\n(Sec. 5.1)\\nPre-training\\nCodeSearchNet[110], Google BigQuery[96], The Pile[78], CodeParrot[254], GitHub Code[254]\\nROOTS[137], The Stack[132], The Stack v2[170]\\nInstruction\\nTuning\\nCommitPackFT [187], Code Alpaca[43], OA-Leet[63], OSS-Instruct[278], Evol-instruction[225]\\nSelf-OSS-Instruct-SC2-Exec-Filter[304]\\nBenchmarks\\nGeneral\\nHumanEval[48], HumanEval+[162], HumanEvalPack[187], MBPP[17]\\nMBPP+[162], CoNaLa[297], Spider[300], CONCODE[113], ODEX[273]\\nCoderEval[299], ReCode[263], StudentEval[19]\\nCompetitions\\nAPPS[95], CodeContests[151]\\nData Science\\nDSP[41], DS-1000[136], ExeDS[107]\\nMultilingual\\nMBXP[16], Multilingual HumanEval[16], HumanEval-X[321], MultiPL-E[39]\\nxCodeEval[128]\\nReasoning\\nMathQA-X[16], MathQA-Python[17], GSM8K[58], GSM-HARD[79]\\nRepository\\nRepoEval[309], Stack-Repo[239], Repobench[167], EvoCodeBench[144]\\nSWE-bench[123], CrossCodeEval[68], SketchEval[308]\\nRecent\\nAdvances',\n",
       " 'Repository\\nRepoEval[309], Stack-Repo[239], Repobench[167], EvoCodeBench[144]\\nSWE-bench[123], CrossCodeEval[68], SketchEval[308]\\nRecent\\nAdvances\\nData\\nSynthesis\\n(Sec. 5.2)\\nSelf-Instruct [268], Evol-Instruct [289], Phi-1[84], Code Alpaca[43], WizardCoder[173]\\nMagicoder[278], StarCoder2-instruct [304]\\nPre-training\\n(Sec. 5.3)\\nModel\\nArchitectures\\nEncoder-Decoder\\nPyMT5[57], PLBART[7], CodeT5[271], JuPyT5[41]\\nAlphaCode[151], CodeRL[139], ERNIE-Code[40]\\nPPOCoder[238], CodeT5+[269], CodeFusion[241]\\nAST-T5[81]\\nDecoder-Only\\nGPT-C[244], GPT-Neo[30], GPT-J[258], Codex[48]\\nCodeGPT[172], CodeParrot[254], PolyCoder[290]\\nCodeGen[193], GPT-NeoX[29], PaLM-Coder[54]\\nInCoder[77], PanGu-Coder[55], PyCodeGPT[306]\\nCodeGeeX[321], BLOOM[140], ChatGPT[196]\\nSantaCoder[9], LLaMA[252], GPT-4[5]\\nCodeGen2[192], replit-code[223], StarCoder[147]\\nWizardCoder[173], phi-1[84], ChainCoder[323]\\nCodeGeeX2[321], PanGu-Coder2[234], Llama 2[253]\\nOctoPack[187], Code Llama[227], MFTCoder[160]',\n",
       " 'CodeGen2[192], replit-code[223], StarCoder[147]\\nWizardCoder[173], phi-1[84], ChainCoder[323]\\nCodeGeeX2[321], PanGu-Coder2[234], Llama 2[253]\\nOctoPack[187], Code Llama[227], MFTCoder[160]\\nphi-1.5[150], CodeShell[285], Magicoder[278]\\nAlphaCode 2[11], StableCode[210], WaveCoder[301]\\nphi-2[182], DeepSeek-Coder[88], StepCoder[71]\\nOpenCodeInterpreter[322], StarCoder 2[170]\\nClaude 3[14], ProCoder[27], CodeGemma[59]\\nCodeQwen[249], Llama3[180]\\nStarCoder2-Instruct[304], Codestral[181]\\nPre-training\\nTasks\\nCLM[88, 147, 173, 278], DAE[7, 269, 271], Auxiliary[40, 269, 271]\\nFine-tuning\\nInstruction\\nTuning\\n(Sec. 5.4)\\nFull Parameter\\nFine-tuning\\nCode Alpaca[43], CodeT5+[271], WizardCoder[173]\\nStarCoder[147], Pangu-Coder2[234], OctoPack[187]\\nCodeGeeX2[321], Magicoder[278], CodeGemma[59]\\nStarCoder2-instruct[304]\\nParameter\\nEfficient\\nFine-tuning\\nCodeUp[121], ASTRAIOS[334]\\nReinforcement\\nLearning\\nwith Feedback\\n(Sec. 5.5)\\nCodeRL[139], CompCoder[266], PPOCoder[238], RLTF[163]\\nPanGu-Coder2[234], StepCoder[71]',\n",
       " 'Parameter\\nEfficient\\nFine-tuning\\nCodeUp[121], ASTRAIOS[334]\\nReinforcement\\nLearning\\nwith Feedback\\n(Sec. 5.5)\\nCodeRL[139], CompCoder[266], PPOCoder[238], RLTF[163]\\nPanGu-Coder2[234], StepCoder[71]\\nPrompting\\nEngineering\\n(Sec. 5.6)\\nReflexion[236], LATS[327], Self-Debugging[51], SelfEvolve[122]\\nTheo X. et al.[195], CodeT[45], LEVER[190], AlphaCodium[224]\\nRepository\\nLevel & Long\\nContext\\n(Sec. 5.7)\\nRepoCoder[309], CoCoMIC[69], RepoHyper[209], RLPG[240]\\nRepoformer[282], RepoFusion[239], ToolGen[259], CodePlan[22]\\nCodeS[308]\\nRetrieval\\nAugmented\\n(Sec. 5.8)\\nHGNN[166], REDCODER[205], ReACC[171], DocPrompting[330]\\nRepoCoder[309], Su et al.[242]\\nAutonomous\\nCoding Agents\\n(Sec. 5.9)\\nAgentCoder [104], MetaGPT[100], CodeAct [265], AutoCodeRover [316], Devin[61]\\nOpenDevin[199], SWE-agent[124], L2MAC[98], OpenDevin CodeAct 1.0[287]\\nEvaluation\\n(Sec. 5.10)\\nMetrics\\nExact Match, BLEU[203], ROUGE[156], METEOR[23], CodeBLEU[221], pass@k[48]',\n",
       " 'OpenDevin[199], SWE-agent[124], L2MAC[98], OpenDevin CodeAct 1.0[287]\\nEvaluation\\n(Sec. 5.10)\\nMetrics\\nExact Match, BLEU[203], ROUGE[156], METEOR[23], CodeBLEU[221], pass@k[48]\\nn@k[151], test case average[95], execution accuracy[218], pass@t[195], perplexity[116]\\nHuman\\nEvaluation\\nCodePlan[22], RepoFusion[239], CodeBLEU[221]\\nLLM-as-a-Judge\\nAlpacaEval[148], MT-bench[320], ICE-Score[332]\\nCode LLMs\\nAlignment\\n(Sec. 5.10.3)\\nGreen[235, 277], Responsibility[168, 292], Efficiency[293], Safety[8, 9, 77, 91, 231, 294, 302], Trustworthiness[120, 202]\\nApplication\\n(Sec. 5.12)\\nGitHub Copilot[48], CodeGeeX[321], CodeWhisperer[12], Codeium[60], CodeArts Snap[234], TabNine[246], Replit[222]\\nFig. 6. Taxonomy of LLMs for code generation.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:17\\nunlock their full potential, in alignment with established scaling law [97, 127]. For a general-purpose\\nLLM, amassing a large-scale corpus of natural language from a variety of sources is imperative.\\nSuch sources include webpages, conversation data, books and news, scientific data, and code\\n[20, 33, 54, 252, 253, 298], while these data are often crawled from the web and must undergo\\nmeticulous and aggressive pre-processing [217, 317]. Fortunately, multiple platforms and websites\\noffer large-scale, open-source, and permissively licensed code corpora, such as GitHub7 and Stack\\nOverflow8. Notably, the number of stars or forks of GitHub repositories has emerged as a valuable\\nmetric for filtering high-quality code datasets. In a similar vein, the quantity of votes on Stack\\nOverflow can serve to discern the most relevant and superior answers.',\n",
       " 'metric for filtering high-quality code datasets. In a similar vein, the quantity of votes on Stack\\nOverflow can serve to discern the most relevant and superior answers.\\nNonetheless, raw datasets are frequently laden with redundant, noisy data and personal infor-\\nmation, eliciting concerns regarding privacy leakage, which may include the names and email\\naddresses of repository contributors [8, 37, 137]. Consequently, it is essential to undertake rigorous\\ndata-cleaning procedures. Typically, this process encompasses exact match deduplication, code\\ndata filtering based on average line length and a defined threshold for the fraction of alphanumeric\\ncharacters, the removal of auto-generated files through keyword searches, and the expunction of\\npersonal user data [132, 254]. Specifically, the standard data preprocessing workflow is depicted in\\nFigure 7.\\nThe development of a proficient LLM for code generation necessitates the utilization of various',\n",
       " 'Figure 7.\\nThe development of a proficient LLM for code generation necessitates the utilization of various\\ntypes of code data at different developmental stages. Therefore, we categorize code data into three\\ndistinct classes: pre-training datasets, instruction-tuning datasets, and benchmarks for performance\\nevaluation. The subsequent subsections will provide a detailed illustration of code data within each\\nclassification.\\n5.1.1\\nPre-training. The remarkable success of bidirectional pre-trained language models (PLMs)\\nsuch as BERT [66] and unidirectional PLMs like GPT [213] has firmly established the practice of\\npre-training on large-scale unlabeled datasets to endow models with a broad spectrum of general\\nknowledge. Extending this principle to the realm of code generation enables LLMs to assimilate\\nfundamental coding principles, including the understanding of code structure dependencies, the',\n",
       " 'knowledge. Extending this principle to the realm of code generation enables LLMs to assimilate\\nfundamental coding principles, including the understanding of code structure dependencies, the\\nsemantics of code identifiers, and the intrinsic logic of code sequences [48, 85, 269, 271]. In light of\\nthis advancement, there has been a proliferation of large-scale unlabeled code datasets proposed\\nto serve as the foundational training ground for LLMs to develop coding proficiency. A brief\\nintroduction of these datasets is as follows, with the statistics available in Table 4.\\n‚Ä¢ CodeSearchNet [110]: CodeSearchNet corpus is a comprehensive dataset, consisting of 2\\nmillion (comment, code) pairs from open-source repositories on GitHub. It includes code\\nand documentation in several programming languages including Go, Java, PHP, Python,\\nJavaScript, and Ruby. The dataset was primarily compiled to promote research into the\\nproblem of code retrieval using natural language.',\n",
       " 'JavaScript, and Ruby. The dataset was primarily compiled to promote research into the\\nproblem of code retrieval using natural language.\\n‚Ä¢ Google BigQuery [96]: the Google BigQuery Public Datasets program offers a full snapshot\\nof the content of more than 2.8 million open source GitHub repositories in BigQuery.\\n‚Ä¢ The Pile [78]: the Pile is an 825 GiB diverse and open source language modeling dataset\\naggregating 22 smaller, high-quality datasets including GitHub, Books3, and Wikipedia (en).\\nIt aims to encompass text from as many modalities as possible, thereby facilitating the\\ndevelopment of models with broader generalization capabilities. For code generation, the\\nGitHub composite is specifically utilized.\\n‚Ä¢ CodeParrot [254]: the CodeParrot dataset contains Python files used to train the code genera-\\ntion model in Chapter 10: Training Transformers from Scratch in the ‚ÄúNLP with Transformers\\n7https://github.com\\n8https://stackoverflow.com',\n",
       " 'tion model in Chapter 10: Training Transformers from Scratch in the ‚ÄúNLP with Transformers\\n7https://github.com\\n8https://stackoverflow.com\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:18\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nQuality Filtering\\n‚Ä¢\\nProgramming Language \\n‚Ä¢\\nStatistic Number\\n‚Ä¢\\nMetric Threshold\\n‚Ä¢\\nKeyword Search\\nDe-duplication\\n‚Ä¢\\nExact Match\\n‚Ä¢\\nSimilarity Metrics\\n‚Ä¢\\nFunction Level\\nPrivacy Reduction\\n‚Ä¢\\nDetect Personally Identifiable \\nInformation (PII)\\n‚Ä¢\\nDelete PII\\nRaw Corpus\\nTokenization\\n‚Ä¢\\nOpen Source Tokenizer\\n‚Ä¢\\nSentencePiece\\n‚Ä¢\\nByte-level BPE\\nPre-training Database\\n# Sum numbers from 1 to 10 \\nand print the result\\ntotal = sum(range(1, 11))\\nprint(total)\\ntotal = 0\\nfor i in range(1, 11):\\ntotal += i\\ntotal = sum(range(1, 11))\\n# Copyright 2024 @ John\\n# Email: csjohn@gmail.com\\n# Institution: HKUST\\ninputs = tokenizer.encode([\"def \\nprint_hello_world():\",...], \\nreturn_tensors=\"pt\").to(\"cuda\")\\n[\\n[755, 1194, 97824, \\n32892, 4658],\\n[755, 4062, 18942, \\n11179, 997, 262, 4304, \\n10442, 264, 1160, 315, \\n5219, 304, 36488, 2015, \\n1701, 279, 17697, 6354, \\n371, 12384,...],...\\n]\\n. . .',\n",
       " '[\\n[755, 1194, 97824, \\n32892, 4658],\\n[755, 4062, 18942, \\n11179, 997, 262, 4304, \\n10442, 264, 1160, 315, \\n5219, 304, 36488, 2015, \\n1701, 279, 17697, 6354, \\n371, 12384,...],...\\n]\\n. . .\\nFig. 7. A diagram depicting the standard data preprocessing workflow utilized in the pre-training phase of\\nLLMs for code generation.\\nbook‚Äù [254]. Created with the GitHub dataset available via Google‚Äôs BigQuery, the CodeParrot\\ndataset includes approximately 22 million Python files and is 180 GB (50 GB compressed) big.\\n‚Ä¢ GitHub Code [254]: the GitHub Code dataset comprises 115M code files derived from GitHub,\\nspanning 32 programming languages and 60 extensions totaling 1TB of data. The dataset\\nwas created from the public GitHub dataset on Google BiqQuery.\\n‚Ä¢ ROOTS [137]: the BigScience ROOTS Corpus is a 1.6TB dataset spanning 59 languages that\\nwas used to train the 176B BigScience Large Open-science Open-access Multilingual (BLOOM)',\n",
       " '‚Ä¢ ROOTS [137]: the BigScience ROOTS Corpus is a 1.6TB dataset spanning 59 languages that\\nwas used to train the 176B BigScience Large Open-science Open-access Multilingual (BLOOM)\\nlanguage model. For the code generation task, the code subset of the ROOTS Corpus will be\\nspecifically utilized.\\n‚Ä¢ The Stack [132]: the Stack contains over 6TB of permissively licensed source code files that\\ncover 358 programming languages. The dataset was compiled as part of the BigCode Project,\\nan open scientific collaboration working on the responsible development of Large Language\\nModels for Code (Code LLMs).\\n‚Ä¢ The Stack v2 [170]: The Stack v2, a dataset created as part of the BigCode Project, contains\\nover 3B files across more than 600 programming and markup languages. The dataset is\\nderived from the Software Heritage archive9, the largest public archive of software source\\ncode and accompanying development history.\\n5.1.2',\n",
       " 'derived from the Software Heritage archive9, the largest public archive of software source\\ncode and accompanying development history.\\n5.1.2\\nInstruction Tuning. Instruction tuning refers to the process of supervised fine-tuning LLMs\\nusing a collection of datasets structured as various instructions, with the purpose of following a\\nwide range of task instructions [56, 200, 229, 274]. This method has demonstrated a considerable\\nimprovement in model performance and an enhanced ability to generalize to unseen tasks that the\\n9https://archive.softwareheritage.org\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:19\\nTable 4. The statistics of some commonly-used pre-training datasets for LLMs aimed at code generation. The\\ncolumn labeled ‚Äò#PL‚Äô indicates the number of programming languages included in each dataset. It should\\nbe noted that in the CodeSearchNet [110] dataset, each file represents a function, and for the Pile [78] and\\nROOTS [137] datasets, only the code components are considered.\\nDataset\\nSize (GB)\\nFiles (M)\\n#PL\\nDate\\nLink\\nCodeSearchNet [110]\\n20\\n6.5\\n6\\n2022-01\\nhttps://huggingface.co/datasets/code_search_net\\nGoogle BigQuery[96]\\n-\\n-\\n-\\n2016-06\\ngithub-on-bigquery-analyze-all-the-open-source-code\\nThe Pile [78]\\n95\\n19\\n-\\n2022-01\\nhttps://huggingface.co/datasets/EleutherAI/pile\\nCodeParrot [254]\\n180\\n22\\n1\\n2021-08\\nhttps://huggingface.co/datasets/transformersbook/codeparrot\\nGitHub Code[254]\\n1,024\\n115\\n32\\n2022-02\\nhttps://huggingface.co/datasets/codeparrot/github-code\\nROOTS [137]\\n163\\n15\\n13\\n2023-03\\nhttps://huggingface.co/bigscience-data',\n",
       " 'GitHub Code[254]\\n1,024\\n115\\n32\\n2022-02\\nhttps://huggingface.co/datasets/codeparrot/github-code\\nROOTS [137]\\n163\\n15\\n13\\n2023-03\\nhttps://huggingface.co/bigscience-data\\nThe Stack [132]\\n3,136\\n317\\n30\\n2022-10\\nhttps://huggingface.co/datasets/bigcode/the-stack\\nThe Stack v2 [170]\\n32K\\n3K\\n619\\n2024-04\\nhttps://huggingface.co/datasets/bigcode/the-stack-v2\\nTable 5. The statistics of several representative datasets used in instruction-tuning LLMs for code generation.\\nThe column labeled ‚Äò#PL‚Äô indicates the number of programming languages encompassed by each dataset.\\nDataset\\nSize\\n#PL\\nDate\\nLink\\nCodeAlpaca-20K [43]\\n20k\\n-\\n2023-03\\nhttps://huggingface.co/datasets/sahil2801/CodeAlpaca-20k\\nCommitPackFT [187]\\n2GB\\n277\\n2023-08\\nhttps://huggingface.co/datasets/bigcode/commitpackft\\nEvol-Instruct-Code-80k [225]\\n80k\\n-\\n2023-07\\nhttps://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1\\nevol-codealpaca-v1 [251]\\n110K\\n-\\n2023-07\\nhttps://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1',\n",
       " '80k\\n-\\n2023-07\\nhttps://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1\\nevol-codealpaca-v1 [251]\\n110K\\n-\\n2023-07\\nhttps://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1\\nMagicoder-OSS-Instruct-75k [278]\\n75k\\nPython, Shell,\\nTypeScript, C++,\\nRust, PHP, Java,\\nSwift, C#\\n2023-12\\nhttps://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K\\nSelf-OSS-Instruct-SC2-Exec-Filter-50k [304]\\n50k\\nPython\\n2024-04\\nhttps://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k\\nmodel has not previously encountered, as evidenced by recent studies [56, 200]. Leveraging the\\nbenefits of instruction tuning, instruction tuning has been expanded into coding domains, especially\\nfor code generation, which involves the automatic generation of the intended code from a natural\\nlanguage description. The promise of instruction tuning in this area has led numerous researchers\\nto develop large-scale instruction-tuning datasets tailored for code generation. Below, we provide an',\n",
       " 'language description. The promise of instruction tuning in this area has led numerous researchers\\nto develop large-scale instruction-tuning datasets tailored for code generation. Below, we provide an\\noverview of several notable datasets tailored for instruction tuning, with their respective statistics\\ndetailed in Table 5.\\n‚Ä¢ CodeAlpaca-20k [43]: CodeAlpaca-20k is a collection of 20K instruction-following data\\ngenerated using the data synthesis techniques termed Self-Instruct outlined in [268], with\\nmodifications for code generation, editing, and optimization tasks instead of general tasks.\\n‚Ä¢ CommitPackFT [187]: CommitPackFT is a 2GB refined version of CommitPack. It is filtered\\nto only include high-quality commit messages that resemble natural language instructions.\\n‚Ä¢ Evol-Instruct-Code-80k [225]: Evol-Instruct-Code-80k is an open-source implementation of\\nEvol-Instruct-Code described in the WizardCoder paper [173], which enhances the fine-tuning',\n",
       " '‚Ä¢ Evol-Instruct-Code-80k [225]: Evol-Instruct-Code-80k is an open-source implementation of\\nEvol-Instruct-Code described in the WizardCoder paper [173], which enhances the fine-tuning\\neffect of pre-trained code large models by adding complex code instructions.\\n‚Ä¢ Magicoder-OSS-Instruct-75k [278]: is a 75k synthetic data generated through OSS-Instruct\\nwith gpt-3.5-turbo-1106 and used to train both Magicoder and Magicoder-S series models.\\n‚Ä¢ Self-OSS-Instruct-SC2-Exec-Filter-50k [304]: Self-OSS-Instruct-SC2-Exec-Filter-50k is gen-\\nerated by StarCoder2-15B using the OSS-Instruct [278] data synthesis approach. It was\\nsubsequently used to fine-tune StarCoder-15B without any human annotations or distilled\\ndata from huge and proprietary LLMs.\\n5.1.3\\nBenchmarks. To rigorously assess the efficacy of LLMs for code generation, the research\\ncommunity has introduced a variety of high-quality benchmarks in recent years. Building on\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:20\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nthe foundational work by [48], numerous variations of the HumanEval dataset and additional\\nbenchmarks have emerged, aiming to evaluate a broader spectrum of code generation capabilities\\nin LLMs. We roughly divide these benchmarks into six distinct categories based on their application\\ncontexts, including general-purpose, competitive programming, data science, multilingual, logical\\nreasoning, and repository-level. It is important to highlight that logical reasoning encompasses math-\\nrelated benchmarks, as it aims to create ‚Äúcode-based solutions‚Äù for solving complex mathematical\\nproblems [50, 79, 326]. This strategy can therefore mitigate the limitations of LLMs in performing\\nintricate mathematical computations. The statistics for these benchmarks are presented in Table 6.\\nGeneral\\n‚Ä¢ HumanEval [48]: HumanEval comprises 164 manually scripted Python programming prob-',\n",
       " 'intricate mathematical computations. The statistics for these benchmarks are presented in Table 6.\\nGeneral\\n‚Ä¢ HumanEval [48]: HumanEval comprises 164 manually scripted Python programming prob-\\nlems, each featuring a function signature, docstring, body, and multiple unit tests.\\n‚Ä¢ HumanEval+ [162]: HumanEval+ extends the original HumanEval [48] benchmark by in-\\ncreasing the scale of the test cases by 80 times. As the test cases increase, HumanEval+ can\\ncatch significant amounts of previously undetected incorrect code synthesized by LLMs.\\n‚Ä¢ HumanEvalPack [187]: expands HumanEval [48] by extending it to encompass three coding\\ntasks across six programming languages, namely code synthesis, code repair, and code\\nexplanation.\\n‚Ä¢ MBPP [17]: MBPP is a collection of approximately 974 Python programming problems, crowd-\\nsourced and designed for entry-level programmers. Each problem comes with an English\\ntask description, a code solution, and three automated test cases.',\n",
       " 'sourced and designed for entry-level programmers. Each problem comes with an English\\ntask description, a code solution, and three automated test cases.\\n‚Ä¢ MBPP+ [162]: MBPP+ enhances MBPP [17] by eliminating ill-formed problems and rectifying\\nproblems with incorrect implementations. The test scale of MBPP+ is also expanded by 35\\ntimes for test augmentation.\\n‚Ä¢ CoNaLa [297]: CoNaLa contains almost 597K data samples for evaluating Python code\\ngeneration. The curated part of CoNaLa is crawled from Stack Overflow, automatically\\nfiltered, and then curated by annotators. The mined part of CoNaLais automatically mined,\\nwith almost 600k examples.\\n‚Ä¢ Spider [300]: Spider is large-scale complex text-to-SQL dataset covering 138 different domains.\\nIt has over 10K questions and 5.6K complex SQL queries on 200 databases. This dataset aims\\nto test a model‚Äôs ability to generalize to SQL queries, database schemas, and new domains.',\n",
       " 'It has over 10K questions and 5.6K complex SQL queries on 200 databases. This dataset aims\\nto test a model‚Äôs ability to generalize to SQL queries, database schemas, and new domains.\\n‚Ä¢ CONCODE [113]: CONCODE is a dataset with over 100K samples consisting of Java classes\\nfrom public GitHub repositories. It provides near zero-shot conditions that can test the\\nmodel‚Äôs ability to generalize to unseen natural language tokens with unseen environments.\\n‚Ä¢ ODEX [273]: ODEX is an open-domain dataset focused on the execution-based generation\\nof Python code from natural language. It features 945 pairs of natural language queries and\\ntheir corresponding Python code, all extracted from StackOverflow forums.\\n‚Ä¢ CoderEval [299]: CoderEval is a pragmatic code generation benchmark that includes 230\\nPython and 230 Java code generation problems. It can be used to evaluate the model perfor-\\nmance in generating pragmatic code beyond just generating standalone functions.',\n",
       " 'Python and 230 Java code generation problems. It can be used to evaluate the model perfor-\\nmance in generating pragmatic code beyond just generating standalone functions.\\n‚Ä¢ ReCode [263]: Recode serves as a comprehensive robustness evaluation benchmark. ReCode\\napplies perturbations to docstrings, function and variable names, code syntax, and code\\nformat, thereby providing multifaceted assessments of a model‚Äôs robustness performance.\\n‚Ä¢ StudentEval [19]: StudentEval is a dataset of 1,749 prompts for 48 problems, authored by 80\\nstudents who have only completed a one-semester Python programming class. Unlike many\\nother benchmarks, it has multiple prompts per problem and multiple attempts by the same\\nparticipant, each problem is also accompanied by a set of instructor-written test cases.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:21\\n‚Ä¢ BigCodeBench [333]: BigCodeBench has 1,140 complex Python programming tasks, covering\\n723 function calls from 139 popular libraries across 7 domains. This benchmark is specifically\\ndesigned to assess LLMs‚Äô ability to call multiple functions from cross-domain libraries and\\nfollow complex instructions to solve programming tasks, helping to bridge the evaluation\\ngap between isolated coding exercises and the real-world programming scenario.\\n‚Ä¢ ClassEval [72]: ClassEval is a manually-crafted benchmark consisting of 100 classes and\\n412 methods for evaluating LLMs in the class-level code generation scenario. Particularly,\\nthe task samples of ClassEval present higher complexities, involving long code generation\\nand sophisticated docstring information, thereby benefiting the evaluation of the LLMs‚Äô\\ncapabilities in generating complicated code.\\n‚Ä¢ NaturalCodeBench [314]: NaturalCodeBench is a comprehensive code benchmark featuring',\n",
       " 'capabilities in generating complicated code.\\n‚Ä¢ NaturalCodeBench [314]: NaturalCodeBench is a comprehensive code benchmark featuring\\n402 high-quality problems in Python and Java. These problems are selected from natural\\nuser queries from online coding services and span 6 distinct domains, shaping an evaluation\\nenvironment aligned with real-world applications.\\nCompetitions\\n‚Ä¢ APPS [95]: The APPS benchmark is composed of 10K Python problems, spanning three levels\\nof difficulty: introductory, interview, and competition. Each entry in the dataset includes a\\nprogramming problem described in English, corresponding ground truth Python solutions,\\ntest cases defined by their inputs and outputs or function names if provided.\\n‚Ä¢ CodeContests [151]: CodeContests is a competitive programming dataset consisting of sam-\\nples from various sources including Aizu, AtCoder, CodeChef, Codeforces, and HackerEarth.\\nThe dataset encompasses programming problems accompanied by test cases in the form of',\n",
       " 'ples from various sources including Aizu, AtCoder, CodeChef, Codeforces, and HackerEarth.\\nThe dataset encompasses programming problems accompanied by test cases in the form of\\npaired inputs and outputs, along with both correct and incorrect human solutions in multiple\\nprogramming languages.\\n‚Ä¢ LiveCodeBench [188]: LiveCodeBench is a comprehensive and contamination-free benchmark\\nfor evaluating a wide array of code-related capabilities of LLMs, including code generation,\\nself-repair, code execution, and test output prediction. It continuously gathers new coding\\nproblems from contests across three reputable competition platforms: LeetCode, AtCoder,\\nand CodeForces. The latest release of the dataset includes 713 problems that were released\\nbetween May 2023 and September 2024.\\nData Science\\n‚Ä¢ DSP [41]: DSP allows for model evaluation based on real data science pedagogical notebooks.\\nIt includes well-structured problems, along with unit tests to verify the correctness of solutions',\n",
       " '‚Ä¢ DSP [41]: DSP allows for model evaluation based on real data science pedagogical notebooks.\\nIt includes well-structured problems, along with unit tests to verify the correctness of solutions\\nand a Docker environment for reproducible execution.\\n‚Ä¢ DS-1000 [136]: DS-1000 has 1K science questions from seven Python libraries, namely NumPy,\\nPandas, TensorFlow, PyTorch, SciPy, Scikit-learn, and Matplotlib. The DS-1000 benchmark\\nfeatures: (1) realistic problems with diverse contexts (2) implementation of multi-criteria\\nevaluation metrics, and (3) defense against memorization.\\n‚Ä¢ ExeDS [107]: ExeDS is a data science code generation dataset specifically designed for execu-\\ntion evaluation. It contains 534 problems with execution outputs from Jupyter Notebooks, as\\nwell as 123K examples for training and validation.\\nMultilingual\\n‚Ä¢ MBXP [16]: MBXP is a multilingual adaptation of the original MBPP [17] dataset. It is created',\n",
       " 'well as 123K examples for training and validation.\\nMultilingual\\n‚Ä¢ MBXP [16]: MBXP is a multilingual adaptation of the original MBPP [17] dataset. It is created\\nusing a framework that translates prompts and test cases from the original Python datasets\\ninto the corresponding data in the targeted programming language.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:22\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n‚Ä¢ Multilingual HumanEval [16]: Multilingual HumanEval is a dataset derived from HumanEval\\n[48]. It is designed to assess the performance of models in a multilingual context. It helps\\nuncover the generalization ability of the given model on languages that are out-of-domain.\\n‚Ä¢ HumanEval-X [321]: HumanEval-X is developed for evaluating the multilingual ability of\\ncode generation models with 820 hand-writing data samples in C++, Java, JavaScript, and Go.\\n‚Ä¢ MultiPL-E [39]: MultiPL-E is a dataset for evaluating LLMs for code generation across 18 pro-\\ngramming languages. It adopts the HumanEval [48] and the MBPP [17] Python benchmarks\\nand uses little compilers to translate them to other languages.\\n‚Ä¢ xCodeEval [128]: xCodeEval is an executable multilingual multitask benchmark consisting of\\n25M examples covering 17 programming languages. Its tasks include code understanding,\\ngeneration, translation, and retrieval.\\nReasoning',\n",
       " '25M examples covering 17 programming languages. Its tasks include code understanding,\\ngeneration, translation, and retrieval.\\nReasoning\\n‚Ä¢ MathQA-X [16] MathQA-X is the multilingual version of MathQA [13]. It is generated by\\nutilizing a conversion framework that converts samples from Python datasets into the target\\nlanguage.\\n‚Ä¢ MathQA-Python [17] MathQA-Python is a Python version of the MathQA benchmark[13].\\nThe benchmark, containing more than 23K problems, is designed to assess the capability of\\nmodels to synthesize code from complex textual descriptions.\\n‚Ä¢ GSM8K [58]: GSM8K is a dataset of 8.5K linguistically diverse grade school math problems.\\nThe dataset is crafted to facilitate the task of question answering on basic mathematical\\nproblems that requires multi-step reasoning.\\n‚Ä¢ GSM-HARD [79]: GSM-HARD is a more challenging version of the GSM8K [58] dataset. It\\nreplaces the numbers in the GSM8K questions with larger, less common numbers, thereby',\n",
       " '‚Ä¢ GSM-HARD [79]: GSM-HARD is a more challenging version of the GSM8K [58] dataset. It\\nreplaces the numbers in the GSM8K questions with larger, less common numbers, thereby\\nincreasing the complexity and difficulty level of the problems.\\n‚Ä¢ CRUXEval [82]: CRUXEval contains 800 Python functions, each paired with an input-output\\nexample. This benchmark supports two tasks: input prediction and output prediction, designed\\nto evaluate the code reasoning, understanding, and execution capabilities of code LLMs.\\nRepository\\n‚Ä¢ RepoEval [309]: RepoEval enables the evaluation of repository-level code completion. It can\\noffer different levels of granularity and improved evaluation accuracy through the use of unit\\ntests.\\n‚Ä¢ Stack-Repo [239]: Stack-Repo is a dataset of 200 Java repositories from GitHub with near-\\ndeduplicated files. These files are augmented with three types of repository contexts: prompt\\nproposal contexts, BM25 Contexts (based on BM25 similarity scores), and RandomNN Con-',\n",
       " 'deduplicated files. These files are augmented with three types of repository contexts: prompt\\nproposal contexts, BM25 Contexts (based on BM25 similarity scores), and RandomNN Con-\\ntexts (obtained using the nearest neighbors in the representation space of an embedding\\nmodel).\\n‚Ä¢ Repobench [167]: Repobench is a benchmark specifically used for evaluating repository-\\nlevel code auto-completion systems. Supporting both Python and Java, it consists of three\\ninterconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion),\\nand RepoBench-P (Pipeline).\\n‚Ä¢ EvoCodeBench [144]: EvoCodeBench is an evolutionary code generation benchmark, con-\\nstructed through a rigorous pipeline and aligned with real-world repositories. This benchmark\\nalso provides comprehensive annotations and robust evaluation metrics.\\n‚Ä¢ SWE-bench [123]: SWE-bench is a dataset that tests a model‚Äôs ability to automatically solve',\n",
       " 'also provides comprehensive annotations and robust evaluation metrics.\\n‚Ä¢ SWE-bench [123]: SWE-bench is a dataset that tests a model‚Äôs ability to automatically solve\\nGitHub issues. The dataset has 2,294 Issue-Pull Request pairs from 12 popular Python reposi-\\ntories.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:23\\nTable 6. The detailed statistics of commonly-used benchmarks used in evaluating LLMs for code generation.\\nThe column labeled ‚Äò#PL‚Äô indicates the number of programming languages included in each dataset. For the\\nsake of brevity, we list the programming languages (PLs) for benchmarks that support fewer than or include\\nfive PLs. For benchmarks with six or more PLs, we provide only a numerical count of the PLs supported.\\nScenario\\nBenchmark\\nSize\\n#PL\\nDate\\nLink\\nGeneral\\nHumanEval [48]\\n164\\nPython\\n2021-07\\nhttps://huggingface.co/datasets/openai_humaneval\\nHumanEval+ [162]\\n164\\nPython\\n2023-05\\nhttps://huggingface.co/datasets/evalplus/humanevalplus\\nHumanEvalPack [187]\\n164\\n6\\n2023-08\\nhttps://huggingface.co/datasets/bigcode/humanevalpack\\nMBPP [17]\\n974\\nPython\\n2021-08\\nhttps://huggingface.co/datasets/mbpp\\nMBPP+ [162]\\n378\\nPython\\n2023-05\\nhttps://huggingface.co/datasets/evalplus/mbppplus\\nCoNaLa [297]\\n596.88K\\nPython\\n2018-05',\n",
       " 'MBPP [17]\\n974\\nPython\\n2021-08\\nhttps://huggingface.co/datasets/mbpp\\nMBPP+ [162]\\n378\\nPython\\n2023-05\\nhttps://huggingface.co/datasets/evalplus/mbppplus\\nCoNaLa [297]\\n596.88K\\nPython\\n2018-05\\nhttps://huggingface.co/datasets/neulab/conala\\nSpider [300]\\n8,034\\nSQL\\n2018-09\\nhttps://huggingface.co/datasets/xlangai/spider\\nCONCODE [113]\\n104K\\nJava\\n2018-08\\nhttps://huggingface.co/datasets/AhmedSSoliman/CONCOD\\nODEX [273]\\n945\\nPython\\n2022-12\\nhttps://huggingface.co/datasets/neulab/odex\\nCoderEval [299]\\n460\\nPython, Java\\n2023-02\\nhttps://github.com/CoderEval/CoderEval\\nReCode [263]\\n1,138\\nPython\\n2022-12\\nhttps://github.com/amazon-science/recode\\nStudentEval [19]\\n1,749\\nPython\\n2023-06\\nhttps://huggingface.co/datasets/wellesley-easel/StudentEval\\nBigCodeBench [333]\\n1,140\\nPython\\n2024-06\\nhttps://huggingface.co/datasets/bigcode/bigcodebench\\nClassEval [72]\\n100\\nPython\\n2023-08\\nhttps://huggingface.co/datasets/FudanSELab/ClassEval\\nNaturalCodeBench [314]\\n402\\nPython, Java\\n2024-05\\nhttps://github.com/THUDM/NaturalCodeBench',\n",
       " 'ClassEval [72]\\n100\\nPython\\n2023-08\\nhttps://huggingface.co/datasets/FudanSELab/ClassEval\\nNaturalCodeBench [314]\\n402\\nPython, Java\\n2024-05\\nhttps://github.com/THUDM/NaturalCodeBench\\nCompetitions\\nAPPS [95]\\n10,000\\nPython\\n2021-05\\nhttps://huggingface.co/datasets/codeparrot/apps\\nCodeContests [151]\\n13,610\\nC++, Python,\\nJava\\n2022-02\\nhttps://huggingface.co/datasets/deepmind/code_contests\\nLiveCodeBench [188]\\n713\\nUpdating\\nPython\\n2024-03\\nhttps://github.com/LiveCodeBench/LiveCodeBench\\nData Science\\nDSP [41]\\n1,119\\nPython\\n2022-01\\nhttps://github.com/microsoft/DataScienceProblems\\nDS-1000 [136]\\n1,000\\nPython\\n2022-11\\nhttps://huggingface.co/datasets/xlangai/DS-1000\\nExeDS [107]\\n534\\nPython\\n2022-11\\nhttps://github.com/Jun-jie-Huang/ExeDS\\nMultilingual\\nMBXP [16]\\n12.4K\\n13\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/mbxp\\nMultilingual HumanEval [16]\\n1.9K\\n12\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/multi-humaneval\\nHumanEval-X [321]\\n820\\nPython, C++,\\nJava, JavaScript,\\nGo\\n2023-03',\n",
       " 'Multilingual HumanEval [16]\\n1.9K\\n12\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/multi-humaneval\\nHumanEval-X [321]\\n820\\nPython, C++,\\nJava, JavaScript,\\nGo\\n2023-03\\nhttps://huggingface.co/datasets/THUDM/humaneval-x\\nMultiPL-E [39]\\n161\\n18\\n2022-08\\nhttps://huggingface.co/datasets/nuprl/MultiPL-E\\nxCodeEval [128]\\n5.5M\\n11\\n2023-03\\nhttps://github.com/ntunlp/xCodeEval\\nReasoning\\nMathQA-X [16]\\n5.6K\\nPython, Java,\\nJavaScript\\n2022-10\\nhttps://huggingface.co/datasets/mxeval/mathqa-x\\nMathQA-Python [17]\\n23,914\\nPython\\n2021-08\\nhttps://github.com/google-research/google-research\\nGSM8K [58]\\n8.5K\\nPython\\n2021-10\\nhttps://huggingface.co/datasets/gsm8k\\nGSM-HARD [79]\\n1.32K\\nPython\\n2022-11\\nhttps://huggingface.co/datasets/reasoning-machines/gsm-hard\\nCRUXEval [82]\\n800\\nPython\\n2024-01\\nhttps://huggingface.co/datasets/cruxeval-org/cruxeval\\nRepository\\nRepoEval [309]\\n3,573\\nPython, Java\\n2023-03\\nhttps://paperswithcode.com/dataset/repoeval\\nStack-Repo [239]\\n200\\nJava\\n2023-06\\nhttps://huggingface.co/datasets/RepoFusion/Stack-Repo',\n",
       " 'Repository\\nRepoEval [309]\\n3,573\\nPython, Java\\n2023-03\\nhttps://paperswithcode.com/dataset/repoeval\\nStack-Repo [239]\\n200\\nJava\\n2023-06\\nhttps://huggingface.co/datasets/RepoFusion/Stack-Repo\\nRepobench [167]\\n27k\\nPython, Java\\n2023-01\\nhttps://github.com/Leolty/repobench\\nEvoCodeBench [144]\\n275\\nPython\\n2024-03\\nhttps://huggingface.co/datasets/LJ0815/EvoCodeBench\\nSWE-bench [123]\\n2,294\\nPython\\n2023-10\\nhttps://huggingface.co/datasets/princeton-nlp/SWE-bench\\nCrossCodeEval [68]\\n10K\\nPython, Java,\\nTypeScript, C#\\n2023-10\\nhttps://github.com/amazon-science/cceval\\nSketchEval [308]\\n20,355\\nPython\\n2024-03\\nhttps://github.com/nl2code/codes\\n‚Ä¢ CrossCodeEval [68]: CrossCodeEval is a diverse and multilingual scope completion dataset\\ncovering four languages: Python, Java, TypeScript, and C#. This benchmark tests the model‚Äôs\\nability to understand in-depth cross-file information and accurately complete the code.\\n‚Ä¢ SketchEval [308]: SketchEval is a repository-oriented benchmark that encompasses data from',\n",
       " 'ability to understand in-depth cross-file information and accurately complete the code.\\n‚Ä¢ SketchEval [308]: SketchEval is a repository-oriented benchmark that encompasses data from\\n19 repositories, each varying in complexity. In addition to the dataset, SketchEval introduces\\na metric, known as SketchBLEU, to measure the similarity between two repositories based\\non their structures and semantics.\\n5.2\\nData Synthesis\\nNumerous studies have demonstrated that high-quality datasets are integral to enhancing the\\nperformance of LLMs in various downstream tasks [33, 133, 179, 280, 286, 328]. For instance, the\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:24\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nSeed Problem\\n<Problem, Solution>\\nEvolved Problem\\nSelf-Instruct\\nEvol-Instruct\\nSolution\\nSeed Problem\\nEvolution Type\\n<Debiased Problem, Solution>\\nOSS-Instruct\\nSeed Code Snippet\\nùêøùêøùëÄ\\nùêøùêøùëÄ\\nùêøùêøùëÄ\\nFig. 8. The comparison among three representative data synthesis methods used for generating instruction\\ndata with LLMs. The Code Alpaca [43] employs the self-instruct method, whereas WizardCoder [173] and\\nMagicoder [278] utilize the Evol-Instruct and OSS-Instruct methods, respectively.\\nLIMA model, a 65B parameter LLaMa language model fine-tuned with a standard supervised loss\\non a mere 1,000 meticulously curated prompts and responses, achieved performance on par with, or\\neven superior to, GPT-4 in 43% of evaluated cases. This figure rose to 58% when compared to Bard\\nand 65% against DaVinci003, all without the use of reinforcement learning or human preference',\n",
       " 'even superior to, GPT-4 in 43% of evaluated cases. This figure rose to 58% when compared to Bard\\nand 65% against DaVinci003, all without the use of reinforcement learning or human preference\\nmodeling [328]. The QuRating initiative strategically selects pre-training data embodying four key\\ntextual qualities ‚Äî writing style, facts & trivia, required expertise, and educational value ‚Äî that\\nresonate with human intuition. Training a 1.3B parameter model on such data resulted in reduced\\nperplexity and stronger in-context learning compared to baseline models [280].\\nDespite these advancements, acquiring quality data remains a significant challenge due to issues\\nsuch as data scarcity, privacy concerns, and prohibitive costs [165, 268]. Human-generated data is\\noften labor-intensive and expensive to produce, and it may lack the necessary scope and detail to\\nnavigate complex, rare, or ambiguous scenarios. As a resolution to these challenges, synthetic data',\n",
       " 'often labor-intensive and expensive to produce, and it may lack the necessary scope and detail to\\nnavigate complex, rare, or ambiguous scenarios. As a resolution to these challenges, synthetic data\\nhas emerged as a viable alternative. By generating artificial datasets that replicate the intricacies\\nof real-world information, models such as GPT-3.5-turbo [196] and GPT-4 [5] have enabled the\\ncreation of rich datasets without the need for human annotation [92, 138, 165, 268]. This approach\\nis particularly beneficial in enhancing the instruction-following capabilities of LLMs, with a focus\\non generating synthetic instruction-based data.\\nA notable example of this approach is the Self-Instruct [268] framework, which employs an off-the-\\nshelf language model to generate a suite of instructions, inputs, and outputs. This data is then refined\\nby removing invalid or redundant entries before being used to fine-tune the model. The empirical',\n",
       " 'by removing invalid or redundant entries before being used to fine-tune the model. The empirical\\nevidence supports the efficacy of this synthetic data generation methodology. Building upon this\\nconcept, the Alpaca [247] model, fine-tuned on 52k pieces of instruction-following data from a 7B\\nparameter LLaMa [252] model, exhibits performance comparable to the text-davinci-003 model.\\nWizardLM [289] introduced the Evol-Instruct technique, which incrementally transforms simple\\ninstructions into more complex variants. The fine-tuned LLaMa model using this technique has\\nshown promising results in comparison to established proprietary LLMs such as ChatGPT [196] and\\nGPT-4 [5], to some extent. Moreover, Microsoft has contributed to this field with their Phi series of\\nmodels, predominantly trained on synthetic high-quality data, which includes Phi-1 (1.3B) [84]\\nfor Python coding, Phi-1.5 (1.3B) [150] for common sense reasoning and language understanding,',\n",
       " 'models, predominantly trained on synthetic high-quality data, which includes Phi-1 (1.3B) [84]\\nfor Python coding, Phi-1.5 (1.3B) [150] for common sense reasoning and language understanding,\\nPhi-2 (2.7B) [182] for advanced reasoning and language understanding, and Phi-3 (3.8B) [4] for\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:25\\ngeneral purposes. These models have consistently outperformed larger counterparts across various\\nbenchmarks, demonstrating the efficacy of synthetic data in model training.\\nDrawing on the successes of data synthesis for general-purpose LLMs, researchers have expanded\\nthe application of synthetic data to the realm of code generation. The Code Alpaca model, as de-\\nscribed in [43], has been fine-tuned on a 7B and 13B LLaMA model using a dataset of 20k instruction-\\nfollowing examples for code generation. This dataset was created by text-davinci-00310 and\\nemployed the Self-Instruct technique [268]. Building on this, the WizardCoder 15B [173] utilizes\\nthe Evol-Instruct technique to create an enhanced dataset of 78k evolved code instruction examples.\\nThis dataset originates from the initial 20k instruction-following dataset used by Code Alpaca\\n[43], which was also generated by text-davinci-003. The WizardCoder model, fine-tuned on',\n",
       " 'This dataset originates from the initial 20k instruction-following dataset used by Code Alpaca\\n[43], which was also generated by text-davinci-003. The WizardCoder model, fine-tuned on\\nthe StarCoder [147] base model, achieved a 57.3% pass@1 on the HumanEval benchmarks. This\\nperformance not only surpasses all other open-source Code LLMs by a significant margin but also\\noutperforms leading closed LLMs such as Anthropic‚Äôs Claude and Google‚Äôs Bard. In a similar vein,\\nMagicoder [278] introduces a novel data synthesis approach termed OSS-INSTRUCT which enlight-\\nens LLMs with open-source code snippets to generate high-quality instruction data for coding tasks.\\nIt aims to address the inherent biases often present in synthetic data produced by LLMs. Building\\nupon CodeLlama [227], the MagicoderS-CL-7B model ‚Äî fine-tuned with 75k synthetic instruction\\ndata using the OSS-INSTRUCT technique and with gpt-3.5-turbo-1106 as the data generator ‚Äî',\n",
       " 'upon CodeLlama [227], the MagicoderS-CL-7B model ‚Äî fine-tuned with 75k synthetic instruction\\ndata using the OSS-INSTRUCT technique and with gpt-3.5-turbo-1106 as the data generator ‚Äî\\nhas outperformed the prominent ChatGPT on the HumanEval Plus benchmark, achieving pass@1\\nof 66.5% versus 65.9%. In a noteworthy development, Microsoft has introduced the phi-1 model [84],\\na more compact LLM of only 1.3B parameters. Despite its smaller size, phi-1 has been trained on\\nhigh-quality textbook data sourced from the web (comprising 6 billion tokens) and supplemented\\nwith synthetic textbooks and exercises generated with GPT-3.5 (1 billion tokens). It has achieved\\npass@1 of 50.6% on HumanEval and 55.5% on MBPP, setting a new state-of-the-art for Python\\ncoding performance among existing small language models (SLMs). The latest contribution to\\nthis field is from the BigCode team, which has presented StarCoder2-15B-instruct [304], the first',\n",
       " 'coding performance among existing small language models (SLMs). The latest contribution to\\nthis field is from the BigCode team, which has presented StarCoder2-15B-instruct [304], the first\\nentirely self-aligned code LLM trained with a transparent and permissive pipeline. This model\\naligns closely with the OSS-INSTRUCT principles established by Magicoder, generating instructions\\nbased on seed functions filtered from the Stack v1 dataset [132] and producing responses through\\nself-validation. Unlike Magicoder, StarCoder2-15B-instruct employs its base model, StarCoder2-15B,\\nas the data generator, thus avoiding reliance on large and proprietary LLMs like GPT-3.5-turbo\\n[196]. Figure 8 illustrates the comparison between Self-Instruct, Evol-Instruct, and OSS-Instruct\\ndata synthesis methods.\\nWhile synthetic data has demonstrated its potential across both small- and large-scale LMs for a\\nvariety of general and specialized tasks, including code generation, it also poses several challenges',\n",
       " 'While synthetic data has demonstrated its potential across both small- and large-scale LMs for a\\nvariety of general and specialized tasks, including code generation, it also poses several challenges\\nthat must be addressed. These challenges include a lack of data diversity [280], the need to ensure\\nthe factuality and fidelity of the information [256, 281], and the potential to amplify existing biases\\nor introduce new ones [24, 89].\\n5.3\\nPre-Training\\n5.3.1\\nModel Architectures. Since the inception of the Transformer architecture for machine transla-\\ntion [257], it has become the de facto backbone for a multitude of LLMs that address a wide range of\\ndownstream tasks. The Transformer and its derivatives owe their prominence to their exceptional\\nability to parallelize computation and their powerful representational capacities [298, 319]. Through\\ninnovative scaling techniques, such as Mixture-of-Experts (MoE) [35, 233] and Depth-Up-Scaling',\n",
       " 'ability to parallelize computation and their powerful representational capacities [298, 319]. Through\\ninnovative scaling techniques, such as Mixture-of-Experts (MoE) [35, 233] and Depth-Up-Scaling\\n(DUS) [130], the capacity of Transformer-based LLMs has expanded to encompass hundreds of\\n10https://platform.openai.com\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:26\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 7. The overview of LLMs with encoder-decoder architectures for code generation.\\nModel\\nInstitution\\nSize\\nVocabulary\\nContext\\nWindow\\nDate\\nOpen Source\\nPyMT5[57]\\nMicrosoft\\n374M\\n50K\\n1024+1024\\n2020-10\\nPLBART[7]\\nUCLA\\n140M\\n50K\\n1024+1024\\n2021-03\\n\"\\nCodeT5 [271]\\nSalesforce\\n60M, 220M, 770M\\n32K\\n512+256\\n2021-09\\n\"\\nJuPyT5[41]\\nMicrosoft\\n350M\\n50K\\n1024+1024\\n2022-01\\nAlphaCode[151]\\nDeepMind\\n284M, 1.1B, 2.8B,\\n8.7B, 41.1B\\n8K\\n1536+768\\n2022-02\\nCodeRL[139]\\nSalesforce\\n770M\\n32K\\n512+256\\n2022-06\\n\"\\nERNIE-Code[40]\\nBaidu\\n560M\\n250K\\n1024+1024\\n2022-12\\n\"\\nPPOCoder[238]\\nVirginia Tech\\n770M\\n32K\\n512+256\\n2023-01\\nCodeT5+[269]\\nSalesforce\\n220M, 770M, 2B,\\n6B, 16B\\n50K\\n2048+2048\\n2023-05\\n\"\\nCodeFusion[241]\\nMicrosoft\\n75M\\n32k\\n128+128\\n2023-10\\n\"\\nAST-T5[81]\\nUC Berkeley\\n226M\\n32k\\n512+200/300\\n2024-01\\n\"\\nbillions or even trillions of parameters. These scaled-up models have exhibited a range of emergent',\n",
       " 'Microsoft\\n75M\\n32k\\n128+128\\n2023-10\\n\"\\nAST-T5[81]\\nUC Berkeley\\n226M\\n32k\\n512+200/300\\n2024-01\\n\"\\nbillions or even trillions of parameters. These scaled-up models have exhibited a range of emergent\\nabilities [97, 127, 275], such as instruction following [200], in-context learning [70], and step-by-step\\nreasoning [105, 276] that were previously unforeseen.\\nIn the domain of code generation using LLMs, the architecture of contemporary models generally\\nfalls into one of two categories: encoder-decoder models, such as CodeT5 [271], CodeT5+ [269],\\nand CodeRL [139]; or decoder-only models, such as Codex [48], StarCoder [147], Code Llama [227],\\nand CodeGemma [59]. These architectures are depicted in Figure 2(b) and (c), respectively. For a\\ncomprehensive overview, Table 7 details the encoder-decoder architectures, while Table 8 focuses\\non the decoder-only models utilized in code generation.\\n5.3.2\\nPre-training Tasks. In the initial phase, language models for code generation are typically',\n",
       " 'on the decoder-only models utilized in code generation.\\n5.3.2\\nPre-training Tasks. In the initial phase, language models for code generation are typically\\ntrained from scratch using datasets consisting of manually annotated pairs of natural language\\ndescriptions and corresponding code snippets, within a supervised learning framework. However,\\nmanual annotation is not only laborious and time-consuming, but the efficacy of the resulting\\nmodels is also constrained by both the volume and the quality of the available annotated data. This\\nlimitation is especially pronounced in the context of low-resource programming languages, such\\nas Swahili and Yoruba, where annotated examples are scarce [38, 46]. In light of these challenges,\\nthere has been a shift towards an alternative training strategy that involves pre-training models on\\nextensive and unlabelled code corpora. This method is aimed at imbuing the models with a broad',\n",
       " 'there has been a shift towards an alternative training strategy that involves pre-training models on\\nextensive and unlabelled code corpora. This method is aimed at imbuing the models with a broad\\nunderstanding of programming knowledge, encompassing elements like identifiers, code structure,\\nand underlying semantics [48]. In this regard, two pre-training tasks have gained prominence\\nfor their effectiveness, namely Causal Language Modeling (CLM), also known as unidirectional\\nlanguage modeling or next-token prediction, and Denoising Autoencoding (DAE). The CLM task\\ncan be applied to both decoder-only and encoder-decoder model architectures, while DAE tasks are\\nspecifically designed for encoder-decoder frameworks. It should also be noted that there is a variety\\nof additional auxiliary pre-training tasks that can further enhance model performance. These\\ninclude Masked Identifier Prediction, Identifier Tagging, Bimodal Dual Generation [271], Text-Code',\n",
       " 'of additional auxiliary pre-training tasks that can further enhance model performance. These\\ninclude Masked Identifier Prediction, Identifier Tagging, Bimodal Dual Generation [271], Text-Code\\nMatching, and Text-Code Contrastive Learning [269]. These tasks contribute to a more nuanced\\nand comprehensive pre-training process, equipping the models with the capabilities necessary to\\nhandle a wide range of code generation scenarios.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:27\\nTable 8. The overview of LLMs with decoder-only architectures for code generation.\\nModel\\nInstitution\\nSize\\nVocabulary\\nContext\\nWindow\\nDate\\nOpen Source\\nGPT-C [244]\\nMicrosoft\\n366M\\n60K\\n1024\\n2020-05\\nCodeGPT [172]\\nMicrosoft\\n124M\\n50K\\n1024\\n2021-02\\n\"\\nGPT-Neo[30]\\nEleutherAI\\n125M, 1.3B, 2.7B\\n50k\\n2048\\n2021-03\\n\"\\nGPT-J [258]\\nEleutherAI\\n6B\\n50k\\n2048\\n2021-05\\n\"\\nCodex [48]\\nOpenAI\\n12M, 25M, 42M,\\n85M, 300M, 679M,\\n2.5B, 12B\\n-\\n4096\\n2021-07\\nCodeParrot [254]\\nHugging Face\\n110M, 1.5B\\n33k\\n1024\\n2021-11\\n\"\\nPolyCoder [290]\\nCMU\\n160M, 400M, 2.7B\\n50k\\n2048\\n2022-02\\n\"\\nCodeGen [193]\\nSalesforce\\n350M, 2.7B, 6.1B,\\n16.1B\\n51k\\n2048\\n2022-03\\n\"\\nGPT-NeoX [29]\\nEleutherAI\\n20B\\n50k\\n2048\\n2022-04\\n\"\\nPaLM-Coder [54]\\nGoogle\\n8B, 62B, 540B\\n256k\\n2048\\n2022-04\\nInCoder [77]\\nMeta\\n1.3B, 6.7B\\n50k\\n2049\\n2022-04\\n\"\\nPanGu-Coder [55]\\nHuawei\\n317M, 2.6B\\n42k\\n1024\\n2022-07\\nPyCodeGPT [306]\\nMicrosoft\\n110M\\n32k\\n1024\\n2022-06\\n\"\\nCodeGeeX [321]\\nTsinghua\\n13B\\n52k\\n2048\\n2022-09\\n\"\\nBLOOM [140]\\nBigScience\\n176B\\n251k\\n-',\n",
       " '2049\\n2022-04\\n\"\\nPanGu-Coder [55]\\nHuawei\\n317M, 2.6B\\n42k\\n1024\\n2022-07\\nPyCodeGPT [306]\\nMicrosoft\\n110M\\n32k\\n1024\\n2022-06\\n\"\\nCodeGeeX [321]\\nTsinghua\\n13B\\n52k\\n2048\\n2022-09\\n\"\\nBLOOM [140]\\nBigScience\\n176B\\n251k\\n-\\n2022-11\\n\"\\nChatGPT [196]\\nOpenAI\\n-\\n-\\n16k\\n2022-11\\n\"\\nSantaCoder [9]\\nHugging Face\\n1.1B\\n49k\\n2048\\n2022-12\\n\"\\nLLaMA [252]\\nMeta\\n6.7B, 13.0B, 32.5B,\\n65.2B\\n32K\\n2048\\n2023-02\\n\"\\nGPT-4 [5]\\nOpenAI\\n-\\n-\\n32K\\n2023-03\\nCodeGen2 [192]\\nSalesforce\\n1B, 3.7B, 7B, 16B\\n51k\\n2048\\n2023-05\\n\"\\nreplit-code [223]\\nreplit\\n3B\\n33k\\n2048\\n2023-05\\n\"\\nStarCoder [147]\\nHugging Face\\n15.5B\\n49k\\n8192\\n2023-05\\n\"\\nWizardCoder [173]\\nMicrosoft\\n15B, 34B\\n49k\\n8192\\n2023-06\\n\"\\nphi-1 [84]\\nMicrosoft\\n1.3B\\n51k\\n2048\\n2023-06\\n\"\\nCodeGeeX2 [321]\\nTsinghua\\n6B\\n65k\\n8192\\n2023-07\\n\"\\nPanGu-Coder2 [234]\\nHuawei\\n15B\\n42k\\n1024\\n2023-07\\nLlama 2 [253]\\nMeta\\n7B, 13B, 70B\\n32K\\n4096\\n2023-07\\n\"\\nOctoCoder [187]\\nHugging Face\\n15.5B\\n49k\\n8192\\n2023-08\\n\"\\nCode Llama [227]\\nMeta\\n7B, 13B, 34B\\n32k\\n16384\\n2023-08\\n\"\\nCodeFuse [160]\\nAnt Group\\n350M, 13B, 34B\\n101k\\n4096\\n2023-09\\n\"\\nphi-1.5 [150]\\nMicrosoft',\n",
       " '\"\\nOctoCoder [187]\\nHugging Face\\n15.5B\\n49k\\n8192\\n2023-08\\n\"\\nCode Llama [227]\\nMeta\\n7B, 13B, 34B\\n32k\\n16384\\n2023-08\\n\"\\nCodeFuse [160]\\nAnt Group\\n350M, 13B, 34B\\n101k\\n4096\\n2023-09\\n\"\\nphi-1.5 [150]\\nMicrosoft\\n1.3B\\n51k\\n2048\\n2023-09\\n\"\\nCodeShell [285]\\nPeking University\\n7B\\n70k\\n8192\\n2023-10\\n\"\\nMagicoder [278]\\nUIUC\\n7B\\n32k\\n16384\\n2023-12\\n\"\\nAlphaCode 2 [11]\\nGoogle DeepMind\\n-\\n-\\n-\\n2023-12\\nStableCode [210]\\nStabilityAI\\n3B\\n50k\\n16384\\n2024-01\\n\"\\nWaveCoder [301]\\nMicrosoft\\n6.7B\\n32k\\n16384\\n2023-12\\n\"\\nphi-2 [182]\\nMicrosoft\\n2.7B\\n51k\\n2048\\n2023-12\\n\"\\nDeepSeek-Coder [88]\\nDeepSeek\\n1.3B, 6.7B, 33B\\n32k\\n16384\\n2023-11\\n\"\\nStarCoder 2 [170]\\nHugging Face\\n15B\\n49k\\n16384\\n2024-02\\n\"\\nClaude 3 [14]\\nAnthropic\\n-\\n-\\n200K\\n2024-03\\nCodeGemma [59]\\nGoogle\\n2B, 7B\\n25.6k\\n8192\\n2024-04\\n\"\\nCode-Qwen [249]\\nQwen Group\\n7B\\n92K\\n65536\\n2024-04\\n\"\\nLlama3 [180]\\nMeta\\n8B, 70B\\n128K\\n8192\\n2024-04\\n\"\\nStarCoder2-Instruct [304]\\nHugging Face\\n15.5B\\n49K\\n16384\\n2024-04\\n\"\\nCodestral [181]\\nMistral AI\\n22B\\n33k\\n32k\\n2024-05\\n\"',\n",
       " 'Qwen Group\\n7B\\n92K\\n65536\\n2024-04\\n\"\\nLlama3 [180]\\nMeta\\n8B, 70B\\n128K\\n8192\\n2024-04\\n\"\\nStarCoder2-Instruct [304]\\nHugging Face\\n15.5B\\n49K\\n16384\\n2024-04\\n\"\\nCodestral [181]\\nMistral AI\\n22B\\n33k\\n32k\\n2024-05\\n\"\\nCausal Language Modeling. In decoder-only LLMs, given a sequence of tokens x = {ùë•1, . . . ,ùë•ùëõ},\\nthe CLM task refers to autoregressively predict the target tokens ùë•ùëñbased on the preceding tokens\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:28\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nùë•<ùëñin a sequence. The causal language modeling objective for training decoder LLMs is to minimize\\nthe following likelihood:\\nLùê∑ùëíùëêùëúùëëùëíùëü‚àíùëúùëõùëôùë¶\\nùê∂ùêøùëÄ\\n(x) = ‚àílog(\\nùëõ\\n√ñ\\nùëñ=1\\nùëÉùúÉ(ùë•ùëñ| x<ùëñ)) =\\nùëõ\\n‚àëÔ∏Å\\nùëñ=1\\n‚àílog ùëÉùúÉ(ùë•ùëñ| x<ùëñ)\\n(16)\\nwhere x<ùëñrepresents the sequence of preceding tokens {ùë•1, . . . ,ùë•ùëñ‚àí1} before xùëñin the input, ùúÉ\\ndenotes the model parameters. The conditional probability ùëÉùúÉ(ùë•ùëñ|x<ùëñ)) is modeled by adding a\\ncausal attention mask to the multi-head self-attention matrix of each Transformer block. To be\\nspecific, causal attention masking is implemented by setting the lower triangular part of the\\nmatrix to 0 and the remaining elements to ‚àí‚àû, ensuring that each token ùë•ùëñattends only to its\\npredecessors and itself. On the contrary, in encoder-decoder LLMs, a pivot token ùë•ùëòis randomly\\nselected in a sequence of tokens and then regarding the context before it as the source sequence',\n",
       " 'predecessors and itself. On the contrary, in encoder-decoder LLMs, a pivot token ùë•ùëòis randomly\\nselected in a sequence of tokens and then regarding the context before it as the source sequence\\nxùëñùëõ= {ùë•1, . . . ,ùë•ùëò} of the encoder and the sequence after it as the target output xùëúùë¢ùë°= {ùë•ùëò+1, . . . ,ùë•ùëõ}\\nof decoder. Formally, the causal language modeling objective for training encoder-decoder LLMs is\\nto minimize loss function as follows:\\nLùê∏ùëõùëêùëúùëëùëíùëü‚àíùê∑ùëíùëêùëúùëëùëíùëü\\nùê∂ùêøùëÄ\\n(x) = ‚àílog(\\nùëõ\\n√ñ\\nùëñ=ùëò+1\\nùëÉùúÉ(ùë•ùëñ| x‚â§ùëò, x<ùëñ)) =\\nùëõ\\n‚àëÔ∏Å\\nùëñ=ùëò+1\\n‚àílog ùëÉùúÉ(ùë•ùëñ| x‚â§ùëò, x<ùëñ)\\n(17)\\nwhere x‚â§ùëòis the source sequence input and x<ùëñdenotes the target sequence autoregressively\\ngenerated so far. During the inference phase, pre-trained LLMs that have been trained on large-\\nscale code corpus can generate code in a zero-shot manner without the need for fine-tuning. This\\nis achieved through the technique of prompt engineering, which guides the model to produce the',\n",
       " 'scale code corpus can generate code in a zero-shot manner without the need for fine-tuning. This\\nis achieved through the technique of prompt engineering, which guides the model to produce the\\ndesired output11 [33, 214]. Additionally, recent studies have explored the use of few-shot learning,\\nalso referred to as in-context learning, to enhance model performance further [145, 206].\\nDenoising Autoencoding. In addition to causal language modeling (CLM), the denoising\\nautoencoding (DAE) task has been extensively applied in pre-training encoder-decoder architectures\\nfor code generation, such as PLBART [7], CodeT5 [271], and its enhanced successor, CodeT5+ [269].\\nFollowing T5 [217] and CodeT5 [271], the DAE refers to initially perturbing the source sequence\\nby introducing randomly masked spans of varying lengths. This corrupted sequence serves as the\\ninput for the encoder. Subsequently, the decoder employs an autoregressive strategy to reconstruct',\n",
       " 'by introducing randomly masked spans of varying lengths. This corrupted sequence serves as the\\ninput for the encoder. Subsequently, the decoder employs an autoregressive strategy to reconstruct\\nthe masked spans, integrating sentinel tokens to facilitate the generation process. This method\\nhas proven effective in improving the model‚Äôs ability to generate semantically and syntactically\\naccurate code by learning robust contextual representations [269, 271]. Formally, the denoising\\nautoencoding objective for training encoder-decoder LLMs is to minimize the following likelihood:\\nLùê∏ùëõùëêùëúùëëùëíùëü‚àíùê∑ùëíùëêùëúùëëùëíùëü\\nùê∑ùê¥ùê∏\\n(x) =\\nùëò\\n‚àëÔ∏Å\\nùëñ=1\\n‚àílog ùëÉùúÉ(xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\nùëñ\\n| x\\\\ùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†, xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\n<ùëñ\\n)\\n(18)\\nwhere ùúÉdenotes the model parameters, x\\\\ùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†is the noisy input with masked spans,\\nxùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†is the masked spans to predict from the decoder with ùëòdenoting the number of\\ntokens in xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†, and xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\n<ùëñ\\nis the span sequence autoregressively generated so far.',\n",
       " 'xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†is the masked spans to predict from the decoder with ùëòdenoting the number of\\ntokens in xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†, and xùëöùëéùë†ùëòùëíùëë_ùë†ùëùùëéùëõùë†\\n<ùëñ\\nis the span sequence autoregressively generated so far.\\nCompared with CLM, the DAE task presents a more challenging scenario, as it necessitates a deeper\\nunderstanding and capture of the intrinsic semantic relationships among token sequences by LLMs\\n[217].\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:29\\nModel learns to perform\\nmany code tasks via natural\\nlanguage instructions\\nInference on\\nunseen code\\ntask\\n(A) Train from Scratch (Transformer)\\n‚Ä¢ Randomly initialized\\nmodel parameters\\n‚Ä¢ Typically requires many\\ncode task-specific examples\\n‚Ä¢ One specialized code model\\nfor each code task\\nImprove performance via\\nfew-shot prompting or\\nprompt engineering\\nUse RL to align\\nwith human\\npreferences\\nInference on\\nunseen code\\ntask\\n(B) Prompting (StarCoder)\\n(C) Pretrain-Finetune (CodeBERT, CodeT5)\\n(E) RLHF (InstructGPT)\\n‚Ä¢ Typically requires many\\ncode task-specific examples\\n‚Ä¢ One specialized code model for\\neach code task\\n(D) Instruction Tuning (WizardCoder)\\nPretrained\\nLM\\nPretrained\\nLM\\nPretrained\\nLM\\nPretrained\\nLM\\nInference\\non task A\\nInference\\non task A\\nInference\\non task A\\nInference\\non task A\\nInference\\non task A\\nTrained LM\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nFinetune\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nRLHF',\n",
       " 'Inference\\non task A\\nInference\\non task A\\nInference\\non task A\\nTrained LM\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nFinetune\\non task A\\nInstruction-tune on\\nmany tasks:\\nB, C, D, ‚Ä¶\\nRLHF\\nTask-specific Knowledge\\nWorld/General Knowledge\\nInstruction-following with\\nMulti-task Learning\\nHuman Preference Alignment\\nModel learns to perform\\nmany code tasks via natural\\nlanguage instructions\\nFig. 9. Comparison of instruction tuning with various fine-tuning strategies and prompting for code tasks,\\nadapted from [274]. For (A), which involves training a Transformer from scratch, please refer to [6] for its use\\nin source code summarization task. In the case of (E), we utilize a representative RLHF [200] as an example.\\nAdditional reinforcement learning methods, such as DPO [216], are also applicable at this stage.\\n5.4\\nInstruction Tuning\\nAfter pre-training LLMs on large-scale datasets, the next phase typically involves augmenting the',\n",
       " '5.4\\nInstruction Tuning\\nAfter pre-training LLMs on large-scale datasets, the next phase typically involves augmenting the\\nmodel‚Äôs ability to process and follow various instructions, known as instruction tuning. Instruction\\ntuning generally refers to the supervised fine-tuning of pre-trained LLMs using datasets comprised\\nof structured examples framed as various natural language instructions [114, 200, 274, 313]. The\\ncomparison of instruction tuning with various fine-tuning strategies and prompting for code tasks\\nis depicted in Figure 9. Two exemplars of instruction data sampled from Code Alpaca [43] are\\ndemonstrated in Figure 10. It capitalizes on the heterogeneity of instruction types, positioning\\ninstruction tuning as a form of multi-task prompted training that significantly enhances the model‚Äôs\\ngeneralization to unseen tasks [56, 200, 229, 274].\\nIn the realm of code generation, natural language descriptions serve as the instructions guiding',\n",
       " 'generalization to unseen tasks [56, 200, 229, 274].\\nIn the realm of code generation, natural language descriptions serve as the instructions guiding\\nthe model to generate corresponding code snippets. Consequently, a line of research on instruction\\ntuning LLMs for code generation has garnered substantial interest across academia and industry.\\nTo perform instruction tuning, instruction data are typically compiled from source code with\\npermissive licenses [110, 132, 170] (refer to Section 5.1.2) or are constructed from synthetic code\\ndata [173, 278, 304] (refer to Section 5.2). These datasets are then utilized to fine-tune LLMs through\\na supervised learning paradigm. However, the substantial computational resources required for\\nfull parameter fine-tuning (FFT) LLM pose a notable challenge, particularly in scenarios with\\nconstrained resources [67, 153]. To mitigate this issue, parameter-efficient fine-tuning (PEFT) has',\n",
       " 'full parameter fine-tuning (FFT) LLM pose a notable challenge, particularly in scenarios with\\nconstrained resources [67, 153]. To mitigate this issue, parameter-efficient fine-tuning (PEFT) has\\nemerged as a compelling alternative strategy, gaining increasing attention for its potential to reduce\\nresource consumption [67]. In the following subsection, we categorize existing works based on\\ntheir instruction-tuning strategies to provide a comprehensive and systematic review.\\n11For more information on prompt engineering, visit https://www.promptingguide.ai\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:30\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\ndef find_primes(n): \\n    prime_list = [2] \\n    for number in range(2, n + 1): \\n        is_prime = True\\n        for k in range(2, number): \\n            if number % k == 0: \\n                is_prime = False \\n        if is_prime: \\n            prime_list.append(number) \\n    return prime_list\\nOutput:\\nInput:\\nInstruction:\\nWrite code to create a list of all \\nprime numbers between 2 and 100.\\nimport re\\nstring = \"This string contains some \\nurls such as https://www.google.com and \\nhttps://www.facebook.com.\"\\nurls = re.findall(\\'http[s]?://(?:[a-zA-\\nZ]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-\\nfA-F][0-9a-fA-F]))+\\', string) \\nprint(urls)\\nN/A\\nThis string contains some urls such as \\nhttps://www.google.com and \\nhttps://www.facebook.com.\\nGenerate a snippet of code to extract \\nall the URLs from the given string.\\nOutput:\\nInput:\\nInstruction:\\nFig. 10. Two exemplars of instruction data sampled from Code Alpaca [43] used to instruction-tune pre-',\n",
       " 'all the URLs from the given string.\\nOutput:\\nInput:\\nInstruction:\\nFig. 10. Two exemplars of instruction data sampled from Code Alpaca [43] used to instruction-tune pre-\\ntrained code LLM to enhance their alignment with natural language instructions. The instruction corpus\\nencompasses a variety of tasks, each accompanied by distinct instructions, such as prime numbers generation\\nand URLs extraction.\\n5.4.1\\nFull Parameter Fine-tuning. Full parameter fine-tuning (FFT) involves updating all parameters\\nwithin a pre-trained model, as shown in Figure 11(a). This approach is often preferred when ample\\ncomputational resources and substantial training data are available, as it typically leads to better\\nperformance. [271] introduces an encoder-decoder pre-trained language model for code generation,\\nnamed CodeT5+. They instruction-tune this model on a dataset comprising 20k instruction samples\\nfrom Code Alpaca [43], resulting in an instruction-following model called InstructCodeT5+, which',\n",
       " 'named CodeT5+. They instruction-tune this model on a dataset comprising 20k instruction samples\\nfrom Code Alpaca [43], resulting in an instruction-following model called InstructCodeT5+, which\\nexhibited improved capabilities in code generation. [173] leverages the Evol-Instruct data synthesis\\ntechnique from WizardLM [289] to evolve 20K code Alpaca [43] instruction samples into a 78K\\ncode instruction dataset. This enriched dataset is then used to fine-tune the StarCoder base model,\\nresulting in WizardCoder, which showcases notable advancements in code generation. In a similar\\nvein, inspired by the successes of WizardCoder [173] and RRHF [303], Pangu-Coder 2 [234] applies\\nthe Evol-Instruct method to generate 68k high-quality instruction samples from the initial 20k Code\\nAlpaca [43] instruction samples. Additionally, they introduces a novel reinforcement learning via\\nRank Responses to align Test & Teacher Feedback (RRTF), which further enhances the performance',\n",
       " 'Alpaca [43] instruction samples. Additionally, they introduces a novel reinforcement learning via\\nRank Responses to align Test & Teacher Feedback (RRTF), which further enhances the performance\\nof Pangu-Coder 2 in code generation. Diverging from synthetic instruction data generation methods,\\nOctoPack [187] utilizes real-world data by curating CommitPack from the natural structure of\\nGit commits, which inherently pair code changes with human-written instructions. This dataset,\\nconsisting of 4 terabytes of Git commits across 350 programming languages, is employed to fine-\\ntune StarCoder [147] and CodeGeeX2 [321], leading to the instruction-following code models of\\nOctoCoder and OctoGeeX for code generation, respectively. The most recent innovation comes\\nfrom Magicoder [278], who proposes OSS-INSTRUCT, a novel data synthesis method that leverages\\nopen-source code snippets to generate high-quality instruction data for code generation. This',\n",
       " 'from Magicoder [278], who proposes OSS-INSTRUCT, a novel data synthesis method that leverages\\nopen-source code snippets to generate high-quality instruction data for code generation. This\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:31\\napproach seeks to reduce the bias often present in synthetic data generated by LLM. In line with\\nOSS-INSTRUCT, the BigCode team introduces StarCoder2-15B-instruct [304], which they claim\\nto be the first entirely self-aligned LLM for code generation, trained with a fully permissive and\\ntransparent pipeline. Moreover, [59] harnesses open-source mathematics datasets, such as MATH\\n[95] and GSM8k [58], along with synthetically generated code following the OSS-INSTRUCT\\n[278] paradigm, to instruction-tune CodeGemma 7B, yielding exceptional results in mathematical\\nreasoning and code generation tasks.\\n5.4.2\\nParameter-Efficient Fine-tuning. To mitigate the extensive computational and resource de-\\nmands inherent in fine-tuning LLMs, the concept of parameter-efficient fine-tuning (PEFT) has\\nemerged to focus on updating a minimal subset of parameters, which may either be a selection of',\n",
       " 'mands inherent in fine-tuning LLMs, the concept of parameter-efficient fine-tuning (PEFT) has\\nemerged to focus on updating a minimal subset of parameters, which may either be a selection of\\nthe model‚Äôs parameters or an array of additional parameters specifically introduced for the tuning\\nprocess [67, 153]. The categorization of these methods is depicted in Figure 11(b), (c), and (d). A\\nplethora of innovative PEFT approaches have been developed, among which BitFit [305], Adapter\\n[102], Prompt tuning [142], Prefix-tuning [149], LoRA [103], IA3 [161], QLoRA [65], and AdaLoRA\\n[312] are particularly noteworthy. A seminal study in this field, LoRA [103], proposes a parameter\\nupdate mechanism for a pre-trained weight matrix ‚Äî such as those found in the key or value\\nprojection matrices of a Transformer block‚Äôs multi-head self-attention layer ‚Äî by factorizing the\\nupdate into two low-rank matrices. Crucially, all original model parameters remain frozen, with',\n",
       " 'projection matrices of a Transformer block‚Äôs multi-head self-attention layer ‚Äî by factorizing the\\nupdate into two low-rank matrices. Crucially, all original model parameters remain frozen, with\\nonly the pair of low-rank matrices being trainable. After fine-tuning, the product of these low-rank\\nmatrices can be seamlessly incorporated into the existing weight matrix through an element-wise\\naddition. This process can be formally described as:\\n(W0 + ŒîW)ùë•= W0ùë•+ ŒîWùë•= Wùëìùëüùëúùëßùëíùëõ\\n0\\nùë•+ ùõº\\nùëüBùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùë¢ùëù\\nAùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùëëùëúùë§ùëõ\\n|                     {z                     }\\nŒîW\\nùë•\\n(19)\\nwhere W0 ‚ààRùëë√óùëòdenotes a pre-trained weight matrix, Bùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùë¢ùëù\\n‚ààRùëë√óùëüand Aùë°ùëüùëéùëñùëõùëéùëèùëôùëí\\nùëëùëúùë§ùëõ\\n‚ààRùëü√óùëòare\\ntwo trainable low-rank matrixes and initialized by a zero matrix and a random Gaussian distribution\\nN (0, ùúé2) respectively, to ensure ŒîW = 0 at the beginning of training. The rank ùëü‚â™min(ùëë,ùëò), the\\nùõº\\nùëüis a scaling coefficient to balance the importance of the LoRA module, like a learning rate.',\n",
       " 'N (0, ùúé2) respectively, to ensure ŒîW = 0 at the beginning of training. The rank ùëü‚â™min(ùëë,ùëò), the\\nùõº\\nùëüis a scaling coefficient to balance the importance of the LoRA module, like a learning rate.\\nDespite the advancements in PEFT methods, their application in code generation remains limited.\\nFor instance, [121] pioneered the use of parameter-efficient instruction-tuning on a Llama 2 [253]\\nmodel with a single RTX 3090 GPU, leading to the development of a multilingual code generation\\nmodel called CodeUp. More recently, ASTRAIOS [334] conducted a thorough empirical examination\\nof parameter-efficient instruction tuning for code comprehension and generation tasks. This study\\nyielded several perceptive observations and conclusions, contributing valuable insights to the\\ndomain.\\n5.5\\nReinforcement Learning with Feedback\\nLLMs have exhibited remarkable instruction-following capabilities through instruction tuning.',\n",
       " 'domain.\\n5.5\\nReinforcement Learning with Feedback\\nLLMs have exhibited remarkable instruction-following capabilities through instruction tuning.\\nHowever, they often produce outputs that are unexpected, toxic, biased, or hallucinated outputs that\\ndo not align with users‚Äô intentions or preferences [118, 200, 272]. Consequently, aligning LLMs with\\nhuman preference has emerged as a pivotal area of research. A notable work is InstructGPT [200],\\nwhich further fine-tunes an instruction-tuned model utilizing reinforcement learning with human\\nfeedback (RLHF) on a dataset where labelers have ranked model outputs in order of quality, from\\nbest to worst. This method has been instrumental in the development of advanced conversational\\nlanguage models, such as ChatGPT [196] and Bard [177]. Despite its success, acquiring high-quality\\nhuman preference ranking data is a resource-intensive process [141]. To address this, Reinforcement\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:32\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTrainable\\nInput\\nLayer N+1\\nüî•\\nLayer N\\nLayer 1\\nLayer 2\\n...\\nInput\\nLayer 2\\nüî•\\nLayer N\\nLayer 1\\n...\\nüî•\\nFrozen\\nInput\\n...\\nLayer 2\\nüî•\\nLayer 1\\nüî•\\nLayer N\\nüî•\\nInput\\nLayer N\\nLayer 1\\nLayer 2\\n‚ñ≥WN\\nüî•\\n‚ñ≥W2\\nüî•\\n‚ñ≥W1\\nüî•\\n...\\n...\\n(a) Full Fine-tuning\\n(b) Specification\\n(c) Addition\\n(d) Reparameterization\\nFig. 11. An illustration of full parameter fine-tuning (FFT) and parameter-efficient fine-tuning (PEFT) methods.\\n(a) refers to the Full Fine-tuning method, which updates all parameters of the base model during fine-tuning.\\n(b) stands for the Specification-based PEFT method that conditionally fine-tunes a small subset of the model\\nparameters while freezing the rest of the model, e.g. BitFit [305]. (c) represents the Addition-based PEFT\\nmethod that fine-tunes the incremental parameters introduced into the base model or input, e.g. Adapter\\n[102], Prefix-tuning [149], and Prompt-tuning [142]. (d) symbolizes the Reparameterization-based method',\n",
       " '[102], Prefix-tuning [149], and Prompt-tuning [142]. (d) symbolizes the Reparameterization-based method\\nwhich reparameterizes existing model parameters by low-rank transformation, e.g. LoRA [103], QLoRA [65],\\nand AdaLoRA [312].\\nLearning from AI Feedback (RLAIF) [21, 141] has been proposed to leverage powerful off-the-shelf\\nLLMs (e.g., ChatGPT [196] and GPT-4 [5]) to simulate human annotators by generating preference\\ndata.\\nBuilding on RLHF‚Äôs success, researchers have explored reinforcement learning with feedback to\\nenhance code generation in LLMs. Unlike RLHF, which relies on human feedback, this approach\\nemploys compilers or interpreters to automatically provide feedback on code samples through code\\nexecution on unit test cases, catalyzing the advancement of this research domain. CodeRL [139]\\nintroduced an actor-critic reinforcement learning framework for code generation. In this setup, the\\nlanguage model serves as the actor-network, while a token-level functional correctness reward',\n",
       " 'introduced an actor-critic reinforcement learning framework for code generation. In this setup, the\\nlanguage model serves as the actor-network, while a token-level functional correctness reward\\npredictor acts as the critic. Generated code is assessed through unit test signals from a compiler,\\nwhich can indicate compiler errors, runtime errors, unit test failures, or passes. CompCoder [266]\\nenhances code compilability by employing compiler feedback, including language model fine-\\ntuning, compilability reinforcement, and compilability discrimination strategies. Subsequently,\\nPPOCoder [238] integrates pre-trained code model CodeT5 [271] with Proximal Policy Optimization\\n(PPO) [230]. This integration not only utilizes execution (i.e., compilers or interpreters) feedback to\\nassess syntactic and functional correctness but also incorporates a reward function that evaluates\\nthe syntactic and semantic congruence between abstract syntax tree (AST) sub-trees and data flow',\n",
       " 'assess syntactic and functional correctness but also incorporates a reward function that evaluates\\nthe syntactic and semantic congruence between abstract syntax tree (AST) sub-trees and data flow\\ngraph (DFG) edges in the generated code against the ground truth. Additionally, the framework\\napplies a KL-divergence penalty to maintain fidelity between the actively learned policy and the\\nreferenced pre-trained model, enhancing the optimization process. More recently, RLTF [163] has\\nproposed an online reinforcement learning framework that provides fine-grained feedback based\\non compiler error information and location, along with adaptive feedback that considers the ratio\\nof passed test cases.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:33\\nDespite these successes, reinforcement learning algorithms face inherent limitations such as\\ninefficiency, instability, extensive resource requirements, and complex hyperparameter tuning,\\nwhich can impede the performance and scalability of LLMs. To overcome these challenges, recent\\nstudies have introduced various variants of RL methods that do not rely on PPO, including DPO\\n[216], RRHF [303], and sDPO [129]. In essence, these methods aim to maximize the likelihood\\nbetween the logarithm of conditional probabilities of preferred and rejected responses, which may\\nbe produced by LLMs with varying capabilities. Inspired by RRHF [303], PanGu-Coder 2 [234]\\nleverages a novel framework, Reinforcement Learning via Rank Responses to align Test & Teacher\\nFeedback (RRTF), significantly enhancing code generation capabilities, as evidenced by pass@1 of\\n62.20% on the HumanEval benchmark.',\n",
       " 'Feedback (RRTF), significantly enhancing code generation capabilities, as evidenced by pass@1 of\\n62.20% on the HumanEval benchmark.\\nTaking a step forward, the integration of more non-differentiable code features, such as coding\\nstyle [44, 178] and readability [34], into the reinforcement learning feedback for LLM-based code\\ngeneration, presents an exciting avenue for future research.\\n5.6\\nPrompting Engineering\\nLarge-scale language models (LLMs) such as GPT-3 and its successors have been trained on large-\\nscale data corpora, endowing them with substantial world knowledge [33, 200, 274]. Despite this,\\ncrafting an effective prompting as a means of communicating with LLMs to harness their full\\npotential remains a long-standing challenge [164]. Recent advancements in prompting engineering\\nhave expanded the capabilities of LLMs, enabling more sophisticated task completion and enhancing\\nboth reliability and performance. Notable techniques include Chain-of-Thought (CoT) [276], Self-',\n",
       " 'have expanded the capabilities of LLMs, enabling more sophisticated task completion and enhancing\\nboth reliability and performance. Notable techniques include Chain-of-Thought (CoT) [276], Self-\\nConsistency [267], Tree-of-Thought (ToT) [295], Program of Thoughts (PoT) [50], Reasoning via\\nPlanning (RAP) [93], ReAct [296], Self-Refine [176], Reflexion [236], and LATS [327]. For instance,\\nCoT significantly improves the LLMs‚Äô ability to perform complex reasoning by providing a few\\nchain-of-thought demonstrations as exemplars in prompting.\\nPrompting engineering is particularly advantageous as it bypasses the need for additional training\\nand can significantly elevate performance. Consequently, numerous studies have leveraged this\\ntechnique for iterative and self-improving (refining) code generation within proprietary LLMs\\nsuch as ChatGPT and GPT-4. Figure 12 illustrates the general pipeline for self-improving code',\n",
       " 'technique for iterative and self-improving (refining) code generation within proprietary LLMs\\nsuch as ChatGPT and GPT-4. Figure 12 illustrates the general pipeline for self-improving code\\ngeneration with LLMs. For instance, Self-Debugging [51] involves prompting an LLM to iteratively\\nrefine a predicted program by utilizing feedback composed of code explanations combined with\\nexecution results, which assists in identifying and rectifying errors. When unit tests are unavailable,\\nthis feedback can rely solely on code explanations. Similarly, LDB [325] prompts LLMs to refine\\ngenerated code by incorporating debugging feedback, which consists of the evaluation of the\\ncorrectness of variable values throughout runtime execution, as assessed by the LLMs. In parallel,\\nSelfEvolve [122] employs a two-stage process where LLMs first generate domain-specific knowledge\\nfor a problem, followed by a trial code. This code is then iteratively refined through interactive',\n",
       " 'SelfEvolve [122] employs a two-stage process where LLMs first generate domain-specific knowledge\\nfor a problem, followed by a trial code. This code is then iteratively refined through interactive\\nprompting and execution feedback. An empirical investigation by [195] provides a comprehensive\\nanalysis of the self-repairing capabilities for code generation in models like Code Llama, GPT-\\n3.5, and GPT-4, using problem sets from HumanEval and APPS. This study yields a series of\\ninsightful observations and findings, shedding light on the self-refinement effectiveness of these\\nLLMs. Moreover, Reflexion [236] introduces a general approach for code generation wherein LLM-\\npowered agents engage in verbal self-reflection on task feedback signals, storing these reflections\\nin an episodic memory buffer to inform and improve decision-making in subsequent interactions.\\nLATS [327] adopts a novel strategy, utilizing LLMs as agents, value functions, and optimizers. It',\n",
       " 'in an episodic memory buffer to inform and improve decision-making in subsequent interactions.\\nLATS [327] adopts a novel strategy, utilizing LLMs as agents, value functions, and optimizers. It\\nenhances decision-making by meticulously constructing trajectories through Monte Carlo Tree\\nSearch (MCTS) algorithms, integrating external feedback, and learning from experience. This\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " \"1:34\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nStep 2: Trajectory\\nStep 3: Evaluation\\nStep 4: Self-Reflection\\nStep 1: Code Task\\nFeedback\\nCode LLM\\nExecutor\\nWrite a Python script to \\nprint all unique elements \\nin a list.\\ndef unique_elements(lst):\\nresult = set(lst)\\n    return list(result)\\nassert unique_elements\\n(['a', 'b', 'c', 'a', 'd']) \\n== ['a', 'b', 'c', 'd'] \\n[‚Ä¶] does not work as \\nexpected because it uses \\nthe built-in `set()` \\nfunction in Python, which \\ndoes not maintain the order \\nof elements.[‚Ä¶]\\n(Optional)\\nassert unique_elements([1, \\n2, 3, 4, 4]) == [1, 2, 3, 4]\\n(Code) LLM\\nFig. 12. An illustration of the self-improving code generation pipeline using prompts for LLMs. This process\\nincorporates iterative self-refinement by integrating execution outcomes and includes an optional self-\\nreflection mechanism to enhance generation quality.\\napproach has demonstrated remarkable results in code generation, achieving a pass@1 of 94.4% on\",\n",
       " 'reflection mechanism to enhance generation quality.\\napproach has demonstrated remarkable results in code generation, achieving a pass@1 of 94.4% on\\nthe HumanEval benchmark with GPT-4.\\nDistinct from the aforementioned methods, CodeT [45] and LEVER [190] prompt LLMs to\\ngenerate numerous code samples, which are then re-ranked based on execution outcomes to select\\nthe optimal solution. Notably, these approaches do not incorporate a self-refinement step to further\\nimprove code generation.\\n5.7\\nRepository Level & Long Context\\nIn contemporary software engineering practices, modifications to a code repository are widespread\\nand encompass a range of activities, including package migration, temporary code edits, and the\\nresolution of GitHub issues. While LLMs showcase impressive prowess in function-level code\\ngeneration, they often falter when grappling with the broader context inherent to a repository,',\n",
       " 'resolution of GitHub issues. While LLMs showcase impressive prowess in function-level code\\ngeneration, they often falter when grappling with the broader context inherent to a repository,\\nsuch as import dependencies, parent classes, and files bearing similar names. These deficiencies\\nresult in suboptimal performance in repository-level code generation, as identified in recent studies\\n[239, 240]. The challenges faced by LLMs in this domain are primarily due to the following factors:\\n‚Ä¢ Code repositories typically contain intricate interdependencies scattered across various\\nfiles, including shared utilities, configurations, and cross-API invocations, which arise from\\nmodular design principles [22, 309].\\n‚Ä¢ Repositories are characterized by their unique structures, naming conventions, and coding\\nstyles, which are essential for maintaining clarity and facilitating ongoing maintenance [44].\\n‚Ä¢ The vast context of an entire repository often exceeds the context length limitations of LLMs,',\n",
       " 'styles, which are essential for maintaining clarity and facilitating ongoing maintenance [44].\\n‚Ä¢ The vast context of an entire repository often exceeds the context length limitations of LLMs,\\nthus hindering their ability to integrate comprehensive contextual information [22].\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:35\\n‚Ä¢ LLMs may not have been adequately trained on extensive sets of repository data, such as\\nproprietary software or projects that are still in development [239].\\nGiven that the scope of a typical software repository encompasses hundreds of thousands of\\ntokens, it is imperative to enhance the capacity of LLMs to handle extensive contexts when they\\nare employed for repository-level code generation. Fortunately, recent advancements in positional\\nencoding techniques, such as ALiBi [211] and RoPE [243], have shown promise in improving the\\nTransformer‚Äôs ability to generalize from shorter training sequences to longer inference sequences\\n[318]. This progress addresses the third challenge mentioned above to a certain degree, thereby\\nenabling better contextualization of coding activities within full repositories.\\nTo further refine LLMs for repository-level code completion, several innovative approaches have',\n",
       " 'enabling better contextualization of coding activities within full repositories.\\nTo further refine LLMs for repository-level code completion, several innovative approaches have\\nbeen introduced. RepoCoder [309] leverages a similarity-based retrieval system within an iterative\\nretrieval-generation paradigm to enrich the context and enhance code completion quality. In a\\nsimilar vein, CoCoMIC [69] employs a cross-file context finder named CCFINDER to pinpoint and\\nretrieve the most relevant cross-file contexts within a repository. RepoHyper [209] introduces a\\nsemantic graph structure, termed RSG, to encapsulate the expansive context of code repositories\\nand uses an ‚ÄúExpand and Refine‚Äù retrieval method to obtain relevant code snippets. Moreover, a\\nframework known as RLPG [240] has been proposed to generate repository-level prompts that\\nintegrate the repository‚Äôs structure with the relevant context across all files. However, the constant',\n",
       " 'framework known as RLPG [240] has been proposed to generate repository-level prompts that\\nintegrate the repository‚Äôs structure with the relevant context across all files. However, the constant\\nreliance on retrieval mechanisms has raised concerns regarding efficiency and robustness, as some\\nretrieved contexts may prove unhelpful or harmful. In response, Repoformer [282] introduces a\\nselective Retrieval-Augmented Generation (RAG) framework that judiciously bypasses retrieval\\nwhen it is deemed redundant. This approach incorporates a self-supervised learning strategy that\\nequips a code LLM with the ability to perform a self-assessment on the utility of retrieval for\\nenhancing the quality of its output, thereby effectively utilizing potentially noisy retrieved contexts.\\nAdditionally, RepoFusion [239] has been developed to train models to combine multiple relevant\\ncontexts from a repository, aiming to produce more precise and context-aware code completions.',\n",
       " 'Additionally, RepoFusion [239] has been developed to train models to combine multiple relevant\\ncontexts from a repository, aiming to produce more precise and context-aware code completions.\\nIn a novel approach, Microsoft‚Äôs CodePlan [22] frames repository-level coding tasks as a planning\\nproblem, generating a multi-step chain of edits (plan) where each step involves invoking an LLM on a\\nspecific code location, considering context from the entire repository, preceding code modifications,\\nand task-specific instructions.\\nAdvancing the state-of-the-art, [308] tackles the formidable challenge of NL2Repo, an endeavor\\nthat seeks to create a complete code repository from natural language requirements. To address\\nthis complex task, they introduce the CodeS framework, which strategically breaks down NL2Repo\\ninto a series of manageable sub-tasks using a multi-layer sketch approach. The CodeS framework',\n",
       " 'this complex task, they introduce the CodeS framework, which strategically breaks down NL2Repo\\ninto a series of manageable sub-tasks using a multi-layer sketch approach. The CodeS framework\\ncomprises three distinct modules: 1) RepoSketcher, for creating a directory structure of the reposi-\\ntory based on given requirements; 2) FileSketcher, for sketching out each file within that structure;\\nand 3) SketchFiller, for fleshing out the specifics of each function within the file sketches [308].\\nAccordingly, a surge of benchmarks tailored for repository-level code generation has emerged,\\nsuch as RepoEval [309], Stack-Repo [239], Repobench [167], EvoCodeBench [144], SWE-bench\\n[123], CrossCodeEval [68], and SketchEval [308]. The detailed statistics and comparisons of these\\nbenchmarks are presented in Table 6.\\nDespite the progress made by these methods in repository-level code generation, significant chal-',\n",
       " 'benchmarks are presented in Table 6.\\nDespite the progress made by these methods in repository-level code generation, significant chal-\\nlenges remain to be addressed. Programming developers are often required to invest considerable\\ntime in editing and debugging [25, 28, 186, 239, 255]. However, the advent of LLM-powered coding\\nagents, such as AutoCodeRover [316], SWE-Agent [124], and OpenDevin [199], has demonstrated\\ntheir potential to tackle complex problems, paving the way for future exploration in this field (for\\nmore details, see Section 5.9).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:36\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nEmbedding \\nModel\\nQuery\\nRetrieved Context\\nCreate a quick-\\nsort algorithm \\nin Python.\\nCombine Prompts and Context\\nCreate a quick-sort algorithm \\nin Python.\\nPlease solve the above problem \\nbased on the following context:\\n{context}\\ndef quick_sort(arr):\\n\"\"\"Sort a list of numbers in ascending \\norder using the Quick-Sort algorithm\"\"\"\\nif len(arr) == 0:\\nreturn []\\npivot = arr[0]\\nleft_arr = [x for x in arr if x < pivot]\\nright_arr = [x for x in arr if x > pivot]\\nreturn quick_sort(left_arr) + [pivot] + \\nquick_sort(right_arr)\\nCode LLM\\nEmbedding \\nModel\\nVector \\nDatabase\\nCode Solution\\nCode Data \\nChunks\\nOpen \\nSource\\nStage 1: Retrieval\\nStage 2: Generation\\nAlgorithm:\\n1. If the input \\narray...already \\nsorted....\\n5. Recursively \\ncall quicksort...\\nFig. 13. A workflow illustration of the Retrieval-Augmented Code Generation (RACG). Upon receiving a query',\n",
       " 'Algorithm:\\n1. If the input \\narray...already \\nsorted....\\n5. Recursively \\ncall quicksort...\\nFig. 13. A workflow illustration of the Retrieval-Augmented Code Generation (RACG). Upon receiving a query\\n(instruction), the retriever selects the relevant contexts from a large-scale vector database. Subsequently, the\\nretrieved contexts are merged with the query, and this combined input is fed into the generator (LLM) to\\nproduce the target code solution.\\n5.8\\nRetrieval Augmented\\nLLMs have exhibited impressive capabilities but are hindered by several critical issues such as\\nhallucination [154, 315], obsolescence of knowledge [115], and non-transparent [32], untraceable\\nreasoning processes [80, 106, 276, 329]. While techniques like instruction-tuning (see Section 5.4)\\nand reinforcement learning with feedback (see Section 5.5) mitigate these issues, they also introduce\\nnew challenges, such as catastrophic forgetting and the requirement for substantial computational',\n",
       " 'and reinforcement learning with feedback (see Section 5.5) mitigate these issues, they also introduce\\nnew challenges, such as catastrophic forgetting and the requirement for substantial computational\\nresources during training [90, 201].\\nRecently, Retrieval-Augmented Generation (RAG) has emerged as an innovative approach to\\novercoming these limitations by integrating knowledge from external databases. Formally defined,\\nRAG denotes a model that, in response to queries, initially sources relevant information from\\nan extensive corpus of documents, and then leverages this retrieved information in conjunction\\nwith the original query to enhance the response‚Äôs quality and accuracy, especially for knowledge-\\nintensive tasks. The RAG framework typically consists of a vector database, a retriever, a re-ranker,\\nand a generator. It is commonly implemented using tools such as LangChain12 and LLamaIndex13.\\nBy performing continuous knowledge updates of the database and the incorporation of domain-',\n",
       " 'and a generator. It is commonly implemented using tools such as LangChain12 and LLamaIndex13.\\nBy performing continuous knowledge updates of the database and the incorporation of domain-\\nspecific data, RAG circumvents the need for re-training LLMs from scratch [80]. Consequently,\\nRAG has substantially advanced LLM performance across a variety of tasks [47, 143].\\nDue to the nature of code, code LLMs are also susceptible to the aforementioned issues that\\naffect general-purpose LLMs. For instance, they may exhibit a hallucination phenomenon when\\ninstructions fall outside the scope of their training data or necessitate the latest programming\\npackages. Given the dynamic nature of publicly available source-code libraries like PyTorch, which\\nundergo frequent expansion and updates, deprecated calling methods can become a significant\\nchallenge. If Code LLMs are not updated in tandem with the latest functions and APIs, this can',\n",
       " 'undergo frequent expansion and updates, deprecated calling methods can become a significant\\nchallenge. If Code LLMs are not updated in tandem with the latest functions and APIs, this can\\nintroduce potential errors and safety risks. Retrieval-Augmented Code Generation (RACG) stands\\n12LangChain facilitates the development of LLM-powered applications. https://www.langchain.com\\n13LLamaIndex is a leading data framework for building LLM applications. https://www.llamaindex.ai\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:37\\nas a promising solution to these concerns. A workflow illustration of the RACG is depicted in\\nFigure 13.\\nDespite its potential, the adoption of RAG for code generation remains limited. Drawing in-\\nspiration from the common practice among programmers of referencing related code snippets,\\n[166] introduced a novel retrieval-augmented mechanism with graph neural networks (GNNs),\\ntermed HGNN, which unites the advantages of similar examples retrieval with the generalization\\ncapabilities of generative models for code summarization, which is the reverse process of code\\ngeneration. [205] pioneered a retrieval augmented framework named REDCODER for code gener-\\nation by retrieving and integrating relevant code snippets from a source-code database, thereby\\nproviding supplementary context for the generation process. Subsequently, a retrieval-augmented',\n",
       " 'ation by retrieving and integrating relevant code snippets from a source-code database, thereby\\nproviding supplementary context for the generation process. Subsequently, a retrieval-augmented\\ncode completion framework termed ReACC [171] is proposed to leverage both lexical copying and\\nsemantic referencing of related code, achieving state-of-the-art performance on the CodeXGLUE\\nbenchmark [172]. In the spirit of how programmers often consult textual resources such as code\\nmanuals and documentation to comprehend functionalities, DocPrompting [330] explicitly utilizes\\ncode documentation by retrieving the relevant documentation pieces based on a natural language\\nquery and then generating the target code by blending the query with the retrieved information.\\nMore recently, RepoCoder [309], an iterative retrieval-generation framework, is proposed for\\nenhancing repository-level code completion by effectively utilizing code analogies across different',\n",
       " 'More recently, RepoCoder [309], an iterative retrieval-generation framework, is proposed for\\nenhancing repository-level code completion by effectively utilizing code analogies across different\\nfiles within a repository to inform and improve code suggestions. Furthermore, breaking away\\nfrom reliance on a singular source of retrieval, [242] developed a multi-faceted ‚Äúknowledge soup‚Äù\\nthat integrates web searches, documentation, execution feedback, and evolved code snippets. Then,\\nit incorporates an active retrieval strategy that iteratively refines the query and enriches the\\nknowledge soup, expanding the scope of information available for code generation.\\nDespite these advancements, several limitations in retrieval-augmented code generation warrant\\nfurther exploration: 1) the quality of the retrieved information significantly impacts overall perfor-\\nmance; 2) the effective integration of retrieved code information with the query needs optimization;',\n",
       " 'mance; 2) the effective integration of retrieved code information with the query needs optimization;\\n3) an over-reliance on retrieved information may lead to inadequate responses that fail to address\\nthe query‚Äôs intent; 4) additional retrieved information necessitates larger context windows for the\\nLLM, resulting in increased computational demands.\\n5.9\\nAutonomous Coding Agents\\nThe advent of LLMs has marked the beginning of a new era of potential pathways toward artificial\\ngeneral intelligence (AGI), capturing significant attention in both academia and industry [108, 261,\\n279, 284]. A rapidly expanding array of applications for LLM-based autonomous agents, including\\nAutoGPT [2], AgentGPT [1], BabyAGI [3], and AutoGen [283], underlines the promise of this\\ntechnology.\\nLLM-powered autonomous agents are systems endowed with sophisticated reasoning abilities,\\nleveraging an LLM as a central computational engine or controller. This allows them to formulate',\n",
       " 'technology.\\nLLM-powered autonomous agents are systems endowed with sophisticated reasoning abilities,\\nleveraging an LLM as a central computational engine or controller. This allows them to formulate\\nand execute problem-solving plans through a series of tool-enabled functions or API calls. Moreover,\\nthese agents are designed to function within a shared environment where they can communicate\\nand engage in cooperative, competitive, or negotiating interactions [104, 261, 283]. The typical\\narchitecture of such an agent encompasses an LLM-based Agent, a memory module, a planning\\ncomponent, and a tool utilization module, as depicted in Figure 14.\\nIn the realm of automated code generation, LLM-powered autonomous agents have demon-\\nstrated remarkable proficiency. For instance, AgentCoder [104] achieved a groundbreaking pass@1\\nof 96.3% on the HumanEval benchmark, forwarding a step closer to the future of automated soft-',\n",
       " 'strated remarkable proficiency. For instance, AgentCoder [104] achieved a groundbreaking pass@1\\nof 96.3% on the HumanEval benchmark, forwarding a step closer to the future of automated soft-\\nware development [111]. The innovative meta-programming framework termed MetaGPT [100]\\nintegrates human workflow efficiencies into LLM-based multi-agent collaboration, as shown in\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:38\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nAgent\\nMemory\\nAction\\nTools\\nPlanning\\nShort-term Memory\\nLong-term Memory\\nCalendar ( )\\nCalculator ( )\\nCode Interpreter ( )\\nSearch ( )\\n...more\\nReflection\\nSelf-critics\\nChain of Thoughts\\nSubgoal Decomposition\\nFig. 14. The general architecture of an LLM-powered autonomous agent system, adapted from [279]. Plan-\\nning: The agent decomposes large tasks into smaller, manageable sub-goals or engages in self-criticism\\nand self-reflection on past actions to learn from mistakes and improve future performance. Memory: This\\ncomponent enables the agent to store and retrieve past information. Tools: The agent is trained to invoke\\nexternal functions or APIs. Action: The agent executes actions, with or without the use of tools, to interact\\nwith the environment. The gray dashed lines represent the data flow within the system.\\nFig. 15. MetaGPT integrates human workflow efficiencies into LLM-based multi-agent collaboration to break',\n",
       " 'with the environment. The gray dashed lines represent the data flow within the system.\\nFig. 15. MetaGPT integrates human workflow efficiencies into LLM-based multi-agent collaboration to break\\ndown complex code-related tasks into specific, actionable procedures. These procedures are then assigned to\\nvarious roles, such as Product Manager, Architect, and Engineer played by LLM. The image is sourced from\\nthe original paper [100].\\nFigure 15. Furthermore, [104] introduces AgentCoder, a multi-agent framework composed of three\\nspecialized agents, each with distinct roles and capabilities. These roles include a programmer agent\\nresponsible for code generation, a test designer agent tasked with generating unit test cases, and\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:39\\na test executor agent that executes the code and provides feedback. This division of labor within\\nAgentCoder promotes more efficient and effective code generation. CodeAct [265] distinguishes\\nitself by utilizing executable Python code to consolidate LLM agent actions within a unified action\\nspace, in contrast to the generation of JSON or textual formats. Additionally, AutoCodeRover [316]\\nis proposed to autonomously resolve GitHub issues for program enhancement.\\nTo address the complexity of tasks within software engineering, two innovative autonomous AI\\nsoftware engineers Devin14[61] and OpenDevin15[199], have been released and rapidly garnered\\nconsiderable interest within the software engineering (SE) and artificial general intelligence (AGI)\\ncommunity. Subsequently, an autonomous system, SWE-agent [124], leverages a language model',\n",
       " 'considerable interest within the software engineering (SE) and artificial general intelligence (AGI)\\ncommunity. Subsequently, an autonomous system, SWE-agent [124], leverages a language model\\nto interact with a computer to address software engineering tasks, successfully resolving 12.5% of\\nissues on the SWE-bench benchmark [123]. L2MAC [98] has been introduced as the first practical,\\nLLM-based, multi-agent, general-purpose stored-program automatic computer that utilizes a von\\nNeumann architecture, designed specifically for the generation of long and consistent outputs.\\nAt the time of writing this survey, OpenDevin has enhanced CodeAct with bash command-based\\ntools, leading to the release of OpenDevin CodeAct 1.0 [287], which sets a new state-of-the-art\\nperformance on the SWE-Bench Lite benchmark [123].\\nDespite these remarkable advancements, the journey toward fully realized AI software engineers\\nemploying LLM-powered autonomous agents is far from complete [261, 284]. Critical aspects',\n",
       " 'Despite these remarkable advancements, the journey toward fully realized AI software engineers\\nemploying LLM-powered autonomous agents is far from complete [261, 284]. Critical aspects\\nsuch as prompt design, context length, agent count, and toolsets call for further refinement and\\noptimization, especially as problem complexities escalate [111].\\n5.10\\nEvaluation\\nDespite the impressive capabilities of LLMs, they exhibit a range of behaviors that are both beneficial\\nand potentially risky. These behaviors can enhance performance across various downstream tasks\\nbut may also introduce reliability and trustworthiness concerns in LLM deployment [42, 48, 290].\\nConsequently, it is imperative to develop precise evaluation approaches to discern the qualitative\\nand quantitive differences between models, thereby encouraging further advancements in LLM\\ncapabilities.\\nEvaluation strategies for LLMs in code generation mirror those for general-purpose LLMs and',\n",
       " 'and quantitive differences between models, thereby encouraging further advancements in LLM\\ncapabilities.\\nEvaluation strategies for LLMs in code generation mirror those for general-purpose LLMs and\\ncan be divided into three principal categories: metrics-based, human-centered, and LLM-based\\napproaches. Detailed benchmarks for these evaluation strategies are presented in Section 5.1.3 and\\nsummarized in Table 6. Subsequent subsections will provide a thorough analysis of each approach.\\n5.10.1\\nMetrics. The pursuit of effective and reliable automatic evaluation metrics for generated\\ncontent is a long-standing challenge within the field of natural language processing (NLP) [49, 156,\\n203]. At the early stage, most works directly leverage token-matching-based metrics, such as Exact\\nMatch, BLEU [203], ROUGE [156], and METEOR [23], which are prevalent in text generation of\\nNLP, to assess the quality of code generation.',\n",
       " 'Match, BLEU [203], ROUGE [156], and METEOR [23], which are prevalent in text generation of\\nNLP, to assess the quality of code generation.\\nWhile these metrics offer a rapid and cost-effective approach for assessing the quality of gener-\\nated code, they often fall short of capturing the syntactical and functional correctness, as well as\\nthe semantic features of the code. To eliminate this limitation, CodeBLEU [221] was introduced, en-\\nhancing the traditional BLEU metric [203] by incorporating syntactic information through abstract\\nsyntax trees (AST) and semantic understanding via data-flow graph (DFG). Despite these improve-\\nments, the metric does not fully resolve issues pertaining to execution errors or discrepancies in\\nthe execution results of the generated code. In light of these challenges, execution-based metrics\\nhave gained prominence for evaluating code generation, including pass@k [48], n@k [151], test\\n14https://www.cognition.ai/introducing-devin',\n",
       " 'have gained prominence for evaluating code generation, including pass@k [48], n@k [151], test\\n14https://www.cognition.ai/introducing-devin\\n15https://github.com/OpenDevin/OpenDevin\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:40\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 9. The performance comparison of LLMs for code generation on the HumanEval [48] benchmark,\\nmeasured by Pass@1. For models with various sizes, we report only the largest size version of each model\\nwith a magnitude of B parameters. ‚Ä° denotes instruction-tuned models.\\nModel\\nSize\\npass@1 (%)\\nAvailability\\nClosed Source\\nGPT-4o-0513 [197]\\n-\\n91.0\\n[API Access]\\nGPT-4-Turbo-0409 [198]\\n-\\n88.2\\n[API Access]\\nGPT-4-1106 [5]\\n-\\n87.8\\n[API Access]\\nGPT-3.5-Turbo-0125 [196]\\n-\\n76.2\\n[API Access]\\nClaude-3.5-Sonnet [14]\\n-\\n92.0\\n[API Access]\\nClaude-3-Opus [14]\\n-\\n84.9\\n[API Access]\\nClaude-3-Sonnet [14]\\n-\\n73.0\\n[API Access]\\nClaude-3-Haiku [14]\\n-\\n75.9\\n[API Access]\\nGemini-1.5-Pro [220]\\n-\\n84.1\\n[API Access]\\nGemini-1.5-Flash [220]\\n-\\n74.3\\n[API Access]\\nGemini-1.0-Ultra [220]\\n-\\n74.4\\n[API Access]\\nGemini-1.0-Pro [220]\\n-\\n67.7\\n[API Access]\\n‚Ä°PanGu-Coder2 [234]\\n15B\\n61.64\\n-\\nPanGu-Coder [55]\\n2.6B\\n23.78\\n-\\nCodex [48]\\n12B\\n28.81\\nDeprecated\\nPaLM-Coder [54]\\n540B\\n36',\n",
       " '-\\n74.4\\n[API Access]\\nGemini-1.0-Pro [220]\\n-\\n67.7\\n[API Access]\\n‚Ä°PanGu-Coder2 [234]\\n15B\\n61.64\\n-\\nPanGu-Coder [55]\\n2.6B\\n23.78\\n-\\nCodex [48]\\n12B\\n28.81\\nDeprecated\\nPaLM-Coder [54]\\n540B\\n36\\n-\\nAlphaCode [151]\\n1.1B\\n17.1\\n-\\nOpen Source\\n‚Ä°Codestral [181]\\n22B\\n81.1\\n[Checkpoint Download]\\n‚Ä°DeepSeek-Coder-V2-Instruct [331]\\n21B (236B)\\n90.2\\n[Checkpoint Download]\\n‚Ä°Qwen2.5-Coder-Instruct [109]\\n7B\\n88.4\\n[Checkpoint Download]\\nQwen2.5-Coder [109]\\n7B\\n61.6\\n[Checkpoint Download]\\n‚Ä°StarCoder2-Instruct [304]\\n15.5B\\n72.6\\n[Checkpoint Download]\\n‚Ä°CodeGemma-Instruct [59]\\n7B\\n56.1\\n[Checkpoint Download]\\nCodeGemma [59]\\n7B\\n44.5\\n[Checkpoint Download]\\nStarCoder 2 [170]\\n15B\\n46.3\\n[Checkpoint Download]\\n‚Ä°WaveCoder-Ultra [301]\\n6.7B\\n79.9\\n[Checkpoint Download]\\n‚Ä°WaveCoder-Pro [301]\\n6.7B\\n74.4\\n[Checkpoint Download]\\n‚Ä°WaveCoder-DS [301]\\n6.7B\\n65.8\\n[Checkpoint Download]\\nStableCode [210]\\n3B\\n29.3\\n[Checkpoint Download]\\nCodeShell [285]\\n7B\\n34.32\\n[Checkpoint Download]\\n‚Ä°CodeQwen1.5-Chat [249]\\n7B\\n83.5\\n[Checkpoint Download]\\nCodeQwen1.5 [249]\\n7B\\n51.8',\n",
       " '[Checkpoint Download]\\nStableCode [210]\\n3B\\n29.3\\n[Checkpoint Download]\\nCodeShell [285]\\n7B\\n34.32\\n[Checkpoint Download]\\n‚Ä°CodeQwen1.5-Chat [249]\\n7B\\n83.5\\n[Checkpoint Download]\\nCodeQwen1.5 [249]\\n7B\\n51.8\\n[Checkpoint Download]\\n‚Ä°DeepSeek-Coder-Instruct [88]\\n33B\\n79.3\\n[Checkpoint Download]\\nDeepSeek-Coder [88]\\n33B\\n56.1\\n[Checkpoint Download]\\nreplit-code [223]\\n3B\\n20.12\\n[Checkpoint Download]\\n‚Ä°MagicoderùëÜ-CL [278]\\n7B\\n70.7\\n[Checkpoint Download]\\n‚Ä°Magicoder-CL [278]\\n7B\\n60.4\\n[Checkpoint Download]\\n‚Ä°WizardCoder [173]\\n33B\\n79.9\\n[Checkpoint Download]\\nCodeFuse [160]\\n34B\\n74.4\\n[Checkpoint Download]\\nPhi-1 [84]\\n1.3B\\n50.6\\n[Checkpoint Download]\\n‚Ä°Code Llama-Instruct [227]\\n70B\\n67.8\\n[Checkpoint Download]\\nCode Llama [227]\\n70B\\n53.0\\n[Checkpoint Download]\\n‚Ä°OctoCoder [187]\\n15.5B\\n46.2\\n[Checkpoint Download]\\nCodeGeeX2 [321]\\n6B\\n35.9\\n[Checkpoint Download]\\n‚Ä°InstructCodeT5+ [269]\\n16B\\n35.0\\n[Checkpoint Download]\\nCodeGen-NL [193]\\n16.1B\\n14.24\\n[Checkpoint Download]\\nCodeGen-Multi [193]\\n16.1B\\n18.32\\n[Checkpoint Download]\\nCodeGen-Mono [193]',\n",
       " '[Checkpoint Download]\\n‚Ä°InstructCodeT5+ [269]\\n16B\\n35.0\\n[Checkpoint Download]\\nCodeGen-NL [193]\\n16.1B\\n14.24\\n[Checkpoint Download]\\nCodeGen-Multi [193]\\n16.1B\\n18.32\\n[Checkpoint Download]\\nCodeGen-Mono [193]\\n16.1B\\n29.28\\n[Checkpoint Download]\\nStarCoder [147]\\n15B\\n33.60\\n[Checkpoint Download]\\nCodeT5+ [271]\\n16B\\n30.9\\n[Checkpoint Download]\\nCodeGen2 [192]\\n16B\\n20.46\\n[Checkpoint Download]\\nSantaCoder [9]\\n1.1B\\n14.0\\n[Checkpoint Download]\\nInCoder [77]\\n6.7B\\n15.2\\n[Checkpoint Download]\\nPolyCoder [290]\\n2.7B\\n5.59\\n[Checkpoint Download]\\nCodeParrot [254]\\n1.5B\\n3.99\\n[Checkpoint Download]\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:41\\ncase average [95], execution accuracy [218], and pass@t [195]. In particular, the pass@k, serving\\nas a principal evaluation metric, assesses the probability that at least one out of ùëòcode samples\\ngenerated by a model will pass all unit tests. An unbiased estimator for pass@k introduced by [48]\\nis defined as:\\npass@k B Etask\\n\"\\n1 ‚àí\\n\\x00ùëõ‚àíùëê\\nùëò\\n\\x01\\n\\x00ùëõ\\nùëò\\n\\x01\\n#\\n(20)\\nwhere ùëõis the total number of sampled candidate code solutions, ùëòis the number of randomly\\nselected code solutions from these candidates for each programming problem, with ùëõ‚â•ùëò, and ùëêis\\nthe count of correct samples within the ùëòselected.\\nNevertheless, these execution-based methods are heavily dependent on the quality of unit tests\\nand are limited to evaluating executable code [307]. Consequently, when unit tests are unavailable,\\ntoken-matching-based metrics are often employed as an alternative for evaluation. Furthermore, in',\n",
       " 'and are limited to evaluating executable code [307]. Consequently, when unit tests are unavailable,\\ntoken-matching-based metrics are often employed as an alternative for evaluation. Furthermore, in\\nscenarios lacking a ground truth label, unsupervised metrics such as perplexity (PPL) [116] can\\nserve as evaluative tools. Perplexity quantifies an LLM‚Äôs uncertainty in predicting new content,\\nthus providing an indirect measure of the model‚Äôs generalization capabilities and the quality of the\\ngenerated code.\\nTaken together, while the aforementioned methods primarily focus on the functional correctness\\nof code, they do not provide a holistic evaluation that encompasses other critical dimensions such\\nas code vulnerability [189], maintainability [15], readability [34], complexity and efficiency [208],\\nstylistic consistency [178], and execution stability [215]. A comprehensive evaluation framework',\n",
       " 'as code vulnerability [189], maintainability [15], readability [34], complexity and efficiency [208],\\nstylistic consistency [178], and execution stability [215]. A comprehensive evaluation framework\\nthat integrates these aspects remains an open area for future research and development in the field\\nof code generation assessment.\\n5.10.2\\nHuman Evaluation. Given the intrinsic characteristics of code, the aforementioned automatic\\nevaluation metrics are inherently limited in their capacity to fully assess code quality. For instance,\\nmetrics specifically designed to measure code style consistency are challenging to develop and\\noften fail to capture this aspect adequately [44]. When it comes to repository-level code generation,\\nthe evaluation of overall code quality is substantially complicated due to the larger scale of the\\ntask, which involves cross-file designs and intricate internal as well as external dependencies, as\\ndiscussed by [22, 239].',\n",
       " 'task, which involves cross-file designs and intricate internal as well as external dependencies, as\\ndiscussed by [22, 239].\\nTo overcome these challenges, conducting human evaluations becomes necessary, as it yields\\nrelatively robust and reliable results. Human assessments also offer greater adaptability across\\nvarious tasks, enabling the simplification of complex and multi-step evaluations. Moreover, human\\nevaluations are essential for demonstrating the effectiveness of certain token-matching-based\\nmetrics, such as CodeBLEU [221]. These studies typically conduct experiments to evaluate the\\ncorrelation coefficient between proposed metrics and quality scores assigned by actual users,\\ndemonstrating their superiority over existing metrics.\\nMoreover, in an effort to better align LLMs with human preferences and intentions, InstructGPT\\n[200] employs human-written prompts and demonstrations, and model output ranking in the',\n",
       " 'Moreover, in an effort to better align LLMs with human preferences and intentions, InstructGPT\\n[200] employs human-written prompts and demonstrations, and model output ranking in the\\nfine-tuning of LLMs using reinforcement learning from human feedback (RLHF). Although similar\\nalignment learning techniques have been applied to code generation, the feedback in this domain\\ntypically comes from a compiler or interpreter, which offers execution feedback, rather than from\\nhuman evaluators. Notable examples include CodeRL [139], PPOCoder [238], RLTF [163], and\\nPanGu-Coder2 [234]. Further information on this topic is available in Section 5.5.\\nNonetheless, human evaluations are not without drawbacks, as they can be prone to certain\\nissues that may compromise their accuracy and consistency. For instance, 1) personalized tastes\\nand varying levels of expertise among human evaluators can introduce biases and inconsistencies',\n",
       " 'issues that may compromise their accuracy and consistency. For instance, 1) personalized tastes\\nand varying levels of expertise among human evaluators can introduce biases and inconsistencies\\ninto the evaluation process; 2) conducting comprehensive and reliable human evaluations often\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:42\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nInstruction\\nCode LLM\\n(Generator)\\n(Code) LLM\\n(Judge)\\nWhich response is better?\\nInstruction: {instruction}\\nResponse 1: {response 1}\\nResponse 2: {response 2}\\nPairwise Comparison\\nRate the response on a scale\\nof 1 to 10.\\nInstruction: {instruction}\\nResponse: {response 1/2}\\nSingle Answer Grading\\nResponse 2 is better\\nThe score is 7\\nResponse 1\\nResponse 2\\nFig. 16. The pipeline of (Code) LLM-as-a-judge for evaluating generated code by Code LLMs. There are\\nprimarily two types of approaches: pairwise comparison and single answer grading.\\nnecessitates a substantial number of evaluators, leading to significant expenses and time-consuming;\\n3) the reproducibility of human evaluations is often limited, which presents challenges in extending\\nprevious evaluation outcomes or monitoring the progress of LLMs, as highlighted by [319].\\n5.10.3\\nLLM-as-a-Judge. The powerful instruction-following capabilities of LLMs have stimulated',\n",
       " 'previous evaluation outcomes or monitoring the progress of LLMs, as highlighted by [319].\\n5.10.3\\nLLM-as-a-Judge. The powerful instruction-following capabilities of LLMs have stimulated\\nresearchers to innovatively investigate the potential of LLM-based evaluations. The LLM-as-a-Judge\\n[320] refers to the application of advanced proprietary LLMs (e.g., GPT4, Gemini, and Claud 3)\\nas proxies for human evaluators. This involves designing prompts with specific requirements\\nto guide LLMs in conducting evaluations, as demonstrated by AlpacaEval [148] and MT-bench\\n[320]. This method reduces reliance on human participation, thereby facilitating more efficient\\nand scalable evaluations. Moreover, LLMs can offer insightful explanations for the assigned rating\\nscores, thereby augmenting the interpretability of evaluations [319].\\nNevertheless, the use of LLM-based evaluation for code generation remains relatively underex-',\n",
       " 'scores, thereby augmenting the interpretability of evaluations [319].\\nNevertheless, the use of LLM-based evaluation for code generation remains relatively underex-\\nplored compared with general-purpose LLM. The pipeline of (Code) LLM-as-a-judge for evaluating\\ngenerated code by Code LLMs is depicted in Figure 16. A recent work [332] introduces the ICE-Score\\nevaluation metric, which instructs LLM for code assessments. This approach attains superior corre-\\nlations with functional correctness and human preferences, thereby eliminating the requirement\\nfor test oracles or references. As the capabilities of LLM continue to improve, we anticipate seeing\\nmore research in this direction.\\nDespite their scalability and explainability, the effectiveness of LLM-based evaluation is con-\\nstrained by the inherent limitations of the chosen LLM. Several studies have shown that most LLMs,\\nincluding GPT-4, suffer from several issues, including position, verbosity, and self-enhancement',\n",
       " 'strained by the inherent limitations of the chosen LLM. Several studies have shown that most LLMs,\\nincluding GPT-4, suffer from several issues, including position, verbosity, and self-enhancement\\nbiases, as well as restricted reasoning ability [320]. Specifically, position bias refers to the tendency\\nof LLMs to disproportionately favor responses that are presented in certain positions, which can\\nskew the perceived quality of answers based on their order of presentation. Meanwhile, verbosity\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:43\\n1.1\\n5.5\\n13 15\\n33\\n70\\n# Parameters (B)\\n0\\n20\\n40\\n60\\n80\\nPass@1 (%)\\nGPT-3.5-Turbo\\nClaude-3-Sonnet\\nClaude-3-Haiku\\nClaude-3-Opus\\nQwen2.5-Coder\\nCodeGemma\\nStarCoder 2\\nWaveCoder\\nCodeFuse\\nCodeShell\\nCodeQwen1.5\\nDeepSeek-Coder\\nphi-1\\nCode Llama\\nCodeGeeX2\\nCodeGeeX\\nPanGu-Coder\\nCodeGen-NL\\nCodeGen-Multi\\nCodeGen-Mono\\nStarCoder\\nCodeT5+\\nSantaCoder\\nInCoder\\nPolyCoder\\nCodeParrot\\nCodestral\\nQwen2.5-Coder-Instruct\\nStarCoder2-Instruct\\nCodeGemma-Instruct\\nCodeQwen1.5-Chat\\nDeepSeek-Coder-Instruct\\nMagicoderS-CL\\nMagicoder-CL\\nWizardCoder\\nCode Llama-Instruct\\nBase\\nInstruct\\nFig. 17. The performance comparison of LLMs for code generation on the MBPP [17] benchmark, measured by\\nPass@1. For models with various sizes, we report only the largest size version of each model with a magnitude\\nof B parameters.\\nbias describes the inclination of LLMs to prefer lengthier responses, even when these are not',\n",
       " 'of B parameters.\\nbias describes the inclination of LLMs to prefer lengthier responses, even when these are not\\nnecessarily of higher quality compared to more concise ones. Self-enhancement bias, on the other\\nhand, is observed when LLMs consistently overvalue the quality of the text they generate [319, 320].\\nMoreover, due to their inherent limitations in tackling complex reasoning challenges, LLMs may not\\nbe entirely reliable as evaluators for tasks that require intensive reasoning, such as those involving\\nmathematical problem-solving. However, these shortcomings can be partially addressed through\\nthe application of deliberate prompt engineering and fine-tuning techniques, as suggested by [320].\\n5.10.4\\nEmpirical Comparison. In this section, we present a performance comparison of LLMs for\\ncode generation using the well-regarded HumanEval, MBPP, and the more practical and chal-\\nlenging BigCodeBench benchmarks. This empirical comparison aims to highlight the progressive',\n",
       " 'code generation using the well-regarded HumanEval, MBPP, and the more practical and chal-\\nlenging BigCodeBench benchmarks. This empirical comparison aims to highlight the progressive\\nenhancements in LLM capabilities for code generation. These benchmarks assess an LLM‚Äôs ability to\\ngenerate source code across various levels of difficulty and types of programming tasks. Specifically,\\nHumanEval focuses on complex code generation, MBPP targets basic programming tasks, and\\nBigCodeBench emphasizes practical and challenging programming tasks.\\nDue to the limitations in computational resources we faced, we have cited experimental results\\nfrom original papers or widely recognized open-source leaderboards within the research community,\\nsuch as the HumanEval Leaderboard 16, EvalPlus Leaderboard 17, Big Code Models Leaderboard\\n18, and BigCodeBench Leaderboard 19. We report performance on HumanEval using the pass@1',\n",
       " 'such as the HumanEval Leaderboard 16, EvalPlus Leaderboard 17, Big Code Models Leaderboard\\n18, and BigCodeBench Leaderboard 19. We report performance on HumanEval using the pass@1\\nmetric, as shown in Table 9, while MBPP and BigCodeBench results are presented with pass@1 in\\nFigures 17 and 18, respectively.\\nWe offer the following insights:\\n16https://paperswithcode.com/sota/code-generation-on-humaneval\\n17https://evalplus.github.io/leaderboard.html\\n18https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard\\n19https://bigcode-bench.github.io/\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:44\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nCodeGemma-7B\\nStarCoder 2-15B\\nCodeGemma-Instruct-7B\\nCodeQwen1.5-Chat-7B\\nWaveCoder-Ultra-6.7B\\nCode Llama-70B\\nStarCoder2-Instruct-15B\\nCodeQwen1.5-7B\\nDeepSeek-Coder-33B\\nMagicoder-S-DS-7B\\nPhi-3-Medium-128K-Instruct-14B\\nQwen2.5-Coder-Instruct-7B\\nCodeGeeX4-9B\\nCode Llama-Instruct-70B\\nClaude-3-Haiku\\nGPT-3.5-Turbo-0125\\nDeepSeek-Coder-Instruct-33B\\nCodestral-22B\\nClaude-3-Sonnet\\nGemini-1.5-Flash\\nGPT-4-0613\\nClaude-3-Opus\\nGemini-1.5-Pro\\nGPT-4-Turbo-0409\\nClaude-3.5-Sonnet\\nDeepSeek-Coder-V2-Instruct-21B (236B)\\nGPT-4o-0513\\n40\\n45\\n50\\n55\\n60\\nPass@1 (%)\\n38.3\\n38.4\\n39.3\\n43.6\\n43.7\\n44\\n45.1\\n45.6\\n46.6\\n47.6\\n48.7\\n48.8\\n49\\n49.6\\n50.1\\n50.6\\n51.1\\n52.5\\n53.8\\n55.1\\n57.2\\n57.4\\n57.5\\n58.2\\n58.6\\n59.7\\n61.1\\nGPT-3.5-Turbo-0125\\nClosed Source\\nOpen Source\\nFig. 18. The performance comparison of LLMs for code generation on the BigCodeBench [333] benchmark,\\nmeasured by Pass@1. For models with various sizes, we report only the largest size version of each model',\n",
       " 'measured by Pass@1. For models with various sizes, we report only the largest size version of each model\\nwith a magnitude of B parameters.\\n‚Ä¢ The performance gap between open-source and closed-source models across the three bench-\\nmarks is gradually narrowing. For instance, on the HumanEval benchmark, DeepSeek-Coder-\\nV2-Instruct with 21B activation parameters and Qwen2.5-Coder-Instruct 7B achieve 90% and\\n88.4% pass@1, respectively. These results are comparable to the much larger closed-source\\nLLMs, such as Claude-3.5-Sonnet, which achieves 92.0% pass@1. On the MBPP benchmark,\\nQwen2.5-Coder-Instruct 7B with 83.5% pass@1 significantly outperforms GPT-3.5-Turbo\\nwith 52.2% pass@1 and closely rivals the closed-source Claude-3-Opus with 86.4% pass@1.\\nOn the BigCodeBench, DeepSeek-Coder-V2-Instruct achieves 59.7%, surpassing all compared\\nclosed-source and open-source LLMs except for slightly falling behind GPT-4o-0513, which\\nachieves 61.1%.',\n",
       " 'On the BigCodeBench, DeepSeek-Coder-V2-Instruct achieves 59.7%, surpassing all compared\\nclosed-source and open-source LLMs except for slightly falling behind GPT-4o-0513, which\\nachieves 61.1%.\\n‚Ä¢ Generally, as the number of model parameters increases, the performance of code LLMs\\nimproves. However, Qwen2.5-Coder-Instruct 7B achieves 88.4% pass@1, outperforming larger\\nmodels like StarCoder2-Instruct 15.5B with 72.6% pass@1, DeepSeek-Coder-Instruct 33B\\nwith 79.3% pass@1, and Code Llama-Instruct 70B with 67.8% pass@1 on the HumanEval\\nbenchmark. Similar trends are observed across the other two benchmarks, suggesting that\\ncode LLMs with 7B parameters may be sufficiently capable for code generation task.\\n‚Ä¢ Instruction-tuned models consistently outperform their base (pretrained) counterparts across\\nthe HumanEval and MBPP benchmarks. For instance, Qwen2.5-Coder-Instruct surpasses\\nQwen2.5-Coder by an average of 26.04%, StarCoder2-Instruct improves upon StarCoder 2',\n",
       " 'the HumanEval and MBPP benchmarks. For instance, Qwen2.5-Coder-Instruct surpasses\\nQwen2.5-Coder by an average of 26.04%, StarCoder2-Instruct improves upon StarCoder 2\\nby an average of 35.20%, and CodeGemma-Instruct enhances CodeGemma by an average\\nof 11.26%. Additionally, DeepSeek-Coder-Instruct outperforms DeepSeek-Coder by an aver-\\nage of 23.71%, while Code Llama-Instruct shows a 13.80% improvement over Code Llama.\\nDetailed results can be found in Table 10. These findings underscore the effectiveness of\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:45\\nTable 10. The performance improvement of instruction-tuned models over their pretrained counterparts\\non the HumanEval, MBPP, and BigCodeBench benchmarks. The last two rows demonstrate the average\\nimprovement on the first two benchmarks and the three benchmarks, respectively.\\nQwen2.5-Coder-\\nInstruct 7B\\nStarCoder2-\\nInstruct 15.5B\\nCodeGemma-\\nInstruct 7B\\nDeepSeek-Coder-\\nInstruct 33B\\nCode Llama-\\nInstruct 70B\\nHumanEval\\n43.51%\\n56.80%\\n26.07%\\n41.35%\\n27.92%\\nMBPP\\n8.58%\\n13.60%\\n-3.56%\\n6.06%\\n-0.32%\\nBigCodeBench\\n-\\n17.45%\\n2.61%\\n9.66%\\n12.73%\\n# Avg. Imp. H. M.\\n26.04%\\n35.20%\\n11.26%\\n23.71%\\n13.80%\\n# Avg. Imp. H. M. B.\\n-\\n29.28%\\n8.37%\\n19.02%\\n13.44%\\ninstruction tuning, although the quality of the instruction tuning dataset plays a critical role\\nin determining model performance [173, 328].\\n‚Ä¢ Performance on the HumanEval benchmark is nearly saturated. However, MBPP, which',\n",
       " 'in determining model performance [173, 328].\\n‚Ä¢ Performance on the HumanEval benchmark is nearly saturated. However, MBPP, which\\ninvolves basic programming tasks, and BigCodeBench, which involves more practical and\\nchallenging programming tasks, demand more capable code LLMs. Additionally, while these\\nbenchmarks primarily evaluate the functional correctness of code, they do not provide\\na comprehensive assessment across other critical dimensions. Developing a more holistic\\nevaluation framework that integrates various aspects remains an open area for future research\\nand development in LLMs for code generation evaluation.\\nDiscussion: We discuss certain code LLMs in Table 9 for clarity: (1) General LLMs accessed via\\nAPI are not specifically trained on large code corpora but achieve state-of-the-art performance\\nin code generation, such as Claude-3.5-Sonnet with 92.0% pass@1 on HumanEval benchmark.\\n(2) AlphaCode targets code generation for more complex and unseen problems that require a',\n",
       " 'in code generation, such as Claude-3.5-Sonnet with 92.0% pass@1 on HumanEval benchmark.\\n(2) AlphaCode targets code generation for more complex and unseen problems that require a\\ndeep understanding of algorithms and intricate natural language, such as those encountered in\\ncompetitive programming. The authors of AlphaCode found that large-scale model sampling to\\nnavigate the search space, such as 1M samples per problem for CodeContests, followed by filtering\\nbased on program behavior to produce a smaller set of submissions, is crucial for achieving good and\\nreliable performance on problems that necessitate advanced reasoning. (3) Phi-1 1.3B is a specialized\\nLLM for code, trained on ‚Äútextbook quality‚Äù data from the web (6B tokens) and synthetically\\ngenerated textbooks and exercises using GPT-3.5 (1B tokens). (4) Code Llama 70B is initialized with\\nLlama 2 model weights and continually pre-trained on 1T tokens from a code-heavy dataset and',\n",
       " 'generated textbooks and exercises using GPT-3.5 (1B tokens). (4) Code Llama 70B is initialized with\\nLlama 2 model weights and continually pre-trained on 1T tokens from a code-heavy dataset and\\nlong-context fine-tuned with approximately 20B tokens. However, Code Llama-Instruct 70B is fine-\\ntuned from Code Llama-Python 70B without long-context fine-tuning, using an additional 260M\\ntokens to better follow human instructions. Surprisingly, these models underperform compared to\\nsmaller parameter Code LLMs like Qwen2.5-Coder-Instruct 7B, DeepSeek-Coder-V2-Instruct 21B,\\nand Codestral 22B across all three benchmarks. The underlying reasons for this discrepancy remain\\nunclear and warrant further exploration. (5) Unlike other open-source Code LLMs, DeepSeek-Coder-\\nV2-Instruct is further pre-trained on DeepSeek-V2 [159], which employs a Mixture-of-Experts (MoE)\\narchitecture with only 21B activation parameters out of 236B parameters, using an additional 6',\n",
       " 'V2-Instruct is further pre-trained on DeepSeek-V2 [159], which employs a Mixture-of-Experts (MoE)\\narchitecture with only 21B activation parameters out of 236B parameters, using an additional 6\\ntrillion tokens composed of 60% source code, 10% math corpus, and 30% natural language corpus.\\nFor a comprehensive understanding of MoE in LLMs, please refer to [36].\\n5.11\\nCode LLMs Alignment\\nThe pre-training of LLMs for next-token prediction, aimed at maximizing conditional generation\\nlikelihood across vast textual corpora, equips these models with extensive world knowledge and\\nemergent capabilities [33]. This training approach enables the generation of coherent and fluent\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:46\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\ntext in response to diverse instructions. Nonetheless, LLMs can sometimes misinterpret human\\ninstructions, produce biased content, or generate factually incorrect information (commonly referred\\nto as hallucinations), which may limit their practical utility [118, 272, 319].\\nAligning LLMs with human intentions and values, known as LLM alignment, has consequently\\nbecome a critical research focus [118, 272]. Key objectives frequently discussed in the context of LLM\\nalignment include robustness, interpretability, controllability, ethicality, trustworthiness, security,\\nprivacy, fairness, and safety. In recent years, significant efforts have been made by researchers\\nto achieve this alignment, employing techniques such as Reinforcement Learning with Human\\nFeedback (RLHF) [200].\\nHowever, the alignment of Code LLMs has not been extensively explored. Compared to text',\n",
       " 'Feedback (RLHF) [200].\\nHowever, the alignment of Code LLMs has not been extensively explored. Compared to text\\ngeneration, aligning code generation with human intentions and values is even more crucial. For\\ninstance, users without programming expertise might prompt Code LLM to generate source code\\nand subsequently execute it on their computers, potentially causing catastrophic damage. Some\\npotential risks include:\\n‚Ä¢ Malware Infection: The code could contain viruses, worms, or trojans that compromise our\\nsystem‚Äôs security.\\n‚Ä¢ Data Loss: It might delete or corrupt important files and data.\\n‚Ä¢ Unauthorized Access: It can create backdoors, allowing attackers to access our system\\nremotely.\\n‚Ä¢ Performance Issues: The code might consume excessive resources, slowing down our\\nsystem.\\n‚Ä¢ Privacy Breaches: Sensitive information, such as passwords or personal data, might be\\nstolen.\\n‚Ä¢ System Damage: It may alter system settings or damage hardware components.',\n",
       " 'system.\\n‚Ä¢ Privacy Breaches: Sensitive information, such as passwords or personal data, might be\\nstolen.\\n‚Ä¢ System Damage: It may alter system settings or damage hardware components.\\n‚Ä¢ Network Spread: It could propagate across networks, affecting other devices.\\n‚Ä¢ Financial Loss: If the code is ransomware, it might encrypt data and demand payment for\\ndecryption.\\n‚Ä¢ Legal Consequences: Running certain types of malicious code can lead to legal repercus-\\nsions.\\nAs illustrated, aligning Code LLMs to produce source code consistent with human preferences and\\nvalues is of paramount importance in software development. A recent study [293] provides the first\\nsystematic literature review identifying seven critical non-functional properties of LLMs for code,\\nbeyond accuracy, including robustness, security, privacy, explainability, efficiency, and usability.\\nThis study is highly pertinent to the alignment of Code LLMs. We recommend readers refer to this\\nsurvey for more detailed insights.',\n",
       " 'This study is highly pertinent to the alignment of Code LLMs. We recommend readers refer to this\\nsurvey for more detailed insights.\\nIn this survey, we identify five core principles that serve as the key objectives for aligning Code\\nLLMs: Green, Responsibility, Efficiency, Safety, and Trustworthiness (collectively referred to as\\nGREST). These principles are examined from a broader perspective. Each category encompasses\\nvarious concepts and properties, which are summarized in Table 11. In the following, we define\\neach principle and briefly introduce a few notable works to enhance understanding.\\nGreen: The Green principle underscores the importance of environmental sustainability in\\nthe development and deployment of LLMs for code generation. This involves optimizing energy\\nconsumption and reducing both the carbon footprint and financial costs associated with training\\nand inference processes. Currently, training, inference, and deployment of Code LLMs are notably',\n",
       " 'consumption and reducing both the carbon footprint and financial costs associated with training\\nand inference processes. Currently, training, inference, and deployment of Code LLMs are notably\\nresource-intensive. For example, training GPT-3, with its 175 billion parameters, required the\\nequivalent of 355 years of single-processor computing time and consumed 284,000 kWh of energy,\\nresulting in an estimated 552.1 tons of CO2 emissions [228]. Furthermore, a ChatGPT-like application,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:47\\nTable 11. Five core principles serve as the key objectives for Code LLMs alignment: Green, Responsibility,\\nEfficiency, Safety, and Trustworthiness (collectively referred to as GREST).\\nPrinciples\\nInvolved Concepts and Properties\\nGreen\\nEnergy Efficiency: Minimizing computational energy use and reduce environmental impact and financial costs.\\nSustainable Materials: Leveraging eco-friendly infrastructure and servers for code generation, lowering long-term expenses.\\nCarbon Footprint: Reducing emissions associated with model training and inference to enhance efficiency and save costs.\\nResource Optimization: Efficiently utilizing computational resources to minimize waste and reduce expenses in code generation.\\nRecycling Management: Responsibly dispose of hardware used in model development to reduce waste management costs.',\n",
       " 'Recycling Management: Responsibly dispose of hardware used in model development to reduce waste management costs.\\nRenewable Energy: Utilizing renewable energy sources for powering training and inference processes to decrease energy costs.\\nLifecycle Assessment: Evaluating the environmental and financial impacts of models from creation to deployment and disposal.\\nResponsibility\\nEthical Considerations: Adhering to ethical guidelines to ensure responsible use and deployment of generated code.\\nAccountability: Establishing clear lines of responsibility for code generation outcomes and potential impacts.\\nUser Education: Providing resources and guidance to help users understand and responsibly use generated code.\\nImpact Assessment: Evaluating the social and technical implications of code generation to minimize negative effects.\\nRegulatory Compliance: Ensuring that generated code adheres to relevant laws (e.g., copyright) and industry regulations.\\nEfficiency',\n",
       " 'Regulatory Compliance: Ensuring that generated code adheres to relevant laws (e.g., copyright) and industry regulations.\\nEfficiency\\nModel Optimization: Streamlining models to reduce computational load and improve speed.\\nPrompt Engineering: Designing effective prompts to generate accurate code efficiently.\\nResource Management: Allocating computational resources wisely to balance speed and cost.\\nInference Optimization: Enhancing the inference process to quickly generate code with minimal latency.\\nParallel Processing: Utilizing parallelism to speed up code generation tasks.\\nCaching Mechanisms: Implementing caching to reuse previous results and reduce redundant computations.\\nEvaluation Metrics: Using precise metrics to assess and improve the efficiency of code outputs.\\nSafety\\nInput Validation: Ensuring inputs (prompts) are safe and sanitized to prevent malicious exploitation.\\nSecurity Audits: Regularly reviewing generated code for vulnerabilities and potential exploits.',\n",
       " 'Input Validation: Ensuring inputs (prompts) are safe and sanitized to prevent malicious exploitation.\\nSecurity Audits: Regularly reviewing generated code for vulnerabilities and potential exploits.\\nMonitoring and Logging: Keeping track of generation outputs to quickly identify and address safety issues.\\nUser Access Control: Limiting access to generation capabilities to trusted users to minimize risk.\\nContinuous Updates: Regularly updating models with the latest safety protocols and security patches.\\nEthical Guidelines: Implementing ethical standards to guide safe and responsible code generation.\\nTrustworthiness\\nReliability: Ensuring that generated code consistently meets functional requirements and performs as expected.\\nTransparency: Providing clear explanations of how code is generated to build user confidence.\\nVerification and Testing: Using rigorous testing frameworks to ensure the generated code accuracy and reliability.',\n",
       " 'Verification and Testing: Using rigorous testing frameworks to ensure the generated code accuracy and reliability.\\nBias Mitigation: Actively working to identify and reduce biases in code generation to ensure fairness and impartiality.\\nUser Feedback Integration: Continuously incorporating user feedback to refine and improve code generation processes.\\nDocumentation: Providing comprehensive documentation for generated code to enhance understanding and trust.\\nwith an estimated usage of 11 million requests per hour, can produce emissions of 12.8k metric\\ntons of CO2 per year, which is 25 times the carbon emissions associated with training GPT-3\\n[53]. To mitigate these costs, several techniques are often employed, such as the development of\\nspecialized hardware (e.g., Tensor Processing Units (TPUs) and Neural Processing Units (NPUs)),\\nmodel compression methods (e.g., quantization and knowledge distillation), parameter-efficient',\n",
       " 'specialized hardware (e.g., Tensor Processing Units (TPUs) and Neural Processing Units (NPUs)),\\nmodel compression methods (e.g., quantization and knowledge distillation), parameter-efficient\\nfine-tuning (PEFT), and the use of renewable energy sources. For instance, Shi et al. [235] applied\\nknowledge distillation to reduce the size of CodeBERT [76] and GraphCodeBERT [86], resulting in\\noptimized models of just 3MB. These models are 160 times smaller than the original large models\\nand significantly reduce energy consumption by up to 184 times and carbon footprint by up to 157\\ntimes. Similarly, Wei et al. [277] utilized quantization techniques for Code LLMs such as CodeGen\\n[193] and Incoder [77] by employing lower-bit integers (e.g., int8). This approach reduced storage\\nrequirements by 67.3% to 70.8%, carbon footprint by 28.8% to 55.0%, and pricing costs by 28.9% to\\n55.0%.\\nResponsibility: The Responsibility principle in the context of Code LLMs underscores the',\n",
       " 'requirements by 67.3% to 70.8%, carbon footprint by 28.8% to 55.0%, and pricing costs by 28.9% to\\n55.0%.\\nResponsibility: The Responsibility principle in the context of Code LLMs underscores the\\nimportance of ethical considerations, fairness, and accountability throughout their lifecycle. This\\ninvolves addressing biases in training data, ensuring fairness and transparency in model decision-\\nmaking, maintaining accountability for outputs, adhering to applicable laws (e.g., copyright),\\nimplementing safeguards against misuse, and providing clear communication about the model‚Äôs\\ncapabilities and limitations. Specifically,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:48\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n‚Ä¢ Bias Mitigation. Biases in code generation can lead to flawed software and reinforce stereo-\\ntypes, potentially causing significant societal impacts. For example, an Code LLM that in-\\nherits biases from its training data may produce source code/software that inadvertently\\ndiscriminates against certain user groups. This can result in applications that fail to meet the\\ndiverse needs of users, promoting exclusionary practices and reinforcing existing stereotypes\\n[168, 185].\\n‚Ä¢ Fairness and Transparency. A lack of fairness and transparency in Code LLM decision-making\\ncan result in biased or suboptimal code solutions. If the model‚Äôs decision-making process is\\nopaque, developers might unknowingly introduce code that favors specific frameworks or\\nlibraries, thereby limiting innovation and diversity in software development. This opacity',\n",
       " 'opaque, developers might unknowingly introduce code that favors specific frameworks or\\nlibraries, thereby limiting innovation and diversity in software development. This opacity\\ncan create unfair advantages and hinder collaborative efforts within tech communities [31].\\n‚Ä¢ Legal Compliance. Compliance with relevant laws, such as licensing and copyright, is crucial\\nwhen using Code LLMs for code generation to avoid legal complications. If an Code LLM\\ngenerates code snippets that inadvertently infringe on existing copyrights, it can lead to\\nlegal disputes and financial liabilities for developers and organizations [292]. Such risks may\\ndiscourage the use of advanced AI tools, thus stifling innovation and affecting growth and\\ncollaboration within the tech community.\\n‚Ä¢ Accountability. Without accountability for code generated by Code LLMs, addressing bugs\\nor security vulnerabilities becomes challenging. If a model generates faulty code leading to',\n",
       " '‚Ä¢ Accountability. Without accountability for code generated by Code LLMs, addressing bugs\\nor security vulnerabilities becomes challenging. If a model generates faulty code leading to\\na security breach, the absence of clear accountability can result in significant financial and\\nreputational damage for companies. This uncertainty can delay critical issue resolution and\\nimpede trust in AI-assisted development [155].\\n‚Ä¢ Misuse Prevention. Failing to implement mechanisms to prevent the misuse of Code LLMs can\\nenable the creation of harmful software. For example, models could be exploited to generate\\nmalware or unauthorized scripts, posing cybersecurity risks. Without proper safeguards,\\nthese models can facilitate malicious activities, threatening both individual and organizational\\nsecurity [184].\\n‚Ä¢ Clear Communication. Without clear communication about a model‚Äôs capabilities and limita-\\ntions, developers may misuse the model or overestimate its abilities. Relying on the model',\n",
       " 'security [184].\\n‚Ä¢ Clear Communication. Without clear communication about a model‚Äôs capabilities and limita-\\ntions, developers may misuse the model or overestimate its abilities. Relying on the model\\nto generate complex, mission-critical code without human oversight can lead to significant\\nsoftware failures. Misunderstanding its limitations can result in faulty implementations and\\nlost productivity [226].\\nTo adhere to this principle, potential mitigation methods include bias detection and mitigation,\\nquantification and evaluation, and adherence to ethical guidelines. Liu et al. [168] propose a new\\nparadigm for constructing code prompts, successfully uncovering social biases in code generation\\nmodels, and developing a dataset along with three metrics to evaluate overall social bias. Recently,\\nXu et al. [292] introduced LiCoEval, an evaluation benchmark for assessing the license compliance\\ncapabilities of LLMs. Additionally, incorporating diverse perspectives in development teams and',\n",
       " 'Xu et al. [292] introduced LiCoEval, an evaluation benchmark for assessing the license compliance\\ncapabilities of LLMs. Additionally, incorporating diverse perspectives in development teams and\\nengaging with stakeholders from various communities can further align Code LLM outputs with\\nethical standards and societal values.\\nEfficiency: The Efficiency principle emphasizes optimizing the performance and speed of Code\\nLLMs for code generation while minimizing the computational resources required for training\\nand inference. For instance, training the GPT-3 model, which consists of 175 billion parameters,\\ndemands substantial resources. It requires approximately 1,024 NVIDIA V100 GPUs, costing around\\n4.6 million and taking approximately 34 days to complete the training process. To address these\\nchallenges, various techniques are employed, including model compression methods such as\\npruning, quantization, and knowledge distillation. Additionally, optimized algorithms like AdamW,',\n",
       " 'challenges, various techniques are employed, including model compression methods such as\\npruning, quantization, and knowledge distillation. Additionally, optimized algorithms like AdamW,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:49\\nparallel strategies such as tensor, pipeline, and data parallelism, and parameter-efficient fine-tuning\\n(PEFT) (see Section 5.4.2) are often utilized. For a comprehensive and detailed discussion on methods\\nto enhance the efficiency of Code LLMs for code generation, please refer to Section 4.5.2, ‚ÄúEfficiency\\nEnhancement‚Äù, in [293].\\nSafety: The Safety principle of Code LLMs is of utmost importance due to their potential\\nto introduce vulnerabilities, errors, or privacy breaches into software systems. Ensuring safety\\ninvolves comprehensive testing and validation processes to detect and mitigate these risks. For\\ninstance, attackers might compromise the training process of LLMs by injecting malicious examples\\ninto the training data, a method known as data poisoning attacks [231]. Even when attackers\\nlack access to the training process, they may employ techniques like the black-box inversion',\n",
       " 'into the training data, a method known as data poisoning attacks [231]. Even when attackers\\nlack access to the training process, they may employ techniques like the black-box inversion\\napproach introduced by Hajipour et al. [91]. This method uses few-shot prompting to identify\\nprompts that coax black-box code generation models into producing vulnerable code. Furthermore,\\nYang et al. [294] and Al-Kaswan et al. [8] reveals that Code LLMs, such as CodeParrot [73], can\\nmemorize training data, potentially outputting personally identifiable information like emails,\\nnames, and IP addresses, thereby posing significant privacy risks. Additionally, Yuan et al. [302]\\ndemonstrate that engaging with ChatGPT and GPT-4 in non-natural languages can circumvent\\nsafety alignment measures, leading to unsafe outcomes, such as ‚ÄúThe steps involved in stealing\\nmoney from a bank.‚Äù. To bolster the safety of LLMs in code generation, it is crucial to detect and',\n",
       " 'safety alignment measures, leading to unsafe outcomes, such as ‚ÄúThe steps involved in stealing\\nmoney from a bank.‚Äù. To bolster the safety of LLMs in code generation, it is crucial to detect and\\neliminate privacy-related information from training datasets. For example, approaches outlined in\\n[77] and [9] utilize carefully crafted regular expressions to identify and remove private information\\nfrom training data. To counteract black-box inversion, implementing prompt filtering mechanisms\\nis recommended to identify and block prompts that might result in insecure code generation.\\nMoreover, adversarial training can enhance the model‚Äôs resilience to malicious prompts. Employing\\nreinforcement learning methods can further align Code LLMs with human preferences, thereby\\nreducing the likelihood of producing harmful outputs.\\nTrustworthiness: The Trustworthiness principle focuses on developing Code LLMs that users',\n",
       " 'reducing the likelihood of producing harmful outputs.\\nTrustworthiness: The Trustworthiness principle focuses on developing Code LLMs that users\\ncan depend on for accurate and reliable code generation, which is crucial for their acceptance and\\nwidespread adoption. Achieving this requires ensuring model transparency, providing explanations\\nfor decisions, and maintaining consistent performance across various scenarios. For instance,\\nJi et al. [120] propose a causal graph-based representation of prompts and generated code to\\nidentify the causal relationships between them. This approach offers insights into the effectiveness\\nof Code LLMs and assists end-users in understanding the generation. Similarly, Palacio et al.\\n[202] introduce ASTxplainer, a tool that extracts and aggregates normalized model logits within\\nAbstract Syntax Tree (AST) structures. This alignment of token predictions with AST nodes',\n",
       " '[202] introduce ASTxplainer, a tool that extracts and aggregates normalized model logits within\\nAbstract Syntax Tree (AST) structures. This alignment of token predictions with AST nodes\\nprovides visualizations that enhance end-user understanding of Code LLM predictions. Therefore,\\nby prioritizing trustworthiness, we can bolster user confidence and facilitate the integration of\\nCode LLMs into diverse coding environments. By adhering to the aforementioned principles as key\\nobjectives for aligning Code LLMs, researchers and developers can create LLMs for code generation\\nthat are not only capable but also ethical, sustainable, and user-centric.\\n5.12\\nApplications\\nCode LLMs have been integrated with development tools and platforms, such as integrated de-\\nvelopment environments (IDEs) and version control systems, improving programming efficiency\\nsubstantially. In this section, we will briefly introduce several widely used applications as coding',\n",
       " 'velopment environments (IDEs) and version control systems, improving programming efficiency\\nsubstantially. In this section, we will briefly introduce several widely used applications as coding\\nassistants. The statistics of these applications are provided in Table 12.\\nGitHub Copilot. GitHub Copilot, powered by OpenAI‚Äôs Codex, is an AI pair programmer that\\nhelps you write better code faster. Copilot suggests whole lines or blocks of code as you type, based\\non the context provided by your existing code and comments. It‚Äôs trained on a dataset that includes\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:50\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nTable 12. The overview of code assistant applications powered by LLMs. The column labeled ‚ÄòPLs‚Äô and ‚ÄòIDEs‚Äô\\nindicate programming languages and integrated development environments, respectively [307].\\nInstitution\\nProducts\\nModel\\nSupported Features\\nSupported PLs\\nSupported IDEs\\nGitHub & OpenAI\\nGitHub Copilot [48]\\nCodex\\nCode Completions, Code Generation,\\nCoding Questions Answering,\\nCode Refactoring, Code Issues Fix,\\nUnit Test Cases Generation,\\nCode Documentation Generation\\nJava, Python, JavaScript, TypeScript,\\nPerl, R, PowerShell, Rust, SQL, CSS,\\nRuby, Julia, C#, PHP, Swift, C++,Go,\\nHTML, JSON, SCSS, .NET, Less,\\nT-SQL, Markdown\\nVisual Studio, VS Code, Neovim,\\nJetBrains IDE\\nZhipu AI\\nCodeGeeX [321]\\nCodeGeeX\\nCode Generation, Code Translation,\\nCode Completion, Code Interpretation,\\nCode Bugs Fix, Comment Generation,\\nAI Chatbot\\nPHP, Go, C, C#, C++, Rust, Perl, CSS,\\nJava, Python, JavaScript, TypeScript,',\n",
       " 'Code Generation, Code Translation,\\nCode Completion, Code Interpretation,\\nCode Bugs Fix, Comment Generation,\\nAI Chatbot\\nPHP, Go, C, C#, C++, Rust, Perl, CSS,\\nJava, Python, JavaScript, TypeScript,\\nObjective C++, Objective C, Pascal,\\nHTML, SQL, Kotlin, R, Shell, Cuda,\\nFortran, Tex, Lean, Scala\\nClion, RubyMine, AppCode, Aqua,\\nIntelliJ IDEA, VS Code, PyCharm,\\nAndroid Studio, WebStorm, Rider,\\nGoLand, DataGrip, DataSpell\\nAmazon\\nCodeWhisperer [12]\\n‚àí\\nCode Completion, Code Explanation,\\nCode Translation,\\nCode Security Identification,\\nCode Suggestion\\nJava, Python, TypeScript, JavaScript,\\nC#\\nJetBrains IDE, VS Code, AWS Cloud9,\\nAWS Lambda\\nCodeium\\nCodeium [60]\\n‚àí\\nCode Completion, Bug Detection,\\nCode Suggestions, AI Chatbot,\\nTest Type Generation,\\nTest Plan Creation,\\nCodebase Search\\nMore than 70 languages in total,\\nincluding but not limited to:\\nC, C#, C++, Dart, CSS, Go, Elixir,\\nHTML, Haskell, Julia, Java, JavaScript,\\nLisp, Kotlin, Lua, Objective-C,\\nPerl, Pascal, PHP, Protobuf,',\n",
       " 'More than 70 languages in total,\\nincluding but not limited to:\\nC, C#, C++, Dart, CSS, Go, Elixir,\\nHTML, Haskell, Julia, Java, JavaScript,\\nLisp, Kotlin, Lua, Objective-C,\\nPerl, Pascal, PHP, Protobuf,\\nR, Python, Ruby, Scala, Rust,\\nSwift, SQL, TS, Vue\\nJetBrains, VSCode, Visual Studio,\\nColab, Jupyter, Deepnote,\\nNotebooks, Databricks, Chrome,\\nVim, Neovim, Eclipse, Emacs,\\nVSCode Web IDEs, Sublime Text\\nHuawei\\nCodeArts Snap [234]\\nPanGu-Coder\\nCode Generation, Code Explanation\\nResearch and Development Knowledge\\nQuestion and Answer\\nCode Comment, Code Debug\\nUnit Test Case Generation\\nJava, Python\\nPyCharm, VS Code, IntelliJ\\nTabnine\\nTabNine [246]\\n‚àí\\nCode Generation, Code Completion,\\nCode Explanation, Bug Fix,\\nCode Recommendation, Code Refactoring,\\nCode Test Generation,\\nDocstring Generation\\nPython, Javascript, Java, TypeScript,\\nHTML, Haskell, Matlab, Kotlin, Sass,\\nGo, PHP, Ruby, C, C#, C++, Swift,\\nRust, CSS, Perl, Angular, Dart, React,\\nObjective C, NodeJS, Scala,\\nSublime, PyCharm, Neovim, Rider,',\n",
       " 'HTML, Haskell, Matlab, Kotlin, Sass,\\nGo, PHP, Ruby, C, C#, C++, Swift,\\nRust, CSS, Perl, Angular, Dart, React,\\nObjective C, NodeJS, Scala,\\nSublime, PyCharm, Neovim, Rider,\\nVS Code, IntelliJ IDE, Visual Studio,\\nPhpStorm, Vim, RubyMine, DataGrip,\\nAndroid Studio, WebStorm, Emacs,\\nClion, Jupyter Notebook, JupyterLab,\\nEclipse, GoLand, AppCode\\nReplit\\nReplit[222]\\nreplit-code\\nCode Completion, Code Editing,\\nCode Generation, Code Explanation,\\nCode Suggestion, Code Test Generation\\nC#, Bash, C, CSS, C++, Java, Go,\\nHTML, JavaScript, Perl, PHP,\\nRuby, Python, R, SQL, Rust\\n‚àí\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:51\\nFig. 19. An exemplar of GitHub Copilot to demonstrate how to use development tools powered by LLMs,\\nincluding powerful GPT 4o, o1-preview (Preview), and o1-mini (Preview). To illustrate its capabilities, we input\\nthe description of the ‚Äú5. Longest Palindromic Substring‚Äù problem from LeetCode into Copilot‚Äôs chat box. The\\ncode generated by Copilot is then submitted to the online judge platform, where it is successfully accepted.\\na significant portion of the public code available on GitHub, which enables it to understand a wide\\nrange of programming languages and coding styles. Copilot not only improves productivity but\\nalso serves as a learning tool by providing programmers with examples of how certain functions\\ncan be implemented or how specific problems can be solved.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:52\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nCodeGeeX. CodeGeeX stands out as a multifaceted programming assistant, proficient in code\\ncompletion, comment generation, code translation, and developer interactions. Its underlying code\\ngeneration LLM has been refined with extensive training on vast amounts of code data, exhibiting\\nsuperior performance on benchmarks like HumanEval, HumanEval-X, and DS1000. Renowned for\\nsupporting multilingual code generation, CodeGeeX plays a pivotal role in enhancing the efficiency\\nof code development.\\nCodeWhisperer. Amazon‚Äôs CodeWhisperer is a versatile, machine learning-driven code genera-\\ntor that offers on-the-fly code recommendations. Tailored to your coding patterns and comments,\\nCodeWhisperer provides personalized suggestions that range from succinct comments to complex\\nfunctions, all aimed at streamlining your coding workflow.\\nCodeium. Codeium is an AI-accelerated coding toolkit that offers a suite of functions, including',\n",
       " 'functions, all aimed at streamlining your coding workflow.\\nCodeium. Codeium is an AI-accelerated coding toolkit that offers a suite of functions, including\\ncode completion, explanation, translation, search, and user chatting. Compatible with over 70\\nprogramming languages, Codeium delivers fast and cutting-edge solutions to coding challenges,\\nsimplifying the development process for its users.\\nCodeArts Snap. Huawei‚Äôs CodeArts Snap is capable of generating comprehensive function-level\\ncode from both Chinese and English descriptions. This tool not only reduces the monotony of\\nmanual coding but also efficiently generates test code, in addition to providing automatic code\\nanalysis and repair services.\\nTabnine. Tabnine is an AI coding assistant that empowers development teams to leverage\\nAI for streamlining the software development lifecycle while maintaining strict standards for\\nprivacy, security, and compliance. With a focus on enhancing coding efficiency, code quality, and',\n",
       " 'AI for streamlining the software development lifecycle while maintaining strict standards for\\nprivacy, security, and compliance. With a focus on enhancing coding efficiency, code quality, and\\ndeveloper satisfaction, Tabnine offers AI-driven automation that is tailored to the needs of your\\nteam. Supporting over one million developers worldwide, Tabnine is applicable across various\\nindustries.\\nReplit. Replit is a multifunctional platform that caters to a diverse array of software development\\nneeds. As a complimentary online IDE, it facilitates code collaboration, and cloud services, and\\nfosters a thriving developer community. Replit also enables users to compile and execute code in\\nmore than 50 programming languages directly within a web browser, eliminating the need for local\\nsoftware installations.\\nTo illustrate the use of development tools powered by LLMs, we employ GitHub Copilot within\\nVisual Studio Code (VS Code) as our example. Note that',\n",
       " 'software installations.\\nTo illustrate the use of development tools powered by LLMs, we employ GitHub Copilot within\\nVisual Studio Code (VS Code) as our example. Note that\\n1‚óãFor details on using the GitHub Copilot extension in VS Code, please refer to the useful\\ndocument at https://code.visualstudio.com/docs/copilot/overview.\\n2‚óãIf you would like to get free access to Copilot as a student, teacher, or open-source maintainer,\\nplease refer to this tutorial at https://docs.github.com/en/copilot/managing-copilot/managing-\\ncopilot-as-an-individual-subscriber/managing-your-copilot-subscription/getting-free-access-\\nto-copilot-as-a-student-teacher-or-maintainer and GitHub education application portal at\\nhttps://education.github.com/discount_requests/application.\\nAs depicted in the upper section of Figure 19, users can interact with Copilot through the chat box in\\nthe lower left corner, where they can inquire about various coding-related tasks. This feature is now',\n",
       " 'the lower left corner, where they can inquire about various coding-related tasks. This feature is now\\nsupported by the advanced capabilities of GPT-4o, o1-preview (Preview), and o1-mini (Preview).\\nFrom the generated content, Copilot demonstrates the ability to plan solutions to coding problems.\\nIt can write code and subsequently explain the generated code to enhance user comprehension.\\nWithin the right-side workspace, users can engage in inline chat conversations to generate or\\nrefactor source code, conduct code explanations, fix coding errors, resolve issues encountered\\nduring terminal command executions, produce documentation comments, and generate unit tests.\\nTo illustrate its capabilities, we input the description of the ‚Äú5. Longest Palindromic Substring‚Äù\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:53\\nproblem from LeetCode into Copilot‚Äôs chat box. The code generated by Copilot is then submitted\\nto the online judge platform, where it is successfully accepted, as shown at the lower section of\\nFigure 19.\\n6\\nCHALLENGES & OPPORTUNITIES\\nAccording to our investigations, the LLMs have revolutionized the paradigm of code generation\\nand achieved remarkable performance. Despite this promising progress, there are still numerous\\nchallenges that need to be addressed. These challenges are mainly caused by the gap between\\nacademia and practical development. For example, in academia, the HumanEval benchmark has\\nbeen established as a de facto standard for evaluating the coding proficiency of LLMs. However,\\nmany works have illustrated the evaluation of HumanEval can‚Äôt reflect the scenario of practical\\ndevelopment [68, 72, 123, 162]. In contrast, these serious challenges offer substantial opportunities',\n",
       " 'many works have illustrated the evaluation of HumanEval can‚Äôt reflect the scenario of practical\\ndevelopment [68, 72, 123, 162]. In contrast, these serious challenges offer substantial opportunities\\nfor further research and applications. In this section, we pinpoint critical challenges and identify\\npromising opportunities, aiming to bridge the research-practicality divide.\\nEnhancing complex code generation at repository and software scale. In practical de-\\nvelopment scenarios, it often involves a large number of complex programming problems of\\nvarying difficulty levels [151, 311]. While LLMs have shown proficiency in generating function-\\nlevel code snippets, these models often struggle with more complex, unseen programming problems,\\nrepository- and software-level problems that are commonplace in real-world software develop-\\nment. To this end, it requires strong problem-solving skills in LLM beyond simply functional-level',\n",
       " 'repository- and software-level problems that are commonplace in real-world software develop-\\nment. To this end, it requires strong problem-solving skills in LLM beyond simply functional-level\\ncode generation. For example, AlphaCode [151] achieved an average ranking in the top 54.3% in\\nprogramming competitions where an understanding of algorithms and complex natural language is\\nrequired to solve competitive programming problems. [123] argues that existing LLMs can‚Äôt resolve\\nreal-world GitHub issues well since the best-performing model, Claude 2, is able to solve a mere\\n1.96% of the issues. The reason for poor performance is mainly attributed to the weak reasoning\\ncapabilities [105], complex internal- and external- dependencies [22], and context length limitation\\nof LLMs [22]. Therefore, the pursuit of models that can handle more complex, repository- and\\nsoftware-level code generation opens up new avenues for automation in software development',\n",
       " 'of LLMs [22]. Therefore, the pursuit of models that can handle more complex, repository- and\\nsoftware-level code generation opens up new avenues for automation in software development\\nand makes programming more productive and accessible.\\nInnovating model architectures tuned to code structures. Due to their scalability and effec-\\ntiveness, Transformer-based LLM architectures have become dominant in solving code generation\\ntask. Nevertheless, they might not be optimally designed to capture the inherent structure and\\nsyntax of programming languages (PLs) [85, 86, 134, 175]. Code has a highly structured nature,\\nwith a syntax that is more rigid than natural language. This presents a unique challenge for LLMs,\\nwhich are often derived from models that were originally designed for natural language processing\\n(NLP). The development of novel model architectures that inherently understand and integrate the',\n",
       " 'which are often derived from models that were originally designed for natural language processing\\n(NLP). The development of novel model architectures that inherently understand and integrate the\\nstructural properties of code represents a significant opportunity to improve code generation and\\ncomprehension. Innovations such as tree-based neural networks [183], which mirror the abstract\\nsyntax tree (AST) representation of code, can offer a more natural way for models to learn and\\ngenerate programming languages. Additionally, leveraging techniques from the compiler theory,\\nsuch as intermediate representations (IR) [152], could enable models to operate on a more abstract\\nand generalizable level, making them effective across multiple programming languages [207]. By\\nexploring architectures beyond the traditional sequential models, researchers can unlock new\\npotentials in code generation.\\nCurating high-quality code data for pre-training and fine-tuning of LLMs. The efficacy',\n",
       " 'potentials in code generation.\\nCurating high-quality code data for pre-training and fine-tuning of LLMs. The efficacy\\nof LLMs largely depends on the quality and diversity of code datasets used during pre-training and\\nfine-tuning phases [133, 280, 328]. Currently, there is a scarcity of large, high-quality datasets that\\nencompass a wide range of programming tasks, styles, and languages. This limitation constrains the\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:54\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nability of LLMs to generalize across unseen programming tasks, different coding environments, and\\nreal-world software development scenarios. The development of more sophisticated data acquisition\\ntechniques, such as automated code repositories mining [158], advanced filtering algorithms, and\\ncode data synthesis [165] (see Section 5.2), can lead to the creation of richer datasets. Collaborations\\nwith industry partners (e.g., GitHub) could also facilitate access to proprietary codebases, thereby\\nenhancing the practical relevance of the training material. Furthermore, the adoption of open-source\\nmodels for dataset sharing can accelerate the collective effort to improve the breadth and depth of\\ncode data available for LLM research.\\nDeveloping comprehensive benchmarks and metrics for coding proficiency evaluation\\nin LLMs. Current benchmarks like HumanEval may not capture the full spectrum of coding',\n",
       " 'Developing comprehensive benchmarks and metrics for coding proficiency evaluation\\nin LLMs. Current benchmarks like HumanEval may not capture the full spectrum of coding\\nskills required for practical software development [191]. Additionally, metrics often focus on\\nsyntactic correctness or functional accuracy, neglecting aspects such as code efficiency [208],\\nstyle [44], readability [34], or maintainability [15]. The design of comprehensive benchmarks that\\nsimulate real-world software development challenges could provide a more accurate assessment\\nof LLMs‚Äô coding capabilities. These benchmarks should include diverse programming tasks of\\nvarying difficulty levels, such as debugging [325], refactoring [237], and optimization [112], and\\nshould be complemented by metrics that evaluate qualitative aspects of code. The establishment of\\ncommunity-driven benchmarking platforms could facilitate continuous evaluation and comparison\\nof LLMs for code generation across the industry and academia.',\n",
       " 'community-driven benchmarking platforms could facilitate continuous evaluation and comparison\\nof LLMs for code generation across the industry and academia.\\nSupport for low-resource, low-level, and domain-specific programming languages. LLMs\\nare predominantly trained in popular high-level programming languages, leaving low-resource, low-\\nlevel, and domain-specific languages underrepresented. This lack of focus restricts the applicability\\nof LLMs in certain specialized fields and systems programming [250]. Intensifying research on\\ntransfer learning and meta-learning approaches may enable LLMs to leverage knowledge from\\nhigh-resource languages to enhance their performance on less common ones [38, 46]. Additionally,\\npartnerships with domain experts can guide the creation of targeted datasets and fine-tuning\\nstrategies to better serve niche markets. The development of LLMs with a capacity for multilingual',\n",
       " 'partnerships with domain experts can guide the creation of targeted datasets and fine-tuning\\nstrategies to better serve niche markets. The development of LLMs with a capacity for multilingual\\ncode generation also presents a significant opportunity for broadening the scope of applications.\\nContinuous learning for LLMs to keep pace with evolving coding knowledge. The\\nsoftware development landscape is continuously evolving, with new languages, frameworks, and\\nbest practices emerging regularly. LLMs risk becoming outdated if they cannot adapt to these\\nchanges and incorporate the latest programming knowledge [115, 264]. While retrieval augmented\\ncode generation mitigates these issues, the performance is limited by the quality of the retrieval\\ncontext While retrieval-augmented code generation offers a partial solution to these issues, its\\neffectiveness is inherently constrained by the quality of retrieved context. [171, 309, 330]. Therefore,',\n",
       " 'effectiveness is inherently constrained by the quality of retrieved context. [171, 309, 330]. Therefore,\\nestablishing mechanisms for continuous learning and updating of LLMs can help maintain their\\nrelevance over time. This could involve real-time monitoring of code repositories to identify trends\\nand innovations, as well as the creation of incremental learning systems that can assimilate new\\ninformation without forgetting previously acquired knowledge. Engaging the LLMs in active\\nlearning scenarios where they interact with human developers may also foster ongoing knowledge\\nacquisition.\\nEnsuring code safety and aligning LLM outputs with human coding preferences. Ensuring\\nthe safety and security of code generated by LLMs is a paramount concern, as is their ability to\\nalign with human preferences and ethical standards. Current models may inadvertently introduce\\nvulnerabilities or generate code that does not adhere to desired norms [48, 293]. Research into',\n",
       " 'align with human preferences and ethical standards. Current models may inadvertently introduce\\nvulnerabilities or generate code that does not adhere to desired norms [48, 293]. Research into\\nthe integration of formal verification tools within the LLM pipeline can enhance the safety of the\\nproduced code. Additionally, developing frameworks for alignment learning that capture and reflect\\nhuman ethical preferences can ensure that the code generation process aligns with societal values\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:55\\n[200, 212]. Transparent and explainable AI methodologies can also contribute to building trust in\\nthe LLM-generated code by making the decision-making process more accessible to developers.\\n7\\nCONCLUSION\\nIn this survey, we provide a systematic literature review, serving as a valuable reference for\\nresearchers investigating the cutting-edge progress in LLMs for code generation. A thorough\\nintroduction and analysis for data curation, the latest advances, performance evaluation, ethical\\nimplications, environmental impact, and real-world applications are illustrated. In addition, we\\npresent a historical overview of the evolution of LLMs for code generation in recent years and\\noffer an empirical comparison using the widely recognized HumanEval, MBPP, and the more\\npractical and challenging BigCodeBench benchmarks to highlight the progressive enhancements',\n",
       " 'offer an empirical comparison using the widely recognized HumanEval, MBPP, and the more\\npractical and challenging BigCodeBench benchmarks to highlight the progressive enhancements\\nin LLM capabilities for code generation. Critical challenges and promising opportunities regarding\\nthe gap between academia and practical development are also identified for future investigation.\\nFurthermore, we have established a dedicated resource website to continuously document and\\ndisseminate the most recent advances in the field. We hope this survey can contribute to a compre-\\nhensive and systematic overview of LLM for code generation and promote its thriving evolution.\\nWe optimistically believe that LLM will ultimately change all aspects of coding and automatically\\nwrite safe, helpful, accurate, trustworthy, and controllable code, like professional programmers,\\nand even solve coding problems that currently cannot be solved by humans.\\nREFERENCES',\n",
       " 'write safe, helpful, accurate, trustworthy, and controllable code, like professional programmers,\\nand even solve coding problems that currently cannot be solved by humans.\\nREFERENCES\\n[1] 2023. AgentGPT: Assemble, configure, and deploy autonomous AI Agents in your browser. https://github.com/\\nreworkd/AgentGPT.\\n[2] 2023. AutoGPT is the vision of accessible AI for everyone, to use and to build on. https://github.com/Significant-\\nGravitas/AutoGPT.\\n[3] 2023. BabyAGI. https://github.com/yoheinakajima/babyagi.\\n[4] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach,\\nAmit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: A highly capable language model\\nlocally on your phone. arXiv preprint arXiv:2404.14219 (2024).\\n[5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,',\n",
       " 'locally on your phone. arXiv preprint arXiv:2404.14219 (2024).\\n[5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\\n(2023).\\n[6] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A Transformer-based Approach for\\nSource Code Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\\n4998‚Äì5007.\\n[7] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified pre-training for program\\nunderstanding and generation. arXiv preprint arXiv:2103.06333 (2021).\\n[8] Ali Al-Kaswan, Maliheh Izadi, and Arie Van Deursen. 2024. Traces of memorisation in large language models for\\ncode. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. 1‚Äì12.',\n",
       " 'code. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. 1‚Äì12.\\n[9] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas\\nMuennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. SantaCoder: don‚Äôt reach for the stars! arXiv preprint\\narXiv:2301.03988 (2023).\\n[10] Miltiadis Allamanis and Charles Sutton. 2014. Mining idioms from source code. In Proceedings of the 22nd acm sigsoft\\ninternational symposium on foundations of software engineering. 472‚Äì483.\\n[11] Google DeepMind AlphaCode Team. 2023. AlphaCode 2 Technical Report. https://storage.googleapis.com/deepmind-\\nmedia/AlphaCode2/AlphaCode2_Tech_Report.pdf.\\n[12] Amazon. 2022. What is CodeWhisperer? https://docs.aws.amazon.com/codewhisperer/latest/userguide/what-is-\\ncwspr.html.\\n[13] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019.',\n",
       " 'cwspr.html.\\n[13] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019.\\nMathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. In Proceedings of the\\n2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers). 2357‚Äì2367.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:56\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[14] Anthropic. 2024.\\nThe Claude 3 Model Family: Opus, Sonnet, Haiku.\\nhttps://www-cdn.anthropic.com/\\nde8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.\\n[15] Luca Ardito, Riccardo Coppola, Luca Barbato, and Diego Verga. 2020. A tool-based perspective on software code\\nmaintainability metrics: a systematic literature review. Scientific Programming 2020 (2020), 1‚Äì26.\\n[16] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad,\\nShiqi Wang, Qing Sun, Mingyue Shang, et al. 2022. Multi-lingual evaluation of code generation models. arXiv preprint\\narXiv:2210.14868 (2022).\\n[17] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie\\nCai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732\\n(2021).',\n",
       " 'Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732\\n(2021).\\n[18] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450\\n(2016).\\n[19] Hannah McLean Babe, Sydney Nguyen, Yangtian Zi, Arjun Guha, Molly Q Feldman, and Carolyn Jane Anderson. 2023.\\nStudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code. arXiv:2306.04556 [cs.LG]\\n[20] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang,\\net al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023).\\n[21] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna\\nGoldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv\\npreprint arXiv:2212.08073 (2022).',\n",
       " 'Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv\\npreprint arXiv:2212.08073 (2022).\\n[22] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok,\\nShashank Shet, et al. 2023. Codeplan: Repository-level coding using llms and planning. arXiv preprint arXiv:2309.12499\\n(2023).\\n[23] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation\\nwith human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine\\ntranslation and/or summarization. 65‚Äì72.\\n[24] Enrico Barbierato, Marco L Della Vedova, Daniele Tessera, Daniele Toti, and Nicola Vanoli. 2022. A methodology for\\ncontrolling bias and fairness in synthetic data generation. Applied Sciences 12, 9 (2022), 4619.\\n[25] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with',\n",
       " '[25] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with\\ncode-generating models. Proceedings of the ACM on Programming Languages 7, OOPSLA1 (2023), 85‚Äì111.\\n[26] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi\\nDu, Zhe Fu, et al. 2024. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint\\narXiv:2401.02954 (2024).\\n[27] Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan, Fangxin Lu, Zili Zhang, Yulei Sui, Xuanhua Shi,\\nand Hai Jin. 2024. Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler\\nFeedback. arXiv preprint arXiv:2403.16792 (2024).\\n[28] Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini Kalliamvakou, Travis Lowdermilk, and\\nIdan Gazit. 2022. Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools.\\nQueue 20, 6 (2022), 35‚Äì57.',\n",
       " 'Idan Gazit. 2022. Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools.\\nQueue 20, 6 (2022), 35‚Äì57.\\n[29] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy,\\nKyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. arXiv\\npreprint arXiv:2204.06745 (2022).\\n[30] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive\\nLanguage Modeling with Mesh-Tensorflow. https://doi.org/10.5281/zenodo.5297715 If you use this software, please\\ncite it using these metadata..\\n[31] Veronika Bogina, Alan Hartman, Tsvi Kuflik, and Avital Shulner-Tal. 2022. Educating software and AI stakeholders\\nabout algorithmic fairness, accountability, transparency and ethics. International Journal of Artificial Intelligence in\\nEducation (2022), 1‚Äì26.',\n",
       " 'about algorithmic fairness, accountability, transparency and ethics. International Journal of Artificial Intelligence in\\nEducation (2022), 1‚Äì26.\\n[32] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,\\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models.\\narXiv preprint arXiv:2108.07258 (2021).\\n[33] Tom B Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).\\n[34] Raymond PL Buse and Westley R Weimer. 2009. Learning a metric for code readability. IEEE Transactions on software\\nengineering 36, 4 (2009), 546‚Äì558.\\n[35] Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, and Jiayi Huang. 2024. Shortcut-connected Expert\\nParallelism for Accelerating Mixture-of-Experts. arXiv preprint arXiv:2404.05019 (2024).\\n[36] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. A survey on mixture of experts.',\n",
       " '[36] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. A survey on mixture of experts.\\narXiv preprint arXiv:2407.06204 (2024).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:57\\n[37] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts,\\nTom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th\\nUSENIX Security Symposium (USENIX Security 21). 2633‚Äì2650.\\n[38] Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg,\\nAbhinav Jangda, and Arjun Guha. 2023. Knowledge Transfer from High-Resource to Low-Resource Programming\\nLanguages for Code LLMs. arXiv preprint arXiv:2308.09895 (2023).\\n[39] Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho\\nYee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. 2022. A scalable and extensible approach to\\nbenchmarking nl2code for 18 programming languages. arXiv preprint arXiv:2208.08227 (2022).',\n",
       " 'Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. 2022. A scalable and extensible approach to\\nbenchmarking nl2code for 18 programming languages. arXiv preprint arXiv:2208.08227 (2022).\\n[40] Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, and Hua Wu. 2022. ERNIE-Code: Beyond english-centric\\ncross-lingual pretraining for programming languages. arXiv preprint arXiv:2212.06742 (2022).\\n[41] Shubham Chandel, Colin B Clement, Guillermo Serrato, and Neel Sundaresan. 2022. Training and evaluating a jupyter\\nnotebook data science assistant. arXiv preprint arXiv:2201.12901 (2022).\\n[42] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang,\\nYidong Wang, et al. 2024. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems\\nand Technology 15, 3 (2024), 1‚Äì45.\\n[43] Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA model for code generation. https://github.',\n",
       " 'and Technology 15, 3 (2024), 1‚Äì45.\\n[43] Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA model for code generation. https://github.\\ncom/sahil280114/codealpaca.\\n[44] Binger Chen and Ziawasch Abedjan. 2023. DUETCS: Code Style Transfer through Generation and Retrieval. In 2023\\nIEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2362‚Äì2373.\\n[45] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. Codet:\\nCode generation with generated tests. arXiv preprint arXiv:2207.10397 (2022).\\n[46] Fuxiang Chen, Fatemeh H Fard, David Lo, and Timofey Bryksin. 2022. On the transferability of pre-trained language\\nmodels for low-resource programming languages. In Proceedings of the 30th IEEE/ACM International Conference on\\nProgram Comprehension. 401‚Äì412.\\n[47] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented',\n",
       " 'Program Comprehension. 401‚Äì412.\\n[47] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented\\ngeneration. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 17754‚Äì17762.\\n[48] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\\nYuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv\\npreprint arXiv:2107.03374 (2021).\\n[49] Stanley F Chen, Douglas Beeferman, and Roni Rosenfeld. 1998. Evaluation metrics for language models. (1998).\\n[50] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling\\ncomputation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588 (2022).\\n[51] Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, and Denny Zhou. 2023. Teaching large language models to self-debug.\\narXiv preprint arXiv:2304.05128 (2023).',\n",
       " '[51] Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, and Denny Zhou. 2023. Teaching large language models to self-debug.\\narXiv preprint arXiv:2304.05128 (2023).\\n[52] Xinyun Chen, Chang Liu, and Dawn Song. 2018. Tree-to-tree neural networks for program translation. Advances in\\nneural information processing systems 31 (2018).\\n[53] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. 2023. Reducing\\nthe Carbon Impact of Generative AI Inference (today and in 2035). In Proceedings of the 2nd workshop on sustainable\\ncomputer systems. 1‚Äì7.\\n[54] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.\\nJournal of Machine Learning Research 24, 240 (2023), 1‚Äì113.\\n[55] Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng',\n",
       " 'Journal of Machine Learning Research 24, 240 (2023), 1‚Äì113.\\n[55] Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng\\nXiao, Bo Shen, Lin Li, et al. 2022. Pangu-coder: Program synthesis with function-level language modeling. arXiv\\npreprint arXiv:2207.11280 (2022).\\n[56] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa\\nDehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine\\nLearning Research 25, 70 (2024), 1‚Äì53.\\n[57] Colin B Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan. 2020. PyMT5:\\nmulti-mode translation of natural language and Python code with transformers. arXiv preprint arXiv:2010.03150\\n(2020).\\n[58] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry',\n",
       " '(2020).\\n[58] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\\nTworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint\\narXiv:2110.14168 (2021).\\n[59] CodeGemma Team, Ale Jakse Hartman, Andrea Hu, Christopher A. Choquette-Choo, Heri Zhao, Jane Fine, Jeffrey\\nHui, Jingyue Shen, Joe Kelley, Joshua Howland, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Nam Nguyen, Paul Michel,\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:58\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\nPeter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Siqi Zuo, Tris Warkentin, and Zhitao\\net al. Gong. 2024. CodeGemma: Open Code Models Based on Gemma. (2024). https://goo.gle/codegemma\\n[60] Codeium. 2023. Free, ultrafast Copilot alternative for Vim and Neovim. https://github.com/Exafunction/codeium.vim.\\n[61] Cognition. 2024. Introducing Devin, the first AI software engineer. https://www.cognition.ai/introducing-devin.\\n[62] Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. The Journal of\\nMachine Learning Research 11 (2010), 3053‚Äì3096.\\n[63] Cognitive Computations. 2023. oa_leet10k. https://huggingface.co/datasets/cognitivecomputations/oa_leet10k.\\n[64] Leonardo De Moura and Nikolaj Bj√∏rner. 2008. Z3: An efficient SMT solver. In International conference on Tools and\\nAlgorithms for the Construction and Analysis of Systems. Springer, 337‚Äì340.',\n",
       " '[64] Leonardo De Moura and Nikolaj Bj√∏rner. 2008. Z3: An efficient SMT solver. In International conference on Tools and\\nAlgorithms for the Construction and Analysis of Systems. Springer, 337‚Äì340.\\n[65] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized\\nllms. Advances in Neural Information Processing Systems 36 (2024).\\n[66] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\\n[67] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min\\nChan, Weize Chen, et al. 2022. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained\\nlanguage models. arXiv preprint arXiv:2203.06904 (2022).\\n[68] Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh',\n",
       " 'language models. arXiv preprint arXiv:2203.06904 (2022).\\n[68] Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh\\nNallapati, Parminder Bhatia, Dan Roth, et al. 2024. Crosscodeeval: A diverse and multilingual benchmark for cross-file\\ncode completion. Advances in Neural Information Processing Systems 36 (2024).\\n[69] Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia,\\nDan Roth, and Bing Xiang. 2022. Cocomic: Code completion by jointly modeling in-file and cross-file context. arXiv\\npreprint arXiv:2212.10007 (2022).\\n[70] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022.\\nA survey on in-context learning. arXiv preprint arXiv:2301.00234 (2022).\\n[71] Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie Shan, Caishuang Huang, Wei Shen, Xiaoran Fan,',\n",
       " 'A survey on in-context learning. arXiv preprint arXiv:2301.00234 (2022).\\n[71] Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie Shan, Caishuang Huang, Wei Shen, Xiaoran Fan,\\nZhiheng Xi, et al. 2024. StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback.\\narXiv preprint arXiv:2402.01391 (2024).\\n[72] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng,\\nand Yiling Lou. 2024. Evaluating large language models in class-level code generation. In Proceedings of the IEEE/ACM\\n46th International Conference on Software Engineering. 1‚Äì13.\\n[73] Hugging Face. 2023. Training CodeParrot from Scratch. https://github.com/huggingface/blog/blob/main/codeparrot.\\nmd.\\n[74] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M Zhang. 2023. Large\\nlanguage models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on',\n",
       " 'language models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on\\nSoftware Engineering: Future of Software Engineering (ICSE-FoSE). IEEE, 31‚Äì53.\\n[75] Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei Tan. 2023. Automated repair of programs\\nfrom large language models. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE,\\n1469‚Äì1481.\\n[76] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu,\\nDaxin Jiang, et al. 2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint\\narXiv:2002.08155 (2020).\\n[77] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke\\nZettlemoyer, and Mike Lewis. 2022. Incoder: A generative model for code infilling and synthesis. arXiv preprint\\narXiv:2204.05999 (2022).',\n",
       " 'Zettlemoyer, and Mike Lewis. 2022. Incoder: A generative model for code infilling and synthesis. arXiv preprint\\narXiv:2204.05999 (2022).\\n[78] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish\\nThite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint\\narXiv:2101.00027 (2020).\\n[79] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.\\nPal: Program-aided language models. In International Conference on Machine Learning. PMLR, 10764‚Äì10799.\\n[80] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023.\\nRetrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023).\\n[81] Linyuan Gong, Mostafa Elhoushi, and Alvin Cheung. 2024. AST-T5: Structure-Aware Pretraining for Code Generation',\n",
       " '[81] Linyuan Gong, Mostafa Elhoushi, and Alvin Cheung. 2024. AST-T5: Structure-Aware Pretraining for Code Generation\\nand Understanding. arXiv preprint arXiv:2401.03003 (2024).\\n[82] Alex Gu, Baptiste Rozi√®re, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I Wang. 2024. Cruxeval:\\nA benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065 (2024).\\n[83] Sumit Gulwani. 2010. Dimensions in program synthesis. In Proceedings of the 12th international ACM SIGPLAN\\nsymposium on Principles and practice of declarative programming. 13‚Äì24.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:59\\n[84] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan\\nJavaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks are all you need. arXiv preprint\\narXiv:2306.11644 (2023).\\n[85] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022. UniXcoder: Unified Cross-Modal\\nPre-training for Code Representation. In Proceedings of the 60th Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers). 7212‚Äì7225.\\n[86] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svy-\\natkovskiy, Shengyu Fu, et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint\\narXiv:2009.08366 (2020).\\n[87] Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023. Longcoder: A long-range pre-trained language',\n",
       " 'arXiv:2009.08366 (2020).\\n[87] Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023. Longcoder: A long-range pre-trained language\\nmodel for code completion. In International Conference on Machine Learning. PMLR, 12098‚Äì12107.\\n[88] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li,\\net al. 2024. DeepSeek-Coder: When the Large Language Model Meets Programming‚ÄìThe Rise of Code Intelligence.\\narXiv preprint arXiv:2401.14196 (2024).\\n[89] Aman Gupta, Deepak Bhatt, and Anubha Pandey. 2021. Transitioning from Real to Synthetic data: Quantifying the\\nbias in model. arXiv preprint arXiv:2105.04144 (2021).\\n[90] Aman Gupta, Anup Shirgaonkar, Angels de Luis Balaguer, Bruno Silva, Daniel Holstein, Dawei Li, Jennifer Marsman,\\nLeonardo O Nunes, Mahsa Rouzbahman, Morris Sharp, et al. 2024. RAG vs Fine-tuning: Pipelines, Tradeoffs, and a\\nCase Study on Agriculture. arXiv preprint arXiv:2401.08406 (2024).',\n",
       " 'Leonardo O Nunes, Mahsa Rouzbahman, Morris Sharp, et al. 2024. RAG vs Fine-tuning: Pipelines, Tradeoffs, and a\\nCase Study on Agriculture. arXiv preprint arXiv:2401.08406 (2024).\\n[91] Hossein Hajipour, Keno Hassler, Thorsten Holz, Lea Sch√∂nherr, and Mario Fritz. 2024. CodeLMSec Benchmark:\\nSystematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models. In 2024 IEEE\\nConference on Secure and Trustworthy Machine Learning (SaTML). IEEE, 684‚Äì709.\\n[92] Perttu H√§m√§l√§inen, Mikke Tavast, and Anton Kunnari. 2023. Evaluating large language models in generating synthetic\\nhci research data: a case study. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems.\\n1‚Äì19.\\n[93] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning\\nwith language model is planning with world model. arXiv preprint arXiv:2305.14992 (2023).',\n",
       " 'with language model is planning with world model. arXiv preprint arXiv:2305.14992 (2023).\\n[94] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In\\nProceedings of the IEEE conference on computer vision and pattern recognition. 770‚Äì778.\\n[95] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\\nSteinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874\\n(2021).\\n[96] Felipe Hoffa. 2016. GitHub on BigQuery: Analyze all the open source code. URL: https://cloud.google.com/blog/topics/\\npublic-datasets/github-on-bigquery-analyze-all-the-open-source-code (2016).\\n[97] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las\\nCasas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language\\nmodels. arXiv preprint arXiv:2203.15556 (2022).',\n",
       " 'Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language\\nmodels. arXiv preprint arXiv:2203.15556 (2022).\\n[98] Samuel Holt, Max Ruiz Luyten, and Mihaela van der Schaar. 2023. L2MAC: Large Language Model Automatic\\nComputer for Unbounded Code Generation. In The Twelfth International Conference on Learning Representations.\\n[99] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration.\\narXiv preprint arXiv:1904.09751 (2019).\\n[100] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing\\nYau, Zijuan Lin, Liyang Zhou, et al. 2023. Metagpt: Meta programming for multi-agent collaborative framework.\\narXiv preprint arXiv:2308.00352 (2023).\\n[101] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang.',\n",
       " 'arXiv preprint arXiv:2308.00352 (2023).\\n[101] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang.\\n2024. Large Language Models for Software Engineering: A Systematic Literature Review. arXiv:2308.10620 [cs.SE]\\n[102] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,\\nMona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International conference on\\nmachine learning. PMLR, 2790‚Äì2799.\\n[103] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\n2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).\\n[104] Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and Heming Cui. 2023. AgentCoder: Multi-Agent-based Code\\nGeneration with Iterative Testing and Optimisation. arXiv preprint arXiv:2312.13010 (2023).',\n",
       " 'Generation with Iterative Testing and Optimisation. arXiv preprint arXiv:2312.13010 (2023).\\n[105] Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. arXiv preprint\\narXiv:2212.10403 (2022).\\n[106] Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards Reasoning in Large Language Models: A Survey. In 61st\\nAnnual Meeting of the Association for Computational Linguistics, ACL 2023. Association for Computational Linguistics\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:60\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n(ACL), 1049‚Äì1065.\\n[107] Junjie Huang, Chenglong Wang, Jipeng Zhang, Cong Yan, Haotian Cui, Jeevana Priya Inala, Colin Clement, Nan\\nDuan, and Jianfeng Gao. 2022. Execution-based evaluation for data science code generation models. arXiv preprint\\narXiv:2211.09374 (2022).\\n[108] Qiuyuan Huang, Naoki Wake, Bidipta Sarkar, Zane Durante, Ran Gong, Rohan Taori, Yusuke Noda, Demetri Ter-\\nzopoulos, Noboru Kuno, Ade Famoti, et al. 2024. Position Paper: Agent AI Towards a Holistic Intelligence. arXiv\\npreprint arXiv:2403.00833 (2024).\\n[109] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai\\nDang, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186 (2024).\\n[110] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet',\n",
       " '[110] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet\\nchallenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).\\n[111] Yoichi Ishibashi and Yoshimasa Nishimura. 2024. Self-Organized Agents: A LLM Multi-Agent Framework toward\\nUltra Large-Scale Code Generation and Optimization. arXiv preprint arXiv:2404.02183 (2024).\\n[112] Shu Ishida, Gianluca Corrado, George Fedoseev, Hudson Yeo, Lloyd Russell, Jamie Shotton, Jo√£o F Henriques, and\\nAnthony Hu. 2024. LangProp: A code optimization framework using Language Models applied to driving. arXiv\\npreprint arXiv:2401.10314 (2024).\\n[113] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Mapping Language to Code in Pro-\\ngrammatic Context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.\\n1643‚Äì1652.',\n",
       " 'grammatic Context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.\\n1643‚Äì1652.\\n[114] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu\\nWang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: Scaling language model instruction meta learning through\\nthe lens of generalization. arXiv preprint arXiv:2212.12017 (2022).\\n[115] Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Jungkyu Choi, and Minjoon\\nSeo. 2022. Towards Continual Knowledge Learning of Language Models. In 10th International Conference on Learning\\nRepresentations, ICLR 2022. International Conference on Learning Representations.\\n[116] Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. 1977. Perplexity‚Äîa measure of the difficulty of speech\\nrecognition tasks. The Journal of the Acoustical Society of America 62, S1 (1977), S63‚ÄìS63.',\n",
       " 'recognition tasks. The Journal of the Acoustical Society of America 62, S1 (1977), S63‚ÄìS63.\\n[117] Susmit Jha, Sumit Gulwani, Sanjit A Seshia, and Ashish Tiwari. 2010. Oracle-guided component-based program\\nsynthesis. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 1. 215‚Äì224.\\n[118] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi\\nZhou, Zhaowei Zhang, et al. 2023. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852 (2023).\\n[119] Ruyi Ji, Jingjing Liang, Yingfei Xiong, Lu Zhang, and Zhenjiang Hu. 2020. Question selection for interactive program\\nsynthesis. In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation.\\n1143‚Äì1158.\\n[120] Zhenlan Ji, Pingchuan Ma, Zongjie Li, and Shuai Wang. 2023. Benchmarking and explaining large language model-\\nbased code generation: A causality-centric approach. arXiv preprint arXiv:2310.06680 (2023).',\n",
       " 'based code generation: A causality-centric approach. arXiv preprint arXiv:2310.06680 (2023).\\n[121] Juyong Jiang and Sunghun Kim. 2023. CodeUp: A Multilingual Code Generation Llama2 Model with Parameter-\\nEfficient Instruction-Tuning. https://github.com/juyongjiang/CodeUp.\\n[122] Shuyang Jiang, Yuhao Wang, and Yu Wang. 2023. Selfevolve: A code evolution framework via large language models.\\narXiv preprint arXiv:2306.02907 (2023).\\n[123] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. 2023.\\nSWE-bench: Can Language Models Resolve Real-world Github Issues?. In The Twelfth International Conference on\\nLearning Representations.\\n[124] Alexander Wettig Kilian Lieret Shunyu Yao Karthik Narasimhan Ofir Press John Yang, Carlos E. Jimenez. 2024.\\nSWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING. (2024).\\nhttps://swe-agent.com/',\n",
       " 'SWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING. (2024).\\nhttps://swe-agent.com/\\n[125] Aravind Joshi and Owen Rambow. 2003. A formalism for dependency grammar based on tree adjoining grammar. In\\nProceedings of the Conference on Meaning-text Theory. MTT Paris, France, 207‚Äì216.\\n[126] Harshit Joshi, Jos√© Cambronero Sanchez, Sumit Gulwani, Vu Le, Gust Verbruggen, and Ivan Radiƒçek. 2023. Repair\\nis nearly generation: Multilingual program repair with llms. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, Vol. 37. 5131‚Äì5140.\\n[127] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford,\\nJeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\\n[128] Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty.',\n",
       " '[128] Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty.\\n2023. xcodeeval: A large scale multilingual multitask benchmark for code understanding, generation, translation and\\nretrieval. arXiv preprint arXiv:2303.03004 (2023).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:61\\n[129] Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, and Chanjun Park. 2024. sDPO:\\nDon‚Äôt Use Your Data All at Once. arXiv preprint arXiv:2403.19270 (2024).\\n[130] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim,\\nHyeonju Lee, Jihoo Kim, et al. 2023. Solar 10.7 b: Scaling large language models with simple yet effective depth\\nup-scaling. arXiv preprint arXiv:2312.15166 (2023).\\n[131] Barbara Kitchenham, O Pearl Brereton, David Budgen, Mark Turner, John Bailey, and Stephen Linkman. 2009.\\nSystematic literature reviews in software engineering‚Äìa systematic literature review. Information and software\\ntechnology 51, 1 (2009), 7‚Äì15.\\n[132] Denis Kocetkov, Raymond Li, LI Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Mu√±oz Ferrandis,\\nSean Hughes, Thomas Wolf, Dzmitry Bahdanau, et al. 2022. The Stack: 3 TB of permissively licensed source code.',\n",
       " 'Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, et al. 2022. The Stack: 3 TB of permissively licensed source code.\\nTransactions on Machine Learning Research (2022).\\n[133] Andreas K√∂pf, Yannic Kilcher, Dimitri von R√ºtte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum,\\nDuc Nguyen, Oliver Stanley, Rich√°rd Nagyfi, et al. 2024. Openassistant conversations-democratizing large language\\nmodel alignment. Advances in Neural Information Processing Systems 36 (2024).\\n[134] Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang. 2023. Is model attention aligned with human\\nattention? an empirical study on large language models for code generation. arXiv preprint arXiv:2306.01220 (2023).\\n[135] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised translation of\\nprogramming languages. arXiv preprint arXiv:2006.03511 (2020).',\n",
       " '[135] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised translation of\\nprogramming languages. arXiv preprint arXiv:2006.03511 (2020).\\n[136] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida\\nWang, and Tao Yu. 2023. DS-1000: A natural and reliable benchmark for data science code generation. In International\\nConference on Machine Learning. PMLR, 18319‚Äì18345.\\n[137] Hugo Lauren√ßon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao,\\nLeandro Von Werra, Chenghao Mou, Eduardo Gonz√°lez Ponferrada, Huu Nguyen, et al. 2022. The bigscience\\nroots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems 35 (2022),\\n31809‚Äì31826.\\n[138] Moritz Laurer. 2024. Synthetic data: save money, time and carbon with open source. https://huggingface.co/blog/\\nsynthetic-data-save-costs.',\n",
       " '31809‚Äì31826.\\n[138] Moritz Laurer. 2024. Synthetic data: save money, time and carbon with open source. https://huggingface.co/blog/\\nsynthetic-data-save-costs.\\n[139] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering\\ncode generation through pretrained models and deep reinforcement learning. Advances in Neural Information\\nProcessing Systems 35 (2022), 21314‚Äì21328.\\n[140] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Iliƒá, Daniel Hesslow, Roman Castagn√©, Alexan-\\ndra Sasha Luccioni, Fran√ßois Yvon, Matthias Gall√©, et al. 2023. Bloom: A 176b-parameter open-access multilingual\\nlanguage model. (2023).\\n[141] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and\\nAbhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint\\narXiv:2309.00267 (2023).',\n",
       " 'Abhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint\\narXiv:2309.00267 (2023).\\n[142] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning.\\narXiv preprint arXiv:2104.08691 (2021).\\n[143] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler,\\nMike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp\\ntasks. Advances in Neural Information Processing Systems 33 (2020), 9459‚Äì9474.\\n[144] Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. 2024. EvoCodeBench: An Evolving Code Generation\\nBenchmark Aligned with Real-World Code Repositories. arXiv preprint arXiv:2404.00599 (2024).\\n[145] Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. 2023. Towards enhancing in-context learning for code generation.\\narXiv preprint arXiv:2303.17780 (2023).',\n",
       " '[145] Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. 2023. Towards enhancing in-context learning for code generation.\\narXiv preprint arXiv:2303.17780 (2023).\\n[146] Li Li, Tegawend√© F Bissyand√©, Mike Papadakis, Siegfried Rasthofer, Alexandre Bartel, Damien Octeau, Jacques Klein,\\nand Le Traon. 2017. Static analysis of android apps: A systematic literature review. Information and Software\\nTechnology 88 (2017), 67‚Äì95.\\n[147] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\\nChristopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161\\n(2023).\\n[148] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B.\\nHashimoto. 2023. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-\\nlab/alpaca_eval.',\n",
       " 'Hashimoto. 2023. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-\\nlab/alpaca_eval.\\n[149] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint\\narXiv:2101.00190 (2021).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:62\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[150] Yuanzhi Li, S√©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks are\\nall you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 (2023).\\n[151] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R√©mi Leblond, Tom Eccles, James Keeling,\\nFelix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science 378, 6624\\n(2022), 1092‚Äì1097.\\n[152] Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang, Qiyi Tang, Sen Nie, and Shi Wu. 2022. Unleashing the power of\\ncompiler intermediate representation to enhance neural program embeddings. In Proceedings of the 44th International\\nConference on Software Engineering. 2253‚Äì2265.\\n[153] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameter-efficient\\nfine-tuning. arXiv preprint arXiv:2303.15647 (2023).',\n",
       " '[153] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameter-efficient\\nfine-tuning. arXiv preprint arXiv:2303.15647 (2023).\\n[154] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak\\nNarayanan, Yuhuai Wu, Ananya Kumar, et al. 2023. Holistic Evaluation of Language Models. Transactions on Machine\\nLearning Research (2023).\\n[155] Andreas Liesenfeld, Alianda Lopez, and Mark Dingemanse. 2023. Opening up ChatGPT: Tracking openness, trans-\\nparency, and accountability in instruction-tuned text generators. In Proceedings of the 5th international conference on\\nconversational user interfaces. 1‚Äì6.\\n[156] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.\\n74‚Äì81.\\n[157] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022. A survey of transformers. AI open 3 (2022),\\n111‚Äì132.',\n",
       " '74‚Äì81.\\n[157] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022. A survey of transformers. AI open 3 (2022),\\n111‚Äì132.\\n[158] Erik Linstead, Paul Rigor, Sushil Bajracharya, Cristina Lopes, and Pierre Baldi. 2007. Mining internet-scale software\\nrepositories. Advances in neural information processing systems 20 (2007).\\n[159] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai,\\nDaya Guo, et al. 2024. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv\\npreprint arXiv:2405.04434 (2024).\\n[160] Bingchang Liu, Chaoyu Chen, Cong Liao, Zi Gong, Huan Wang, Zhichao Lei, Ming Liang, Dajun Chen, Min Shen,\\nHailian Zhou, et al. 2023. Mftcoder: Boosting code llms with multitask fine-tuning. arXiv preprint arXiv:2311.02303\\n(2023).\\n[161] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel.',\n",
       " '(2023).\\n[161] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel.\\n2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural\\nInformation Processing Systems 35 (2022), 1950‚Äì1965.\\n[162] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is your code generated by chatgpt really\\ncorrect? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing\\nSystems 36 (2024).\\n[163] Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. 2023. Rltf: Reinforcement learning\\nfrom unit test feedback. arXiv preprint arXiv:2307.04349 (2023).\\n[164] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt,\\nand predict: A systematic survey of prompting methods in natural language processing. Comput. Surveys 55, 9 (2023),\\n1‚Äì35.',\n",
       " 'and predict: A systematic survey of prompting methods in natural language processing. Comput. Surveys 55, 9 (2023),\\n1‚Äì35.\\n[165] Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang,\\nDenny Zhou, et al. 2024. Best Practices and Lessons Learned on Synthetic Data for Language Models. arXiv preprint\\narXiv:2404.07503 (2024).\\n[166] Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, and Yang Liu. 2020. Retrieval-Augmented Generation for Code\\nSummarization via Hybrid GNN. In International Conference on Learning Representations.\\n[167] Tianyang Liu, Canwen Xu, and Julian McAuley. 2023.\\nRepobench: Benchmarking repository-level code auto-\\ncompletion systems. arXiv preprint arXiv:2306.03091 (2023).\\n[168] Yan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji Zhang, Daoguang Zan, Jian-Guang Lou, Pin-Yu Chen, and Tsung-Yi\\nHo. 2023. Uncovering and quantifying social biases in code generation. Advances in Neural Information Processing',\n",
       " 'Ho. 2023. Uncovering and quantifying social biases in code generation. Advances in Neural Information Processing\\nSystems 36 (2023), 2368‚Äì2380.\\n[169] Yue Liu, Chakkrit Tantithamthavorn, Li Li, and Yepang Liu. 2022. Deep learning for android malware defenses: a\\nsystematic literature review. Comput. Surveys 55, 8 (2022), 1‚Äì36.\\n[170] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang,\\nDmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. 2024. StarCoder 2 and The Stack v2: The Next Generation. arXiv\\npreprint arXiv:2402.19173 (2024).\\n[171] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy. 2022. ReACC: A Retrieval-\\nAugmented Code Completion Framework. In Proceedings of the 60th Annual Meeting of the Association for Computa-\\ntional Linguistics (Volume 1: Long Papers). 6227‚Äì6240.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:63\\n[172] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain,\\nDaxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and\\ngeneration. arXiv preprint arXiv:2102.04664 (2021).\\n[173] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin,\\nand Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. In The Twelfth\\nInternational Conference on Learning Representations.\\n[174] Michael R Lyu, Baishakhi Ray, Abhik Roychoudhury, Shin Hwei Tan, and Patanamon Thongtanunam. 2024. Automatic\\nProgramming: Large Language Models and Beyond. arXiv preprint arXiv:2405.02213 (2024).\\n[175] Wei Ma, Mengjie Zhao, Xiaofei Xie, Qiang Hu, Shangqing Liu, Jie Zhang, Wenhan Wang, and Yang Liu. 2022. Are',\n",
       " '[175] Wei Ma, Mengjie Zhao, Xiaofei Xie, Qiang Hu, Shangqing Liu, Jie Zhang, Wenhan Wang, and Yang Liu. 2022. Are\\nCode Pre-trained Models Powerful to Learn Code Syntax and Semantics? arXiv preprint arXiv:2212.10017 (2022).\\n[176] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\\nShrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. Advances in\\nNeural Information Processing Systems 36 (2024).\\n[177] James Manyika and Sissie Hsiao. 2023. An overview of Bard: an early experiment with generative AI. AI. Google\\nStatic Documents 2 (2023).\\n[178] Vadim Markovtsev, Waren Long, Hugo Mougard, Konstantin Slavnov, and Egor Bulychev. 2019. STYLE-ANALYZER:\\nfixing code style inconsistencies with interpretable unsupervised algorithms. In 2019 IEEE/ACM 16th International\\nConference on Mining Software Repositories (MSR). IEEE, 468‚Äì478.',\n",
       " 'fixing code style inconsistencies with interpretable unsupervised algorithms. In 2019 IEEE/ACM 16th International\\nConference on Mining Software Repositories (MSR). IEEE, 468‚Äì478.\\n[179] Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. 2022. Generating training data with language models: Towards\\nzero-shot language understanding. Advances in Neural Information Processing Systems 35 (2022), 462‚Äì477.\\n[180] Meta. 2024. Introducing Meta Llama 3: The most capable openly available LLM to date. https://ai.meta.com/blog/meta-\\nllama-3/.\\n[181] MistralAI. 2024. Codestral. https://mistral.ai/news/codestral/.\\n[182] S√©bastien Bubeck Mojan Javaheripi. 2023. Phi-2: The surprising power of small language models. https://www.\\nmicrosoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models.\\n[183] Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang. 2014. TBCNN: A tree-based convolutional neural network for\\nprogramming language processing. arXiv preprint arXiv:1409.5718 (2014).',\n",
       " '[183] Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang. 2014. TBCNN: A tree-based convolutional neural network for\\nprogramming language processing. arXiv preprint arXiv:1409.5718 (2014).\\n[184] Zahra Mousavi, Chadni Islam, Kristen Moore, Alsharif Abuadbba, and M Ali Babar. 2024. An investigation into\\nmisuse of java security apis by large language models. In Proceedings of the 19th ACM Asia Conference on Computer\\nand Communications Security. 1299‚Äì1315.\\n[185] Spyridon Mouselinos, Mateusz Malinowski, and Henryk Michalewski. 2022. A simple, yet effective approach to\\nfinding biases in code generation. arXiv preprint arXiv:2211.00609 (2022).\\n[186] Hussein Mozannar, Gagan Bansal, Adam Fourney, and Eric Horvitz. 2022. Reading between the lines: Modeling user\\nbehavior and costs in AI-assisted programming. arXiv preprint arXiv:2210.14306 (2022).\\n[187] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru',\n",
       " '[187] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru\\nTang, Leandro Von Werra, and Shayne Longpre. 2023. Octopack: Instruction tuning code large language models.\\narXiv preprint arXiv:2308.07124 (2023).\\n[188] King Han Naman Jain, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik\\nSen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for\\ncode. arXiv preprint arXiv:2403.07974 (2024).\\n[189] Antonio Nappa, Richard Johnson, Leyla Bilge, Juan Caballero, and Tudor Dumitras. 2015. The attack of the clones: A\\nstudy of the impact of shared code on vulnerability patching. In 2015 IEEE symposium on security and privacy. IEEE,\\n692‚Äì708.\\n[190] Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. 2023. Lever:',\n",
       " '692‚Äì708.\\n[190] Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. 2023. Lever:\\nLearning to verify language-to-code generation with execution. In International Conference on Machine Learning.\\nPMLR, 26106‚Äì26128.\\n[191] Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz,\\nCaiming Xiong, et al. 2023. L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language\\nModels. arXiv preprint arXiv:2309.17446 (2023).\\n[192] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. 2023. Codegen2: Lessons for\\ntraining llms on programming and natural languages. arXiv preprint arXiv:2305.02309 (2023).\\n[193] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022.\\nCodegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474\\n(2022).',\n",
       " 'Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474\\n(2022).\\n[194] Changan Niu, Chuanyi Li, Bin Luo, and Vincent Ng. 2022. Deep learning meets software engineering: A survey on\\npre-trained models of source code. arXiv preprint arXiv:2205.11739 (2022).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:64\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[195] Theo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. 2023. Is Self-Repair\\na Silver Bullet for Code Generation?. In The Twelfth International Conference on Learning Representations.\\n[196] OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/chatgpt.\\n[197] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/.\\n[198] OpenAI. 2024. New models and developer products announced at DevDay. https://openai.com/index/new-models-\\nand-developer-products-announced-at-devday/.\\n[199] OpenDevin. 2024. OpenDevin: Code Less, Make More. https://github.com/OpenDevin/OpenDevin.\\n[200] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.',\n",
       " 'Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.\\nAdvances in neural information processing systems 35 (2022), 27730‚Äì27744.\\n[201] Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023. Fine-tuning or retrieval? comparing\\nknowledge injection in llms. arXiv preprint arXiv:2312.05934 (2023).\\n[202] David N Palacio, Alejandro Velasco, Daniel Rodriguez-Cardenas, Kevin Moran, and Denys Poshyvanyk. 2023. Eval-\\nuating and explaining large language models for code using syntactic structures. arXiv preprint arXiv:2308.03873\\n(2023).\\n[203] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation\\nof machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics.\\n311‚Äì318.\\n[204] Nikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr, and Sergey Mechtaev.',\n",
       " '311‚Äì318.\\n[204] Nikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr, and Sergey Mechtaev.\\n2024. The Fact Selection Problem in LLM-Based Program Repair. arXiv preprint arXiv:2404.05520 (2024).\\n[205] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval Augmented\\nCode Generation and Summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021.\\n2719‚Äì2734.\\n[206] Arkil Patel, Siva Reddy, Dzmitry Bahdanau, and Pradeep Dasigi. 2023. Evaluating In-Context Learning of Libraries\\nfor Code Generation. arXiv preprint arXiv:2311.09635 (2023).\\n[207] Indraneil Paul, Jun Luo, Goran Glava≈°, and Iryna Gurevych. 2024. IRCoder: Intermediate Representations Make\\nLanguage Models Robust Multilingual Code Generators. arXiv preprint arXiv:2403.03894 (2024).\\n[208] Norman Peitek, Sven Apel, Chris Parnin, Andr√© Brechmann, and Janet Siegmund. 2021. Program comprehension and',\n",
       " '[208] Norman Peitek, Sven Apel, Chris Parnin, Andr√© Brechmann, and Janet Siegmund. 2021. Program comprehension and\\ncode complexity metrics: An fmri study. In 2021 IEEE/ACM 43rd International Conference on Software Engineering\\n(ICSE). IEEE, 524‚Äì536.\\n[209] Huy N Phan, Hoang N Phan, Tien N Nguyen, and Nghi DQ Bui. 2024. RepoHyper: Better Context Retrieval Is All You\\nNeed for Repository-Level Code Completion. arXiv preprint arXiv:2403.06095 (2024).\\n[210] Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, Ashish Datta, Maksym Zhu-\\nravinskyi, Dakota Mahan, Marco Bellagente, Carlos Riquelme, et al. 2024. Stable Code Technical Report. arXiv\\npreprint arXiv:2404.01226 (2024).\\n[211] Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input\\nlength extrapolation. arXiv preprint arXiv:2108.12409 (2021).',\n",
       " '[211] Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input\\nlength extrapolation. arXiv preprint arXiv:2108.12409 (2021).\\n[212] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023. Fine-tuning\\naligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693\\n(2023).\\n[213] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by\\ngenerative pre-training. (2018).\\n[214] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are\\nunsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.\\n[215] Steven Raemaekers, Arie Van Deursen, and Joost Visser. 2012. Measuring software library stability through historical\\nversion analysis. In 2012 28th IEEE international conference on software maintenance (ICSM). IEEE, 378‚Äì387.',\n",
       " 'version analysis. In 2012 28th IEEE international conference on software maintenance (ICSM). IEEE, 378‚Äì387.\\n[216] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct\\npreference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing\\nSystems 36 (2024).\\n[217] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\nPeter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine\\nlearning research 21, 140 (2020), 1‚Äì67.\\n[218] Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. 2022. Evaluating the text-to-sql capabilities of large\\nlanguage models. arXiv preprint arXiv:2204.00498 (2022).\\n[219] Aurora Ramirez, Jose Raul Romero, and Christopher L Simons. 2018. A systematic review of interaction in search-based',\n",
       " 'language models. arXiv preprint arXiv:2204.00498 (2022).\\n[219] Aurora Ramirez, Jose Raul Romero, and Christopher L Simons. 2018. A systematic review of interaction in search-based\\nsoftware engineering. IEEE Transactions on Software Engineering 45, 8 (2018), 760‚Äì781.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:65\\n[220] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Sori-\\ncut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding\\nacross millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024).\\n[221] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco,\\nand Shuai Ma. 2020. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297\\n(2020).\\n[222] Replit. 2016. Idea to software, fast. https://replit.com.\\n[223] Replit. 2023. replit-code-v1-3b. https://huggingface.co/replit/replit-code-v1-3b.\\n[224] Tal Ridnik, Dedy Kredo, and Itamar Friedman. 2024. Code Generation with AlphaCodium: From Prompt Engineering\\nto Flow Engineering. arXiv preprint arXiv:2401.08500 (2024).',\n",
       " '[224] Tal Ridnik, Dedy Kredo, and Itamar Friedman. 2024. Code Generation with AlphaCodium: From Prompt Engineering\\nto Flow Engineering. arXiv preprint arXiv:2401.08500 (2024).\\n[225] Nick Roshdieh. 2023. Evol-Instruct-Code-80k. https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1.\\n[226] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D Weisz. 2023. The programmer‚Äôs\\nassistant: Conversational interaction with a large language model for software development. In Proceedings of the\\n28th International Conference on Intelligent User Interfaces. 491‚Äì514.\\n[227] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal\\nRemez, J√©r√©my Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950\\n(2023).\\n[228] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy',\n",
       " '(2023).\\n[228] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy\\nKepner, Devesh Tiwari, and Vijay Gadepally. 2023. From words to watts: Benchmarking the energy costs of large\\nlanguage model inference. In 2023 IEEE High Performance Extreme Computing Conference (HPEC). IEEE, 1‚Äì9.\\n[229] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud\\nStiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask Prompted Training Enables Zero-Shot Task Generalization.\\nIn ICLR 2022-Tenth International Conference on Learning Representations.\\n[230] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization\\nalgorithms. arXiv preprint arXiv:1707.06347 (2017).\\n[231] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. 2021. You autocomplete me: Poisoning vulnera-',\n",
       " 'algorithms. arXiv preprint arXiv:1707.06347 (2017).\\n[231] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. 2021. You autocomplete me: Poisoning vulnera-\\nbilities in neural code completion. In 30th USENIX Security Symposium (USENIX Security 21). 1559‚Äì1575.\\n[232] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv\\npreprint arXiv:1803.02155 (2018).\\n[233] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017.\\nOutrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538\\n(2017).\\n[234] Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang\\nZhao, et al. 2023. Pangu-coder2: Boosting large language models for code with ranking feedback. arXiv preprint\\narXiv:2307.14936 (2023).',\n",
       " 'Zhao, et al. 2023. Pangu-coder2: Boosting large language models for code with ranking feedback. arXiv preprint\\narXiv:2307.14936 (2023).\\n[235] Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, and David Lo. 2024. Greening large language models\\nof code. In Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society.\\n142‚Äì153.\\n[236] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language\\nagents with verbal reinforcement learning. Advances in Neural Information Processing Systems 36 (2024).\\n[237] Atsushi Shirafuji, Yusuke Oda, Jun Suzuki, Makoto Morishita, and Yutaka Watanobe. 2023. Refactoring Programs\\nUsing Large Language Models with Few-Shot Examples. arXiv preprint arXiv:2311.11690 (2023).\\n[238] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based code generation using\\ndeep reinforcement learning. arXiv preprint arXiv:2301.13816 (2023).',\n",
       " '[238] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based code generation using\\ndeep reinforcement learning. arXiv preprint arXiv:2301.13816 (2023).\\n[239] Disha Shrivastava, Denis Kocetkov, Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. 2023. RepoFusion:\\nTraining Code Models to Understand Your Repository. arXiv preprint arXiv:2306.10998 (2023).\\n[240] Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. 2023. Repository-level prompt generation for large language\\nmodels of code. In International Conference on Machine Learning. PMLR, 31693‚Äì31715.\\n[241] Mukul Singh, Jos√© Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, and Gust Verbruggen. 2023. Codefusion: A\\npre-trained diffusion model for code generation. arXiv preprint arXiv:2310.17680 (2023).\\n[242] Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, and Tao Yu. 2024. ARKS: Active',\n",
       " '[242] Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, and Tao Yu. 2024. ARKS: Active\\nRetrieval in Knowledge Soup for Code Generation. arXiv preprint arXiv:2402.12317 (2024).\\n[243] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer\\nwith rotary position embedding. Neurocomputing 568 (2024), 127063.\\n[244] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: Code generation\\nusing transformer. In Proceedings of the 28th ACM joint meeting on European software engineering conference and\\nsymposium on the foundations of software engineering. 1433‚Äì1443.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:66\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[245] Marc Szafraniec, Baptiste Roziere, Hugh Leather, Francois Charton, Patrick Labatut, and Gabriel Synnaeve. 2022.\\nCode translation with compiler representations. In Proceedings of the Eleventh International Conference on Learning\\nRepresentations: ICLR.\\n[246] TabNine. 2018. AI Code Completions. https://github.com/codota/TabNine.\\n[247] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.\\nHashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_\\nalpaca.\\n[248] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\\nMorgane Rivi√®re, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and\\ntechnology. arXiv preprint arXiv:2403.08295 (2024).',\n",
       " 'Morgane Rivi√®re, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and\\ntechnology. arXiv preprint arXiv:2403.08295 (2024).\\n[249] Qwen Team. 2024. Code with CodeQwen1.5. https://qwenlm.github.io/blog/codeqwen1.5.\\n[250] Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt,\\nand Siddharth Garg. 2023. Benchmarking large language models for automated verilog rtl code generation. In 2023\\nDesign, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 1‚Äì6.\\n[251] theblackcat102. 2023.\\nThe evolved code alpaca dataset.\\nhttps://huggingface.co/datasets/theblackcat102/evol-\\ncodealpaca-v1.\\n[252] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste\\nRozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.\\narXiv preprint arXiv:2302.13971 (2023).',\n",
       " 'Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.\\narXiv preprint arXiv:2302.13971 (2023).\\n[253] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.\\narXiv preprint arXiv:2307.09288 (2023).\\n[254] Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. 2022. Natural language processing with transformers. \" O‚ÄôReilly\\nMedia, Inc.\".\\n[255] Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022. Expectation vs. experience: Evaluating the usability\\nof code generation tools powered by large language models. In Chi conference on human factors in computing systems\\nextended abstracts. 1‚Äì7.\\n[256] Boris Van Breugel, Zhaozhi Qian, and Mihaela Van Der Schaar. 2023. Synthetic data, real errors: how (not) to publish',\n",
       " 'extended abstracts. 1‚Äì7.\\n[256] Boris Van Breugel, Zhaozhi Qian, and Mihaela Van Der Schaar. 2023. Synthetic data, real errors: how (not) to publish\\nand use synthetic data. In International Conference on Machine Learning. PMLR, 34793‚Äì34808.\\n[257] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\\nPolosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).\\n[258] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https:\\n//github.com/kingoflolz/mesh-transformer-jax.\\n[259] Chong Wang, Jian Zhang, Yebo Feng, Tianlin Li, Weisong Sun, Yang Liu, and Xin Peng. 2024. Teaching Code LLMs to\\nUse Autocompletion Tools in Repository-Level Code Generation. arXiv preprint arXiv:2401.06391 (2024).\\n[260] Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. 2024. Software testing with',\n",
       " '[260] Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. 2024. Software testing with\\nlarge language models: Survey, landscape, and vision. IEEE Transactions on Software Engineering (2024).\\n[261] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen,\\nYankai Lin, et al. 2024. A survey on large language model based autonomous agents. Frontiers of Computer Science 18,\\n6 (2024), 1‚Äì26.\\n[262] Simin Wang, Liguo Huang, Amiao Gao, Jidong Ge, Tengfei Zhang, Haitao Feng, Ishna Satyarth, Ming Li, He Zhang, and\\nVincent Ng. 2022. Machine/deep learning for software engineering: A systematic literature review. IEEE Transactions\\non Software Engineering 49, 3 (2022), 1188‚Äì1231.\\n[263] Shiqi Wang, Li Zheng, Haifeng Qian, Chenghao Yang, Zijian Wang, Varun Kumar, Mingyue Shang, Samson Tan,\\nBaishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, and Bing Xiang. 2022.',\n",
       " 'Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, and Bing Xiang. 2022.\\nReCode: Robustness Evaluation of Code Generation Models. (2022). https://doi.org/10.48550/arXiv.2212.10264\\n[264] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. 2023. Knowledge editing for large language\\nmodels: A survey. arXiv preprint arXiv:2310.16218 (2023).\\n[265] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024. Executable code\\nactions elicit better llm agents. arXiv preprint arXiv:2402.01030 (2024).\\n[266] Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, and Qun Liu.\\n2022. Compilable Neural Code Generation with Compiler Feedback. In Findings of the Association for Computational\\nLinguistics: ACL 2022. 9‚Äì19.\\n[267] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny',\n",
       " 'Linguistics: ACL 2022. 9‚Äì19.\\n[267] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny\\nZhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171\\n(2022).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:67\\n[268] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.\\n2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. In The 61st Annual Meeting Of The\\nAssociation For Computational Linguistics.\\n[269] Yue Wang, Hung Le, Akhilesh Gotmare, Nghi Bui, Junnan Li, and Steven Hoi. 2023. CodeT5+: Open Code Large\\nLanguage Models for Code Understanding and Generation. In Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing. 1069‚Äì1088.\\n[270] Yanlin Wang and Hui Li. 2021. Code completion by modeling flattened abstract syntax trees as graphs. In Proceedings\\nof the AAAI conference on artificial intelligence, Vol. 35. 14015‚Äì14023.\\n[271] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-',\n",
       " 'of the AAAI conference on artificial intelligence, Vol. 35. 14015‚Äì14023.\\n[271] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-\\nDecoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing. 8696‚Äì8708.\\n[272] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and\\nQun Liu. 2023. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966 (2023).\\n[273] Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. 2022. Execution-based evaluation for open-domain\\ncode generation. arXiv preprint arXiv:2212.10481 (2022).\\n[274] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\\nQuoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).',\n",
       " 'Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).\\n[275] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\\nDenny Zhou, Donald Metzler, et al. 2022. Emergent Abilities of Large Language Models. Transactions on Machine\\nLearning Research (2022).\\n[276] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.\\nChain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing\\nsystems 35 (2022), 24824‚Äì24837.\\n[277] Xiaokai Wei, Sujan Kumar Gonugondla, Shiqi Wang, Wasi Ahmad, Baishakhi Ray, Haifeng Qian, Xiaopeng Li, Varun\\nKumar, Zijian Wang, Yuchen Tian, et al. 2023. Towards greener yet powerful code generation via quantization: An\\nempirical study. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the',\n",
       " 'empirical study. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the\\nFoundations of Software Engineering. 224‚Äì236.\\n[278] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need.\\narXiv preprint arXiv:2312.02120 (2023).\\n[279] Lilian Weng. 2023. LLM-powered Autonomous Agents. lilianweng.github.io (Jun 2023). https://lilianweng.github.io/\\nposts/2023-06-23-agent/\\n[280] Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024. QuRating: Selecting High-Quality Data for\\nTraining Language Models. arXiv preprint arXiv:2402.09739 (2024).\\n[281] Erroll Wood, Tadas Baltru≈°aitis, Charlie Hewitt, Sebastian Dziadzio, Thomas J Cashman, and Jamie Shotton. 2021.\\nFake it till you make it: face analysis in the wild using synthetic data alone. In Proceedings of the IEEE/CVF international\\nconference on computer vision. 3681‚Äì3691.',\n",
       " 'Fake it till you make it: face analysis in the wild using synthetic data alone. In Proceedings of the IEEE/CVF international\\nconference on computer vision. 3681‚Äì3691.\\n[282] Di Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Krishna Ramanathan, and Xiaofei Ma. 2024. Repoformer: Selective\\nRetrieval for Repository-Level Code Completion. arXiv preprint arXiv:2403.10059 (2024).\\n[283] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang,\\nand Chi Wang. 2023. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv\\npreprint arXiv:2308.08155 (2023).\\n[284] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin,\\nEnyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint\\narXiv:2309.07864 (2023).\\n[285] Rui Xie, Zhengran Zeng, Zhuohao Yu, Chang Gao, Shikun Zhang, and Wei Ye. 2024. CodeShell Technical Report.',\n",
       " 'arXiv:2309.07864 (2023).\\n[285] Rui Xie, Zhengran Zeng, Zhuohao Yu, Chang Gao, Shikun Zhang, and Wei Ye. 2024. CodeShell Technical Report.\\narXiv preprint arXiv:2403.15747 (2024).\\n[286] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S Liang. 2023. Data selection for language models via\\nimportance resampling. Advances in Neural Information Processing Systems 36 (2023), 34201‚Äì34227.\\n[287] Bowen Li Xingyao Wang and Graham Neubig. 2024. Introducing OpenDevin CodeAct 1.0, a new State-of-the-art in\\nCoding Agents. https://www.cognition.ai/introducing-devin.\\n[288] Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang Huang, and Lu Zhang. 2017. Precise condition\\nsynthesis for program repair. In 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE,\\n416‚Äì426.\\n[289] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.',\n",
       " '416‚Äì426.\\n[289] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.\\nWizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 (2023).\\n[290] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large\\nlanguage models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming.\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:68\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n1‚Äì10.\\n[291] Junjielong Xu, Ying Fu, Shin Hwei Tan, and Pinjia He. 2024. Aligning LLMs for FL-free Program Repair. arXiv preprint\\narXiv:2404.08877 (2024).\\n[292] Weiwei Xu, Kai Gao, Hao He, and Minghui Zhou. 2024. A First Look at License Compliance Capability of LLMs in\\nCode Generation. arXiv preprint arXiv:2408.02487 (2024).\\n[293] Zhou Yang, Zhensu Sun, Terry Zhuo Yue, Premkumar Devanbu, and David Lo. 2024. Robustness, security, privacy,\\nexplainability, efficiency, and usability of large language models for code. arXiv preprint arXiv:2403.07506 (2024).\\n[294] Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsun Kim, Donggyun Han, and David Lo. 2024. Unveiling\\nmemorization in code models. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering.\\n1‚Äì13.\\n[295] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of',\n",
       " '1‚Äì13.\\n[295] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of\\nthoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems\\n36 (2024).\\n[296] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct:\\nSynergizing Reasoning and Acting in Language Models. In International Conference on Learning Representations\\n(ICLR).\\n[297] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned\\ncode and natural language pairs from stack overflow. In Proceedings of the 15th international conference on mining\\nsoftware repositories. 476‚Äì486.\\n[298] Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim,\\nMunhyong Kim, Sungju Kim, et al. 2024. HyperCLOVA X Technical Report. arXiv preprint arXiv:2404.01954 (2024).',\n",
       " 'Munhyong Kim, Sungju Kim, et al. 2024. HyperCLOVA X Technical Report. arXiv preprint arXiv:2404.01954 (2024).\\n[299] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao\\nXie. 2024. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings\\nof the 46th IEEE/ACM International Conference on Software Engineering. 1‚Äì12.\\n[300] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle\\nRoman, et al. 2018. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing\\nand Text-to-SQL Task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.\\n3911‚Äì3921.\\n[301] Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng Yin. 2023.\\nWavecoder: Widespread and versatile enhanced instruction tuning with refined data generation. arXiv preprint',\n",
       " 'Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation. arXiv preprint\\narXiv:2312.14187 (2023).\\n[302] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2023.\\nGpt-4 is too smart to be safe: Stealthy chat with llms via cipher. arXiv preprint arXiv:2308.06463 (2023).\\n[303] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. Rrhf: Rank responses to\\nalign language models with human feedback without tears. arXiv preprint arXiv:2304.05302 (2023).\\n[304] Jiawei Liu Yifeng Ding Naman Jain Harm de Vries Leandro von Werra Arjun Guha Lingming Zhang Yuxiang Wei,\\nFederico Cassano. 2024. StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation.\\nhttps://github.com/bigcode-project/starcoder2-self-align.\\n[305] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021.\\nBitfit: Simple parameter-efficient fine-tuning for',\n",
       " 'https://github.com/bigcode-project/starcoder2-self-align.\\n[305] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021.\\nBitfit: Simple parameter-efficient fine-tuning for\\ntransformer-based masked language-models. arXiv preprint arXiv:2106.10199 (2021).\\n[306] Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-\\nGuang Lou. 2022. CERT: continual pre-training on sketches for library-oriented code generation. arXiv preprint\\narXiv:2206.06888 (2022).\\n[307] Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, and Jian-Guang Lou. 2023.\\nLarge Language Models Meet NL2Code: A Survey. In Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers). 7443‚Äì7464.\\n[308] Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan,',\n",
       " 'Computational Linguistics (Volume 1: Long Papers). 7443‚Äì7464.\\n[308] Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan,\\net al. 2024. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. arXiv preprint arXiv:2403.16443\\n(2024).\\n[309] Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\\n2023. RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. In Proceedings of\\nthe 2023 Conference on Empirical Methods in Natural Language Processing. 2471‚Äì2484.\\n[310] Jialu Zhang, Jos√© Pablo Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, and Gust Verbruggen.\\n2024. Pydex: Repairing bugs in introductory python assignments using llms. Proceedings of the ACM on Programming\\nLanguages 8, OOPSLA1 (2024), 1100‚Äì1124.\\n[311] Jialu Zhang, De Li, John Charles Kolesar, Hanyuan Shi, and Ruzica Piskac. 2022. Automated feedback generation',\n",
       " 'Languages 8, OOPSLA1 (2024), 1100‚Äì1124.\\n[311] Jialu Zhang, De Li, John Charles Kolesar, Hanyuan Shi, and Ruzica Piskac. 2022. Automated feedback generation\\nfor competition-level code. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " 'A Survey on Large Language Models for Code Generation\\n1:69\\nEngineering. 1‚Äì13.\\n[312] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. 2023.\\nAdaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning\\nRepresentations.\\n[313] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang,\\nFei Wu, et al. 2023. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792 (2023).\\n[314] Shudan Zhang, Hanlin Zhao, Xiao Liu, Qinkai Zheng, Zehan Qi, Xiaotao Gu, Xiaohan Zhang, Yuxiao Dong, and Jie\\nTang. 2024. NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts.\\narXiv preprint arXiv:2405.04520 (2024).\\n[315] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong',\n",
       " 'arXiv preprint arXiv:2405.04520 (2024).\\n[315] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong\\nChen, et al. 2023. Siren‚Äôs song in the AI ocean: a survey on hallucination in large language models. arXiv preprint\\narXiv:2309.01219 (2023).\\n[316] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. 2024. AutoCodeRover: Autonomous Program\\nImprovement. arXiv preprint arXiv:2404.05427 (2024).\\n[317] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023. Unifying the\\nperspectives of nlp and software engineering: A survey on language models for code. arXiv preprint arXiv:2311.07989\\n(2023).\\n[318] Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bin Qin, and Ting Liu. 2023. Length Extrapolation of Transformers: A\\nSurvey from the Perspective of Position Encoding. arXiv preprint arXiv:2312.17044 (2023).',\n",
       " 'Survey from the Perspective of Position Encoding. arXiv preprint arXiv:2312.17044 (2023).\\n[319] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\\nZhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).\\n[320] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,\\nDacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\\nInformation Processing Systems 36 (2024).\\n[321] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li,\\net al. 2023. Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x. In\\nProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 5673‚Äì5684.',\n",
       " 'Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 5673‚Äì5684.\\n[322] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. 2024.\\nOpenCodeInterpreter: Integrating Code Generation with Execution and Refinement. arXiv preprint arXiv:2402.14658\\n(2024).\\n[323] Wenqing Zheng, SP Sharan, Ajay Kumar Jaiswal, Kevin Wang, Yihan Xi, Dejia Xu, and Zhangyang Wang. 2023.\\nOutline, then details: Syntactically guided coarse-to-fine code generation. In International Conference on Machine\\nLearning. PMLR, 42403‚Äì42419.\\n[324] Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen. 2023. A survey of\\nlarge language models for code: Evolution, benchmarking, and future trends. arXiv preprint arXiv:2311.10372 (2023).\\n[325] Li Zhong, Zilong Wang, and Jingbo Shang. 2024. LDB: A Large Language Model Debugger via Verifying Runtime\\nExecution Step-by-step. arXiv preprint arXiv:2402.16906 (2024).',\n",
       " '[325] Li Zhong, Zilong Wang, and Jingbo Shang. 2024. LDB: A Large Language Model Debugger via Verifying Runtime\\nExecution Step-by-step. arXiv preprint arXiv:2402.16906 (2024).\\n[326] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie\\nZhan, et al. 2023. Solving challenging math word problems using gpt-4 code interpreter with code-based self-\\nverification. arXiv preprint arXiv:2308.07921 (2023).\\n[327] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023. Language agent tree\\nsearch unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406 (2023).\\n[328] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili\\nYu, et al. 2024. Lima: Less is more for alignment. Advances in Neural Information Processing Systems 36 (2024).',\n",
       " 'Yu, et al. 2024. Lima: Less is more for alignment. Advances in Neural Information Processing Systems 36 (2024).\\n[329] Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui,\\nOlivier Bousquet, Quoc V Le, et al. 2022. Least-to-Most Prompting Enables Complex Reasoning in Large Language\\nModels. In The Eleventh International Conference on Learning Representations.\\n[330] Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang, and Graham Neubig. 2022. DocPrompting: Generating Code by\\nRetrieving the Docs. In The Eleventh International Conference on Learning Representations.\\n[331] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong\\nMa, et al. 2024. DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence. arXiv\\npreprint arXiv:2406.11931 (2024).\\n[332] Terry Yue Zhuo. 2024. ICE-Score: Instructing Large Language Models to Evaluate Code. In Findings of the Association',\n",
       " 'preprint arXiv:2406.11931 (2024).\\n[332] Terry Yue Zhuo. 2024. ICE-Score: Instructing Large Language Models to Evaluate Code. In Findings of the Association\\nfor Computational Linguistics: EACL 2024. 2232‚Äì2242.\\n[333] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf,\\nHaolan Zhan, Junda He, Indraneil Paul, et al. 2024. Bigcodebench: Benchmarking code generation with diverse\\nfunction calls and complex instructions. arXiv preprint arXiv:2406.15877 (2024).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '1:70\\nJuyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim\\n[334] Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian Liu, and Niklas\\nMuennighoff. 2024. Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models. arXiv preprint\\narXiv:2401.00788 (2024).\\nJ. ACM, Vol. 37, No. 4, Article 1. Publication date: August 2018.',\n",
       " '33\\nFlashFill++: Scaling Programming by Example by Cutting to\\nthe Chase\\nJOS√â CAMBRONERO‚àó, Microsoft, USA\\nSUMIT GULWANI‚àó, Microsoft, USA\\nVU LE‚àó, Microsoft, USA\\nDANIEL PERELMAN‚àó, Microsoft, USA\\nARJUN RADHAKRISHNA‚àó, Microsoft, USA\\nCLINT SIMON‚àó, Microsoft, USA\\nASHISH TIWARI‚àó, Microsoft, USA\\nProgramming-by-Examples (PBE) involves synthesizing an intended program from a small set of user-provided\\ninput-output examples. A key PBE strategy has been to restrict the search to a carefully designed small\\ndomain-specific language (DSL) with effectively-invertible (EI) operators at the top and effectively-enumerable\\n(EE) operators at the bottom. This facilitates an effective combination of top-down synthesis strategy (which\\nbackpropagates outputs over various paths in the DSL using inverse functions) with a bottom-up synthesis\\nstrategy (which propagates inputs over various paths in the DSL). We address the problem of scaling synthesis',\n",
       " 'strategy (which propagates inputs over various paths in the DSL). We address the problem of scaling synthesis\\nto large DSLs with several non-EI/EE operators. This is motivated by the need to support a richer class of\\ntransformations and the need for readable code generation. We propose a novel solution strategy that relies\\non propagating fewer values and over fewer paths.\\nOur first key idea is that of cut functions that prune the set of values being propagated by using knowledge\\nof the sub-DSL on the other side. Cuts can be designed to preserve completeness of synthesis; however, DSL\\ndesigners may use incomplete cuts to have finer control over the kind of programs synthesized. In either case,\\ncuts make search feasible for non-EI/EE operators and efficient for deep DSLs. Our second key idea is that of\\nguarded DSLs that allow a precedence on DSL operators, which dynamically controls exploration of various',\n",
       " 'guarded DSLs that allow a precedence on DSL operators, which dynamically controls exploration of various\\npaths in the DSL. This makes search efficient over grammars with large fanouts without losing recall. It also\\nmakes ranking simpler yet more effective in learning an intended program from very few examples. Both\\ncuts and precedence provide a mechanism to the DSL designer to restrict search to a reasonable, and possibly\\nincomplete, space of programs.\\nUsing cuts and gDSLs, we have built FlashFill++, an industrial-strength PBE engine for performing rich\\nstring transformations, including datetime and number manipulations. The FlashFill++ gDSL is designed to\\nenable readable code generation in different target languages including Excel‚Äôs formula language, PowerFx,\\nand Python. We show FlashFill++ is more expressive, more performant, and generates better quality code than\\ncomparable existing PBE systems. FlashFill++ is being deployed in several mass-market products ranging',\n",
       " 'comparable existing PBE systems. FlashFill++ is being deployed in several mass-market products ranging\\nfrom spreadsheet software to notebooks and business intelligence applications, each with millions of users.\\nCCS Concepts: ‚Ä¢ Software and its engineering ‚ÜíProgramming by example; Domain specific languages.\\n‚àóAuthors in alphabetic order\\nAuthors‚Äô addresses: Jos√© Cambronero, Microsoft, USA, jcambronero@microsoft.com; Sumit Gulwani, Microsoft, USA,\\nsumitg@microsoft.com; Vu Le, Microsoft, USA, levu@microsoft.com; Daniel Perelman, Microsoft, USA, danpere@microsoft.\\ncom; Arjun Radhakrishna, Microsoft, USA, arradha@microsoft.com; Clint Simon, Microsoft, USA, clint.simon@microsoft.\\ncom; Ashish Tiwari, Microsoft, USA, astiwar@microsoft.com.\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and',\n",
       " 'provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,\\ncontact the owner/author(s).\\n¬© 2023 Copyright held by the owner/author(s).\\n2475-1421/2023/1-ART33\\nhttps://doi.org/10.1145/3571226\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:2\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nAdditional Key Words and Phrases: programming by example, domain-specific languages, string transforma-\\ntions\\nACM Reference Format:\\nJos√© Cambronero, Sumit Gulwani, Vu Le, Daniel Perelman, Arjun Radhakrishna, Clint Simon, and Ashish\\nTiwari. 2023. FlashFill++: Scaling Programming by Example by Cutting to the Chase. Proc. ACM Program.\\nLang. 7, POPL, Article 33 (January 2023), 30 pages. https://doi.org/10.1145/3571226\\n1\\nINTRODUCTION\\nProgramming-by-examples (PBE) has seen tremendous interest and progress in the last decade [Gul-\\nwani et al. 2017]. A variety of approaches have been proposed targeting various applications. Starting\\nfrom purely symbolic techniques, the field has explored neural [Devlin et al. 2017] and neurosym-\\nbolic approaches [Chaudhuri et al. 2021; Kalyan et al. 2018; Rahmani et al. 2021; Verbruggen et al.',\n",
       " 'from purely symbolic techniques, the field has explored neural [Devlin et al. 2017] and neurosym-\\nbolic approaches [Chaudhuri et al. 2021; Kalyan et al. 2018; Rahmani et al. 2021; Verbruggen et al.\\n2021]. In this paper, we present novel symbolic techniques to improve the scalability of PBE systems.\\nPBE applications range from enabling non-experts to author programs for spreadsheet data\\nmanipulation [Gulwani et al. 2012] or application creation in a low-code/no-code setting [Lukes\\net al. 2021], to improving productivity of data scientists for data wrangling tasks [Le and Gulwani\\n2014; Miltner et al. 2018] and even automating professional developers‚Äô repeated edits [Pan et al.\\n2021; Rolim et al. 2017]. A flagship application for PBE is that of string transformations, for\\ninstance, converting ‚ÄòAlan Turing‚Äô to ‚Äòturing, alan‚Äô‚Äîsuch tasks are very common and are\\nwell described by examples [Gulwani 2011]. We focus on such string transformations, though our',\n",
       " 'instance, converting ‚ÄòAlan Turing‚Äô to ‚Äòturing, alan‚Äô‚Äîsuch tasks are very common and are\\nwell described by examples [Gulwani 2011]. We focus on such string transformations, though our\\ntechnical contributions are more generally applicable to any grammar-based synthesis setting.\\nMost PBE engines work by defining a program search space, and then employing some strategy\\nto search over it. The program space is often defined by a domain-specific language (DSL), which\\nfixes a finite set of operators and all the different ways they can be composed to create programs. A\\nkey challenge in PBE is scaling the search to very large program spaces. DSL designers have to\\nbuild DSLs that are expressive enough to be useful, yet small enough to keep the program search\\nspace small. This tension in DSL design has hindered broader applications of program synthesis.\\nUsers implicitly advocate for larger DSLs as they want synthesizers to produce programs that are',\n",
       " 'space small. This tension in DSL design has hindered broader applications of program synthesis.\\nUsers implicitly advocate for larger DSLs as they want synthesizers to produce programs that are\\ncloser to the ones they would manually write, i.e., ones that use a large variety of functions that\\nare available in general purpose programming languages. On the other hand, these larger DSLs\\n(a) make the search space large and the synthesis slow, and (b) more importantly, allow the large\\nnumber of functions to be combined in unintuitive ways to produce undesirable programs. Program\\nsynthesis research has mainly focused on completeness, i.e., ensuring that we find a program when\\none exists, and insisting on completeness for large DSLs exacerbates these problems. We introduce\\ntwo new mechanisms, cuts and precedence, by which DSL designers can control the program search\\nspace even as the DSL itself grows in size. This not only eases the job of the DSL designer, but also',\n",
       " 'two new mechanisms, cuts and precedence, by which DSL designers can control the program search\\nspace even as the DSL itself grows in size. This not only eases the job of the DSL designer, but also\\nenables them to build synthesizers based on very expressive DSLs.\\nChallenges. Let us say we are given an input-output example: a tuple of input values and one\\noutput value. There are two commonly used search strategies to find programs that would generate\\nthe output value using the input values: bottom-up and top-down.\\nBottom-up (BU) search starts with the inputs and generates all possible values that can be computed\\nfrom the inputs using all possible (partial) programs in the search space. It does so by applying the\\nexecutable semantics functions of the DSL operators. Thus, information flows from the inputs, and all\\ncomputed intermediate results are completely oblivious to the output. The BU strategy is effective',\n",
       " 'computed intermediate results are completely oblivious to the output. The BU strategy is effective\\nonly when the sets of values generated in each intermediate stage remains small. This happens\\nwhen there are only a small number of leaf constants and each operator is effectively-enumerable\\n(EE) ‚Äì namely, it has a small arity and many-to-one semantics, thus having a small dynamic fan-out.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:3\\nTop-down (TD) search starts with the output and applies the inverse semantics of the operators,\\nso-called witness functions, to generate intermediate values that can generate the output value. Thus,\\ninformation flows from the output value, and at every stage, the intermediate values are computed\\nsolely based on the output and are completely oblivious to the input values. The TD strategy is\\neffective only when these intermediate sets are small. This happens only when every operator is\\neffectively invertible (EI) ‚Äì namely, it allows for effective (inverse) computation of various inputs\\nthat can yield a given output.\\nIf the DSL has both non-EE and non-EI operators, then neither bottom-up nor top-down strategies\\nare effective. Recently, it was observed that one could scale synthesis to larger DSLs by combining\\nthe two strategies [Lee 2021]. If there is a partition of the DSL such that the sub-DSL closer to',\n",
       " 'are effective. Recently, it was observed that one could scale synthesis to larger DSLs by combining\\nthe two strategies [Lee 2021]. If there is a partition of the DSL such that the sub-DSL closer to\\nthe start symbol has EI operators, and the rest contains EE operators, then the two strategies\\ncan be combined to yield a meet-in-the-middle strategy at that cut [Lee 2021]. However, this new\\nstrategy still has only a limited form of information flow between the inputs and the output. In\\nfact, the DSLs used in Duet [Lee 2021] are relatively small, albeit larger than those in FlashFill and\\nFlashMeta [Gulwani 2011; Polozov and Gulwani 2015]. These latter systems are based on a TD\\nstrategy over very small DSLs.\\nAn alternate way to scale PBE synthesis is based on abstraction and refinement types [Feng et al.\\n2017; Guo et al. 2020; Polikarpova et al. 2016; Wang et al. 2017]. The idea behind abstraction is that',\n",
       " '2017; Guo et al. 2020; Polikarpova et al. 2016; Wang et al. 2017]. The idea behind abstraction is that\\ninstead of computing the exact set of values that can be generated (in either top-down or bottom-up\\nstrategy), we compute overapproximations of the sets of values. This approach works when high\\nquality abstractions can be quickly generated. Typically, it is still ‚Äúone-sided‚Äù ‚Äì either the inputs flow\\nto intermediate values or the output value flows backwards to intermediate values. Furthermore,\\nabstractions of compositions of operators are computed by composing the abstractions of operators,\\nwhich loses accuracy as the composition depth grows. We overcome some of these shortcomings\\nin our work; however, abstraction-based approaches are inherently complementary.1\\nOur Contribution. In this paper, we present two novel techniques - cuts and precedence - to\\neffectively address the scalability challenges of PBE synthesis. The executable semantics functions',\n",
       " 'Our Contribution. In this paper, we present two novel techniques - cuts and precedence - to\\neffectively address the scalability challenges of PBE synthesis. The executable semantics functions\\n(that are the basis of BU) and the witness functions (that are basis of TD) are defined to allow\\ninformation to flow in one direction. We introduce cuts that prune values generated by witness\\nfunctions guided by the values that sub-DSLs could possibly compute on the inputs. This concept\\ninherently builds in bi-directional information flow in its definition, and in fact, generalizes the\\nsemantics functions and witness functions. A DSL designer can author a cut function based on their\\nintuition of the form of values that can be computed at a nonterminal using the inputs, and then\\nrestricting them to those that would be relevant for the output. Unlike abstractions, cuts are not\\ncomputed compositionally. They are provided for whole sub-DSLs; thus, they avoid information loss',\n",
       " 'restricting them to those that would be relevant for the output. Unlike abstractions, cuts are not\\ncomputed compositionally. They are provided for whole sub-DSLs; thus, they avoid information loss\\naccumulated by composing lossy abstractions. This is similar to how accelerations avoid information\\nloss in program analysis by capturing the effect (composition or transitive closure) of multiple state\\ntransitions by a single ‚Äúmeta transition‚Äù [Finkel 1987; Karp and Miller 1969].\\nA top-down strategy would get stuck at a non-EI operator. However, a cut function for an\\nargument of that non-EI operator can help unblock TD synthesis. As a special case, a cut for that\\nargument can be generated using bottom-up enumeration, in which case we get the meet-in-the-\\nmiddle strategy [Lee 2021]. However, cuts may be generated by other means based on the DSL\\ndesigner‚Äôs insight. In general, we get a novel search strategy, middle-out synthesis, which uses cuts',\n",
       " 'middle strategy [Lee 2021]. However, cuts may be generated by other means based on the DSL\\ndesigner‚Äôs insight. In general, we get a novel search strategy, middle-out synthesis, which uses cuts\\n1In this paper, we focus exclusively on synthesis approaches based on concrete values: the specification is a concrete IO\\nexample, and the semantics functions (and the inverse semantics) are given on concrete values (and not abstract values or\\nrefinement types). More specifically, we are in the context of version-space algebra (VSA) driven synthesis, and hence the\\nterms top-down and bottom-up are always used in that context.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:4\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nto reduce the original PBE problem over a large DSL (with potentially non-EI and non-EE operators)\\ninto simpler PBE problems over (smaller depth) sub-DSLs with only EI or only EE operators.\\nOur second key idea is to introduce precedence over operators in the grammar of domain-specific\\nlanguages. Precedence is a natural concept in grammars and arises naturally when the DSL designer\\nwants to prefer certain operators over others. We show that if the precedence is a series-parallel\\npartial order, then it can be encoded as an ordering on grammar rules to create a guarded DSL (gDSL),\\nand program search can be performed directly on the gDSL without compromising soundness or\\ncompleteness, while gaining efficiency. The ordering on rules in a gDSL is interpreted as a mandate\\nto explore a certain branch only when higher-ordered branches have failed to return a result. This',\n",
       " 'completeness, while gaining efficiency. The ordering on rules in a gDSL is interpreted as a mandate\\nto explore a certain branch only when higher-ordered branches have failed to return a result. This\\nhas two major advantages. First, it makes the search more scalable by dynamically using different\\nunderapproximations of the DSL to be explored. Second, it makes ranking simpler to write for DSL\\ndesigners because the precedence already builds in a default ranking over programs.\\nCuts and precedence provide DSL designers two new mechanisms to control the program search\\nspace, beyond what they get through designing DSLs. Our contributions include:\\n‚Ä¢ A new algorithmic approach for PBE (middle-out synthesis) that leverages a novel cut rule to\\nspeed up synthesis over large DSLs and to handle non-EI and non-EE operators.\\n‚Ä¢ A new formalism of guarded DSLs that supports operator precedence, and an extension of',\n",
       " 'speed up synthesis over large DSLs and to handle non-EI and non-EE operators.\\n‚Ä¢ A new formalism of guarded DSLs that supports operator precedence, and an extension of\\nour synthesis approach to gDSLs that scales synthesis to large DSLs and gives ranking based\\non path orderings [Dershowitz and Jouannaud 1990] for free.\\n‚Ä¢ A new and expressive system FlashFill++ for string transformations that supports datetime &\\nnumber manipulations and is designed for readable code generation.\\n‚Ä¢ An extensive comparison of FlashFill++ with existing state-of-the-art PBE systems for string\\ntransformations (FlashFill [Gulwani 2011], SmartFill [Chen et al. 2021a], and Duet [Lee 2021])\\nthat shows improvements in expressiveness, learning performance, and code readability.\\n2\\nOVERVIEW\\n2.1\\nNew Challenges in PBE for String Transformations\\nWe first discuss some challenges faced by the current generation of PBE tools for string trans-',\n",
       " '2\\nOVERVIEW\\n2.1\\nNew Challenges in PBE for String Transformations\\nWe first discuss some challenges faced by the current generation of PBE tools for string trans-\\nformations. These challenges were compiled in collaboration with two key industrial deployers\\nof PBE: the Microsoft Excel and the Microsoft PowerBI teams. To compile these challenges, we\\ninterviewed several product managers in these teams, interacted with both expert and novice users\\nof the FlashFill feature, and analyzed online help posts.\\nGenerating Readable Code. Consider the task of transforming the input pair (\"David Walker\",\\n\"623179\") to the output string \"D-6231#walker\". Any string processing library would contain\\nmany redundant methods for extracting \"Walker\" from \"David Walker\". For example, in Python,\\nwe could use the split method to accomplish the task. Alternatively, we could use the find method\\nalong with string slicing, or use regular expressions.',\n",
       " 'we could use the split method to accomplish the task. Alternatively, we could use the find method\\nalong with string slicing, or use regular expressions.\\nIn contrast to the design of string processing libraries, the prevailing wisdom in DSL design for\\nsynthesis has been to work with a minimal number of operators [Gulwani 2016]. For example,\\nFlashFill [Gulwani 2011] and the more recent Duet [Lee 2021] DSLs contain only 3 and 5 functions\\nthat directly operate on strings, respectively. Smaller DSLs lead to smaller program search spaces,\\nyielding better synthesis performance and effective ranking [Polozov and Gulwani 2015]. Following\\nthis minimalism to an extreme can lead to a DSL whose programs translate to very unnatural and\\nunreadable programs in target languages like Python (see Figure 1) or PowerFx (see Figure 2).\\nOne straightforward approach to readability is writing a good translator from the synthesis DSL',\n",
       " 'unreadable programs in target languages like Python (see Figure 1) or PowerFx (see Figure 2).\\nOne straightforward approach to readability is writing a good translator from the synthesis DSL\\nto the target language. However, if the semantic gap between the DSL and the target languages‚Äô\\noperators is large, then ‚Äúreadable translation‚Äù itself becomes a new and nontrivial synthesis problem.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:5\\n# Python program generated by FlashFill\\nimport regex\\ndef transformation_text_program(input1 , input2 ):\\ncomputed_value_83 = regex.search(r\"\\\\p{Lu}+\", input1 ). group (0)\\nindex1_83 = regex.search(r\"[-.\\\\p{Lu}\\\\p{Ll}0 -9]+\", input2 ). start ()\\ncomputed_value_0_83 = input2[index1_83 :( len(input2) + -2)]\\nkth_match_83 =\\nl i s t (regex.finditer(r\"[-.\\\\p{Lu}\\\\p{Ll}0 -9]+\", input1 ))[ -1]\\ncomputed_value_1_83 = kth_match_83.group (0). lower ()\\nreturn computed_value_83+\"-\"+computed_value_0_83+\"#\"+computed_value_1_83\\n# Python program generated by FlashFill ++\\ndef formula(i1 , i2):\\ns1 = i1[:1]\\ns2 = i2[:4]\\ns3 = i1.split(\" \")[1]. lower()\\nreturn s1 + \"-\" + s2 + \"#\" + s3\\n# Python program generated by FlashFill ++ and renamed by Codex\\ndef formula(name , number ):\\nfirst_initial = name [:1]\\nnumber_prefix = number [:4]\\nlast_name = name.split(\" \")[1]. lower()\\nreturn first_initial + \"-\" + number_prefix + \"#\" + last_name',\n",
       " 'def formula(name , number ):\\nfirst_initial = name [:1]\\nnumber_prefix = number [:4]\\nlast_name = name.split(\" \")[1]. lower()\\nreturn first_initial + \"-\" + number_prefix + \"#\" + last_name\\nFig. 1. The python programs generated by FlashFill and FlashFill++ for the task of transforming (\"David\\nWalker\", \"623179\") to \"D-6231#walker\". FlashFill++‚Äôs program is much more readable and its readability\\nis further improved by renaming variables using a pretrained large language model, as shown.\\nOur insight is that to effectively generate readable code the DSLs should not be designed with the\\nsingle-minded goal of efficient learning, but also pay heed to the target languages.\\nWhile generating readable code is challenging, the need is sorely felt in industrial PBE tools‚Äî\\nusers are more likely to trust and use PBE tools if they produce idiomatic, readable code. Quoting\\none study participant in [Drosos et al. 2020]: ‚Äúdon‚Äôt know what is going on there, so I don‚Äôt know if I',\n",
       " 'one study participant in [Drosos et al. 2020]: ‚Äúdon‚Äôt know what is going on there, so I don‚Äôt know if I\\ncan trust it if I want to extend it to other tasks. I saw my examples were correctly transformed, but\\nbecause the code is hard to read, I would not be able to trust what it is doing‚Äù. The lack of readable\\ncode is one of the primary challenges preventing a broader adoption of PBE technologies.\\nMultiple Target Languages. The need for readable code generation is compounded by the\\nproliferation of different target languages, each with their own set of operations; see Figure 3.\\nThese target languages range across standard programming languages (e.g., Python, R), individual\\nlibraries (e.g., Pandas, PySpark), data query languages (e.g., SQL), and custom application-specific\\nlanguages (e.g., Google Sheets & Excel formula languages, PowerBI‚Äôs M language). Apart from the\\nobvious benefit, multiple target support can also help with learning: seeing the same program in',\n",
       " 'languages (e.g., Google Sheets & Excel formula languages, PowerBI‚Äôs M language). Apart from the\\nobvious benefit, multiple target support can also help with learning: seeing the same program in\\nmultiple languages helps with cross-language knowledge transfer [Shrestha et al. 2018].\\nDate-Time and Numeric Transformations. Most string PBE technologies cannot natively\\nhandle date-time and numeric operations efficiently, leading to situations like transforming ‚Äòjan‚Äô\\nto ‚ÄòJanember‚Äô given the input-output example ‚Äònov‚Äô ‚Ü¶‚Üí‚ÄòNovember‚Äô. Duet [Lee 2021] does allow\\nfor limited numeric operators, but still lacks support for important data-processing operations\\nsuch as rounding and bucketing. According to the Microsoft Excel team, date-time and numeric\\noperations (of the kind shown in Figure 3) are among the most requested FlashFill features. However,\\nas illustrated in Section 2.2, these operations are not amenable to standard synthesis techniques.',\n",
       " 'operations (of the kind shown in Figure 3) are among the most requested FlashFill features. However,\\nas illustrated in Section 2.2, these operations are not amenable to standard synthesis techniques.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:6\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\n# PowerFx formula generated by FlashFill\\nConcatenate(\\nMid(Left(input1 , Match(input1 , \"\\\\p{Lu}+\"). StartMatch\\n+ Len(Match(input1 , \"\\\\p{Lu}+\"). FullMatch) - 1),\\nMatch(input1 , \"\\\\p{Lu}+\"). StartMatch),\\nConcatenate(\"-\",\\nConcatenate(Mid(Left(input2 ,Len(input2)-2), Match(input2 ,\"[0 -9]+\"). StartMatch),\\nConcatenate(\"#\",\\nLower(Mid(\\nLeft(input1 ,\\nFirst(LastN(MatchAll(input1 , \"[\\\\p{Lu}\\\\p{Ll}]+\"), 1)). StartMatch\\n+ Len(First(LastN(MatchAll(input1 , \"[\\\\p{Lu}\\\\p{Ll}]+\"), 1)). FullMatch )-1),\\nLast(MatchAll(input1 , \"[\\\\p{Lu}\\\\p{Ll}]+\")). StartMatch ))))))\\n# PowerFx formula generated by FlashFill ++\\nLeft(input1 , 1) & \"-\" & Left(input2 , 4) & \"#\"\\n& Lower(Last(FirstN(Split(input1 , \" \"), 2)). Result)\\nFig. 2. PowerFx formulas generated by FlashFill and FlashFill++ to transform (\"David Walker\", \"623179\") into\\n\"D-6231#walker\". The latter is much more readable than the former.',\n",
       " 'Fig. 2. PowerFx formulas generated by FlashFill and FlashFill++ to transform (\"David Walker\", \"623179\") into\\n\"D-6231#walker\". The latter is much more readable than the former.\\nPerforming operations over datetimes and numbers allows our system to handle use cases like\\nthe one detailed in Fig. 4(a), which presents 911 call records that need to be transformed. Each call\\nlog, shown in the Input column, contains an (optional) address, the township, the call date and time,\\nfollowed by possible annotations indicating the specific 911 station that addressed the call. Let us\\nsuppose that a data scientist wants to extract the date (2015-12-11) and time (13:34:52) from each log,\\nand map it to the corresponding weekday (Fri) and the 3-hour window (12PM - 3PM), as shown in\\nthe Output column. Performing this transformation requires string processing to extract candidate\\ndates and times, parsing these substrings into appropriate datatypes, performing type-specific',\n",
       " 'the Output column. Performing this transformation requires string processing to extract candidate\\ndates and times, parsing these substrings into appropriate datatypes, performing type-specific\\ntransformations on the extracted values, and then formatting them into an appropriate output\\nstring value. This is beyond the capabilities of current synthesizers. Our system can synthesize the\\nintended program (shown in Fig. 4(b)) from just the first example. This program is readable and\\nalso serves educational value (e.g. teaching the API of the popular datetime Python library).\\n2.2\\nOverview of FlashFill++\\nWe now show how our novel techniques address the various challenges from subsection 2.1.\\nExtended Domain-Specific Language. The main strength of FlashFill++ compared to previous\\nsystems is its expanded DSL containing over 40 operators, including 25 for just strings and the rest\\nfor datetime and numbers, such as for rounding and bucketing; see Figure 7. Contrast this with the',\n",
       " 'systems is its expanded DSL containing over 40 operators, including 25 for just strings and the rest\\nfor datetime and numbers, such as for rounding and bucketing; see Figure 7. Contrast this with the\\nnumbers 3 and 5 mentioned previously for FlashFill and Duet.\\nThis extended DSL supports more expressive and more readable programs. Contrast the code\\ngenerated by FlashFill++ in Fig. 1 and 2 to that generated by FlashFill to see the clear difference an\\nextended DSL makes. However, expanding the DSL comes with its own set of challenges: (a) Given\\nthe larger search space, standard synthesis techniques fall short on efficiency. (b) The larger search\\nspace also complicates ranking‚Äîthe problem of picking the best (or intended) program among all\\nthe ones consistent with the examples. (c) Handling numeric and date-time operators requires new\\nsynthesis techniques. Next, we discuss some novel strategies to address these challenges.',\n",
       " 'the ones consistent with the examples. (c) Handling numeric and date-time operators requires new\\nsynthesis techniques. Next, we discuss some novel strategies to address these challenges.\\nCuts and Middle-Out Synthesis. Our new DSL contains several non-EI operators (required for\\nnumber and datetime operations) that inhibit use of a top-down synthesis strategy across those\\noperators. Furthermore, bottom-up synthesis is not feasible for the sub-DSLs below those operators\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:7\\nRound to last day of month: 2/5/2020 =‚áí2/29/2020\\nfrom datetime import datetime\\nfrom dateutil.relativedelta import *\\ndef formula(i1):\\nmonth_start = datetime(i1.year ,i1.month ,1)\\nmonth_end = month_start\\n+ relativedelta(months =1)\\nreturn month_end - relativedelta(days =1)\\nEOMONTH(A1)\\nWith({ monthStart:\\nDate(Year(i1), Month(i1), 1)\\n},\\nDateAdd(\\nDateAdd(\\nmonthStart , 1, \"\" Months \"\"),\\n-1, \"\"Days\"\"\\n))\\nRound to start of quarter: 2/5/2020 =‚áí1/1/2020\\nfrom datetime import datetime\\ndef formula(i1):\\nquarter = (i1.month - 1) // 3 + 1\\nreturn datetime(i1.year ,3* quarter -2,1)\\nEOMONTH(\\nDATE(YEAR(A1),\\nROUNDUP(MONTH(A1)/3,\\n0)*3,1),\\n0)\\nWith({ quarter:\\nRoundUp(Month(i1) / 3, 0)\\n},\\nDate(Year(i1),quarter *3-2,1)\\n+ Time(0, 0, 0))\\nDays since start of year: 2/5/2029 =‚áí36\\ndef formula(i1):\\nreturn i1.timetuple (). tm_yday\\nA1 - DATE(YEAR(A1), 1, 1) + 1\\nDateDiff(\\nDate(Year(i1),1,1),i1) + 1\\nCreate year-quarter string: 4/5/1983 =‚áí‚Äò1983-Q2‚Äô',\n",
       " 'def formula(i1):\\nreturn i1.timetuple (). tm_yday\\nA1 - DATE(YEAR(A1), 1, 1) + 1\\nDateDiff(\\nDate(Year(i1),1,1),i1) + 1\\nCreate year-quarter string: 4/5/1983 =‚áí‚Äò1983-Q2‚Äô\\nfrom datetime import datetime\\ndef formula(i1):\\nquarter = (i1.month -1)//3 + 1\\nreturn i1.strftime(\"%Y\") +\\n\"-Q\"+f\"{quarter :01.0f}\"\\nYEAR(A1) & \"-Q\" &\\n& ROUNDUP(MONTH(A1)/3, 0)\\nText(i1 , \"yyyy\", \"en -US\")\\n& \"-Q\" &\\nText(\\nRoundUp(Month(i1) /3, 0),\\n\"0\",\\n\"en -US\")\\nExtract number, convert and round: ‚ÄòYour Total: $1,2564.45‚Äô =‚áí12564.5ùëë\\nfrom decimal import *\\ndef formula(i1):\\nsource = Decimal( str ( float (\\ni1.split(\"$\")[-1]. replace(\",\", \"\"))))\\ndelta = Decimal(\"0.5\")\\nreturn\\nfloat (( source / delta)\\n.quantize(0, ROUND_CEILING) * delta)\\nROUNDUP(\\nNUMBERVALUE(\\nRIGHT(A1 ,\\nLEN(A1)-FIND(\"$\", A1))\\n) / 0.5,\\n0\\n) * 0.5\\nRoundUp(Value(\\nLast(\\nSplit(i1 , \"$\")\\n).Result ,\\n\"en -US\"\\n) * 2, 0) / 2\\nFig. 3. Code produced by FlashFill++ for various date-time and rounding scenarios in Python (left), Excel\\n(center), and PowerFx (right) respectively.',\n",
       " 'Split(i1 , \"$\")\\n).Result ,\\n\"en -US\"\\n) * 2, 0) / 2\\nFig. 3. Code produced by FlashFill++ for various date-time and rounding scenarios in Python (left), Excel\\n(center), and PowerFx (right) respectively.\\ndue to enumeration blowup, rendering a meet-in-the-middle strategy infeasible. We propose a\\nnovel middle-out synthesis strategy that uses cuts to deal with such non-EI operators.\\nConsider the number parsing, rounding, and formatting subset of the FlashFill++ DSL below\\ndecimal roundNumber := RoundNumber(parseNumber, roundNumDesc)\\ndecimal parseNumber := ParseNumber(substr, locale) | ...\\nstring substr\\n:= ...\\nFix the input-output example ‚ü®‚ÄúThe price is $24.58 and 46 units are available.‚Äù ‚Ü¶‚Üí\\n24.00‚ü©for the non-terminal roundNumber. We first discuss the short-comings of both bottom-\\nup and top-down synthesis in this case.\\nTop-Down Synthesis using Witness Functions. In FlashMeta-style programming-by-example, the',\n",
       " 'up and top-down synthesis in this case.\\nTop-Down Synthesis using Witness Functions. In FlashMeta-style programming-by-example, the\\nprimary deductive tools are witness functions. Given a specification in the form of an input-output\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:8\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\n(a)\\nInput\\nOutput\\nCEDAR AVE & COTTAGE AVE; HORSHAM; 2015-12-11 @ 13:34:52;\\nFri, 12PM - 3PM\\nRT202 PKWY; MONTGOMERY; 2016-01-13 @ 09:05:41-Station:STA18;\\nWed, 9AM - 12PM\\n; UPPER GWYNEDD; 2015-12-11 @ 21:11:18;\\nFri, 9PM - 12AM\\n(b)\\ndef derive_value(_input ):\\ntext = _input.split(\";\")[2]\\npart_0 = text.split(\" \")[0]; part_1 = text.split(\" \")[2][:8]\\ndate = datetime.datetime.strptime(part_0 , \"%Y-%m-%d\")\\ntime = datetime.datetime.strptime(part_1 , \"%H:%M:%S\")\\nbase_value = datetime.timedelta(hours=time.hour , minutes=time.minute ,\\nseconds=time.second , microseconds=time.microsecond)\\ndelta_value = datetime.timedelta(hours =3)\\ntime_str = (time - base_value % delta_value ). strftime(\"%#I%p\")\\nrounded_up_next = (time - base_value % delta_value) + delta_value\\ncomputed_value = time_str + \"-\" + rounded_up_next.strftime(\"%#I%p\")\\nreturn date.strftime(\"%a\") + \", \" + computed_value',\n",
       " 'rounded_up_next = (time - base_value % delta_value) + delta_value\\ncomputed_value = time_str + \"-\" + rounded_up_next.strftime(\"%#I%p\")\\nreturn date.strftime(\"%a\") + \", \" + computed_value\\nFig. 4. (a) A task to map the 911-call logs in the Input column to weekday/time buckets in the Output column.\\n(b) Python function synthesized by our approach for this task from just one example.\\nexample ùëñ‚Ü¶‚Üíùëúand a top-level operator ùêπ, a witness function for position ùëògenerates a sub-\\nspecification for the ùëòùë°‚Ñéparameter for ùêπ. For example, if the top-level operator is concat(ùëÅ1, ùëÅ2)\\nand the input-output example is ùëñ‚Ü¶‚Üí‚Äúabc‚Äù, the witness function for the 1ùë†ùë°position will return\\n{‚Äúa‚Äù, ‚Äúab‚Äù} (assuming we do not consider the trivial case of appending an empty string). In the\\nexample we are considering, writing a witness functions for the RoundNumber operator is not as\\nstraight-forward‚Äîthere are an infinite set of numbers that can round to 24.00. A standard top-down',\n",
       " 'example we are considering, writing a witness functions for the RoundNumber operator is not as\\nstraight-forward‚Äîthere are an infinite set of numbers that can round to 24.00. A standard top-down\\nprocedure cannot handle this infinite-width witness function.\\nBottom-Up Synthesis. The other major paradigm for programming-by-example is bottom-up syn-\\nthesis: here, the synthesizer starts enumerating programs ‚Äì starting from constants and iteratively\\napplying operators from the grammar on the previously generated programs ‚Äì and checks if any\\nof the enumerated programs satisfies the given input-output example. An efficient bottom-up\\nsynthesizer will avoid enumerating all programs using observational equivalence‚Äîthat is, it only\\ngenerate programs that produce different outputs for the given input. In our running example, the\\nsynthesizer will begin by generating substr sub-programs and concrete values for locale and',\n",
       " 'generate programs that produce different outputs for the given input. In our running example, the\\nsynthesizer will begin by generating substr sub-programs and concrete values for locale and\\nroundNumberDesc. However, enumerating such sub-programs is expensive‚Äîthe fragment of the\\nDSL reachable from the non-terminal substr is large. In fact, string operations like substr are\\nbest handled using witness functions.\\nMiddle-Out Synthesis using Cuts. Examining our input-output example by hand, it is easy to see that\\nin any valid program the output of ParseNumber should be derived from a numerical substring\\nin the input. Intuitively, it does not matter what or how complex the substr sub-program is; we\\ncan be confident that the output of the ParseNumber will be either 24.58 or 46. Cuts capture this\\nsimple intuition‚Äîfor a given input-output example ùëñ‚Ü¶‚Üíùëúand a non-terminal ùëÅthat expands to\\nùëì(ùëÅ1, ùëÅ2), the cut for ùëÅ1 (say) in the context of ùëÅwill be a set of values {ùëú1,ùëú2, . . . ,ùëúùëõ} such that',\n",
       " 'simple intuition‚Äîfor a given input-output example ùëñ‚Ü¶‚Üíùëúand a non-terminal ùëÅthat expands to\\nùëì(ùëÅ1, ùëÅ2), the cut for ùëÅ1 (say) in the context of ùëÅwill be a set of values {ùëú1,ùëú2, . . . ,ùëúùëõ} such that\\nin any desired program ùëÉgenerated by ùëÅ, the output of the sub-program corresponding to ùëÅ1 will\\nbe one of the ùëúùëòfor ùëò‚àà{1, 2, . . . ,ùëõ}.\\nGiven that the output of ParseNumber will be either 24.58 or 46, the synthesizer has two sub-\\ntasks for the 24.58 case (the 46 case will be similar): (a) synthesizing a program for ùëùùëéùëüùë†ùëíùëÅùë¢ùëöùëèùëíùëü\\nfor the example ùëñ‚Ü¶‚Üí24.58, and (b) synthesizing a program for ùëüùëúùë¢ùëõùëëùëÅùë¢ùëöùëèùëíùëüfor the example\\nùëñ‚Ü¶‚Üí24.00 using a modified DSL, which is generated dynamically in middle-out synthesis, where\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:9\\nparseNumber ‚Üí24.58 is the only rule whose head is parseNumber. Both sub-tasks can now be\\nrecursively solved, possibly using either the top-down strategy or the bottom-up strategy.\\nGuarded Context-Free Grammars. The larger program space resulting from an extended DSL\\nwith a wide range of operators poses both efficiency and ranking challenges. However, human\\nprogrammers often encounter the same challenge when writing their own implementations and\\ndecide between these operators and programs using simple rules of thumb, which can be leveraged\\nboth for improving search efficiency and ranking. For example, ‚Äúif a task can be done using a date-\\ntime function, do not use string transformation functions‚Äù or ‚Äúif a task can be done using string\\nindexing, do not use regular expressions‚Äù. To mimic this kind of coarse reasoning, we introduce\\nthe notion of gDSLs. In a gDSL, the production rules for each non-terminal are ordered (with a',\n",
       " 'indexing, do not use regular expressions‚Äù. To mimic this kind of coarse reasoning, we introduce\\nthe notion of gDSLs. In a gDSL, the production rules for each non-terminal are ordered (with a\\npartial order |‚ä≤), with production rules earlier in the order preferred to ones later in the order. For\\nexample, the rule concat := segment |‚ä≤Concat(segment, concat) expresses that we always\\nprefer programs that do not use the Concat operation to ones that do. During synthesis for a gDSL\\nrule ùëÅ‚Üíùõº|‚ä≤ùõΩthe branch ùõΩis explored only if the branch ùõºfails to produce a program. This\\ngreatly improves the performance of synthesis and the FlashFill++ synthesis times are competitive\\nwith other synthesis techniques that work with significantly smaller DSLs.\\nApart from improving the efficiency of search, gDSLs also simplify the task of writing ranking\\nfunctions. Intuitively, the precedence in the guarded rules induce a ranking on programs, and any',\n",
       " 'Apart from improving the efficiency of search, gDSLs also simplify the task of writing ranking\\nfunctions. Intuitively, the precedence in the guarded rules induce a ranking on programs, and any\\nadditional ranking function only needs to order the remaining incomparable programs. Precedences\\nrank programs by a lexicographic path ordering (LPO) [Dershowitz and Jouannaud 1990], and our\\nfinal ranker will be a lexicographic combination of LPO and base arithmetic ranker ‚Äì such program\\nrankers have not been used in program synthesis before.\\n3\\nBACKGROUND: PROGRAMMING BY EXAMPLE\\nWe now define the problem of programming-by-example and discuss common solutions.\\n3.1\\nDomain-Specific Languages\\nWe use domain-specific languages (DSLs) to specify the set of target programs for a synthesizer.\\nFormally, a DSL D is given by ‚ü®N, T, F, R, Vin, ùë£out‚ü©where:\\n‚Ä¢ N is a finite set of non-terminal symbols (or non-terminals). The symbol ùë£out is a special start',\n",
       " 'Formally, a DSL D is given by ‚ü®N, T, F, R, Vin, ùë£out‚ü©where:\\n‚Ä¢ N is a finite set of non-terminal symbols (or non-terminals). The symbol ùë£out is a special start\\nnon-terminal in N that represents the output of a program in the DSL.\\n‚Ä¢ T is a set of terminal symbols (or terminals) that is partitioned as Vin ‚à™O into inputs Vin and\\nvalues O. The set Vin contains special terminals, in1, in2, . . ., that represent the input symbols\\nin a program of the DSL. The set O contains constant values.\\n‚Ä¢ F is a finite set of function symbols (or operations). Each operation f ‚ààF has a fixed arity\\nArity(f). The semantics of f, denoted by JfK, is a mapping from OArity(f) to O.\\n‚Ä¢ R is a set of rules of the form ùëÅ‚Üíf(ùë£1, . . . , ùë£ùëò) or ùëÅ‚Üíùë£0 where ùëÅ‚ààN, f ‚ààF , ùë£0 ‚ààT,\\nand ùë£1, . . . , ùë£ùëò‚ààN ‚à™T.\\nThe formalism above is untyped for ease of reading. In practice, the FlashFill++ DSL is typed‚Äî values\\ncan be integers, floats, strings, Booleans, and date-time objects, and each non-terminal, terminal,',\n",
       " 'can be integers, floats, strings, Booleans, and date-time objects, and each non-terminal, terminal,\\nand operator have specific type signatures.\\nEvery terminal or nonterminal ùë£generates a set L(ùë£) of programs defined recursively as follows:\\n(a) L(inùëò) = {inùëò} for all input symbols inùëò‚ààVin, (b) L(ùëú) = {ùëú} for all values ùëú‚ààO, and\\n(c) L(ùëÅ) =\\n\\x08f(ùëÉ1, . . . , ùëÉùëõ) | ùëÅ‚Üíf(ùë£1, . . . , ùë£ùëõ) ‚ààR, ‚àÄùëñ.ùëÉùëñ‚ààL(ùë£ùëñ)\\n\\t\\n‚à™\\n\\x08\\nùëÉ| ùëÅ‚Üíùë£‚ààR, ùë£‚ààT, ùëÉ‚àà\\nL(ùë£)\\n\\t\\n. The set of programs defined by the whole DSL L(D) is L(ùë£out).\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:10\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nExample 3.1. The following is a simple DSL Dùê¥for affine arithmetic expressions over naturals\\nN: (a) Here, the terminals T are {input1, input2, 0, 1, 2, . . .}, with input1, input2 being the spe-\\ncial input terminals in Vin. (b) The non-terminals N are {output, addend, const}, with output\\nbeing the output symbol ùë£out. (c) The operators F are Plus and Times. (d) The rules R are given\\nby {output ‚ÜíPlus(addend, output), output ‚Üíconst, addend ‚ÜíTimes(const, input1),\\naddend ‚ÜíTimes(const, input2), const ‚Üí0 | 1 | 2 | . . .}. Note that the declaration uint\\nconst; in the listing is shorthand for a set of rules for the form const ‚Üíùëòfor each ùëò‚ààN.\\nPlus(Times(5, input1), 3) is a sample program in L(Dùê¥).\\n‚ñ°\\n@input uint input1, input2;\\n@start uint output := Plus(addend, output) | const;\\nuint addend := Times(const, input1) | Times(const, input2);\\nuint const;\\n3.2\\nSynthesis Tasks and Solutions',\n",
       " '‚ñ°\\n@input uint input1, input2;\\n@start uint output := Plus(addend, output) | const;\\nuint addend := Times(const, input1) | Times(const, input2);\\nuint const;\\n3.2\\nSynthesis Tasks and Solutions\\nA state ùëÜis a valuation of all input symbols in a DSL, i.e., ùëÜ= {in1 ‚Ü¶‚Üíùëú1, . . . , inùëò‚Ü¶‚Üíùëúùëò} where inùëñ\\nare input symbols and ùëúùëñare values. An example Ex is a pair ùëÜ‚Ü¶‚Üíùëúof a state ùëÜand value ùëú.\\nA synthesis task ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©is given by: (a) a term ùõº, which is either a nonterminal, terminal, or\\nright-hand side of a rule, (b) a set R of rules, and (c) an example ùëÜ‚Ü¶‚Üíùëú. A solution of the synthesis\\ntask ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©is a program ùëÉsuch that: (a) JùëÉK(ùëÜ) = ùëú, and (b) ùëÉ‚ààL(ùõº). Here, JùëÉK : OVin ‚Ü¶‚ÜíO\\nrepresents the standard semantics of a program. Formally, JùëÉK(ùëÜ) is recursively defined as: (1)\\nJinùëñK(ùëÜ) = ùëÜ(inùëñ), (2) JùëúK(ùëÜ) = ùëúfor every value ùëú, (3) Jf(ùë£1, ùë£2)K(ùëÜ) = JfK(Jùë£1K(ùëÜ), Jùë£2K(ùëÜ)) for every\\noperator f. To keep the presentation simple, the synthesis task is defined to contain one input-output',\n",
       " 'operator f. To keep the presentation simple, the synthesis task is defined to contain one input-output\\nexample. In practice, a synthesis task often involves multiple examples. It is straight-forward to\\nextend our technique for this, as done in our implementation.\\nExample 3.2. A sample synthesis task for the affine expression DSL Dùê¥is ‚ü®output, R,ùëÜ‚Ü¶‚Üí7‚ü©\\nwhere ùëÜ= {input1 ‚Ü¶‚Üí2, input2 ‚Ü¶‚Üí0}. The program Plus(Times(3, input1), 1) is a solution of\\nthis task. Here Times and Plus have their usual arithmetic semantics.\\n‚ñ°\\nGiven a synthesis task ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, a synthesizer generates a program set PS such that every\\nprogram in the set PS is a solution of the synthesis task, which we denote by the assertion PS |=\\n‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. Note that it is vacuously true that ‚àÖ|= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, and so practical synthesizers\\nstrive to establish the above assertion for nonempty sets PS. The notation Ã∏|= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©denotes\\nthat there is no nonempty set PS that is a solution for the synthesis task.\\n3.3',\n",
       " 'strive to establish the above assertion for nonempty sets PS. The notation Ã∏|= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©denotes\\nthat there is no nonempty set PS that is a solution for the synthesis task.\\n3.3\\nBottom-Up and Top-Down Synthesis\\nThere are two main approaches for solving the synthesis task: bottom-up and top-down.\\nThe bottom-up (BU) approach enumerates programs generated by different nonterminals of the\\ngrammar and collects the values that those programs compute on the input state ùëÜ. More precisely,\\nfor each nonterminal ùëÅ‚Ä≤, the BU approach computes the bottom-up value set buùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) given by\\n{JùëÉ‚Ä≤K(ùëÜ) | ùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤)}. These sets are computed starting from the leaf (terminals) of the grammar\\nand moving up to the root (start symbol ùë£out). Success is declared if the output value ùëúis found to\\nbe in the set buùë£out (ùëÜ‚Ü¶‚Üíùëú). Note that the BU procedure is not guided by the output value ùëú.\\nThe top-down (TD) approach starts with the output value ùëúthat needs to be generated at start',\n",
       " 'be in the set buùë£out (ùëÜ‚Ü¶‚Üíùëú). Note that the BU procedure is not guided by the output value ùëú.\\nThe top-down (TD) approach starts with the output value ùëúthat needs to be generated at start\\nsymbol ùë£out, and for every nonterminal ùëÅ‚Ä≤, it computes the set of values that flow to ùëúat ùë£out.\\nMore formally, we say a value ùëú‚Ä≤ at ùëÅ‚Ä≤ flows to value ùëúat ùëÅif either (a) ùëÅ‚Üíùëì(. . . , ùëÅ‚Ä≤, . . .) is a\\ngrammar rule and JùëìK(ùëú1, . . . ,ùëú‚Ä≤, . . . ,ùëúùëò) = ùëúfor some values ùëú1, . . . ,ùëúùëò, or (b) there exist ùëú‚Ä≤‚Ä≤ and\\nùëÅ‚Ä≤‚Ä≤ s.t. ùëú‚Ä≤ at ùëÅ‚Ä≤ flows to ùëú‚Ä≤‚Ä≤ at ùëÅ‚Ä≤‚Ä≤ and ùëú‚Ä≤‚Ä≤ at ùëÅ‚Ä≤‚Ä≤ flows to ùëúat ùëÅ. For each nonterminal ùëÅ‚Ä≤, the\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:11\\nTD approach computes a top-down value set tdùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) that contains the values ùëú‚Ä≤ at ùëÅ‚Ä≤ that\\nflow into the value ùëúat ùë£out. The top-down value sets are computed using witness functions. An\\noperator f with arity ùëõis associated with ùëõwitness functions - one for each argument position. The\\nùëò-th witness function, WFf,ùëò: Oùëò‚Ü¶‚Üí2O, for operator f maps the desired output and ùëò‚àí1 values\\nfor previous arguments to possible values for the ùëò-th argument. Such a parameterized collection\\nof witness functions is sound (complete) if ùëúùëò‚ààWFf,ùëò(ùëú,ùëú1, . . . ,ùëúùëò‚àí1) for ùëò= 1, . . . ,ùëõimplies (is\\nimplied by) JfK(ùëú1, . . . ,ùëúùëõ) = ùëú. For example, if the top-level operator is Plus and the output is 7, the\\npossible arguments for Plus would be (0, 7), (1, 6), (2, 5), . . ., and hence, WFPlus,1(7) = {0, 1, 2, . . .}\\nand WFPlus,2(7,ùë•) = {7 ‚àíùë•}.\\nTop-down and bottom-up synthesis are both efficient and practical in different scenarios. TD',\n",
       " 'and WFPlus,2(7,ùë•) = {7 ‚àíùë•}.\\nTop-down and bottom-up synthesis are both efficient and practical in different scenarios. TD\\nsynthesis performs well when operators are effectively invertible (EI), i.e., the witness functions\\nWFf,ùëòfor each operator f produces sets that are finite and manageable in size. BU synthesis performs\\nwell when the grammar is effectively enumerable (EE), i.e., both the number of constants in the\\ngrammar is bounded and the number of different intermediate values produced is manageable.\\nNote that the focus in this paper is exclusively on synthesis approaches based on concrete values:\\nthe semantics and witness functions are given on concrete values, and not abstract values or types.\\nAbstractions and types can make top-down strategies work on grammars with non-EI operators,\\nfor example [Feng et al. 2017; Polikarpova et al. 2016], but require additional machinery, such as\\ntype systems, abstract domains, and constraint solvers.',\n",
       " 'for example [Feng et al. 2017; Polikarpova et al. 2016], but require additional machinery, such as\\ntype systems, abstract domains, and constraint solvers.\\nExample 3.3. The affine expression DSL from Example 3.1 is neither effectively invertible nor\\neffectively enumerable. The operator Plus has a witness function WFPlus,1 which produces a set\\nof size ùëõ+ 1 for an example ùëÜ‚Ü¶‚Üíùëõ. Further, it contains an infinite number of constants (i.e., the\\nnon-negative integers) which make bottom-up approach infeasible. In the FlashFill++ DSL, many\\nstring operators are not effectively invertible. E.g., the LowerCase operator‚Äôs witness function that\\ncan produce a set that is exponential in the input‚Äôs length: WFLowerCase,1(‚Äòabc‚Äô) produces a set of\\nsize 8, i.e., {‚ÄòABC‚Äô, ‚ÄòABc‚Äô, ‚ÄòAbC‚Äô, ‚ÄòaBC‚Äô, ‚ÄòabC‚Äô, ‚ÄòaBc‚Äô, ‚ÄòAbc‚Äô, ‚Äòabc‚Äô}.\\n‚ñ°\\nIn Section 4, we introduce cuts that can be used to decompose the synthesis problem to enable\\nmiddle-out synthesis, which can learn over deep DSLs ‚Äì DSLs that can generate programs with',\n",
       " '‚ñ°\\nIn Section 4, we introduce cuts that can be used to decompose the synthesis problem to enable\\nmiddle-out synthesis, which can learn over deep DSLs ‚Äì DSLs that can generate programs with\\nlarge depth ‚Äì where neither bottom-up nor top-down is feasible. In Section 5, we introduce gDSLs,\\nwhich provide a precedence-based mechanism to help learning scale to broad DSLs ‚Äì DSLs with\\nseveral options for a single nonterminal.\\n4\\nCUTS AND MIDDLE-OUT SYNTHESIS\\nTop-down and bottom-up approaches, as well as their combination, struggle to scale to large DSLs.\\nCuts can help scale synthesis. If we want to generate a value ùëúat nonterminal ùëÅ, and another\\nnonterminal ùëÅ‚Ä≤ is on the path from ùëÅto the terminals, then a cut at ùëÅ‚Ä≤ returns values to generate\\nat ùëÅ‚Ä≤ that can help with generating ùëúat ùëÅ.\\nDefinition 4.1 (Cuts). Given a synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and a non-terminal ùëÅ‚Ä≤ ‚ààN, a cut\\nCutùëÅ‚Ä≤,ùëÅfor ùëÅ‚Ä≤ in the context ùëÅ, maps an example, ùëÜ‚Ü¶‚Üíùëú, to a set of values. Such a function is',\n",
       " 'Definition 4.1 (Cuts). Given a synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and a non-terminal ùëÅ‚Ä≤ ‚ààN, a cut\\nCutùëÅ‚Ä≤,ùëÅfor ùëÅ‚Ä≤ in the context ùëÅ, maps an example, ùëÜ‚Ü¶‚Üíùëú, to a set of values. Such a function is\\ncomplete if for every solution ùëÉfor the task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, whenever ùëÉcontains a sub-program\\nùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤), then JùëÉ‚Ä≤K(ùëÜ) ‚ààCutùëÅ‚Ä≤,ùëÅ(ùëÜ‚Ü¶‚Üíùëú).\\nNote that ùëÅneed not be the start symbol of the grammar and ùëúneed not be the original output\\nvalue in the input-output example. Typically, we define cuts CutùëÅ‚Ä≤,ùëÅwhen ùëÅ‚Üíùëì(ùëÅ‚Ä≤, ùëÅ‚Ä≤‚Ä≤) is a\\ngrammar rule. Such a cut can be used in place of the witness function for the first argument of ùëì.\\nLet us illustrate cuts through an example.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:12\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nMO.Cut\\nPS1 |= ‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©\\nPS2 |= ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©\\n√ò\\nùëú1\\nPS2[ùëú1 ‚Ü¶‚ÜíPS1] |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nif ùëú1 ‚ààCutùëÅ1,ùëÅ(ùëÜ‚Ü¶‚Üíùëú)\\nFig. 5. The cut inference rule that enables middle-out program synthesis.\\nExample 4.2. Consider again the synthesis task from Section 2.2, where the input-output example\\nwas ùëÜ‚Ü¶‚Üí24.00 and the input state ùëÜwas ‚ü®in1 ‚Ü¶‚Üí‚ÄúThe price is $24.58 and 46 units are\\navailable.‚Äù ‚ü©. Suppose we want to synthesize a program from this example starting from the\\nnonterminal roundNumber of the FlashFill++ DSL (Figure 7). One potential cut for parseNumber in\\nthe context roundNumber could work by scanning the input for any maximal substrings that are\\nnumerical constants and returning them (as a number). Here, it would return {24.58, 46}. A more\\nsophisticated cut could additionally look at the output 24.00 and only return the set {24.58}, as it is',\n",
       " 'numerical constants and returning them (as a number). Here, it would return {24.58, 46}. A more\\nsophisticated cut could additionally look at the output 24.00 and only return the set {24.58}, as it is\\nthe only value in the string that can be rounded down to 24.00. These cuts are not complete as they\\ndo not include 24.5, which can also be extracted from the input and rounded to 24.\\n‚ñ°\\nRecall that we model synthesizers as generating nonempty program sets PS and asserting\\nPS |= ‚ü®ùõº, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. Figure 5 presents a new inference rule, called the cut rule, that can be used\\nto generate such assertions. This rule can be used in conjunction with any synthesizer (such\\nas those based on top-down or bottom-up approach). The cut rule uses a cut for a nonterminal\\nùëÅ1 in the context of ùëÅto decompose the overall synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©into two subtasks\\n‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©and ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©. The first, or inner, subtask tries to find a program',\n",
       " 'ùëÅ1 in the context of ùëÅto decompose the overall synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©into two subtasks\\n‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©and ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©. The first, or inner, subtask tries to find a program\\nin L(ùëÅ1) that maps ùëÜto ùëú1, whereas the second, or outer, subtask tries to find a program in L(ùëÅ)\\nthat maps ùëÜto ùëúassuming that we have a program to maps ùëÜto ùëú1. The notation R with ùëÅ1 ‚Üíùëú1\\nsimply means we remove all old rules in R of the form ùëÅ1 ‚Üíùõºand only have one rule ùëÅ1 ‚Üíùëú1.\\nThe cut rule also shows how the solutions to the two subtasks are combined to generate a solution\\nfor the original synthesis task.\\nTheorem 4.3. [Soundness] If program sets PS1 and PS2 are such that PS1 |= ‚ü®ùëÅ1, R,ùëÜ‚Ü¶‚Üíùëú1‚ü©and\\nPS2 |= ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©, then PS2[ùëú1 ‚Ü¶‚ÜíPS1] |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. Furthermore, [complete-\\nness] if a program ùëÉis a solution for the synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and the program ùëÉcontains a\\nsubprogram ùëÉ1 ‚ààL(ùëÅ‚Ä≤) that maps the input ùëÜto a value ùëú1 (i.e., JùëÉ1K(ùëÜ) = ùëú1), then ùëú1 will be in the',\n",
       " 'ness] if a program ùëÉis a solution for the synthesis task ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and the program ùëÉcontains a\\nsubprogram ùëÉ1 ‚ààL(ùëÅ‚Ä≤) that maps the input ùëÜto a value ùëú1 (i.e., JùëÉ1K(ùëÜ) = ùëú1), then ùëú1 will be in the\\ncut CutùëÅ1,ùëÅ(ùëÜ‚Ü¶‚Üíùëú) assuming that the cut is complete, and moreover, the program ùëÉ[ùëÉ1 ‚Ü¶‚Üíùëú1] is a\\nsolution for the task ‚ü®ùëÅ, R with ùëÅ1 ‚Üíùëú1,ùëÜ‚Ü¶‚Üíùëú‚ü©.\\nWe use the term middle-out synthesis to describe the synthesis approach that uses the Rule MO.Cut\\nto perform synthesis. Note that the subproblems created by Rule MO.Cut can be solved using either\\nthe top-down approach, or the bottom-up approach, or the middle-out approach, or a hybrid\\ncombination of the approaches. One common strategy is: after applying Rule MO.Cut, we solve\\nthe outer subtask using bottom-up or hybrid approach, and for each ùëú1 for which the outer has a\\nsolution, we solve the inner subtask using the top-down or hybrid approach. Note that the cut rule\\ncan be used multiple times to solve a synthesis task.',\n",
       " 'solution, we solve the inner subtask using the top-down or hybrid approach. Note that the cut rule\\ncan be used multiple times to solve a synthesis task.\\nExample 4.4. Consider the synthesis task ‚ü®roundNumber, R,ùëÜ‚Ü¶‚Üí24.00‚ü©from Example 4.2. Us-\\ning the fact that 24.58 was in the cut for parseNumber, we can use MO.Cut rule from Figure 5\\nand get the subtasks ‚ü®parseNumber, R,ùëÜ‚Ü¶‚Üí24.58‚ü©and ‚ü®roundNumber, R‚Ä≤,ùëÜ‚Ü¶‚Üí24.00‚ü©, where R‚Ä≤\\nis R with parseNumber ‚Üí24.58. The second subproblem now has only one rule for parseNumber,\\nwhich directly generates 24.58. The first subproblem now has to generate 24.58 from the input, and\\nthe second subproblem has to round 24.58 to 24.00.\\n‚ñ°\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:13\\n4.1\\nGeneralizing Top-Down and Bottom-Up Synthesis\\nExamining the top-down and bottom-up synthesis approaches closely, it can be seen that top-\\ndown synthesis is purely output driven and bottom-up synthesis is purely input driven. Cuts neatly\\ngeneralize both these approaches, allowing us the possibility to use a set of values influenced by\\nboth: (a) the set ùêµof values reachable through forward semantics from the input values (bottom-up\\nsearch), and (b) the set ùëáof values reachable through inverse semantics from the output values\\n(top-down search).\\nRecall that the set buùëÅ‚Ä≤ is the set of values that bottom-up enumeration generates corresponding\\nto nonterminal ùëÅ‚Ä≤ and the set tdùëÅ,ùëÅ‚Ä≤ is the set of values that arise at ùëÅ‚Ä≤ by repeatedly applying\\n(precise) witness functions starting from ùëÅ. Let real value set rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) be the set of values\\nJùëÉ‚Ä≤K(ùëÜ) where ùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤) is a sub-program of a program ùëÉ‚ààL(ùëÅ) such that JùëÉK(ùëÜ) = ùëú. Clearly,',\n",
       " '(precise) witness functions starting from ùëÅ. Let real value set rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) be the set of values\\nJùëÉ‚Ä≤K(ùëÜ) where ùëÉ‚Ä≤ ‚ààL(ùëÅ‚Ä≤) is a sub-program of a program ùëÉ‚ààL(ùëÅ) such that JùëÉK(ùëÜ) = ùëú. Clearly,\\nit can be seen that rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) ‚äÜtdùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú) ‚à©buùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú). Restating the definition of\\ncomplete cuts, a cut is complete if and only if the set it returns is a superset of rvùëÅ,ùëÅ‚Ä≤(ùëÜ‚Ü¶‚Üíùëú). Both\\ntop-down and bottom-up search for synthesis are special cases of cut-based middle-out synthesis.\\nTheorem 4.5. [Cuts generalize top-down and bottom-up value sets.] Given a synthesis problem\\n‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and a non-terminal ùëÅ‚Ä≤, both the functions buùëÅ‚Ä≤ and tdùëÅ,ùëÅ‚Ä≤ are complete cuts for the\\nnon-terminal ùëÅ‚Ä≤ in the context of ùëÅ.\\nIn the light of this theorem, restricting the cut in the middle-out synthesis rule from Figure 5 to\\nonly being buùëÅ‚Ä≤ produces the state-of-the-art combination of top-down and bottom-up synthesis\\nDuet [Lee 2021]. While the above theorem states that TD and BU analyses produce complete cuts,',\n",
       " 'only being buùëÅ‚Ä≤ produces the state-of-the-art combination of top-down and bottom-up synthesis\\nDuet [Lee 2021]. While the above theorem states that TD and BU analyses produce complete cuts,\\nnot all complete cuts are (overapproximations of) top-down value set or bottom-up value set.\\nExample 4.6. Building on Example 4.2, consider now the example ùëÜ‚Ü¶‚Üí24.58, but we want\\nto synthesize a program from this example starting from the nonterminal parseNumber, which\\nhas a rule parseNumber ‚ÜíParseNumber(substr, locale). One potential cut for substr in the\\ncontext parseNumber could be obtained by scanning the input string for any substrings that are\\nnumerical constants and returning them (as a string). Here it returns a set containing ‚Äú24.58‚Äù and\\n‚Äú46‚Äù and all substrings of these two strings that are valid numbers. This complete cut is not an\\noverapproximation of the bottom-up values that substr can generate since there are many more',\n",
       " '‚Äú46‚Äù and all substrings of these two strings that are valid numbers. This complete cut is not an\\noverapproximation of the bottom-up values that substr can generate since there are many more\\nsubstrings in an input. However, it is complete in the context parseNumber because these are the\\nonly strings that can be parsed as numbers.\\n‚ñ°\\n4.2\\nComputing Cuts\\nWe can use top-down value sets or bottom-up value sets as the cuts, as shown in Theorem 4.5;\\nhowever, if we do that, we only replicate top-down, bottom-up, and Duet‚Äôs meet-in-the-middle\\nsynthesis [Lee 2021]. DSL designers can provide more refined cuts that would enable going beyond\\ncurrent methods. How can designers author more refined cuts? For most operators, using witness\\nfunctions to perform top-down synthesis might suffice. Specialized cuts are only required when we\\nhave not-effectively-invertible operators that have either no witness function or very inefficient\\nwitness functions.',\n",
       " 'have not-effectively-invertible operators that have either no witness function or very inefficient\\nwitness functions.\\nIn practice, DSL designers do not necessarily have to use complete cuts. Looking back at Exam-\\nple 4.6, the set {‚Äú24.58‚Äù, ‚Äú46‚Äù} is a reasonable, but incomplete cut, because any program that extracts\\na number from a strict substring, say ‚Äú4.5‚Äù, of these two strings would be a contrived program. Cuts\\nare reminiscent of interpolants or invariants from program analysis: a cut at ùëÅ‚Ä≤ in the context of ùëÅ\\nis the denotation of an invariant that holds true at ùëÅ‚Ä≤ for all programs that compute the desired\\noutput at ùëÅ. We next describe a few cut functions used in FlashFill++ along with the DSL designer‚Äôs\\nintuition about the DSL that helped construct that cut function.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:14\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nCut for LowerCase. When computing a cut for a nonterminal ùëÅ‚Ä≤ in context of nonterminal ùëÅ, we\\nneed to consider all paths from ùëÅ‚Ä≤ to ùëÅin the grammar. We focus on one path at a time, and take a\\nunion if there are multiple paths. Consider the grammar rule single := LowerCase(concat) in the\\nFlashFill++ DSL (Figure 7), and ignore the other paths between nonterminals single and concat\\nfor now. Clearly, the witness function is given as:\\nWFLowerCase,1(ùë¶) = {ùë•| lowercase(ùë•) = ùë¶}\\nThe returned set contains 2len(ùë¶) strings. In contrast, the cut could be much smaller. Ignoring other\\npaths between the two nonterminals, one possible cut is:\\nCutconcat,single(ùëÜ‚Ü¶‚Üíùë¶) = {ùë•| lowercase(ùë•) = ùë¶and each char of ùë•is in some input in ùëÜ}.\\nHere the DSL designer uses their knowledge that the non-terminal concat can only generate strings',\n",
       " 'Cutconcat,single(ùëÜ‚Ü¶‚Üíùë¶) = {ùë•| lowercase(ùë•) = ùë¶and each char of ùë•is in some input in ùëÜ}.\\nHere the DSL designer uses their knowledge that the non-terminal concat can only generate strings\\nwhose characters are in some input. One exception to this invariance rule are strings that are\\ngenerated as ‚Äúconstant strings\", and hence this cut is not complete, but it may be a reasonable\\ncompromise between completeness and efficiency. An alternative cut could be:\\n{ùë•| lowercase(ùë•) = ùë¶and for each char ùë•[ùëñ] of ùë•, ùë•[ùëñ] is in some input or ùë•[ùëñ] == ùë¶[ùëñ]}.\\nFinally, the designer can further refine the cut to only include those strings that contain large\\nchunks of substrings of some input. Consider input ‚ÄòAmal Ahmed <AMAL@CCS.NEU.EDU>‚Äô and\\noutput ‚Äòamal‚Äô. The witness function would return 16 values, all variations of the string ‚Äòamal‚Äô\\nwith each letter optionally capitalized. However, a cut would only return 2 values ‚ÄòAmal‚Äô and',\n",
       " 'output ‚Äòamal‚Äô. The witness function would return 16 values, all variations of the string ‚Äòamal‚Äô\\nwith each letter optionally capitalized. However, a cut would only return 2 values ‚ÄòAmal‚Äô and\\n‚ÄòAMAL‚Äô, i.e., those variations that occur contiguously in the input. The same kind of reasoning\\ncan be used on all other paths from concat to single that go via the operators UpperCase and\\nProperCase, and thus we can get a cut for concat in the context of single.\\nCut for Concat. Consider the grammar rule concat := segment|Concat(segment, concat) in the\\nFlashFill++ DSL (Figure 7). The witness function for (the first argument of) Concat is given by\\nWFConcat,1(ùë¶) = {ùë•| ùë•is a prefix of ùë¶}\\nThe DSL designer knows the invariant that every string generated by segment is either a substring\\nof an input, or a string representation of date or number, or a constant string. So, a possible cut,\\nCutsegment,concat(ùëÜ‚Ü¶‚Üíùë¶), could be:\\n{ùë•| ùë•is maximal prefix of ùë¶either contained in some input or a number or a date}.',\n",
       " 'Cutsegment,concat(ùëÜ‚Ü¶‚Üíùë¶), could be:\\n{ùë•| ùë•is maximal prefix of ùë¶either contained in some input or a number or a date}.\\nThis cut is not complete since segment could generate a constant string, but the DSL designer may\\nprefer to use this cut and fallback on the witness function only if the above choices fail to work.\\nCut for RoundNumber and FormatDateTime. Example 4.2 presented the cut for parseNumber\\nin the context roundNumber. The witness function for roundNumber on the input ùëÜ‚Ü¶‚Üí24.00 will\\nhave to return an infinite set of floating point values that can all round to 24.00. However, the DSL\\ndesigner knows that parseNumber can only generate a number that occurs in the input, i.e., 24.58 or\\n46, and furthermore, numbers such as 4.5 are not reasonable choices. Hence, the designer can pick the\\ncut CutparseNumber,roundNumber(ùëÜ‚Ü¶‚Üíùë¶):\\n{ùë•| ùë•is a maximally long number extracted from a substring of an input}.\\nHere the DSL designer used their knowledge about the form of values that a nonterminal can',\n",
       " '{ùë•| ùë•is a maximally long number extracted from a substring of an input}.\\nHere the DSL designer used their knowledge about the form of values that a nonterminal can\\ngenerate to construct a cut. Another such example is the cut, CutasDate,formatDate(ùëÜ‚Ü¶‚Üíùë¶), which\\ncan be computed as:\\n{ùë•| ùë•is a maximally long datetime value extracted from a substring of an input}.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:15\\nCut for Const in the Affine Grammar. The DSL designer of arithmetic expressions given in\\nExample 3.1 can provide a cut for const in the context output by noting the monotonicity invariant:\\nvalues computed by subexpressions are smaller than values computed by the whole expression. Consider\\nthe input stateùëÜ= ‚ü®in1 ‚Ü¶‚Üí2, in2 ‚Ü¶‚Üí0‚ü©and the input-output exampleùëÜ‚Ü¶‚Üí7. Since the output is 7, we\\ncan restrict the potential values for the coefficient of in1 (which is 2) to at most 3 values, i.e., {0, 1, 2, 3}\\nas any larger value will make the product exceed the value 7. Thus, the cut, Cutconst,output(ùëÜ‚Ü¶‚Üíùë¶),\\nis computed as:\\n{ùë•‚ààN | 0 ‚â§ùë•‚â§‚åä\\nùë¶\\nùëÜ[in1] ‚åã}.\\nUsing this cut we can now perform bottom-up synthesis, whereas it wasn‚Äôt possible before since\\nthere are an infinite set of possible values for the non-terminal const and the grammar is not\\neffectively enumerable.',\n",
       " 'there are an infinite set of possible values for the non-terminal const and the grammar is not\\neffectively enumerable.\\nWhenever there is a nontrivial invariant that holds for all values that can be generated at a\\ncertain nonterminal, we can usually exploit that invariant to design a cut for that nonterminal.\\nWhile we have focused mainly on the FlashFill++ DSL to illustrate this process of designing good\\ncuts, the ideas extend to DSLs for any other domain.\\n5\\nPRECEDENCE IN DOMAIN-SPECIFIC LANGUAGES\\nWe first define the problem of synthesis in presence of precedence over operators. We then introduce\\ngDSLs, which extend the notion of DSL (Section 3.1) with precedence. We then present the gDSL\\nfor FlashFill++ and inference rules that solve the PBE synthesis problem over gDSLs.\\n5.1\\nSynthesis with Preference\\nDSL designers who want to translate programs generated in a DSL into a popular target language,',\n",
       " '5.1\\nSynthesis with Preference\\nDSL designers who want to translate programs generated in a DSL into a popular target language,\\nsuch as Python, typically want the DSL to contain all operators from the target language libraries.\\nThese operators are often redundant. For example, substrings of a string can be extracted using\\nabsolute index positions, or regular expressions, or finding locations of constant substrings in the\\nstring, or splitting a string by certain delimiters. In such cases, whenever a task is achievable in\\nmany different ways, we get a so-called broad DSL. For broad DSLs, DSL designers often have a\\npreference for which operators to use. For example, they may prefer split over find, which they\\nmay prefer in turn over using regular expressions or absolute indices. As another example, DSL\\ndesigners may prefer transforming a string containing ‚ÄúJan‚Äù to ‚ÄúJanuary‚Äù by treating the substring',\n",
       " 'may prefer in turn over using regular expressions or absolute indices. As another example, DSL\\ndesigners may prefer transforming a string containing ‚ÄúJan‚Äù to ‚ÄúJanuary‚Äù by treating the substring\\naround ‚ÄúJan‚Äù in the input as a datetime object and working with them, rather than generating\\n‚ÄúJanuary‚Äù by concatenating ‚ÄúJan‚Äù with a constant string ‚Äúuary‚Äù.\\nExample 5.1. Consider the task of extracting the second column from a line in a comma-separated\\nvalues (CSV) file, specified by the input-output example ‚ÄòWA, Olympia, UTC-8‚Äô ‚Ü¶‚Üí‚ÄòOlympia‚Äô.\\nIn the FlashFill++ DSL, this can be done using the program Split(x, ‚Äò,‚Äô, 2), where ùë•is the input,\\nor using the program Slice(x, Find(x, ‚Äò,‚Äô, 1, 0), Find(x, ‚Äò,‚Äô, 2, 0)). A traditional VSA-based syn-\\nthesizer would (possibly implicitly) produce both programs, assign scores to both using a ranking\\nfunction, and return the better ranked one. However, typically we strictly prefer programs that use',\n",
       " 'thesizer would (possibly implicitly) produce both programs, assign scores to both using a ranking\\nfunction, and return the better ranked one. However, typically we strictly prefer programs that use\\nthe Split operator over the Slice operator. Ideally, a synthesizer should not even examine Slice\\nprograms when an equivalent Split program exists. Similar preference is also seen in Python\\nprogrammers who often prefer to use str.split over regex.find or str.find.\\n‚ñ°\\nThis motivates the need to perform synthesis over a broad DSL where there is preference over\\noperators. Suppose a DSL designer has a DSL and a preference over operators and terminals. Let\\nŒ£ := F ‚à™T be the collection of all operators and terminal symbols in the DSL and let ‚âªŒ£ be a\\nprecedence relation on the symbols f ‚ààŒ£. We make the assumption that (A1) the DSL designer\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:16\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nonly provides precedence over operators that occur as alternates for the same nonterminal; that is,\\nif the designer sets f1 ‚âªŒ£ f2, then ùëÅ‚Üíf1(. . .) ‚ààR and ùëÅ‚Üíf2(. . .) ‚ààR are two rules with the\\nsame nonterminal ùëÅin the grammar. We also assume that (A2) the relation ‚âªŒ£ is a strict partial\\norder (irreflexive, asymmetric, transitive) to ensure that the DSL designers preference is consistent.\\nWe first need to formalize what it means for a synthesizer to satisfy the operator precedence ‚âªŒ£\\nprovided by the DSL designer. For this, we need to lift ‚âªŒ£ to a precedence on the set of programs ùëÉ\\ngenerated by the DSL. However, this is not easy since it is not clear which of f1(f2(in)) or f‚Ä≤\\n1(f‚Ä≤\\n2(in))\\nto prefer if the DSL designer says f1 ‚âªŒ£ f‚Ä≤\\n1 and f‚Ä≤\\n2 ‚âªŒ£ f2. We resolve this issue by saying that the\\npreference for the operator occurring ‚Äúabove‚Äù in the program is more important than anything',\n",
       " 'to prefer if the DSL designer says f1 ‚âªŒ£ f‚Ä≤\\n1 and f‚Ä≤\\n2 ‚âªŒ£ f2. We resolve this issue by saying that the\\npreference for the operator occurring ‚Äúabove‚Äù in the program is more important than anything\\nbelow. In the above example, we give more weight to f1 ‚âªŒ£ f‚Ä≤\\n1 and hence we want f1(f2(in)) to be\\npreferred over f‚Ä≤\\n1(f‚Ä≤\\n2(in)). This follows the intuition that operators at the top in, say, the FlashFill++\\nDSL, such as Concat, FormatNumber, or FormatDateTime, are more influential in determining the\\nhigh-level strategy for solving a task than operators at the bottom, such as Split or Slice. Hence,\\nwe extend the user provided ‚âªŒ£ to ‚âªùëí\\nŒ£‚äá‚âªŒ£ so that whenever ùëÅ0 ‚Üíf1(. . . , ùëÅ1, . . .) and ùëÅ1 ‚Üíf2(. . .)\\nare rules in the DSL, then f1 ‚âªùëí\\nŒ£ f2.\\nExample 5.2. Consider the synthesis task from Example 4.2 of generating ‚Äò24.00‚Äô from the input\\nstring. One possible program is Concat(ùëù1, ‚Äò.00‚Äô), where ùëù1 is a subprogram that extracts ‚Äò24‚Äô',\n",
       " 'Œ£ f2.\\nExample 5.2. Consider the synthesis task from Example 4.2 of generating ‚Äò24.00‚Äô from the input\\nstring. One possible program is Concat(ùëù1, ‚Äò.00‚Äô), where ùëù1 is a subprogram that extracts ‚Äò24‚Äô\\nfrom the input string. A second possible program is Segment(FormatNumber(ùëù2, fmt_desc) that\\ngenerates ‚Äò24.00‚Äô by formatting a number computed by program ùëù2. Here, Segment is a dummy\\nidentity operator having higher preference than Concat in the FlashFill++ gDSL (Figure 7). In the\\nFlashFill++ DSL, the second program is preferred since it does not use concatenation, irrespective of\\nhow ùëù1 and ùëù2 work‚Äîat the top-level concatenation is strictly less preferred. Note that here ùëù1 is\\nlikely to be significantly smaller and simpler than ùëù2 as it is just extracting the string ‚Äò24‚Äô, while\\nùëù2 is extracting a number and then rounding it. A traditional arithmetic ranking function (as used\\nin FlashFill and FlashMeta) intuitively computes the score of programs as a weighted sum of the',\n",
       " 'ùëù2 is extracting a number and then rounding it. A traditional arithmetic ranking function (as used\\nin FlashFill and FlashMeta) intuitively computes the score of programs as a weighted sum of the\\nscore of its sub-programs, and hence, will need to be tuned carefully to ensure that the smaller\\nconcat program is scored worse than the larger segment program.\\n‚ñ°\\nWe want the relation ‚âªùëí\\nŒ£ to be a strict partial order. However, in general, it may not be a strict\\npartial order due to cycles in the grammar (where some ùëÅ0 generates a term containing ùëÅ0), which\\nagain makes ‚âªùëí\\nŒ£ violate irreflexivity or transitivity. We make the reasonable assumption that there\\nare no cycles since we often limit the depth of terms being synthesized and then the assumption\\ncan be satisfied by renaming the nonterminals. Thus, without loss of much generality, we can\\nassume that the extended precedence ‚âªùëí\\nŒ£ on Œ£ is a strict partial order.',\n",
       " 'can be satisfied by renaming the nonterminals. Thus, without loss of much generality, we can\\nassume that the extended precedence ‚âªùëí\\nŒ£ on Œ£ is a strict partial order.\\nNow we formalize synthesizing in presence of precedence ‚âªŒ£ by lifting the precedence ‚âªŒ£ on\\nŒ£ to ‚âªùëí\\nŒ£, and then to a preference on programs (trees) over Œ£ using the well-known lexicographic\\npath ordering (LPO), ‚âªùëôùëùùëú, which is defined as follows [Dershowitz and Jouannaud 1990]: Given\\nprograms ùëÉ1 := f1(ùëÉ11, . . . , ùëÉ1ùëò) and ùëÉ2 := f2(ùëÉ21, . . . , ùëÉ2ùëô), we have ùëÉ1 ‚âªùëôùëùùëúùëÉ2 if either (a) f1 ‚âªùëí\\nŒ£ f2\\nand ùëÉ1 ‚âªùëôùëùùëúùëÉ2ùëñfor all ùëñ, or (b) f1 = f2 and there exists a ùëös.t. ùëÉ1ùëñ= ùëÉ2ùëñfor ùëñ< ùëöand ùëÉ1ùëö‚âªùëôùëùùëúùëÉ2ùëö,\\nor (c) ùëÉ1ùëñ‚âªùëôùëùùëúùëÉ2 for some ùëñ.\\nDefinition 5.3. Let ‚™∞base be the base ordering to rank programs that are unordered by ‚âªùëôùëùùëú. Given\\na DSL D with precedence ‚âªŒ£ on the set Œ£ := F ‚à™T of all operators and terminal symbols in D, and\\n‚™∞base, the PBE synthesis with precedence problem is to find the maximally ranked program by the',\n",
       " 'a DSL D with precedence ‚âªŒ£ on the set Œ£ := F ‚à™T of all operators and terminal symbols in D, and\\n‚™∞base, the PBE synthesis with precedence problem is to find the maximally ranked program by the\\nlexicographic combination of ‚âªùëôùëùùëúand ‚™∞base that satisfies a given example, where ‚âªùëôùëùùëúis the LPO\\ninduced by the extended precedence ‚âªùëí\\nŒ£.2\\n2Given orderings ‚âª1 and ‚âª2, the lexicographic combination ‚âª‚âª1,‚âª2 is defined as follows: ùë†‚âª‚âª1,‚âª2 ùë°if either (a) ùë†‚âª1 ùë°, or\\n(b) ùë†‚äÅ1 ùë°and ùë°‚äÅ1 ùë†and ùë†‚âª2 ùë°.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:17\\nWe solve the PBE synthesis with precedence problem by extending DSLs to gDSLs and modifying\\nthe inference rules to correctly handle the precedence.\\n5.2\\nGuarded Domain-Specific Languages\\nA guarded domain-specific language (gDSL) is a DSL D = ‚ü®N, T, F, R, Vin, ùë£out‚ü©where the set R of\\nrules can additionally contain guarded rules of the form ùëÅ‚Üíùõº1 |‚ä≤ùõº2 |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤ùõºùëòwhere each ùõºùëñis\\neither f(ùë£1, . . . , ùë£ùëõ) or ùë£0, where f ‚ààF , ùë£0 ‚ààT, and ùë£1, . . . , ùë£ùëõ‚ààN ‚à™T. A non-terminal ùëÅcan have\\nany number of guarded rules associated with it, each with possibly different values of ùëò‚â•1.\\nThe rules in a regular DSL can be viewed as a special case of guarded rules where ùëò= 1 (in the\\ndefinition of guarded production rules above.) When ùëò> 1, a guarded rule ùëÅ‚Üíùõº1 |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤ùõºùëò\\nassociated with the nonterminal ùëÅhas ùëòalternates on the right-hand side that are ordered. The ùëñ-th',\n",
       " 'definition of guarded production rules above.) When ùëò> 1, a guarded rule ùëÅ‚Üíùõº1 |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤ùõºùëò\\nassociated with the nonterminal ùëÅhas ùëòalternates on the right-hand side that are ordered. The ùëñ-th\\nalternate ùõºùëñyields a (regular) rule ùëÅ‚Üíùõºùëñ. We call ùëÅ‚Üíùõºùëñthe ùëñ-th constituent rule of the original\\nguarded rule. Define Rùëêas the collection of all constituent rules of rules in R; that is, Rùëê:= {ùëÅ‚Üí\\nùõºùëñ| ùëÅ‚Üí(. . . |‚ä≤ùõºùëñ|‚ä≤. . .) ‚ààR}. We call the (regular) DSL Dùëê:= ‚ü®N, T, F, Rùëê, Vin, ùë£out‚ü©obtained\\nfrom a gDSL D := ‚ü®N, T, F, R, Vin, ùë£out‚ü©a constituent DSL of the gDSL D.\\nGiven an instance of the PBE synthesis with precedence problem, we can annotate the given DSL\\nwith precedence to get a gDSL. We assume that the precedence relation ‚âªŒ£ satisfies Assumptions (A1)\\nand (A2). Furthermore, we also assume that (A3) the precedence is a series-parallel partial order\\n(SPPO) [B√©chet et al. 1997]. Under Assumption (A3), the precedence can be encoded in gDSL',\n",
       " 'and (A2). Furthermore, we also assume that (A3) the precedence is a series-parallel partial order\\n(SPPO) [B√©chet et al. 1997]. Under Assumption (A3), the precedence can be encoded in gDSL\\nby introducing new nonterminals where necessary. Specifically, if we have a maximal chain\\nf1 ‚âªŒ£ f2 ‚âªŒ£ . . . ‚âªŒ£ fùëòover alternate operators ùëÅ‚Üíf1(. . .)| ¬∑ ¬∑ ¬∑ |fùëò(. . .) in the DSL, then we\\nadd a guarded rule ùëÅ‚Üíf1(. . .) |‚ä≤¬∑ ¬∑ ¬∑ |‚ä≤fùëò(. . .). However, if certain alternate operators are left\\nincomparable by the DSL designers, then we introduce a new nonterminal. For example, if f1 ‚âªŒ£ f3\\nand f2 ‚âªŒ£ f3, but there is no preference between f1 and f2, then we introduce a new nonterminal\\nùëÅ‚Ä≤ and have ùëÅ‚ÜíùëÅ‚Ä≤ |‚ä≤f3(. . .) and ùëÅ‚Ä≤ ‚Üíf1(. . .)|f2(. . .) in the gDSL. Any SPPO ‚âªŒ£ can thus be\\nencoded in the gDSL.\\nExample 5.4. Consider the DSL for affine arithmetic from Example 3.1. Suppose the DSL designer\\nwants the precedence input1 ‚âªŒ£ input2. Then, we can replace the two rules for nonterminal',\n",
       " 'Example 5.4. Consider the DSL for affine arithmetic from Example 3.1. Suppose the DSL designer\\nwants the precedence input1 ‚âªŒ£ input2. Then, we can replace the two rules for nonterminal\\naddend by a single guarded rule: addend ‚ÜíTimes(const, input1) |‚ä≤Times(const, input2) to\\nget a gDSL, Dùëî\\nùê¥. In the LPO induced by the extension ‚âªùëí\\nŒ£ of this preference, the program ùëÉ1 :=\\nPlus(Times(3, input1), 1) is preferred over ùëÉ2 := Plus(Times(3, input2), 7).\\n‚ñ°\\nLet ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©be a synthesis task, where R is the rules of a gDSL D. We say a program ùëÉis\\na solution for this task if\\n(a) ùëÉis a solution for the task ‚ü®ùë£out, Rùëê,ùëÜ‚Ü¶‚Üíùëú‚ü©(in the constituent DSL Dùëê), and\\n(b) for any other ùëÉ‚Ä≤ that is a solution in the constituent DSL, ùëÉ‚Ä≤ ‚äÅùëôùëùùëúùëÉ, where ‚âªùëôùëùùëúis the LPO\\ninduced by the extended precedence ‚âªùëí\\nŒ£ coming from the guarded rules.\\nCondition (a) says that ùëÉshould be a solution for the synthesis problem ignoring the precedence.',\n",
       " 'induced by the extended precedence ‚âªùëí\\nŒ£ coming from the guarded rules.\\nCondition (a) says that ùëÉshould be a solution for the synthesis problem ignoring the precedence.\\nCondition (b) says that the ordering in the rules should be interpreted as an ordering on programs\\nusing the induced LPO, and we should ignore programs smaller in this ordering.\\nExample 5.5. Consider the gDSL Dùëî\\nùê¥and programs ùëÉ1, ùëÉ2 from Example 5.4. Consider the synthesis\\ntask ‚ü®ùë£out, R, ‚ü®input1 ‚Ü¶‚Üí2, input2 ‚Ü¶‚Üí0‚ü©‚Ü¶‚Üí7‚ü©from Example 3.2, but with R now coming from the\\ngDSL Dùëî\\nùê¥. Both programs, ùëÉ1 and ùëÉ2, map the input state to 7. However, ùëÉ2 is now not a solution in\\nDùëî\\nùê¥because there exists ùëÉ1 that is preferred. The program ùëÉ3 := 7 also maps the input state to 7.\\nThe (derivations of) ùëÉ1 and ùëÉ3 are incomparable; and in fact, both are solutions in Dùëî\\nùê¥.\\n‚ñ°\\nIf a solution for the unguarded DSL exists, then there will be a solution that is maximal and\\nhence a solution for the gDSL will exist.',\n",
       " 'ùê¥.\\n‚ñ°\\nIf a solution for the unguarded DSL exists, then there will be a solution that is maximal and\\nhence a solution for the gDSL will exist.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:18\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nGuarded.If\\nPS1 |= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nùëÅ‚Üíùõº1 |‚ä≤ùõº2 ‚ààR\\nPS1 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nGuarded.Else\\nÃ∏|= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nùëÅ‚Üíùõº1 |‚ä≤ùõº2 ‚ààR\\nPS2 |= ‚ü®ùõº2, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nPS2 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©\\nFig. 6. Extending top-down synthesis for guarded rules.\\nTheorem 5.6 (Precedence preserves solvability.). Let D be a gDSL based on precedence ‚âªŒ£\\nthat is a strict partial order. Let ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùë£‚ü©be a synthesis task. This task has a solution in D iff\\nthere is a solution in Dùëê.\\nIf we can compute all solutions for a synthesis task over a gDSL, we can order them by any ‚™∞base\\nordering to solve the PBE synthesis with precedence problem.\\n5.3\\nPBE Synthesis Rules for Guarded DSLs\\nFigure 6 contains two inference rules that describe how guarded rules are handled in top-down,\\nbottom-up, or middle-out synthesis. If ùëÅ‚Üíùõº1 |‚ä≤ùõº2 is a guarded rule in R and we can (recursively)\\nprove ùëÉùëÜ1 |= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, then Rule Guarded.If can be used to assert ùëÉùëÜ1 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. (This',\n",
       " 'bottom-up, or middle-out synthesis. If ùëÅ‚Üíùõº1 |‚ä≤ùõº2 is a guarded rule in R and we can (recursively)\\nprove ùëÉùëÜ1 |= ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, then Rule Guarded.If can be used to assert ùëÉùëÜ1 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©. (This\\nassertion will be nontrivial only if ùëÉùëÜ1 ‚â†‚àÖ.) In case there is no program that solves ‚ü®ùõº1, R,ùëÜ‚Ü¶‚Üíùëú‚ü©,\\nand we can (recursively) prove ùëÉùëÜ2 |= ‚ü®ùõº2, R,ùëÜ‚Ü¶‚Üíùëú‚ü©, then Rule Guarded.Else can be used to assert\\nùëÉùëÜ2 |= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©.\\nRecall that the notation ùëÉùëÜ|= ‚ü®ùëÅ, R,ùëÜ‚Ü¶‚Üíùëú‚ü©simply asserts that every program in ùëÉùëÜsolves the\\ngiven synthesis task, and does not require ùëÉùëÜto contain all solutions. The Rule Guarded.Else has a\\ncondition that a certain synthesis task be unsolvable. If we have the ability to compute all solutions\\n(such as, using version-space algebras, or VSAs), then that can help in determining when a problem\\nis unsolvable, but other synthesis approaches that can establish infeasibility can also be used.\\nExample 5.7. Continuing from Example 5.5, consider the task of generating 7 from inputs 2',\n",
       " 'is unsolvable, but other synthesis approaches that can establish infeasibility can also be used.\\nExample 5.7. Continuing from Example 5.5, consider the task of generating 7 from inputs 2\\nand 0. Say we reduce the original task to the proof obligation ùëã|= ‚ü®addend, R,ùëÜ‚Ü¶‚Üíùë•‚ü©, where\\nwe pick say 6 for ùë•. Now, we have a guarded rule for addend and we can use Rule Guarded.If\\nto get the proof obligation ùëã|= ‚ü®Times(const, input1), R,ùëÜ‚Ü¶‚Üí6‚ü©. This subtask has a solution\\nùëã= {Times(3, input1)}, and hence we will not consider the option Times(const, input2).\\n‚ñ°\\nPerforming synthesis over the gDSL is equivalent to solving PBE synthesis with precedence.\\nTheorem 5.8. Let ‚âªŒ£ be a precedence on Œ£ := F ‚à™T that satisfies Assumptions (A1), (A2), and (A3).\\nLet D := ‚ü®N, T, F, R, Vin, ùë£out‚ü©be a gDSL that encodes ‚âªŒ£. Let Dùëê:= ‚ü®N, T, F, Rùëê, Vin, ùë£out‚ü©be\\nits (unguarded) constituent DSL. Let ‚™∞base be an ordering on programs and let ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©be a\\nsynthesis task. Then, the following are equivalent:',\n",
       " 'its (unguarded) constituent DSL. Let ‚™∞base be an ordering on programs and let ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©be a\\nsynthesis task. Then, the following are equivalent:\\n(1) {ùëÉ} |= ‚ü®ùë£out, R,ùëÜ‚Ü¶‚Üíùëú‚ü©and ùëÉis maximal w.r.t ‚™∞base among all such solutions.\\n(2) The program ùëÉis a solution in Dùëêfor the task ‚ü®ùë£out, Rùëê,ùëÜ‚Ü¶‚Üíùëú‚ü©that is maximal w.r.t a lexicographic\\ncombination of ‚âªùëôùëùùëú(induced by ‚âªŒ£) and ‚™∞base.\\nTheorem 5.8 shows that using a ranker ‚™∞base with a gDSL D has the same effect as using a\\ncomplex ranker (lexicographic combination of ‚âªùëôùëùùëúand ‚™∞base) with a regular DSL Dùëê. This shows\\nthat our gDSL-based approach solves the PBE synthesis with precedence problem (under some\\nassumptions). Theorem 5.8 also explains why the ranker ‚™∞base used with a gDSL D can be very\\nsimple compared to what is needed with a regular DSL Dùëê. Designing a good complex ranking\\nfunction has traditionally been very challenging in PBE, taking many developer-months to converge',\n",
       " 'simple compared to what is needed with a regular DSL Dùëê. Designing a good complex ranking\\nfunction has traditionally been very challenging in PBE, taking many developer-months to converge\\non a good ranking function [Kalyan et al. 2018; Natarajan et al. 2019; Singh and Gulwani 2015]. In\\ncontrast, FlashFill++ uses the power of gDSLs (Theorem 5.8) to reduce the requirements on its base\\nranker, which was developed significantly faster.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:19\\nlanguage FlashFillPP;\\n@start string output := single |> If(cond, single, output)\\n// conditions\\nbool cond := pred |> And(pred, cond);\\nbool pred := StartsWith(x, matchPattern) |> EndsWith(x, matchPattern)\\n|> Contains(x, matchPattern) |> ...;\\n// single branch\\nstring single := concat | LowerCase(concat) | UpperCase(concat) | ProperCase(concat);\\n// optional concatenation of substrings\\nstring concat := segment |> Concat(segment, concat)\\n// substring logic\\nstring segment := substr | formatNumber | formatDate | constStr;\\n// format substring as a number\\nstring formatNumber := FormatNumber(roundNumber, numFmtDesc);\\ndecimal roundNumber := parseNumber |> RoundNumber(parseNumber, roundNumDesc);\\ndecimal parseNumber := AsNumber(row, columnName) |> ParseNumber(substr, locale);\\n// format substring as a date\\nstring formatDate := FormatDateTime(asDate, dateFmtDesc);',\n",
       " 'decimal parseNumber := AsNumber(row, columnName) |> ParseNumber(substr, locale);\\n// format substring as a date\\nstring formatDate := FormatDateTime(asDate, dateFmtDesc);\\nDateTime asDate := AsDateTime(row, columnName) |> ParseDateTime(substr, parseDateFmtDesc);\\n// find a substring within the input x\\nstring substr := Split(x, splitDelimiter, splitInstance)\\n|> Slice(x, pos, pos)\\n|> MatchFull(x, matchPattern, matchInstance);\\n// find a position within the input x\\nint pos := End(x) |> Abs(x, absPos)\\n|> Find(x, findDelimiter, findInstance, findOffset)\\n|> Match(x, matchPattern, matchInstance)\\n|> MatchEnd(x, matchPattern, matchInstance);\\n// literal terminals\\nFmtNumDescriptor numFmtDesc; RndNumDescriptor roundNumDesc;\\nFmtDateTimeDescriptor dateFmtDes, parseDateFmtDesc;\\nstring constStr, splitDelimiter, findDelimiter;\\nint splitInstance, findInstance, matchInstance, findOffset;\\nRegex matchPattern; int absPos;',\n",
       " 'FmtDateTimeDescriptor dateFmtDes, parseDateFmtDesc;\\nstring constStr, splitDelimiter, findDelimiter;\\nint splitInstance, findInstance, matchInstance, findOffset;\\nRegex matchPattern; int absPos;\\nFig. 7. A fragment of the gDSL for FlashFill++. | choices are unguarded, |> choices are guarded.\\n5.4\\nGuarded DSL for FlashFill++\\nWe now describe the FlashFill++ gDSL and compare it to FlashFill. Figure 7 shows a major part of the\\nDSL. FlashFill++ shares the top level rules that perform conditional statements, case conversion, and\\nstring concatenation with FlashFill. Conditionals enable if-then-else logic. The condition is one or\\nmore conjunctive predicates based on properties of the input string. Case conversion transforms a\\nstring into lower-, upper-, or proper-case. Concatenation concatenates two strings.\\nAlthough FlashFill can perform some datetime and number operations using text manipulation\\n(such as \"01/01/2020\" ‚Üí\"2020\" or \"10.01\" ‚Üí\"10\"), it is unable to express other sophisticated',\n",
       " 'Although FlashFill can perform some datetime and number operations using text manipulation\\n(such as \"01/01/2020\" ‚Üí\"2020\" or \"10.01\" ‚Üí\"10\"), it is unable to express other sophisticated\\ndatetime and number operations as it does not incorporate operations over those datatypes, but\\nrather treats them as standard strings. For instance, FlashFill cannot get the day of week from a date\\n(such as \"01/01/2020\" ‚Üí\"Wednesday\"), or round up a number (e.g., \"10.49\" ‚Üí\"10.5\"). This\\nmotivates us to add new rules to support richer datetime (rules parseDate and formatDate) and\\nnumber (rules parseNumber and formatNumber) transformations.\\nThe next major differences are in the substr and pos rules. FlashFill has a single Slice operator\\nwhich selects a substring defined by its start and end positions. These positions can be defined\\neither as absolute positions or with the complex RegPos operator which finds the ùëòth place in the',\n",
       " 'which selects a substring defined by its start and end positions. These positions can be defined\\neither as absolute positions or with the complex RegPos operator which finds the ùëòth place in the\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:20\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nstring where the left- and right substrings match the two given regular expressions. While this is\\nexpressive enough to cover any desired substring selection and all of the operators in FlashFill++\\ncan technically be expressed in terms of it, this introduces a challenge for an industrial synthesizer\\nthat targets different languages for code generation: not all platforms of interest support regular\\nexpressions natively (e.g. Microsoft Excel‚Äôs formula language does not support regular expressions).\\nIn contrast, when designing FlashFill++ we chose a wider collection of more natural operators that\\nare closer to what developers use in practice when working with target languages ‚Äì this removes\\nthe mismatch between synthesis DSL and code generation target language.\\nIn particular, instead of only allowing substrings to be defined as a Slice with their start and',\n",
       " 'the mismatch between synthesis DSL and code generation target language.\\nIn particular, instead of only allowing substrings to be defined as a Slice with their start and\\nend positions, FlashFill++ adds a Split operator to select the ùëòth element in a sequence generated\\nby splitting the input string by some delimiters. We also add a MatchFull operator to find the ùëòth\\nmatch of a regular expression. Additionally, in FlashFill++ the pos rule replaces the operator RegPos\\n(which relies on a pair of regular expressions to identify a position) with a Find of a constant string\\nin the input and a Match/MatchEnd of a regular expression.\\nExample Guarded Rules in the FlashFill++ DSL. We go over a few cases of guarded rules in FlashFill++\\nto show they capture natural intuitions and rules of thumb in the string transformation domain.\\n‚Ä¢ Single segments over concats. The guarded rule segment |‚ä≤Concat(segment, concat) ensures',\n",
       " 'to show they capture natural intuitions and rules of thumb in the string transformation domain.\\n‚Ä¢ Single segments over concats. The guarded rule segment |‚ä≤Concat(segment, concat) ensures\\nthat we try to synthesize programs that generate the whole output at once, before generating\\nprograms that produce a prefix and a suffix of the output and then combine them. Whenever\\nsuch a program exists, this guarded rule potentially saves the exploration of a huge portion\\nof the program space. This single guarded rule plays a crucial role in keeping FlashFill++\\nperformant since witness function of the concat operator produces 2(ùëõ‚àí1) subtasks, one\\neach for the prefix and suffix at each point where the output string can be split.\\n‚Ä¢ Splits over slices. As illustrated Example 5.1, FlashFill++ strictly prefers programs that use\\nthe Split operator over programs that use the Slice operator. Program in data wrangling\\nscenarios very commonly begin by extracting the appropriate part of the input from a',\n",
       " 'the Split operator over programs that use the Slice operator. Program in data wrangling\\nscenarios very commonly begin by extracting the appropriate part of the input from a\\ndelimited file record (CSVs, TSVs, etc). In all such cases, a split program more closely follows\\nthe human intuition of ‚Äúextract the ùëõùë°‚Ñécolumn delimited by‚Äù as compared to a slice program.\\nNote that the split operator is a ‚Äúhigher level‚Äù construct preferred over the ‚Äúlow-level‚Äù slice.\\n‚Ä¢ Input numbers over rounded numbers. The RoundNumber operator in Figure 7 is guarded by\\nparseNumber, meaning that we can only generate a program that rounds a number if no\\nnumber in the input can be used directly to produce the same output.\\n6\\nEXPERIMENTAL EVALUATION\\nWe now present our experimental results, including an ablation study and survey-based user study.\\n6.1\\nMethodology\\nWe used 3 publicly available benchmark sets ‚Äì Duet string benchmarks [Lee 2021], Playgol [Cropper',\n",
       " '6.1\\nMethodology\\nWe used 3 publicly available benchmark sets ‚Äì Duet string benchmarks [Lee 2021], Playgol [Cropper\\n2019], and Prose [PROSE 2022] ‚Äì covering a range of string transformations, including datetime\\nand number formatting.\\nWe compare FlashFill++ with three systems: two publicly available state-of-the-art synthesis\\nsystems that are deployed in productions, namely FlashFill [Gulwani 2011] and SmartFill [Chen\\net al. 2021a; Google 2021], and one, Duet, from a recent publication [Lee 2021]. To experiment with\\nFlashFill, we implemented it on top of the FlashMeta framework [Polozov and Gulwani 2015]. We\\ncarry out our FlashFill, FlashFill++, and Duet experiments on a machine with 2 CPUs & 8GB RAM. To\\nexperiment with SmartFill, we employ Google Sheets in Chrome and use it to solve the subset of\\ntasks that are suitable for its spreadsheet environment.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:21\\nTable 1. Comparing different tools (rows) on the 3 public benchmarks (columns) based on (1) number of\\nbenchmarks correctly solved (Columns 2‚Äì5) and (2) average number of examples required on the successful\\nbenchmarks (Columns 6‚Äì8). The total number of benchmarks attempted are shown in brackets.\\nNumber benchmarks solved\\nAverage #examples required\\nDuet (205)\\nPlaygol (327)\\nProse (354)\\nTotal (886)\\nDuet\\nPlaygol\\nProse\\nFlashFill\\n139\\n264\\n172\\n575\\n1.41\\n1.26\\n1.54\\nFlashFill++\\n159\\n307\\n353\\n819\\n1.69\\n1.46\\n1.52\\nDuet\\n102\\n211\\n166\\n479\\n1.97\\n2.11\\n2.01\\nSmartFill\\n37 (158)\\n34 (327)\\n18 (296)\\n89 (781)\\n2.75\\n2.85\\n2.83\\nSince SmartFill is not exposed in the Google Sheets API, we rely on browser-automation using\\nSelenium [Selenium 2022] for SmartFill evaluation. Moreover, we remove problematic benchmark\\ntasks and use 158, 327, and 296 tasks from the Duet, Playgol, and Prose benchmarks, respectively,',\n",
       " 'Selenium [Selenium 2022] for SmartFill evaluation. Moreover, we remove problematic benchmark\\ntasks and use 158, 327, and 296 tasks from the Duet, Playgol, and Prose benchmarks, respectively,\\nfor SmartFill evaluation. The tasks removed were unsuitable for browser automation (e.g. too many\\nrows or new line characters).\\nThe Duet tool [Lee 2021] accepts a DSL as part of its input. Different Duet benchmarks used\\nslightly different DSLs. For fair comparison, we set a single fixed DSL. We obtained the fixed DSL\\nby taking all commonly-used rules in the string transformation benchmarks of Duet. Second, Duet\\nhas some hyperparameters, for which we picked the best setting after some experimentation.\\n6.2\\nExpressiveness and Performance\\nA key feature of FlashFill++ is improved expressiveness. Table 1 (Columns 2‚Äì5) shows the number\\nof benchmark tasks that the various tools (rows) can correctly solve. FlashFill++ produces a correct',\n",
       " 'A key feature of FlashFill++ is improved expressiveness. Table 1 (Columns 2‚Äì5) shows the number\\nof benchmark tasks that the various tools (rows) can correctly solve. FlashFill++ produces a correct\\nprogram for most number of benchmarks. FlashFill is limited due to a lack of datetime and number\\nformatting. The DSL used in the Duet tool has no datetime support, and only limited support for\\nnumber and string formatting. The SmartFill tool is a neural-based general purpose tool and has\\nthe weakest numbers here. Since our SmartFill experiments rely on browser-based interaction, it is\\npossible that the underlying synthesizer can solve more benchmarks but these are not exposed to\\nthe UI. However, our setup reflects the experience that a user would face.\\nOur DSL is expressive, covering a large class of string, datetime and number transformations;\\nthus showing the added value from using cuts and guarded rules.',\n",
       " 'Our DSL is expressive, covering a large class of string, datetime and number transformations;\\nthus showing the added value from using cuts and guarded rules.\\nWhile increasing expressiveness enables users to accomplish more tasks, there is a risk of reducing\\nperformance on existing tasks. To this end, we consider the minimum number of examples required\\nfor a synthesizer to learn a correct program. To find that number, we use a counter-example guided\\n(CEGIS) loop which provides the next failing I/O example in every iteration to the synthesizer.\\nWe use a time out of 20 seconds. Table 1 Columns 6‚Äì8 present the average number of examples\\nthe various tools used across the benchmarks where they were successful. Here FlashFill has the\\nbest numbers, which indicates that when it works (for string benchmarks), it learns with very\\nfew examples. Our tool FlashFill++ takes only slightly more examples on average, partly because',\n",
       " 'best numbers, which indicates that when it works (for string benchmarks), it learns with very\\nfew examples. Our tool FlashFill++ takes only slightly more examples on average, partly because\\ndatetime and number formatting typically requires more examples for intent disambiguation. For\\nexample, the string ‚Äò2/3/2020‚Äô can either be the 2ùëõùëëof March or the 3ùëõùëëof February. The Duet and\\nSmartFill tools take more examples in general. We emphasize that the performance of FlashFill++ is\\ngood here because it solves more problems (presumably much harder instances) and yet it doesn‚Äôt\\nuse too many more examples (the harder instances did not make the averages much worse).\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:22\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\n0\\n2500\\n5000\\n7500\\n10000 12500\\nBest Duet or FF Synthesis Time (ms)\\n‚àí2\\n‚àí1\\n0\\n1\\n2\\nlog10(Best / FlashFill++)\\nOutcome\\nFlashFill++ better\\nFlashFill++ worse\\n(a) log10( min(FlashFill,Duet)\\nFlashFill++\\n) vs min(FlashFill, Duet).\\nBenchmark classes\\nDuet\\nPlaygol\\nProse\\nOverall\\nFlashFill\\n689.7\\n1757.0\\n734.1\\n1116.4\\nFlashFill++\\n203.0\\n217.1\\n246.4\\n228.5\\nDuet\\n264.0\\n558.4\\n1351.7\\n766.74\\n(b) Average time (in ms) taken by the tools\\n(rows) over successful benchmarks from the three\\nsources (columns), and the average for each tool\\nover the entire suite (last column).\\nFig. 8. FlashFill++ is generally faster ‚Äì up to two orders of magnitude ‚Äì than the best of FlashFill and Duet,\\nand the slowdowns are mostly on fast benchmarks.\\nDespite solving more tasks, FlashFill++ continues to require a reasonable number of examples\\ncompared to baselines, showing that it generalizes to unseen examples and does not overfit.',\n",
       " 'Despite solving more tasks, FlashFill++ continues to require a reasonable number of examples\\ncompared to baselines, showing that it generalizes to unseen examples and does not overfit.\\nWe next compare synthesis times (averaged over 5 runs). In particular, we compute the synthesis\\ntime when the tools are given the minimum number of examples they required to produce the\\ncorrect program. Figure 8 shows the results. We restrict this experiment to FlashFill, Duet, and\\nFlashFill++, as synthesis time for SmartFill is unobservable through the browser.\\nFigure 8a compares FlashFill++ with the faster of FlashFill and Duet. We focus on benchmarks that\\nare solved by FlashFill++, and by either FlashFill or Duet. The y-axis displays the log base 10 of the\\nratio of the best of FlashFill and Duet synthesis time to the FlashFill++ synthesis time. A higher value\\nrepresents a larger reduction in synthesis time. On the x-axis we display the best of FlashFill and',\n",
       " 'represents a larger reduction in synthesis time. On the x-axis we display the best of FlashFill and\\nDuet synthesis time (in milliseconds) for that benchmark task. FlashFill++ reduces synthesis time\\nin 63% of the benchmarks, and the remaining 37% happen to be benchmarks where synthesis is\\nfast (< 500ùëöùë†in most cases) and slowdown is likely not observable in a user-facing application. In\\n37.3% cases, FlashFill++ is at least one order of magnitude faster and in 18% cases it is at least two\\norders of magnitude faster. Table 8b shows the average synthesis time for various tools across the\\nbenchmark classes. We averaged over the benchmarks that the tool solved. We see that FlashFill++\\nhas better averages despite solving more (presumably harder) benchmarks.\\nFlashFill++ is faster on average despite solving more (presumably harder) benchmarks and better\\nthan the best of the baselines on most hard instances.',\n",
       " 'FlashFill++ is faster on average despite solving more (presumably harder) benchmarks and better\\nthan the best of the baselines on most hard instances.\\nFinally, we evaluate the gain from using gDSLs. We create FlashFill++‚àíby replacing gDSL used\\nin FlashFill++ by a regular DSL ( |‚ä≤operator is treated as the usual |). We compare FlashFill++ and\\nFlashFill++‚àíon synthesis time and minimum examples required metrics. Figure 9b summarize the\\nresults. We note that gDSLs reduce synthesis time in 91% of the benchmark tasks, give more than\\n3x speedup in 20% of cases, and generate better performance across the benchmarks and metrics.\\nPrecedences in gDSLs consistently help FlashFill++ in improving both search (synthesis time)\\nand ranking (minimum number of examples).\\n6.3\\nCode Readability\\nTraditionally DSL design has focused on efficacy of the learning and ranking process, and not on\\nreadable code generation. We evaluated the extent to which FlashFill++ enables such readable code.',\n",
       " 'Traditionally DSL design has focused on efficacy of the learning and ranking process, and not on\\nreadable code generation. We evaluated the extent to which FlashFill++ enables such readable code.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:23\\nDuet\\nPlaygol\\nProse\\nOverall\\nAverage Synthesis Time\\nFlashFill++\\n203.0\\n217.1\\n246.4\\n228.5\\nFlashFill++ ‚àí\\n255.0\\n350.6\\n410.3\\n361.1\\nAverage #examples required\\nFlashFill++\\n1.69\\n1.46\\n1.52\\n1.53\\nFlashFill++ ‚àí\\n1.76\\n1.54\\n1.56\\n1.59\\n(a) Synthesis time and no. of examples required to learn.\\n0\\n2000\\n4000\\n6000\\n8000\\nSynthesis Time FF++ (Ablated) (ms)\\n‚àí0.50\\n‚àí0.25\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\nlog10(FF++ (Ablated) / FlashFill++)\\nOutcome\\nFlashFill++ worse\\nFlashFill++ better\\n(b) Synthesis Time Ratios.\\nFig. 9. FlashFill++ improves over an ablation that removes the use of gDSLs, denoted FlashFill++‚àí.\\nFirst, we compared the code complexity for programs synthesized by FlashFill++ and FlashFill for\\ntwo target languages: Python & PowerFx, the low/no-code language of the Power platform [PowerFx\\n2021]. On average, FlashFill++‚Äôs Python is 76% shorter and uses 83% fewer functions, whereas',\n",
       " 'two target languages: Python & PowerFx, the low/no-code language of the Power platform [PowerFx\\n2021]. On average, FlashFill++‚Äôs Python is 76% shorter and uses 83% fewer functions, whereas\\nFlashFill++‚Äôs PowerFx is 57% shorter and uses 55% fewer functions. We next compared the Google\\nSheets formula language code generated by SmartFill with Excel code generated by FlashFill++. We\\nfound that FlashFill++ does better in ‚âà60% of the formulas generated and is at parity for ‚âà20% of\\nthe formulas. We did not compare with Duet since it generates programs only in its DSL and not in\\nany target language.\\nNext, we carried out a survey-based user study where we asked users to read and compare\\ndifferent Python functions synthesized by alternative approaches. In our study, we further augment\\nFlashFill++ with a procedure to rename variables. This part of the system is optional, and is only\\nadded to further underscore the benefits of readable code generation. To perform variable renaming',\n",
       " 'FlashFill++ with a procedure to rename variables. This part of the system is optional, and is only\\nadded to further underscore the benefits of readable code generation. To perform variable renaming\\nwe use the few-shot learning capability of Codex [Chen et al. 2021b], a pretrained large language\\nmodel, and iteratively provide the following prompt [Gao et al. 2021]: (1) two samples of the\\nrenaming task, where each sample contains I/O examples, FlashFill++ program, and the renamed\\nprogram, (2) the current renaming task, which contains the examples and the FlashFill++ program\\nto be renamed, and (3) partially renamed program up to the next non-renamed variable.\\nWe sampled 10 tasks from our benchmarks, with probability proportional to the number of\\nidentifier renaming calls made to Codex. For each task, we displayed the Python code generated by\\nFlashFill, FlashFill++, and FlashFill++ with Codex (anonymized as A, B, and C). For the first 5 tasks,',\n",
       " 'FlashFill, FlashFill++, and FlashFill++ with Codex (anonymized as A, B, and C). For the first 5 tasks,\\nthe participants were asked (on a 7-point Likert scale) the extent to which they agreed with the\\nstatements ‚ÄúFlashFill++ is more readable than FlashFill‚Äù and ‚ÄúFlashFill++ with Codex is more readable\\nthan FlashFill++‚Äù. For the last 5 tasks, the participants answered (on a 7-point Likert scale) the extent\\nto which they agreed with the statement ‚ÄúX is similar to the code I write‚Äù, where X was replaced\\nwith the corresponding (anonymized) system name.\\nFigure 10a shows that participants found code generated by FlashFill++ (without identifier renam-\\ning) was more readable than code generated by FlashFill for the same task. Adding Codex-based\\nrenaming further improved readability with most participants at least somewhat agreeing.\\nFigure 10b shows that participants strongly disagreed that FlashFill code is similar to the code',\n",
       " 'renaming further improved readability with most participants at least somewhat agreeing.\\nFigure 10b shows that participants strongly disagreed that FlashFill code is similar to the code\\nthey write. In contrast, most participants at least somewhat agreed that FlashFill++ code is similar\\nto the code they write. Adding identifier renaming resulted in improvements, across all five tasks.\\nWe also provided an open-ended text box for additional feedback with each task. Here are some\\nillustrative excerpts, where we have replaced the anonymized system names (A,B,C) with meaningful\\ncounterparts: FlashFill: ‚Äúis a mess‚Äù, FlashFill++: ‚Äúvery readable‚Äù, FlashFill++ +Codex: \"parameter name is\\nmore self describing‚Äù; ‚ÄúFlashFill is just confusing while FlashFill++ and/or FlashFill++ with Codex are\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:24\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nD3 D2 D1 N S1 S2 S3\\nResponse\\n0\\n50\\n100\\n150\\n200\\n# of Participants\\ncomparison\\nFF++ > FF\\nFF++ w/Codex > FF++\\n(a) Statement: X is more readable than Y (denoted\\nas ùëã> ùëå). Most participants found FlashFill++\\nbetter than FlashFill. Adding Codex further im-\\nproved it.\\nD3 D2 D1 N S1 S2 S3\\nResponse\\n0\\n50\\n100\\n150\\n# of Participants\\nApproach\\nFF\\nFF++\\nFF++ w/Codex\\n(b) Statement: X is similar to the code I write. Par-\\nticipants did not find FlashFill similar. FlashFill++\\nwas closer, but FlashFill++ +Codex most similar.\\nFig. 10. Survey: D3-S3 Strongly disagree/agree. FF=FlashFill, FF++=FlashFill++, FF++ w/Codex=FlashFill++\\nwith Codex.\\nquite simple and direct‚Äù; and ‚ÄúFlashFill is very badly written, and FlashFill++ with Codex‚Äôs parameter\\nname tells a much better story‚Äù.\\n7\\nDISCUSSION\\n7.1\\nRelated Work\\nCuts are closely related to the widely-studied concept of accelerations in program analysis. Accel-',\n",
       " 'name tells a much better story‚Äù.\\n7\\nDISCUSSION\\n7.1\\nRelated Work\\nCuts are closely related to the widely-studied concept of accelerations in program analysis. Accel-\\nerations are used to capture the effect (transitive closure) of multiple state transitions by a single\\n‚Äúmeta transition‚Äù [Finkel 1987; Karp and Miller 1969]. It has often been used to handle numerical\\nupdates in programs, especially when more classical abstract interpretation techniques either do\\nnot converge or become very imprecise [Bardin et al. 2008; Boigelot 2003; Jeannet et al. 2014].\\nOur use of cuts inherits its motivation and purpose from these works, but applies them to PBE.\\nWhereas in program analysis, accelerations had success mostly on numerical domains, in PBE we\\nfind cuts helpful more generally. Currently, we assume cuts are provided by the DSL designer, but\\nautomatically generating them remains an interesting topic for future research.',\n",
       " 'find cuts helpful more generally. Currently, we assume cuts are provided by the DSL designer, but\\nautomatically generating them remains an interesting topic for future research.\\nIn PBE, cuts help speed-up search (by guiding top-down propagation across non-EI operators\\nbased on abstracting behavior of inner sub-DSLs). Other ways to speed-up search include using\\ntypes and other forms of abstractions [Guo et al. 2020; Osera and Zdancewic 2015; Wang et al.\\n2018], or combining search strategies [Lee 2021]. The difference between cuts and abstraction-based\\nmethods in synthesis is the same as the difference between accelerations and abstraction in program\\nanalysis. We need cuts for only some operators, whereas abstract transformers are required for\\nall operators. Moreover, the methods are not incompatible ‚Äì a promising direction would be to\\ncombine them.\\nMiddle-out synthesis, enabled by cuts, is a new way to combine bottom-up [Alur et al. 2013,',\n",
       " 'all operators. Moreover, the methods are not incompatible ‚Äì a promising direction would be to\\ncombine them.\\nMiddle-out synthesis, enabled by cuts, is a new way to combine bottom-up [Alur et al. 2013,\\n2017] and top-down [Gulwani 2011; Polozov and Gulwani 2015] synthesis. It is very different from\\nthe meet-in-the-middle way of combining them where search starts simultaneously from the bottom\\n(enumerating subprograms that generate new values) and from the top (back propagating the\\noutput) until we find values that connect the two [Gulwani et al. 2011; Lee 2021]. Helped by the\\njump provided by cuts, middle-out synthesis starts at the middle creating two subproblems that can\\nbe solved using either approach. Meet-in-the-middle approach in [Lee 2021] guides the top-down\\nsearch based on the values propagated by bottom-up, similar to middle-out synthesis; however, our\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:25\\ncuts are more general and can handle more scenarios because they overcome issues (not effectively\\nenumerable operators and large number of constants) that may make partial bottom-up fail.\\nMiddle-out synthesis can be viewed as a divide-and-conquer strategy for synthesis. The coopera-\\ntive synthesis framework in [Huang et al. 2020] proposes 3 such strategies that are used when the\\noriginal synthesis problem remains unsolved. However, [Huang et al. 2020] works on complete\\nlogical specifications, and not on input-output examples.\\nAt a very abstract level, top-down synthesis and middle-out synthesis can both be viewed as\\nan approach that synthesizes a sketch and then fills the sketch in a PBE setting [Feng et al. 2017;\\nPolikarpova et al. 2016; Wang et al. 2017, 2020]. In this context, Scythe [Wang et al. 2017] uses\\noverapproximation of the set of values that are computed by partial programs to synthesize a sketch.',\n",
       " 'Polikarpova et al. 2016; Wang et al. 2017, 2020]. In this context, Scythe [Wang et al. 2017] uses\\noverapproximation of the set of values that are computed by partial programs to synthesize a sketch.\\nUnlike our work, [Wang et al. 2017] is exclusively focused on bottom-up synthesis. Since [Wang\\net al. 2017] is bottom-up, its approximations get coarser as the program gets deeper. Scythe, tends\\nto do well for shallow programs. FlashFill++‚Äôs use of cuts is more ‚Äúon-demand‚Äù and thus not affected\\nby depth of programs. In fact, FlashFill++ can synthesize deep programs, with the largest solution in\\nour benchmark suite having a depth of 24, with over 10% of our benchmarks requiring programs\\nwith a depth of at least 10. Furthermore, [Wang et al. 2017] is exclusively focused on SQL ‚Äì its\\nmain contribution is an approach to overapproximate SQL queries that abstracts carefully selected\\nnonterminals. In contrast, our formalization is not fixed to any particular DSL, but relies on the',\n",
       " 'main contribution is an approach to overapproximate SQL queries that abstracts carefully selected\\nnonterminals. In contrast, our formalization is not fixed to any particular DSL, but relies on the\\nDSL designer to provide the cuts.\\nMorpheus [Feng et al. 2017] and Synquid [Polikarpova et al. 2016] overapproximate each com-\\nponent to prune partial programs to synthesize sketches that are subsequently filled. Morpheus\\nis specialized to tables and uses the distinction between value and table transformations. In con-\\ntrast, our framework is more general as it allows the use of approximations (cuts) for only certain\\nfunctions (wherever they are provided); however, we cannot (and do not) prune partial programs.\\nWe always work on concrete values ‚Äì there is no abstract domain involved. We do not use SMT\\nsolvers, whereas SMT solvers are a key component of [Feng et al. 2017; Polikarpova et al. 2016].\\nPrecedence and gDSLs. Precedences or priorities have been used in many grammar formalisms,',\n",
       " 'solvers, whereas SMT solvers are a key component of [Feng et al. 2017; Polikarpova et al. 2016].\\nPrecedence and gDSLs. Precedences or priorities have been used in many grammar formalisms,\\nbut mainly for achieving disambiguation while parsing in ambiguous grammars [Aasa 1995; Aho\\net al. 1973; Earley 1974; Heering et al. 1989]. Disambiguation here refers to preferring the parse\\nùëé+(ùëè‚àóùëê) over (ùëé+ùëè)‚àóùëêfor the same string ùëé+ùëè‚àóùëê. In contrast, we use gDSLs to compare derivations\\nof different strings (programs). Furthermore, in the work on filters and SDF [Heering et al. 1989],\\nthe semantics of the precedence relation ‚âªon rules is different: there ùëÜ1 ‚Üíùë§1 ‚âªùëÜ2 ‚Üíùë§2 means\\nthat one can not use ùëÜ2 ‚Üíùë§2 as a child of ùëÜ1 ‚Üíùë§1 in a parse tree [van den Brand et al. 2002]. In\\nour case, we disallow precedence on rules with different left-hand nonterminals. Nevertheless, our\\nprecedence can be viewed as a specialized filter in the terminology of [van den Brand et al. 2002].',\n",
       " 'our case, we disallow precedence on rules with different left-hand nonterminals. Nevertheless, our\\nprecedence can be viewed as a specialized filter in the terminology of [van den Brand et al. 2002].\\nFarzan and Nicolet [Farzan and Nicolet 2021] use a sub-grammar to optimize search. This can be\\nmodeled using our precedence operator. They use constraint-based synthesis (using SMT solvers)\\nwhere the interest is in any one correct program. Ranking is not of interest there, whereas we use\\nprecedence in the context of top-down synthesis where we synthesize program sets and rank them.\\nThe interaction of precedence in the grammar and the program ranking is one of our contributions.\\nCasper [Ahmad and Cheung 2018] uses a hierarchy of grammars growing in size for synthesis,\\nmaking search efficient. This hierarchy is dynamically generated - guided by the input-output\\nexample. Our precedence-based approach provides a different mechanism to constrain search. The',\n",
       " 'making search efficient. This hierarchy is dynamically generated - guided by the input-output\\nexample. Our precedence-based approach provides a different mechanism to constrain search. The\\nvalue of our approach is that it is easily integrated with the underlying synthesis framework, giving\\nsynthesis-engine-builders more flexibility in controlling search and ranking. Precedence is also\\nintuitive for DSL designers because they must think only locally to decide if the operators need a\\nprecedence relation. Technically speaking, our notion of precedence implicitly represents a lattice\\nof grammars rather than a strict linear hierarchy.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:26\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nEarlier work on ‚ÄúParsing expression grammars‚Äù [Ford 2004] introduces grammars that are like\\nCFGs, but contain features such as prioritized choice (precedence), greedy repetitions, etc., but\\nit does so for parsing, whereas our focus is on top-down, bottom-up, and combination synthesis\\ntechniques. Our novelty is in supporting precedence in our synthesis framework, and formally\\nworking out how it impacts ranking and search.\\nUsing precedence is one way to handle potentially redundant operators in the DSL and prune\\nsearch space. The other way is to explicitly write the equivalence relation on programs and\\nonly consider programs that are canonical representatives of each equivalence class [Osera and\\nZdancewic 2015; Smith and Albarghouthi 2019; Udupa et al. 2013]. The gDSL approach is low\\noverhead, but may consider equivalent programs during search. However, this is by design as our',\n",
       " 'Zdancewic 2015; Smith and Albarghouthi 2019; Udupa et al. 2013]. The gDSL approach is low\\noverhead, but may consider equivalent programs during search. However, this is by design as our\\ngoal is to generate whatever program leads to most naturally readable code.\\nPrecedence of grammar rules can be viewed as a specialized case of probabilistic context-free\\ngrammars (pCFGs) that have been used to bias the enumeration of programs through their gram-\\nmar [Lee et al. 2018; Liang et al. 2010; Menon et al. 2013]. While specialized, precedence is actually\\npreferable in many scenarios where we want to synthesize not just any program that works on the\\ninput-output examples, but one that has other desirable properties, such as, the program generalizes\\nto unseen inputs and has a readable translation in target language. As such, precedence in gDSLs\\ngive designers of synthesizers a clean way to state their ranking preference. Weights in a pCFG',\n",
       " 'to unseen inputs and has a readable translation in target language. As such, precedence in gDSLs\\ngive designers of synthesizers a clean way to state their ranking preference. Weights in a pCFG\\nhave to be learned from data or manually set ‚Äì both are daunting tasks compared to writing a gDSL.\\nThe contrast between pCFGs and gDSLs is akin to that between neural and symbolic approaches.\\nFlashFill [Gulwani 2011] demonstrated the effectiveness of inductive synthesis at tackling\\ncomplex string transformation. FlashMeta [Polozov and Gulwani 2015] recognized that many\\npopular inductive synthesizers [Gulwani 2011; Le and Gulwani 2014] could be decomposed into\\ndomain-specific features, such as the DSL operators and their semantics, and general (shareable)\\ndeductive steps. We used the FlashMeta framework to implement FlashFill, based on the original\\npaper [Gulwani 2011], for our experiments. We also built FlashFill++ the same way, which extends',\n",
       " 'deductive steps. We used the FlashMeta framework to implement FlashFill, based on the original\\npaper [Gulwani 2011], for our experiments. We also built FlashFill++ the same way, which extends\\nthe capabilities of FlashFill to include new operations like date and number formatting, and also\\nfocuses on generating readable code.\\nIn recent work [Verbruggen et al. 2021], FlashFill has been combined with a pre-trained language\\nmodel (LM), GPT3, to facilitate semantic transformations, such as converting the city \"San Francisco\"\\nto the state \"CA\". Complementing FlashFill‚Äôs syntactic effectiveness with GPT3‚Äôs ability to do\\nsemantic transformation is interesting, but orthogonal to the problem here. However, we do exploit\\na LM to (optionally) generate meaningful variable names for our user study on code readability.\\nTrust and readability. Wrex [Drosos et al. 2020] argues that readable code is indispensable',\n",
       " 'a LM to (optionally) generate meaningful variable names for our user study on code readability.\\nTrust and readability. Wrex [Drosos et al. 2020] argues that readable code is indispensable\\nfor users of synthesis technology. Wrex employed hand-written rewrite rules to produce readable\\ncode for their user study. However, this is not an approach that easily extends to all scenarios and\\nlarger languages. FlashFill++ is inspired by Wrex [Drosos et al. 2020] to address the readable code\\ngeneration challenge in a more general and scalable way: redesigning the DSL used with a focus\\non enabling readable code generation, rather than post-processing.\\nZhang et al [Zhang et al. 2021, 2020] introduced a system for interpretable synthesis, where the\\nuser interacts with the synthesizer. This approach is complementary to FlashFill++.\\n7.2\\nLimitations\\nThe concepts of cuts and precedence have been developed exclusively for synthesis approaches',\n",
       " 'user interacts with the synthesizer. This approach is complementary to FlashFill++.\\n7.2\\nLimitations\\nThe concepts of cuts and precedence have been developed exclusively for synthesis approaches\\nbased on concrete values, so-called version space algebra (VSA) based methods, in this paper. The\\nterm top-down, respectively bottom-up, has been used for techniques that are based on propagation\\nof concrete output (respectively, input) values through partial sketches (respectively, programs)\\ntypically represented using VSAs. In systems that represent approximations of the sets of values\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:27\\n(at each component) using abstractions or refinement types (e.g., Synquid, Morpheus, etc), cuts\\ncan potentially address the issue of abstractions becoming too coarse as they are composed from\\nabstractions of sub-components. This is similar to how interpolants can be used to compute tight\\ninvariants in program verification when successive application of abstract transformers lead to\\noverly general invariants. Studying broader implications of these ideas is left for future work.\\nNontrivial cuts that go beyond bottom-up or top-down approaches can only be computed if there\\nare nontrivial invariants that hold (about the values that are generated) at certain intermediate\\nnonterminals in the grammar. Moreover, the DSL designer must be aware of these invariants. The\\nDSL designer can choose to write cuts that are incomplete in theory, but reasonable in practice,',\n",
       " 'nonterminals in the grammar. Moreover, the DSL designer must be aware of these invariants. The\\nDSL designer can choose to write cuts that are incomplete in theory, but reasonable in practice,\\nguided by their understanding of the domain. Nontrivial cuts are likely to exist in large DSLs where\\ndifferent forms of values flow on different paths. We have not done a formal study of how difficult\\nit is for a DSL designer to write useful cut functions‚Äîthis is beyond the scope of the current paper\\nwhich just lays down the foundations for cuts.\\nDesigning a guarded DSL requires the DSL designer to have clear preference for certain operators\\nover other alternatives, and moreover, any precedence on operators closer to the start symbol (in the\\ngrammar) should override any precedence on operators closer to the leaves of the program tree. The\\n‚Äúhigher-up‚Äù operators in any DSL typically establish the high-level tactic of the program, and hence',\n",
       " 'grammar) should override any precedence on operators closer to the leaves of the program tree. The\\n‚Äúhigher-up‚Äù operators in any DSL typically establish the high-level tactic of the program, and hence\\nthis requirement often holds. Precedences in a grammar are likely to exist if it contains redundant\\noperators that are some preferred compositions of other low-level operators in the grammar. Further,\\nthere are certain technical assumptions we make about precedences. The assumption that the\\nprecedence is a series-parallel partial order is not required if we start with the gDSL (rather than\\nstart with precedences), which is what we do in practice. The assumption that the reachability\\nrelation on the grammar nonterminals be acyclic is required only to provide a clean mathematical\\ninterpretation of the ranking induced by gDSL on programs (terms) as a path ordering. In the\\npresence of cycles, the synthesis rule and the full system can still be used without problems, but',\n",
       " 'interpretation of the ranking induced by gDSL on programs (terms) as a path ordering. In the\\npresence of cycles, the synthesis rule and the full system can still be used without problems, but\\nranking cannot be described in a simple way.\\n8\\nCONCLUSION\\nWe introduced two techniques, cuts and precedence through guarded DSLs, that DSL designers can\\nuse to prune search in programming by example. Cuts enable a novel synthesis strategy: middle-\\nout synthesis. This strategy allows FlashFill++ to support synthesis tasks that require non-EI/EE\\noperators, such as datetime and numeric transformations. The use of precedence through gDSLs\\nallows us to increase the size of our DSL, by incorporating redundant operators, which facilitate\\nreadable code generation in different target languages. We compare our tool to existing state-of-\\nthe-art PBE systems, FlashFill, Duet, and SmartFill, on three public benchmark datasets and show',\n",
       " 'readable code generation in different target languages. We compare our tool to existing state-of-\\nthe-art PBE systems, FlashFill, Duet, and SmartFill, on three public benchmark datasets and show\\nthat FlashFill++ can solve more tasks, in less time, and without substantially increasing the number\\nof examples required. We also perform a survey-based study on code readability, confirming that\\nthe programs synthesized by FlashFill++ are more readable than those generated by FlashFill.\\nREFERENCES\\nAnnika Aasa. 1995. Precedences in specifications and implementations of programming languages. Theoretical Computer\\nScience 142, 1 (1995), 3‚Äì26.\\nMaaz Bin Safeer Ahmad and Alvin Cheung. 2018. Automatically Leveraging MapReduce Frameworks for Data-Intensive\\nApplications. In Proc. 2018 International Conference on Management of Data, SIGMOD Conference. ACM, 1205‚Äì1220.\\nhttps://doi.org/10.1145/3183713.3196891',\n",
       " 'Applications. In Proc. 2018 International Conference on Management of Data, SIGMOD Conference. ACM, 1205‚Äì1220.\\nhttps://doi.org/10.1145/3183713.3196891\\nAlfred Aho, S. Johnson, and Jeffrey Ullman. 1973. Deterministic parsing of ambiguous grammars. Commun. ACM 18 (01\\n1973), 441‚Äì452. https://doi.org/10.1145/360933.360969\\nRajeev Alur, Rastislav Bod√≠k, Garvit Juniwal, Milo M. K. Martin, Mukund Raghothaman, Sanjit A. Seshia, Rishabh Singh,\\nArmando Solar-Lezama, Emina Torlak, and Abhishek Udupa. 2013. Syntax-Guided Synthesis. In Formal Methods in\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:28\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nComputer-Aided Design, FMCAD 2013. 1‚Äì8.\\nRajeev Alur, Arjun Radhakrishna, and Abhishek Udupa. 2017. Scaling Enumerative Program Synthesis via Divide and\\nConquer. In TACAS. 319‚Äì336.\\nS√©bastien Bardin, Alain Finkel, J√©r√¥me Leroux, and Laure Petrucci. 2008. FAST: acceleration from theory to practice. Int. J.\\nSoftw. Tools Technol. Transf. 10, 5 (2008), 401‚Äì424. https://doi.org/10.1007/s10009-008-0064-3\\nDenis B√©chet, Philippe de Groote, and Christian Retor√©. 1997. A Complete Axiomatisation for the Inclusion of Series-Parallel\\nPartial Orders. In Rewriting Techniques and Applications, 8th Int. Conf., RTA-97 (Lecture Notes in Computer Science,\\nVol. 1232). Springer, 230‚Äì240. https://doi.org/10.1007/3-540-62950-5_74\\nBernard Boigelot. 2003. On iterating linear transformations over recognizable sets of integers. Theor. Comput. Sci. 309, 1-3\\n(2003), 413‚Äì468. https://doi.org/10.1016/S0304-3975(03)00314-1',\n",
       " 'Bernard Boigelot. 2003. On iterating linear transformations over recognizable sets of integers. Theor. Comput. Sci. 309, 1-3\\n(2003), 413‚Äì468. https://doi.org/10.1016/S0304-3975(03)00314-1\\nSwarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, and Yisong Yue. 2021. Neu-\\nrosymbolic Programming. Found. Trends Program. Lang. 7, 3 (2021), 158‚Äì243.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards,\\nYuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser,\\nMohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,',\n",
       " 'Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,\\nIgor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua\\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter\\nWelinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021b. Evaluating Large\\nLanguage Models Trained on Code. CoRR abs/2107.03374 (2021). arXiv:2107.03374 https://arxiv.org/abs/2107.03374\\nXinyun Chen, Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, and Denny Zhou. 2021a. Spreadsheet-\\nCoder: Formula Prediction from Semi-structured Context. In Proceedings of the 38th International Conference on Machine\\nLearning (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 1661‚Äì1672.\\nhttps://proceedings.mlr.press/v139/chen21m.html',\n",
       " 'Learning (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 1661‚Äì1672.\\nhttps://proceedings.mlr.press/v139/chen21m.html\\nAndrew Cropper. 2019. Playgol: Learning Programs Through Play. In Proceedings of the Twenty-Eighth International Joint\\nConference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, Sarit Kraus (Ed.). ijcai.org, 6074‚Äì6080.\\nhttps://doi.org/10.24963/ijcai.2019/841 https://github.com/andrewcropper/ijcai19-playgol.\\nNachum Dershowitz and Jean-Pierre Jouannaud. 1990. Rewrite Systems. In Handbook of Theoretical Computer Science,\\nVolume B: Formal Models and Semantics. Elsevier and MIT Press, 243‚Äì320.\\nJacob Devlin, Rudy Bunel, Rishabh Singh, Matthew J. Hausknecht, and Pushmeet Kohli. 2017. Neural Program Meta-Induction.\\nIn NIPS, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and\\nRoman Garnett (Eds.). 2080‚Äì2088.',\n",
       " 'In NIPS, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and\\nRoman Garnett (Eds.). 2080‚Äì2088.\\nIan Drosos, Titus Barik, Philip J. Guo, Robert DeLine, and Sumit Gulwani. 2020. Wrex: A Unified Programming-by-Example\\nInteraction for Synthesizing Readable Code for Data Scientists. In Proceedings of the 2020 CHI Conference on Human\\nFactors in Computing Systems (Honolulu, HI, USA) (CHI ‚Äô20). Association for Computing Machinery, New York, NY, USA,\\n1‚Äì12. https://doi.org/10.1145/3313831.3376442\\nJay Earley. 1974. Ambiguity and Precedence in Syntax Description. Acta Informatica 4 (1974), 183‚Äì192.\\nAzadeh Farzan and Victor Nicolet. 2021. Phased synthesis of divide and conquer programs. In PLDI ‚Äô21: 42nd ACM SIGPLAN\\nInternational Conference on Programming Language Design and Implementation. ACM, 974‚Äì986. https://doi.org/10.1145/\\n3453483.3454089',\n",
       " 'International Conference on Programming Language Design and Implementation. ACM, 974‚Äì986. https://doi.org/10.1145/\\n3453483.3454089\\nYu Feng, Ruben Martins, Jacob Van Geffen, Isil Dillig, and Swarat Chaudhuri. 2017. Component-based synthesis of table\\nconsolidation and transformation tasks from examples. In Proc. 38th ACM SIGPLAN Conference on Programming Language\\nDesign and Implementation, PLDI. ACM, 422‚Äì436. https://doi.org/10.1145/3062341.3062351\\nAlain Finkel. 1987. A Generalization of the Procedure of Karp and Miller to Well Structured Transition Systems. In Proc.\\n14th Intl. Colloquium on Automata, Languages and Programming, ICALP87 (Lecture Notes in Computer Science, Vol. 267),\\nThomas Ottmann (Ed.). Springer, 499‚Äì508. https://doi.org/10.1007/3-540-18088-5_43\\nBryan Ford. 2004. Parsing expression grammars: a recognition-based syntactic foundation. In Proc. 31st ACM SIGPLAN-',\n",
       " 'Thomas Ottmann (Ed.). Springer, 499‚Äì508. https://doi.org/10.1007/3-540-18088-5_43\\nBryan Ford. 2004. Parsing expression grammars: a recognition-based syntactic foundation. In Proc. 31st ACM SIGPLAN-\\nSIGACT Symposium on Principles of Programming Languages, POPL. ACM, 111‚Äì122. https://doi.org/10.1145/964001.964011\\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained Language Models Better Few-shot Learners. In\\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\\nConference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, Online,\\n3816‚Äì3830. https://doi.org/10.18653/v1/2021.acl-long.295\\nGoogle. 2021. SpreadSheetCoder. https://github.com/google-research/google-research/tree/master/spreadsheet_coder\\nSumit Gulwani. 2011. Automating string processing in spreadsheets using input-output examples. In POPL. 317‚Äì330.',\n",
       " 'Sumit Gulwani. 2011. Automating string processing in spreadsheets using input-output examples. In POPL. 317‚Äì330.\\nSumit Gulwani. 2016. Programming by Examples - and its applications in Data Wrangling. In Dependable Software Systems\\nEngineering. 137‚Äì158.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'FlashFill++: Scaling PBE by Cutting to the Chase\\n33:29\\nSumit Gulwani, William R. Harris, and Rishabh Singh. 2012. Spreadsheet data manipulation using examples. Commun.\\nACM 55, 8 (2012), 97‚Äì105.\\nS. Gulwani, V. Korthikanti, and A. Tiwari. 2011. Synthesizing geometry constructions. In Proc. ACM Conf. on Prgm. Lang.\\nDesgn. and Impl. PLDI. 50‚Äì61.\\nSumit Gulwani, Oleksandr Polozov, and Rishabh Singh. 2017. Program Synthesis. Foundations and Trends in Programming\\nLanguages 4, 1-2 (2017), 1‚Äì119.\\nZheng Guo, Michael James, David Justo, Jiaxiao Zhou, Ziteng Wang, Ranjit Jhala, and Nadia Polikarpova. 2020. Program\\nsynthesis by type-guided abstraction refinement. Proc. ACM Program. Lang. 4, POPL (2020), 12:1‚Äì12:28. https://doi.org/\\n10.1145/3371080\\nJan Heering, P. R. H. Hendriks, Paul Klint, and J. Rekers. 1989. The syntax definition formalism SDF - reference manual.\\nACM SIGPLAN Notices 24, 11 (1989), 43‚Äì75.',\n",
       " '10.1145/3371080\\nJan Heering, P. R. H. Hendriks, Paul Klint, and J. Rekers. 1989. The syntax definition formalism SDF - reference manual.\\nACM SIGPLAN Notices 24, 11 (1989), 43‚Äì75.\\nKangjing Huang, Xiaokang Qiu, Peiyuan Shen, and Yanjun Wang. 2020. Reconciling enumerative and deductive program\\nsynthesis. In Proc. 41st ACM SIGPLAN Intl. Conf. on Programming Language Design and Implementation, PLDI, Alastair F.\\nDonaldson and Emina Torlak (Eds.). ACM, 1159‚Äì1174. https://doi.org/10.1145/3385412.3386027\\nBertrand Jeannet, Peter Schrammel, and Sriram Sankaranarayanan. 2014. Abstract acceleration of general linear loops.\\nIn The 41st Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL. ACM, 529‚Äì540.\\nhttps://doi.org/10.1145/2535838.2535843\\nAshwin Kalyan, Abhishek Mohta, Alex Polozov, Dhruv Batra, Prateek Jain, and Sumit Gulwani. 2018. Neural-Guided\\nDeductive Search for Real-Time Program Synthesis from Examples. In 6th International Conference on Learning Repre-',\n",
       " 'Deductive Search for Real-Time Program Synthesis from Examples. In 6th International Conference on Learning Repre-\\nsentations (ICLR) (6th international conference on learning representations (iclr) ed.). https://www.microsoft.com/en-\\nus/research/publication/neural-guided-deductive-search-real-time-program-synthesis-examples/\\nRichard M. Karp and Raymond E. Miller. 1969. Parallel Program Schemata. J. Comput. Syst. Sci. 3, 2 (1969), 147‚Äì195.\\nhttps://doi.org/10.1016/S0022-0000(69)80011-5\\nVu Le and Sumit Gulwani. 2014. FlashExtract: A Framework for Data Extraction by Examples. In PLDI. 542‚Äì553.\\nWoosuk Lee. 2021. Combining the top-down propagation and bottom-up enumeration for inductive program synthesis.\\nProc. ACM Program. Lang. 5, POPL (2021), 1‚Äì28. https://doi.org/10.1145/3434335 https://github.com/wslee/duet.\\nWoosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. 2018. Accelerating search-based program synthesis using',\n",
       " 'Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. 2018. Accelerating search-based program synthesis using\\nlearned probabilistic models. In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and\\nImplementation, PLDI, Jeffrey S. Foster and Dan Grossman (Eds.). ACM, 436‚Äì449. https://doi.org/10.1145/3192366.3192410\\nPercy Liang, Michael I. Jordan, and Dan Klein. 2010. Learning Programs: A Hierarchical Bayesian Approach. In Proceedings\\nof the 27th International Conference on Machine Learning (ICML-10), Johannes F√ºrnkranz and Thorsten Joachims (Eds.).\\nOmnipress, 639‚Äì646.\\nDylan Lukes, John Sarracino, Cora Coleman, Hila Peleg, Sorin Lerner, and Nadia Polikarpova. 2021. Synthesis of web\\nlayouts from examples. In ESEC/FSE ‚Äô21: 29th ACM Joint European Software Engineering Conference and Symposium on the\\nFoundations of Software Engineering, Athens, Greece, August 23-28, 2021, Diomidis Spinellis, Georgios Gousios, Marsha',\n",
       " 'Foundations of Software Engineering, Athens, Greece, August 23-28, 2021, Diomidis Spinellis, Georgios Gousios, Marsha\\nChechik, and Massimiliano Di Penta (Eds.). ACM, 651‚Äì663.\\nAditya Krishna Menon, Omer Tamuz, Sumit Gulwani, Butler W. Lampson, and Adam Kalai. 2013. A Machine Learning\\nFramework for Programming by Example. In Proceedings of the 30th International Conference on Machine Learning, ICML\\n(JMLR Workshop and Conference Proceedings, Vol. 28). JMLR.org, 187‚Äì195. http://proceedings.mlr.press/v28/menon13.html\\nAnders Miltner, Kathleen Fisher, Benjamin C. Pierce, David Walker, and Steve Zdancewic. 2018. Synthesizing bijective\\nlenses. Proc. ACM Program. Lang. 2, POPL (2018), 1:1‚Äì1:30.\\nNagarajan Natarajan, Danny Simmons, Naren Datha, Prateek Jain, and Sumit Gulwani. 2019. Learning Natural Programs\\nfrom a Few Examples in Real-Time. In AIStats. https://www.microsoft.com/en-us/research/publication/learning-natural-\\nprograms-from-a-few-examples-in-real-time/',\n",
       " 'from a Few Examples in Real-Time. In AIStats. https://www.microsoft.com/en-us/research/publication/learning-natural-\\nprograms-from-a-few-examples-in-real-time/\\nPeter-Michael Osera and Steve Zdancewic. 2015. Type-and-example-directed program synthesis. In Proc. 36th ACM SIGPLAN\\nConf. on Programming Language Design and Implementation. ACM, 619‚Äì630. https://doi.org/10.1145/2737924.2738007\\nRangeet Pan, Vu Le, Nachiappan Nagappan, Sumit Gulwani, Shuvendu K. Lahiri, and Mike Kaufman. 2021. Can Program\\nSynthesis be Used to Learn Merge Conflict Resolutions? An Empirical Analysis. In 43rd IEEE/ACM International Conference\\non Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021. IEEE, 785‚Äì796.\\nNadia Polikarpova, Ivan Kuraj, and Armando Solar-Lezama. 2016. Program synthesis from polymorphic refinement\\ntypes. In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation,',\n",
       " 'types. In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation,\\nPLDI 2016, Santa Barbara, CA, USA, June 13-17, 2016, Chandra Krintz and Emery D. Berger (Eds.). ACM, 522‚Äì538.\\nhttps://doi.org/10.1145/2908080.2908093\\nOleksandr Polozov and Sumit Gulwani. 2015. FlashMeta: A Framework for Inductive Program synthesis. In OOPSLA/SPLASH.\\n107‚Äì126.\\nPowerFx 2021. PowerFx: The low code programming language. https://powerapps.microsoft.com/en-us/blog/introducing-\\nmicrosoft-power-fx-the-low-code-programming-language-for-everyone/. Accessed: 2021-11-19.\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " '33:30\\nJ. Cambronero, S. Gulwani, V. Le, D. Perelman, A. Radhakrishna, C. Simon, A. Tiwari\\nMicrosoft PROSE. 2022. PROSE public benchmark suite. https://github.com/microsoft/prose-benchmarks.\\nKia Rahmani, Mohammad Raza, Sumit Gulwani, Vu Le, Daniel Morris, Arjun Radhakrishna, Gustavo Soares, and Ashish\\nTiwari. 2021. Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis.\\nProc. ACM Program. Lang. 5, OOPSLA (2021), 1‚Äì29.\\nReudismam Rolim, Gustavo Soares, Loris D‚ÄôAntoni, Oleksandr Polozov, Sumit Gulwani, Rohit Gheyi, Ryo Suzuki, and Bj√∂rn\\nHartmann. 2017. Learning syntactic program transformations from examples. In ICSE. IEEE / ACM, 404‚Äì415.\\nSelenium. 2022. Selenium. https://github.com/SeleniumHQ/selenium\\nNischal Shrestha, Titus Barik, and Chris Parnin. 2018. It‚Äôs Like Python But: Towards Supporting Transfer of Programming\\nLanguage Knowledge. In 2018 IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC, J√°come',\n",
       " 'Language Knowledge. In 2018 IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC, J√°come\\nCunha, Jo√£o Paulo Fernandes, Caitlin Kelleher, Gregor Engels, and Jorge Mendes (Eds.). IEEE Computer Society, 177‚Äì185.\\nhttps://doi.org/10.1109/VLHCC.2018.8506508\\nRishabh Singh and Sumit Gulwani. 2015. Predicting a Correct Program in Programming by Example. In CAV. 398‚Äì414.\\nCalvin Smith and Aws Albarghouthi. 2019. Program Synthesis with Equivalence Reduction. In VMCAI, Constantin Enea\\nand Ruzica Piskac (Eds.).\\nAbhishek Udupa, Arun Raghavan, Jyotirmoy V. Deshmukh, Sela Mador-Haim, Milo M. K. Martin, and Rajeev Alur. 2013.\\nTRANSIT: specifying protocols with concolic snippets. In ACM SIGPLAN Conference on Programming Language Design\\nand Implementation, PLDI, Hans-Juergen Boehm and Cormac Flanagan (Eds.). ACM, 287‚Äì296. https://doi.org/10.1145/\\n2491956.2462174\\nMark van den Brand, Jeroen Scheerder, Jurgen J. Vinju, and Eelco Visser. 2002. Disambiguation Filters for Scannerless',\n",
       " '2491956.2462174\\nMark van den Brand, Jeroen Scheerder, Jurgen J. Vinju, and Eelco Visser. 2002. Disambiguation Filters for Scannerless\\nGeneralized LR Parsers. In Compiler Construction, 11th Intl. Conf, CC 2002, Part of ETAPS, Proceedings (Lecture Notes in\\nComputer Science, Vol. 2304). Springer, 143‚Äì158.\\nGust Verbruggen, Vu Le, and Sumit Gulwani. 2021. Semantic programming by example with pre-trained models. Proc. ACM\\nProgram. Lang. 5, OOPSLA (2021), 1‚Äì25. https://doi.org/10.1145/3485477\\nChenglong Wang, Alvin Cheung, and Rastislav Bod√≠k. 2017. Synthesizing highly expressive SQL queries from input-output\\nexamples. In Proc. 38th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI. ACM,\\n452‚Äì466. https://doi.org/10.1145/3062341.3062365\\nXinyu Wang, Isil Dillig, and Rishabh Singh. 2018. Program synthesis using abstraction refinement. Proc. ACM Program.\\nLang. 2, POPL (2018), 63:1‚Äì63:30. https://doi.org/10.1145/3158151',\n",
       " 'Xinyu Wang, Isil Dillig, and Rishabh Singh. 2018. Program synthesis using abstraction refinement. Proc. ACM Program.\\nLang. 2, POPL (2018), 63:1‚Äì63:30. https://doi.org/10.1145/3158151\\nYuepeng Wang, Rushi Shah, Abby Criswell, Rong Pan, and Isil Dillig. 2020. Data Migration using Datalog Program Synthesis.\\nProc. VLDB Endow. 13, 7 (2020), 1006‚Äì1019. https://doi.org/10.14778/3384345.3384350\\nTianyi Zhang, Zhiyang Chen, Yuanli Zhu, Priyan Vaithilingam, Xinyu Wang, and Elena L. Glassman. 2021. Interpretable\\nProgram Synthesis. Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3411764.3445646\\nTianyi Zhang, London Lowmanstone, Xinyu Wang, and Elena L. Glassman. 2020. Interactive Program Synthesis by\\nAugmented Examples. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology\\n(Virtual Event, USA) (UIST ‚Äô20). Association for Computing Machinery, New York, NY, USA, 627‚Äì648. https://doi.org/10.\\n1145/3379337.3415900',\n",
       " '(Virtual Event, USA) (UIST ‚Äô20). Association for Computing Machinery, New York, NY, USA, 627‚Äì648. https://doi.org/10.\\n1145/3379337.3415900\\nReceived 2022-07-07; accepted 2022-11-07\\nProc. ACM Program. Lang., Vol. 7, No. POPL, Article 33. Publication date: January 2023.',\n",
       " 'Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nContents lists available at ScienceDirect \\nComputer Languages, Systems & Structures \\njournal homepage: www.elsevier.com/locate/cl \\nSystematic mapping study of template-based code generation \\nEugene Syriani ‚àó, Lechanceux Luhunu , Houari Sahraoui \\nDepartment of computer science and operations research (DIRO), University of Montreal, Montreal, Quebec, Canada \\na r t i c l e \\ni n f o \\nArticle history: \\nReceived 18 August 2017 \\nRevised 24 November 2017 \\nAccepted 30 November 2017 \\nAvailable online 7 December 2017 \\nKeywords: \\nCode generation \\nSystematic mapping study \\nModel-driven engineering \\na b s t r a c t \\nContext: Template-based code generation (TBCG) is a synthesis technique that produces \\ncode from high-level speciÔ¨Åcations, called templates. TBCG is a popular technique in \\nmodel-driven engineering (MDE) given that they both emphasize abstraction and automa-',\n",
       " 'code from high-level speciÔ¨Åcations, called templates. TBCG is a popular technique in \\nmodel-driven engineering (MDE) given that they both emphasize abstraction and automa- \\ntion. Given the diversity of tools and approaches, it is necessary to classify existing TBCG \\ntechniques to better guide developers in their choices. \\nObjective: The goal of this article is to better understand the characteristics of TBCG tech- \\nniques and associated tools, identify research trends, and assess the importance of the role \\nof MDE in this code synthesis approach. \\nMethod: We survey the literature to paint an interesting picture about the trends and uses \\nof TBCG in research. To this end, we follow a systematic mapping study process. \\nResults: Our study shows, among other observations, that the research community has \\nbeen diversely using TBCG over the past 16 years. An important observation is that TBCG \\nhas greatly beneÔ¨Åted from MDE. It has favored a template style that is output-based and',\n",
       " 'been diversely using TBCG over the past 16 years. An important observation is that TBCG \\nhas greatly beneÔ¨Åted from MDE. It has favored a template style that is output-based and \\nhigh-level modeling languages as input. TBCG is mainly used to generate source code and \\nhas been applied to many domains. \\nConclusion: TBCG is now a mature technique and much research work is still conducted \\nin this area. However, some issues remain to be addressed, such as support for template \\ndeÔ¨Ånition and assessment of the correctness and quality of the generated code. \\n¬© 2017 Elsevier Ltd. All rights reserved. \\n1. Introduction \\nCode generation has been around since the 1950s, taking its origin in early compilers [1] . Since then, software organi- \\nzations have been relying on code synthesis techniques in order to reduce development time and increase productivity [2] . \\nAutomatically generating code is an approach where the same generator can be reused to produce many different artifacts',\n",
       " 'Automatically generating code is an approach where the same generator can be reused to produce many different artifacts \\naccording to the varying inputs it receives. It also helps detecting errors in the input artifact early on before the generated \\ncode is compiled, when the output is source code. \\nThere are many techniques to generate code, such as programmatically [3] , using a meta-object protocol [4] , or aspect- \\noriented programming [5] . Since the mid-1990‚Äôs, template-based code generation (TBCG) emerged as an approach requiring \\nless effort for the programmers to develop code generators. Templates favor reuse following the principle of write once, \\nproduce many . The concept was heavily used in web designer software (such as Dreamweaver) to generate web pages \\nand Computer Aided Software Engineering (CASE) tools to generate source code from UML diagrams. Many development \\n‚àóCorresponding author.',\n",
       " 'and Computer Aided Software Engineering (CASE) tools to generate source code from UML diagrams. Many development \\n‚àóCorresponding author. \\nE-mail addresses: syriani@iro.umontreal.ca (E. Syriani), luhunukl@iro.umontreal.ca (L. Luhunu), sahraoui@iro.umontreal.ca (H. Sahraoui). \\nhttps://doi.org/10.1016/j.cl.2017.11.003 \\n1477-8424/¬© 2017 Elsevier Ltd. All rights reserved.',\n",
       " '44 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nenvironments started to include a template mechanism in their framework such as Microsoft Text Template Transformation \\nToolkit (T4) 1 for .NET and Velocity 2 for Apache. \\nModel-driven engineering (MDE) has advocated the use of model-to-text transformations as a core component of its \\nparadigm [6] . TBCG is a popular technique in MDE given that they both emphasize abstraction and automation. MDE tools, \\nsuch as Acceleo 3 and Xpand 4 , allow developers to generate code from high-level models without worrying on how to parse \\nand traverse input models. We can Ô¨Ånd today TBCG applied in a plethora of computer science and engineering research. \\nThe software engineering research community has focused essentially on primary studies proposing new TBCG tech- \\nniques, tools and applications. However, to the best of our knowledge, there is no classiÔ¨Åcation, characterization, or',\n",
       " 'niques, tools and applications. However, to the best of our knowledge, there is no classiÔ¨Åcation, characterization, or \\nassessment of these studies available yet. Therefore, in this paper, we conducted a systematic mapping study (SMS) of \\nthe literature in order to understand the trends, identify the characteristics of TBCG, assess the popularity of existing tools \\nwithin the research community, and determine the inÔ¨Çuence that MDE has had on TBCG. We are interested in various \\nfacets of TBCG, such as characterizing the templates, the inputs, and outputs, along with the evolution of the amount of \\npublications using TBCG over the past 16 years. \\nThe remainder of this paper is organized as follows. In Section 2 , we introduce the necessary background on TBCG \\nand discuss related work. In Section 3 , we elaborate on the methodology we followed for this SMS, and on the results of',\n",
       " 'and discuss related work. In Section 3 , we elaborate on the methodology we followed for this SMS, and on the results of \\nthe paper selection phase. The following sections report the results: the trends and evolution of TBCG in Section 4 , the \\ncharacteristics of TBCG according to our classiÔ¨Åcation scheme in Section 5 , the relationships between the different facets \\nin Section 6 , how TBCG tools have been used in primary studies in Section 7 , and the relation between MDE and TBCG \\nin Section 8 . In Section 9 , we answer our research questions and discuss limitations of the study. Finally, we conclude in \\nSection 10 . \\n2. Background and related work \\nWe brieÔ¨Çy explain TBCG in the context of MDE and discuss related work on secondary studies about code generation. \\n2.1. Code generation \\nIn this paper, we view code generation as in automatic programming [1] rather than compilers. The underlying principle',\n",
       " '2.1. Code generation \\nIn this paper, we view code generation as in automatic programming [1] rather than compilers. The underlying principle \\nof automatic programming is that a user deÔ¨Ånes what he expects from the program and the program should be automat- \\nically generated by a software without any assistance by the user. This generative approach is different from a compiler \\napproach. Compilers produce code executable by a computer from a speciÔ¨Åcation conforming to a programming language, \\nwhereas automatic programming transforms user speciÔ¨Åcations into code which often conforms to a programming language. \\nCompilers have a phase called code generation that retrieves an abstract syntax tree produced by a parser and translates it \\ninto machine code or bytecode executable by a virtual machine. Compared to code generation as in automatic programming, \\ncompilers can be regarded as tasks or services that are incorporated in or post-positioned to code generators [7] .',\n",
       " 'compilers can be regarded as tasks or services that are incorporated in or post-positioned to code generators [7] . \\nCode generation is an important model-to-text transformation that ensures the automatic transformation of a model into \\ncode. Organization are adopting the use of code generation since it reduces the development process time and increases \\nthe productivity. Generating the code using the most appropriate technique is even more crucial since it is the key to \\nbeneÔ¨Åt from all the advantages code synthesis offers to an organization. Nowadays, TBCG has raised to be the most popular \\nsynthesis technique available. Using templates can quickly become a complex task especially when the model should satisfy \\na certain condition before a template fragment is executed. \\nAs Balzer [8] states, there are many advantages to code generation. The effort of the user is reduced as he has fewer',\n",
       " 'a certain condition before a template fragment is executed. \\nAs Balzer [8] states, there are many advantages to code generation. The effort of the user is reduced as he has fewer \\nlines to write: speciÔ¨Åcations are shorter than the program that implements them. SpeciÔ¨Åcations are easier to write and \\nto understand for a user, given that they are closer to the application and domain concepts. Writing speciÔ¨Åcations is less \\nerror-prone than writing the program directly, since the expert is the one who writes the speciÔ¨Åcation rather than another \\nprogrammer. \\nThese advantages are in fact the pillar principles of MDE and domain-speciÔ¨Åc modeling. Floch et al. [9] observed many \\nsimilarities between MDE and compilers research and principles. Thus, it is not surprising to see that many, though not \\nexclusively, code generation tools came out of the MDE community. The advantages of code generation should be contrasted',\n",
       " 'exclusively, code generation tools came out of the MDE community. The advantages of code generation should be contrasted \\nwith some of its limitations. For example, there are issues related to integration of generated code with manually written \\ncode and to evolving speciÔ¨Åcations that require to re-generate the code [10] . Sometimes, relying too much on code genera- \\ntors may produce an overly general solution that may not necessarily be optimal for a speciÔ¨Åc problem. \\n1 https://msdn.microsoft.com/en-us/library/bb126445.aspx \\n2 http://velocity.apache.org/ \\n3 http://www.eclipse.org/acceleo/ \\n4 http://wiki.eclipse.org/Xpand',\n",
       " 'E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n45 \\nconforms to\\nuses\\ninput\\ninput\\ngenerates\\nTemplate\\n<%context class%>\\npublic class <%name%> { String id; }\\nDesign-time input\\nClass\\nname:string\\nRuntime input\\nPerson\\nOutput\\npublic class Person { String id; }\\nTemplate engine\\nFig. 1. Components of TBCG. \\n2.2. Model-to-text transformations \\nIn MDE, model transformations can have different purposes [11] , such as translating, simulating, or reÔ¨Åning models. One \\nparticular kind of model transformation is devoted to code generation with model-to-text transformations (M2T) [12] . In \\ngeneral, M2T transforms a model (often in the form of a graph structure) into a linearized textual representation. M2T is \\nused to produce various text artifacts, e.g., to generate code, to serialize models, to generate documentation and reports, or \\nto visualize and explore models. As such, code generation is a special case of M2T, where the output artifacts are executable',\n",
       " 'to visualize and explore models. As such, code generation is a special case of M2T, where the output artifacts are executable \\nsource code. There are commonly Ô¨Åve different code generation approaches present in the literature [7,12] : \\nVisitor based approaches consist of programmatically traversing the internal representation of the input, while relying \\non an API dedicated to manipulate the input, to write the output to a text stream. This requires to program directly \\nthe creation of the output Ô¨Åles and storing the strings while navigating through the data structure of the input. The \\nimplementation typically makes use of the visitor design pattern [13] . This approach is used in [3] to generate Java \\ncode from a software architecture description language. \\nMeta-programming is a language extension approach, such as using a meta-object protocol, that relies on introspection',\n",
       " 'code from a software architecture description language. \\nMeta-programming is a language extension approach, such as using a meta-object protocol, that relies on introspection \\nin order to access the abstract syntax tree of the source program. For example, in OpenJava [4] , a Java meta-program \\ncreates a Java Ô¨Åle, compiles it on the Ô¨Çy, and loads the generated program in its own run-time. \\nIn-line generation relies on a preprocessor that generates additional code to expand the existing one, such as with the \\nC++ standard template library or C macro preprocessor instructions. The generation instructions can be deÔ¨Åned at a \\nhigher-level of abstraction, either using a dedicated language distinct from the base language (e.g., for macros) or as \\na dedicated library as in [14] . \\nCode annotations are in-line descriptions that added to statement declarations (e.g., class deÔ¨Ånition) that can either be',\n",
       " 'a dedicated library as in [14] . \\nCode annotations are in-line descriptions that added to statement declarations (e.g., class deÔ¨Ånition) that can either be \\ninternally transformed into more expanded code (e.g., attributes in C#) or that are processed by a tool other than the \\ncompiler of the language (e.g., the speciÔ¨Åcation of documentation comments processed by Javadoc). This approach is \\nused in [15] . \\nTemplate based is described below. \\n2.3. Template-based code generation \\nThe literature agrees on a general deÔ¨Ånition of M2T code generation [12] and on templates. J√∂rges [7] identiÔ¨Åes three \\ncomponents in TBCG: the data, the template, and the output. However, there is another component that is not mentioned, \\nwhich is the meta-information the generation logic of the template relies on. Therefore, we conducted this study according \\nto the following notion of TBCG. \\nFigure 1 summarizes the main concepts of TBCG. We consider TBCG as a synthesis technique that uses templates in order',\n",
       " 'to the following notion of TBCG. \\nFigure 1 summarizes the main concepts of TBCG. We consider TBCG as a synthesis technique that uses templates in order \\nto produce a textual artifact, such as source code, called the output . A template is an abstract and generalized representation \\nof the textual output it describes. It has a static part , text fragments that appear in the output ‚Äúas is‚Äù. It also has a dynamic \\npart embedded with splices of meta-code that encode the generation logic. Templates are executed by the template engine \\n(sometimes refereed to as template processor ) to compute the dynamic part and replace meta-codes by static text according \\nto run-time input . The design-time input deÔ¨Ånes the meta-information which the run-time input conforms to. The dynamic \\npart of a template relies on the design-time input to query the run-time input by Ô¨Åltering the information retrieved and',\n",
       " 'part of a template relies on the design-time input to query the run-time input by Ô¨Åltering the information retrieved and \\nperforming iterative expansions on it. Therefore, TBCG relies on a design-time input that is used to deÔ¨Åne the template \\nand a run-time input on which the template is applied to produce the output. For example, a TBCG engine that takes as \\nrun-time input an XML document relies on an XML schema as design-time input. DeÔ¨Ånition 1 summarizes our deÔ¨Ånition of \\nTBCG.',\n",
       " '46 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nDeÔ¨Ånition 1. A synthesis technique is a TBCG if it consists of a set of templates speciÔ¨Åed in a formalism, where the speciÔ¨Å- \\ncation of a template is based on a design-time input such that, when executed, it reads a run-time input to produce a textual \\noutput . \\nFor example, the work in [16] generates a C# API from Ecore models using Xpand. According to DeÔ¨Ånition 1 , the tem- \\nplates of this TBCG example are Xpand templates, the design-time input is the metamodel of Ecore, the run-time input is \\nan Ecore model, and the output is a C# project Ô¨Åle and C# classes. \\n2.4. Literature reviews on code generation \\nIn evidence-based software engineering [17] , a systematic literature review is a secondary study that reviews primary \\nstudies with the aim of synthesizing evidence related to a speciÔ¨Åc research question. Several forms of systematic reviews',\n",
       " 'studies with the aim of synthesizing evidence related to a speciÔ¨Åc research question. Several forms of systematic reviews \\nexist depending on the depth of reviewing primary studies and on the speciÔ¨Åcities of research questions. Unlike conven- \\ntional systematic literature reviews that attempt to answer a speciÔ¨Åc question, a SMS aim at classifying and performing a \\nthematic analysis on a topic [18] . SMS is a secondary study method that has been adapted from other disciplines to soft- \\nware engineering in [19] and later evolved by Petersen et al. in [20] . A SMS is designed to provide a wide overview of \\na research area, establish if research evidence exists on a speciÔ¨Åc topic, and provide an indication of the quantity of the \\nevidence speciÔ¨Åc to the domain. \\nOver the years, there have been many primary studies on code generation. However, we could not Ô¨Ånd any secondary \\nstudy on TBCG explicitly. Still, the following are closely related secondary studies.',\n",
       " 'study on TBCG explicitly. Still, the following are closely related secondary studies. \\nMehmood et al. [21] performed a SMS regarding the use of aspect-oriented modeling for code generation, which is not \\nbased on templates. They analyzed 65 papers mainly based on three main categories: the focus area, the type of research, \\nand the type of contribution. The authors concluded that this synthesis technique is still immature. The study shows that \\nno work has been reported to use or evaluate any of the techniques proposed. \\nGurunule et al. [22] presented a comparison of aspect orientation and MDE techniques to investigate how they can each \\nbe used for code generation. The authors found that further research in these areas can lead to signiÔ¨Åcant advancements in \\nthe development of software systems. Unlike Mehmood et al. [21] , they did not follow a systematic and repeatable process.',\n",
       " 'the development of software systems. Unlike Mehmood et al. [21] , they did not follow a systematic and repeatable process. \\nDominguez et al. [23] performed a systematic literature review of studies that focus on code generation from state ma- \\nchine speciÔ¨Åcations. The study is based on a set of 53 papers, which have been classiÔ¨Åed into two groups: pattern-based \\nand not pattern-based. The authors do not take template-based approaches into consideration. \\nBatot et al. [24] performed a systematic mapping study on model transformations solving a concrete problem that have \\nbeen published in the literature. They analyzed 82 papers based on a classiÔ¨Åcation scheme that is general to any model \\ntransformation approach, which includes M2T. They conclude that concrete model transformations have been pulling out \\nfrom the research literature since 2009 and are being considered as development tasks. They also found that 22% of their',\n",
       " 'from the research literature since 2009 and are being considered as development tasks. They also found that 22% of their \\ncorpus solve concrete problems using reÔ¨Ånement and code synthesis techniques. Finally, they found that research in model \\ntransformations is heading for a more stable and grounded validation. \\nThere are other studies that attempted to classify code generation techniques. However, they did not follow a systematic \\nand repeatable process. For example, Czarnecki et al. [12] proposed a feature model providing a terminology to characterize \\nmodel transformation approaches. They distinguished two categories for M2T approaches: those that are visitor-based and \\nthose that are template-based; the latter being in line with DeÔ¨Ånition 1 . The authors found that many new approaches \\nto model-to-model transformation have been proposed recently, but relatively little experience is available to assess their \\neffectiveness in practical applications.',\n",
       " 'to model-to-model transformation have been proposed recently, but relatively little experience is available to assess their \\neffectiveness in practical applications. \\nRose et al. [25] extended the feature model of Czarnecki et al. to focus on template-based M2T tools. Their classiÔ¨Åcation \\nis centered exclusively on tool-dependent features. Their goal is to help developers when they are faced to choose between \\ndifferent tools. This study is close to the work of Czarnecki in [12] but focuses only on a feature model for M2T. The \\ndifference with our study is that it focuses on a feature diagram and deals with tool-dependent features only. \\nThere are also other systematic reviews performed on other related topics. For example, the study in [26] performed a \\nSMS on domain-speciÔ¨Åc modeling languages (DSL). They analyzed 390 papers to portray the published literature on DSLs,',\n",
       " 'SMS on domain-speciÔ¨Åc modeling languages (DSL). They analyzed 390 papers to portray the published literature on DSLs, \\nwhich is comparable to the size of our corpus. Also, the study in [27] presents a systematic literature review on software \\nproduct lines engineering in the context of DSLs. Similar to our study, they propose a deÔ¨Ånition for the life-cycle of language \\nproduct lines and use it to analyze how the literature has an impact this life-cycle. Another SMS recently published in \\n[28] presents a taxonomy of source code labeling. \\n3. Research methods \\nIn order to analyze the topic of TBCG, we conducted a SMS following the process deÔ¨Åned by Petersen et al. in [20] and \\nsummarized in Fig. 2 . The deÔ¨Ånition of research question is discussed in Section 3.1 . The search conduction is described \\nin Section 3.2 . We present the screening of papers in Section 3.3 . The relevant papers are obtained based on the criteria',\n",
       " 'in Section 3.2 . We present the screening of papers in Section 3.3 . The relevant papers are obtained based on the criteria \\npresented in Section 3.3.1 and Section 3.3.2 . The elaboration of the classiÔ¨Åcation scheme is described in Section 3.4 . Finally, \\nwe detail the selection of the papers in Section 3.5 .',\n",
       " 'E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n47 \\nProcess Steps\\nOutcomes\\nDefinition of\\nResearch Question\\nConduct Search\\nScreening of Papers\\nKeywording using\\nAbstracts\\nData Extraction and\\nMapping Process\\nSystematic Map\\nClassification\\nScheme\\nRelevant Papers\\nAll Papers\\nReview Scope\\nFig. 2. The systematic mapping study process we followed from Peterson et al. \\n3.1. Objectives \\nThe objective of this study is to obtain an overview of the current research in the area of TBCG and to characterize the \\ndifferent approaches that have been developed. We deÔ¨Åned four research questions to set the scope of this study: \\n1. What are the trends in template-based code generation? We are interested to know how this technique has evolved over \\nthe years through research publications. \\n2. What are the characteristics of template-based code generation approaches? We want to identify major characteristics of \\nthese techniques and their tendencies.',\n",
       " '2. What are the characteristics of template-based code generation approaches? We want to identify major characteristics of \\nthese techniques and their tendencies. \\n3. To what extent are template-based code generation tools being used in research? We are interested in identifying popular \\ntools and their uses. \\n4. What is the place of MDE in template-based code generation? We seek to determine whether and how MDE has inÔ¨Çuenced \\nTBCG. \\n3.2. Selection of source \\nWe delimited the scope of the search to be regular publications that mention TBCG as at least one of the approaches \\nused for code generation and published between 20 0 0‚Äì2016. Therefore, this includes publications where code generation is \\nnot necessarily the main contribution. For example, Buchmann et al. [29] used TBCG to obtain ATL code while their main \\nfocus was implementing a higher-order transformation. Given that not all publications have the term ‚Äúcode generation‚Äù in',\n",
       " 'focus was implementing a higher-order transformation. Given that not all publications have the term ‚Äúcode generation‚Äù in \\ntheir title, we retrieved publications based on their title, abstract, or full text (when available) mentioning the keywords \\n‚Äútemplate‚Äù and its variations, ‚Äúcode‚Äù, and ‚Äúgeneration‚Äù and synonyms with their variations (e.g., ‚Äúsynthesis‚Äù). We used the \\nfollowing search query for all digital libraries, using their respective syntax: \\ntemplat ‚àóAND ‚Äúcode generat ‚àó‚Äù OR ‚Äúcode synthesi ‚àó‚Äù\\nWe validated our search with a sample of 100 pilot papers we preselected. These papers were chosen from papers we \\napriori knew should be included and resulting from a preliminary pilot search online. We iterated over different versions of \\nthe search strings until all pilot papers could be retrieved from the digital libraries. \\n3.3. Screening procedure \\nScreening is the most crucial phase in a SMS [20] . We followed a two-stage screening procedure: automatic Ô¨Åltering,',\n",
       " '3.3. Screening procedure \\nScreening is the most crucial phase in a SMS [20] . We followed a two-stage screening procedure: automatic Ô¨Åltering, \\nthen title and abstract screening. In order to avoid the exclusion of papers that should be part of the Ô¨Ånal corpus, we \\nfollowed a strict screening procedure. With four reviewers at our disposal, each article is screened by at least two reviewers \\nindependently. When both reviewers of a paper disagree upon the inclusion or exclusion of the paper, a physical discussion \\nis required. If the conÔ¨Çict is still unresolved, an additional senior reviewer is involved in the discussion until a consensus \\nis reached. To determine a fair exclusion process, a senior reviewer reviews a sample of no less than 20% of the excluded \\npapers at the end of the screening phase, to make sure that no potential paper is missed. \\n3.3.1. Inclusion criteria',\n",
       " 'papers at the end of the screening phase, to make sure that no potential paper is missed. \\n3.3.1. Inclusion criteria \\nA paper is included if it explicitly indicates the use of TBCG or if it proposes a TBCG technique. We also include papers \\nif the name of a TBCG tool appears in the title, abstract, or content. ‚ÄúUse‚Äù is taken in a large sense when it is explicit that \\na TBCG contributes to the core of the paper (not in the related work). \\n3.3.2. Exclusion criteria \\nResults from the search were Ô¨Årst Ô¨Åltered automatically to discard records that were outside the scope of this study: \\npapers not in computer science, not in the software engineering domain, with less than two pages of length (e.g., proceed-',\n",
       " '48 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nings preface), not peer-reviewed (e.g., white papers), not written in English, or not published between the years 20 0 0 and \\n2016. Then, papers were excluded through manual inspection based on the following criteria: \\n‚Ä¢ No code generation. There is no code generation technique used. \\n‚Ä¢ Not template-based code generation. Code generation is mentioned, but the considered technique is not template-based \\naccording to DeÔ¨Ånition 1 . \\n‚Ä¢ Not a paper. This exclusion criterion spans papers that were not caught by the automatic Ô¨Åltering. For example, some \\npapers had only the abstract written in English and the content of the paper in another language. Additionally, there \\nwere 24 papers where the full text was not accessible online. \\nFor the Ô¨Årst two criteria, when the abstract did not give enough details about the code generation approach, a quick',\n",
       " 'were 24 papers where the full text was not accessible online. \\nFor the Ô¨Årst two criteria, when the abstract did not give enough details about the code generation approach, a quick \\nlook at the full text helped clear any doubts on whether to exclude the paper or not. Reviewers were conservative on that \\nmatter. \\n3.4. ClassiÔ¨Åcation scheme \\nWe elaborated a classiÔ¨Åcation scheme by combining our general knowledge with the information extracted from the \\nabstracts during the screening phase. We classify all papers along different categories that are of interest in order to answer \\nour research questions. This helps analyzing the overall results and gives an overview of the trends and characteristics of \\nTBCG. The categories we classiÔ¨Åed the corpus with are the following: \\n‚Ä¢ Template style : We characterize the level of customization and expressiveness of the templates used in the code gen-',\n",
       " 'TBCG. The categories we classiÔ¨Åed the corpus with are the following: \\n‚Ä¢ Template style : We characterize the level of customization and expressiveness of the templates used in the code gen- \\neration approach. PredeÔ¨Åned style is reserved for approaches where the template used for code generation is deÔ¨Åned \\ninternally to the tool. However, a subset of the static part of the template is customizable to vary slightly the gener- \\nated output. This is, for example, the case for common CASE tools where there is a predeÔ¨Åned template to synthesize \\na class diagram into a number of programming languages. Nevertheless, the user can specify what construct to use for \\nmany-cardinality associations, e.g., Array or ArrayList for Java templates. Output-based style covers templates that \\nare syntactically based on the actual target output. In contrast with the previous style, output-based templates offer full',\n",
       " 'are syntactically based on the actual target output. In contrast with the previous style, output-based templates offer full \\ncontrol on how the code is generated, both on the static and dynamic parts. The generation logic is typically encoded in \\nmeta-code as in the example of Fig. 1 . Rule-based style puts the focus of the template on computing the dynamic part \\nwith the static part being implicit. The template lists declarative production rules that are applied on-demand by the \\ntemplate engine to obtain the Ô¨Ånal target output. For example, this is used to render the concrete textual syntax from \\nthe abstract syntax of a model using a grammar. \\n‚Ä¢ Input type : We characterize the language of the design-time input that is necessary to develop templates. The run-time \\ninput is an instance that conforms to it. General-purpose modeling language is for generic languages reusable across dif-',\n",
       " 'input is an instance that conforms to it. General-purpose modeling language is for generic languages reusable across dif- \\nferent domains that are not programming languages, such as UML. Domain-speciÔ¨Åc modeling language is for languages \\ntargeted for a particular domain, for example, where the run-time input is a Simulink model. Schema is for structured \\ndata deÔ¨Ånitions, such as XML or database schema. Programming language is for well-deÔ¨Åned programming languages, \\nwhere the run-time input is source code. \\n‚Ä¢ Output type : We characterize the output of the code generator (more than one category can be selected when multiple \\nartifacts are generated). Source code is executable code conforming to a speciÔ¨Åc programming language. Structured data \\nis for code that is not executable, such as HTML. \\n‚Ä¢ Application scale : We characterize the scale of the artifact on which the TBCG approach is applied with respect to the',\n",
       " 'is for code that is not executable, such as HTML. \\n‚Ä¢ Application scale : We characterize the scale of the artifact on which the TBCG approach is applied with respect to the \\nevaluation or illustration cases in the paper. The presence of a formal case study that either relied on multiple data \\nsources or subjects qualiÔ¨Åed as large scale application. Small scale was used mainly when there was only one example or \\ntoy examples in the paper that illustrated the TBCG. No application if for when the code generation was not applied on \\nany example. \\n‚Ä¢ Application domain : We classify the general domain TBCG has been applied on. For example, this includes Software engi- \\nneering, Embedded systems, Compilers, Bio-medicine , etc. \\n‚Ä¢ Orientation : We distinguish industrial papers, where at least one author is aÔ¨Éliated to industry, from academic papers \\notherwise. \\n‚Ä¢ Tool : We capture the tool used for TBCG. If a tool is not clearly identiÔ¨Åed in a paper or the TBCG is programmed directly,',\n",
       " 'otherwise. \\n‚Ä¢ Tool : We capture the tool used for TBCG. If a tool is not clearly identiÔ¨Åed in a paper or the TBCG is programmed directly, \\nwe classify the tool as unspeciÔ¨Åed . We consider a tool to be popular within the research community when it is used in \\nat least 1% of the papers. Otherwise, we classify it as other . \\n‚Ä¢ MDE : We determine whether the part of the solution where TBCG is applied in the paper follows MDE techniques and \\nprinciples. A good indication is if the design-time input is a metamodel. \\n3.5. Paper selection \\nTable 1 summarizes the Ô¨Çow of information through the selection process of this study. This section explains how we \\nobtained the Ô¨Ånal corpus of papers to classify and analyze.',\n",
       " 'E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n49 \\nTable 1 \\nEvolution of paper corpus during the study process. \\nPhase \\nNumber of papers \\nCollection \\nEngineering Village \\n4043 \\nScopus \\n932 \\nSpringerLink \\n2671 \\nInitial corpus \\n5131 \\nScreening \\nExcluded during screening \\n4553 \\nIncluded \\n578 \\nClassiÔ¨Åcation \\nExcluded during classiÔ¨Åcation \\n99 \\nFinal corpus \\n481 \\n3.5.1. Paper collection \\nThe paper collection step was done in two phases: querying and automatic duplicates removal. There are several \\nonline databases that index software engineering literature. For this study, we considered three main databases to maxi- \\nmize coverage: Engineering Village 5 , Scopus 6 , and SpringerLink 7 . The Ô¨Årst two cover typical software engineering editors \\n( IEEE Xplore , ACM Digital Library , Elsevier ). However, from past experiences [24] , they do not include all of Springer pub-',\n",
       " '( IEEE Xplore , ACM Digital Library , Elsevier ). However, from past experiences [24] , they do not include all of Springer pub- \\nlications. We used the search string from Section 3.2 to retrieve all papers from these three databases. We obtained 7 646 \\ncandidate papers that satisfy the query and the options of the search stated in Section 3.3.2 . We then removed automatically \\nall duplicates using EndNote software. This resulted in 5 131 candidate papers for the screening phase. \\n3.5.2. Screening \\nBased on the exclusion criteria stated in Section 3.3.2 , each candidate paper was screened by at least two reviewers to \\ndecide on its inclusion. To make the screening phase more eÔ¨Écient, we used a home-made tool [30] . After all the reviewers \\ncompleted screening the papers they were assigned, the tool calculates an inter-rater agreement coeÔ¨Écient. In our case, the \\nCohens Kappa coeÔ¨Écient was 0.813. This high value shows that the reviewers were in almost perfect agreement.',\n",
       " 'Cohens Kappa coeÔ¨Écient was 0.813. This high value shows that the reviewers were in almost perfect agreement. \\nAmong the initial corpus of candidate papers, 4556 were excluded, 551 were included and 24 received conÔ¨Çicting ratings. \\nDuring the screening, the senior reviewer systematically veriÔ¨Åed each set of 100 rejected papers for sanity check. A total \\nof 7 more papers were included back hence the rejected papers were reduced to 4549. Almost all cases of conÔ¨Çicts were \\nabout a disagreement on whether the code generation technique of a paper was using templates or not. These conÔ¨Çicts were \\nresolved in physical meetings and 20 of them were Ô¨Ånally included for a total of 578 papers and 4553 excluded. \\nAmong the excluded papers, 52% were rejected because no code generation was used. We were expecting such a high \\nrate because terms such as ‚Äútemplates‚Äù are used in many other Ô¨Åelds, like biometrics. Also, many of these papers were',\n",
       " 'rate because terms such as ‚Äútemplates‚Äù are used in many other Ô¨Åelds, like biometrics. Also, many of these papers were \\nreferring to the C++ standard template library [31] , which is not about code generation. We counted 34% papers excluded \\nbecause they were not using templates . Examples of such papers are cited in Section 2.2 . Also, more than a quarter of the \\npapers were in the compilers or embedded system domains, where code generation is programmed imperatively rather than \\ndeclaratively speciÔ¨Åed using a template mechanism. Finally, 5% of the papers were considered as not a paper . In fact, this \\ncriterion was in place to catch papers that escaped the automatic Ô¨Åltering from the databases. \\n3.5.3. Eligibility during classiÔ¨Åcation \\nOnce the screening phase over, we thoroughly analyzed the full text of the remaining 578 papers to classify them \\naccording to our classiÔ¨Åcation scheme. Doing so allowed us to conÔ¨Årm that the code generation approach was effectively',\n",
       " 'according to our classiÔ¨Åcation scheme. Doing so allowed us to conÔ¨Årm that the code generation approach was effectively \\ntemplate-based according to DeÔ¨Ånition 1 . We encountered papers that used multiple TBCG tools: they either compared tools \\nor adopted different tools for different tasks. We classiÔ¨Åed each of these papers as a single publication, but incremented the \\noccurrence corresponding to the tools referred to in the paper. This is the case of [32] where the authors use Velocity and \\nXSLT for code generation. Velocity generates Java and SQL code, while XSLT generates the control code. \\nWe excluded 99 additional papers. During screening, we detected situations where the abstract suggested the imple- \\nmentation of TBCG, whereas the full text proved otherwise. In most of the cases, the meaning of TBCG differed from the \\ndescription presented in Section 2.3 . As shown in [33] the terms template-based and generation are used in the context of',\n",
       " 'description presented in Section 2.3 . As shown in [33] the terms template-based and generation are used in the context of \\nnetworking and distributed systems. We also encountered circumstances where the tool mentioned in the abstract requires \\nthe explicit use of another component to be considered as TBCG, such as Simulink TLC, as in [34] . \\nThe Ô¨Ånal corpus 8 considered for this study contains 481 papers. \\n5 https://www.engineeringvillage.com/ \\n6 https://www.scopus.com/ \\n7 http://link.springer.com/ \\n8 The complete list of papers is available online at http://www-ens.iro.umontreal.ca/ ‚àºluhunukl/survey/classiÔ¨Åcation.html',\n",
       " '50 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\n# of papers\\nFig. 3. Evolution of papers in the corpus. \\nTable 2 \\nMost popular venues. \\nVenue \\nVenue & publication type \\n# Papers \\nModel Driven Engineering Languages and Systems ( Models ) \\nMDE \\nConference \\n26 \\nSoftware and Systems Modeling ( Sosym ) \\nMDE \\nJournal \\n26 \\nEuropean Conference on Modeling Foundations and Applications ( Ecmfa ) \\nMDE \\nConference \\n19 \\nGenerative and Transformational Techniques in Software Engineering ( Gttse ) \\nSoft. eng. \\nConference \\n11 \\nGenerative Programming: Concepts & Experience ( Gpce ) \\nSoft. eng. \\nConference \\n8 \\nInternational Conference on Computational Science and Applications ( Iccsa ) \\nOther \\nConference \\n8 \\nSoftware Language Engineering ( Sle ) \\nMDE \\nConference \\n7 \\nLeveraging Applications of Formal Methods, VeriÔ¨Åcation and Validation ( Isola ) \\nOther \\nConference \\n7 \\nAutomated Software Engineering ( Ase )',\n",
       " 'MDE \\nConference \\n7 \\nLeveraging Applications of Formal Methods, VeriÔ¨Åcation and Validation ( Isola ) \\nOther \\nConference \\n7 \\nAutomated Software Engineering ( Ase ) \\nSoft. eng. \\nJournal + Conference \\n5 + 2 \\nInternational Conference on Web Engineering ( Icwe ) \\nOther \\nConference \\n6 \\nEvaluation of Novel Approaches to Software Engineering ( Enase ) \\nSoft. eng. \\nConference \\n5 \\n4. Evolution of TBCG \\nWe start with a thorough analysis of the trends in TBCG in order to answer the Ô¨Årst research question. \\n4.1. General trend \\nFigure 3 reports the number of papers per year, averaging around 28. The general trend indicates that the number of \\npublications with at least one TBCG method started increasing in 2002 to reach a Ô¨Årst local maximum in 2005 and then \\nremained relatively constant until 2012. This increase coincides with the early stages of MDE and the Ô¨Årst edition of the \\nModels conference, previous called Uml conference. This is a typical trend where a research community gets carried away',\n",
       " 'Models conference, previous called Uml conference. This is a typical trend where a research community gets carried away \\nby the enthusiasm of a new potentially interesting domain, which leads to more publications. However, the most proliÔ¨Åc \\nperiod was in 2013, where we notice a signiÔ¨Åcant peak with 2.4 times the average numbers of publications observed in the \\nprevious years. Fig. 3 then shows sudden a decrease in 2015. \\nResorting to statistical methods, the high coeÔ¨Écient of variability and modiÔ¨Åed Thompson Tau test indicate that 2013 \\nand 2015 are outliers in the range 2005‚Äì2016, where the average is 37 papers per year. The sudden isolated peak in 2013 \\nis the result of a special event or popularity of TBCG. The following decrease in the amount of papers published should not \\nbe interpreted as a decline in interest in TBCG, but that some event happened around 2013 which boosted publications,',\n",
       " 'be interpreted as a decline in interest in TBCG, but that some event happened around 2013 which boosted publications, \\nand then it went back to the steady rate of publication as previous years. In fact, 2016 is one standard deviation above the \\naverage. \\n4.2. Publications and venues \\nWe analyzed the papers based on the type of publication and the venue of their publication. MDE venues account for \\nonly 22% of the publications, so are software engineering venues, while the majority (56%) were published in other venues. \\nTable 2 shows the most popular venues that have at least Ô¨Åve papers. These top venues account for just more than a quarter \\nof the total number of publications. Among them, MDE venues account for 60% of the papers. Models , Sosym , and Ecmfa \\nare the three most popular venues 9 with a total of 66 publications between them. This is very signiÔ¨Åcant given that the \\n9 We grouped Uml conference with Models and Ecmda-fa with Ecmfa .',\n",
       " 'E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n51 \\nTable 3 \\nDistribution of the template style facet. \\nOutput-based \\nPredeÔ¨Åned \\nRule-based \\n72% \\n24% \\n4% \\n0\\n10\\n20\\n30\\n40\\n50\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nOutput-based\\nPredefined\\nRule-based\\n# of papers\\nFig. 4. Template style evolution. \\naverage is only 1.67 paper per venue with a standard deviation of 2.63. Also, 43% of venues had only one paper using TBCG, \\nwhich is the case for most of the other venues. \\nThe peak in 2013 was mainly inÔ¨Çuenced by MDE and software engineering venues. However the drop in 2015 is the result \\nof an accumulation of the small variations among the other venues. Since 2014, MDE venues account for 10‚Äì12 papers per \\nyear, while only 6‚Äì7 in software engineering. \\nAs for the publication type, conference publications have been dominating at 64%. Journal article account for 24% of all',\n",
       " 'year, while only 6‚Äì7 in software engineering. \\nAs for the publication type, conference publications have been dominating at 64%. Journal article account for 24% of all \\npapers. The remaining papers were published as book chapters, workshops or other publication type. Interestingly, we notice \\na steady increase in journal articles, reaching a maximum of 15 in 2016. \\n5. Characteristics of template-based code generation \\nWe examine the characteristics of TBCG using the classiÔ¨Åcation scheme presented in Section 3.4 . \\n5.1. Template style \\nAs Table 3 illustrates, the vast majority of the publications follow the output-based style. This consists of papers like [35] , \\nwhere Xpand is used to generate workÔ¨Çow code used to automate modeling tools. There, it is the Ô¨Ånal output target text \\nthat drives the development of the template. This high score is expected since output-based style is the original template',\n",
       " 'that drives the development of the template. This high score is expected since output-based style is the original template \\nstyle for TBCG as depicted in Fig. 4 . This style has always been the most popular style since 20 0 0. \\nThe predeÔ¨Åned style is the second most popular. Most of these papers generate code using a CASE tool, such as [36] that \\nuses Rhapsody to generate code to map UML2 semantics to Java code with respect to association ends. Apart from CASE \\ntools, we also classiÔ¨Åed papers like [37] as predeÔ¨Åned style since the output code is already Ô¨Åxed as HTML and the pro- \\ngrammer uses the tags to change some values based on the model. Each year, around 28% of the papers were using the \\npredeÔ¨Åned style, except for a peak of 39% in 2005, given the popularity of CASE tools then. \\nWe found 19 publications that used rule-based style templates. This includes papers like [38] which generates Java code',\n",
       " 'We found 19 publications that used rule-based style templates. This includes papers like [38] which generates Java code \\nwith Stratego from a DSL. A possible explanation of such a low score is that this is the most diÔ¨Écult template style to \\nimplement. It had a maximum of two papers per year throughout the study period. \\n5.2. Input type \\nGeneral purpose languages account for almost half of the design-time input of the publications, as depicted in Table 4 . \\nUML (class) diagrams, which are used as metamodels for code generation, are the most used for 87% of these papers as in \\n[35] . Other popular general-purpose languages that were used are, for example, the architecture analysis and design lan- \\nguage [39] and feature diagrams [40] . The schema category comes second with 21% of the papers. For example, a database \\nTable 4 \\nDistribution of the input type facet. \\nGeneral purpose \\nSchema \\nDomain speciÔ¨Åc \\nProgramming language \\n48% \\n22% \\n20% \\n10%',\n",
       " '52 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nGeneral purpose\\nSchema\\nDomain Specific\\nProgramming Language\\n# of papers\\nFig. 5. Evolution of the design-time input type. \\nschema is used as input at design-time in [41] to generate Java for a system that demonstrates that template can im- \\nprove software development. Also, an XML schema is used in [42] as design-time input to produce C programs in order to \\nimplement an approach that can eÔ¨Éciently support all the conÔ¨Åguration options of an application in embedded systems. \\nDSLs are almost at par with schema. They have been gaining popularity and gradually reducing the gap with general-purpose \\nlanguages. For example in [43] , a custom language is given as the design input in order to generate C and C++ to develop \\na TBCG approach dedicated to real-time systems. The least popular design-time input type is programming language . This',\n",
       " 'a TBCG approach dedicated to real-time systems. The least popular design-time input type is programming language . This \\nincludes papers like [44] where T4 is used to generate hardware description (VHDL) code for conÔ¨Ågurable hardware. In this \\ncase, the input is another program on which the template depends. \\nOver the years, the general-purpose category has dominated the input type facet, as depicted in Fig. 5 . 2003 and 2006 \\nwere the only exceptions where schema obtained slightly more publications. We also notice a shift from schema to domain- \\nspeciÔ¨Åc design-time input types. Domain-speciÔ¨Åc input started increasing in 2009 but never reached the same level as \\ngeneral purpose. Programming language input maintained a constant level, with an average of 1% per year. Interestingly, in \\n2011, there were more programming languages used than DSLs. \\n5.3. Output type \\nAn overwhelming majority of the papers use TBCG to generate source code (81%), in contrast with 19% was structured',\n",
       " '2011, there were more programming languages used than DSLs. \\n5.3. Output type \\nAn overwhelming majority of the papers use TBCG to generate source code (81%), in contrast with 19% was structured \\ndata (though some papers output both types). Table 5 shows the distribution of the output languages that appeared in more \\nthan Ô¨Åve papers, representing 74% of the corpus. This includes papers like [45] where Java code is generated an adaptable \\naccess control tool for electronic medical records. Java and C are the most targeted programming languages. Writing a \\nprogram manually often requires proved abilities, especially with system and hardware languages, such as VHDL [46] . This \\nis why 8% of all papers generate low level source codes. Generation of structured data includes TBCG of mainly XML and \\nHTML Ô¨Åles. For example [47] produces both HTML and XML as parts of the web component to ease regression testing. In',\n",
       " 'HTML Ô¨Åles. For example [47] produces both HTML and XML as parts of the web component to ease regression testing. In \\naddition, we found that around 4% of the papers generate combinations of at least two output types. This includes papers \\nsuch as [48] that generate both C# and HTML from a domain-speciÔ¨Åc model. \\nStructured data output remained constant over the years, unlike source code which follows the general trend. \\n5.4. Application scale \\nAs depicted in Table 6 , most papers applied TBCG on large scale examples. This result indicates that TBCG is a technique \\nwhich scales with larger amounts of data. This includes papers like [49] that uses Acceleo to generate hundreds of lines \\nTable 5 \\nDistribution of the most popular output languages. \\nJava \\nC \\nHTML \\nC ++ \\nC# \\nXML \\nVHDL \\nSQL \\nAspectJ \\nSystemC \\n33% \\n11% \\n7% \\n7% \\n4% \\n3% \\n3% \\n2% \\n2% \\n2% \\nTable 6 \\nDistribution of application scale facet \\nLarge scale \\nSmall scale \\nNo application \\n63% \\n32% \\n5%',\n",
       " 'E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n53 \\n0\\n10\\n20\\n30\\n40\\n50\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nLarge scale\\nSmall scale\\nNo application\\n# of papers\\nFig. 6. Application scale evolution. \\nFig. 7. Distribution of application domain facet. \\nof aspect-oriented programming code. Small scale obtains 32% of the papers. This is commonly found in research papers \\nthat only need a small and simple example to illustrate their solution. This is the case in [50] in which a small concocted \\nexample shows the generation process with the Epsilon Generation Language (EGL) 10 . No application was used in 5% of the \\npublications. This includes papers like [51] where authors just mention that code synthesis is performed using a tool named \\nMako-template. Even though the number of publications without an actual application is very low, this demonstrates that',\n",
       " 'Mako-template. Even though the number of publications without an actual application is very low, this demonstrates that \\nsome authors have still not adopted good practice to show an example of the implementation. This is important, especially \\nwhen the TBCG approach is performed with a newly developed tool. \\nAs depicted in Fig. 6 , most papers illustrated their TBCG using large scale applications up to 2015. In this period, while \\nit follows the general trend of papers, the other two categories remained constant over the years. However, in 2016, we \\nobserve a major increase of small scale for TBCG. \\n5.5. Application domain \\nThe tree map in Fig. 7 highlights the fact that TBCG is used in many different areas. Software engineering obtains more \\nthan half of the papers with 55% of the publications. We have grouped in this category other related areas like ontolo- \\ngies, information systems or software product lines. This is expected given that the goal of TBCG is to synthesize software',\n",
       " 'gies, information systems or software product lines. This is expected given that the goal of TBCG is to synthesize software \\napplications. For example, the work in [52] uses the Rational CASE tool to generate Java programs in order to implement an \\napproach that transforms UML state machine to behavioral code. The next category is embedded systems which obtains 13% \\nof papers. Embedded systems often require low level hardware code diÔ¨Écult to write. Some even consider code generation \\nto VHDL as a compilation rather than automatic programming. In this category, we found papers like [53] in which Velocity \\nis used to produce Verilog code to increase the speed of simulation. Web technology related application domains account for \\n8% of the papers. It consists of papers like [54] where the authors worked to enhance the development dynamic web sites. \\nNetworking obtains 4% of the papers, such as [55] where code is generated for a telephony service network. Compiler obtains',\n",
       " 'Networking obtains 4% of the papers, such as [55] where code is generated for a telephony service network. Compiler obtains \\n1% of the papers, such as [56] where a C code is generated and optimized for an Intel C compiler. It is interesting to note \\nthat several papers were applied in domains such as bio-medicine [57] , artiÔ¨Åcial intelligence [58] , and graphics [59] . \\n10 http://www.eclipse.org/epsilon/doc/egl/',\n",
       " '54 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nWe combined application domains with a single paper into the other category. This regroups domains such as agronomy, \\neducation, and Ô¨Ånance. It is important to mention that the domain discussed in this category corresponds to the domain of \\napplication of TBCG employed, which differs from the publication venue. \\n5.6. Orientation \\nA quarter (24%) of the papers in the corpus are authored by a researcher from industry . The remaining 76% are written \\nonly by academics . This is a typical distribution since industrials tend to not publish their work. This result shows that TBCG \\nis used in industry as in [16] . Industry oriented papers have gradually increased since 2003 until they reached a peak in \\n2013. \\n6. Relations between characteristics \\nTo further characterize the trends observed in Section 5 , we identiÔ¨Åed signiÔ¨Åcant and interesting relations between the',\n",
       " '2013. \\n6. Relations between characteristics \\nTo further characterize the trends observed in Section 5 , we identiÔ¨Åed signiÔ¨Åcant and interesting relations between the \\ndifferent facets of the classiÔ¨Åcation scheme, using SPSS. \\n6.1. Statistical correlations \\nA Shapiro‚ÄìWilk test of each category determined that the none of them are normally distributed. Therefore, we opted \\nfor the Spearman two-tailed test of non-parametric correlations with a signiÔ¨Åcance value of 0.05 to identify correlations \\nbetween the trends of each category. The only signiÔ¨Åcantly strong correlations we found statistically are between the two \\ninput types, and between MDE and input type. \\nWith no surprise, the correlation between run-time and design time input is the strongest among all, with a correlation \\ncoeÔ¨Écient of 0.944 and a p- value of less than 0.001. This concurs with the results found in Section 5.2 . An example is',\n",
       " 'coeÔ¨Écient of 0.944 and a p- value of less than 0.001. This concurs with the results found in Section 5.2 . An example is \\nwhen the design-time input is UML, the run-time input is always a UML diagram as in [57] . Such a strong relationship is \\nalso noticeable in [60] with programming languages and source code, as well as in [58] when a schema design is used for \\nstructured data. As a result, all run-time input categories are correlated to the same categories as for design-time input. We \\nwill therefore treat these two facets together as input type . \\nThere is a strong correlation of coeÔ¨Écient of 0.738 and a p-value of less than 0.001 between input type and MDE . As \\nexpected, more than 90% of the papers using general purpose and domain speciÔ¨Åc inputs are follow the MDE approach. \\n6.2. Other interesting relations \\nWe also found weak but statistically signiÔ¨Åcant correlations between the remaining facets. We discuss the result here. \\n6.2.1. Template style',\n",
       " '6.2. Other interesting relations \\nWe also found weak but statistically signiÔ¨Åcant correlations between the remaining facets. We discuss the result here. \\n6.2.1. Template style \\nFigure 8 shows the relationship between template style, design-time input, and output types. We found that for \\nthe predeÔ¨Åned templates, there are twice as many papers that use schema input than domain speciÔ¨Åc. However, for \\n168\\n72\\n79\\n26\\n62\\n13\\n24\\n18\\n13\\n3\\n3\\nOutput-based\\nPredefined\\nRule-based\\nGeneral \\npurpose\\nDomain \\nspecific\\nSchema\\nProg. \\nlanguage\\nSource \\ncode\\nStructured \\ndata\\nDesign-time input type\\nOutput type\\nFig. 8. Relation between template style (vertical) and input/output types (horizontal).',\n",
       " 'E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n55 \\nSource \\ncode\\nStructured \\ndata\\nGeneral \\npurpose\\nDomain \\nspecific\\nSchema\\nProg. \\nlanguage\\nFig. 9. Relation between output (vertical) and design-time input (horizontal) types showing the number of papers in each intersection. \\nTable 7 \\nDistribution of the tool facet. \\nNamed MDE \\nUnspeciÔ¨Åed \\nOther \\nNamed not MDE \\n41% \\n28% \\n23% \\n8% \\noutput-based, domain speciÔ¨Åc inputs are used slightly more often. We also notice that general purpose input is never used \\nwith rule-based templates. The output type follows the same general distribution regardless of the template style. \\nAll rule-based style approaches have included a sample application. Meanwhile, the proportion of small scale was twice \\nmore important for predeÔ¨Åned templates (51%) then for output-based (27%). \\nWe found that popular tools were used twice as often on output-based templates (58%) than on predeÔ¨Åned templates',\n",
       " 'more important for predeÔ¨Åned templates (51%) then for output-based (27%). \\nWe found that popular tools were used twice as often on output-based templates (58%) than on predeÔ¨Åned templates \\n(23%). Rule-based templates never employed a tool that satisÔ¨Åed our popularity threshold, but used other tools such as \\nStratego. \\nWe found that all papers using a rule-based style template do not follow an MDE approach. On the contrary, 70% of the \\noutput-based style papers and 56% of the predeÔ¨Åned ones follow an MDE approach. \\nFinally, we found that for each template style, the number of papers authored by an industry researcher Ô¨Çuctuated \\nbetween 22‚Äì30%. \\n6.2.2. Input type \\nThe bubble chart in Fig. 9 illustrates the tendencies between input and output types. It is clear that source code is \\nthe dominant generated artifact regardless of the input type. Source code is more often generated from general purpose',\n",
       " 'the dominant generated artifact regardless of the input type. Source code is more often generated from general purpose \\nand domain speciÔ¨Åc inputs than from schema and programming languages. Also, the largest portion of structured data is \\ngenerated from a schema input. \\nMoving on to input type and application scale, we found that small scales are used 40% of the time when the input is a \\nprogramming language. The number of papers with no sample application is very low (5%) regardless of the template style. \\nFinally, 74% of papers using large scale applications use a domain speciÔ¨Åc input, which is slightly higher than those using a \\ngeneral purpose input with 71%. \\n6.2.3. Output type, application scale, and orientation \\nAs we compared output type to orientation, we found that industrials generate slightly more source code than academics: \\n89% vs. 80%. However, academics generate more structured data than industrials: 18% vs. 6% and 3% vs. 1%., respectively. We',\n",
       " '89% vs. 80%. However, academics generate more structured data than industrials: 18% vs. 6% and 3% vs. 1%., respectively. We \\nfound that 65% of the papers without application are from the academy. \\n7. Template-based code generation tools \\nTable 7 shows that half of the papers used a popular TBCG tool ( named ). The other half used less popular tools (the other \\ncategory), did not mention any TBCG tool, or implemented the code generation directly for the purpose of the paper. \\n7.1. Popular tools in research \\nFigure 10 shows the distribution of popular tools used in at least 1% of the papers, i.e., Ô¨Åve papers. We see that only \\n6/14 popular tools follow MDE approaches. Acceleo and Xpand are the most popular with respectively 16% and 15% of the \\npapers using them. Their popularity is probably due to their simple syntax and ease of use [61] and the fact that they are \\nMDE tools [16] . They both have an OCL-like language for the dynamic part and rely on a metamodel speciÔ¨Åed in Ecore as',\n",
       " 'MDE tools [16] . They both have an OCL-like language for the dynamic part and rely on a metamodel speciÔ¨Åed in Ecore as \\ndesign-time input.',\n",
       " '56 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\nFig. 10. Popular tools. \\nEGL also has a structure similar to the other model-based tools. It is natively integrated with languages from the Epsilon \\nfamily, thus relies on the Epsilon Object Language for its dynamic part. MOFScript is another popular model-based tool that \\nonly differs in syntax from the others. Xtend2 is the least used popular model-based tool. It is both an advanced form of \\nXpand and a simpliÔ¨Åed syntactical version of Java. \\nXSLT is the third most popular tool used. It is suitable for XML documents only. Some use it for models represented in \\ntheir XMI format, as it is the case in [62] . XSLT follows the template and Ô¨Åltering strategy. It matches each tag of the input \\ndocument and applies the corresponding template. \\nJET [63] and Velocity [53] are used as often as each other on top of being quite similar. The main difference is that JET',\n",
       " 'document and applies the corresponding template. \\nJET [63] and Velocity [53] are used as often as each other on top of being quite similar. The main difference is that JET \\nuses an underlying programming language (Java) for the dynamic part. In JET, templates are used to developers generate the \\nJava code speciÔ¨Åc for the synthesis of code to help developers implement the code generation. \\nStringTemplate [64] has its own template structure. It can be embedded into a Java code where strings to be output are \\ndeÔ¨Åned using templates. Note that all the tools mentioned above use an output-based template style. \\nThe most popular CASE tools for TBCG are Fujaba [65] , Rational [66] , and Rhapsody [67] . One of the features they offer is \\nto generate different target languages from individual UML elements. All CASE tools (even counting the other category) have \\nbeen used in a total of 39 papers, which puts them at par with Xpand. CASE tools are mostly popular for design activities;',\n",
       " 'been used in a total of 39 papers, which puts them at par with Xpand. CASE tools are mostly popular for design activities; \\ncode generation is only one of their many features. CASE tools have a predeÔ¨Åned template style. \\nSimulink TLC is the only rule-based tool among the most popular ones. As a rule-based approach, it has a different \\nstructure compared to the above mentioned tools. Its main difference is that the developer writes the directives to be \\nfollowed by Simulink in order to render the Ô¨Ånal C code from S-functions. \\nWe notice that the most popular tools are evenly distributed between model-based tools (Acceleo, Xpand) and code- \\nbased tools (JET, XSLT). Surprisingly, XSLT, which has been around the longest, is less popular than Xpand. This is undoubt- \\nedly explained by the advantages that MDE has to offer [7,8] . \\n7.2. UnspeciÔ¨Åed and other tools \\nAs depicted in Table 7 , 28% of the papers did not specify the tool that was used, as in [68] where the authors introduce',\n",
       " '7.2. UnspeciÔ¨Åed and other tools \\nAs depicted in Table 7 , 28% of the papers did not specify the tool that was used, as in [68] where the authors introduce \\nthe concept of a meta-framework to resolve issues involved in extending the life of applications. Furthermore, 23% of the \\npapers used less popular tools, present in less than Ô¨Åve papers, such as T4 [44] and Cheetah [56] , a python powered template \\nmainly used for web developing. Like JET, Cheetah templates generate Python classes, while T4 is integrated with .NET \\ntechnology. Some CASE tools were also in this category, such as AndroMDA [69] . Other examples of less popular tools are \\nGroovy template [47] , Meta-Aspect-J [70] , and Jinja2 [71] . The fact that new or less popular tools are still abundantly used \\nsuggests that research in TBCG is still active with new tools being developed or evolved. \\n7.3. Trends of tools used',\n",
       " 'suggests that research in TBCG is still active with new tools being developed or evolved. \\n7.3. Trends of tools used \\nEach one of these tools had a different evolution over the years. UnspeciÔ¨Åed tools were prevalent before 2004 and then \\nkept a constant rate of usage until a drop since 2014. We notice a similar trend for CASE tools that were the most popular \\nin 2005 before decreasing until 2009. They only appear in at most three papers per year after 2010. The use of the most \\npopular tool, Xpand, gradually increased since 2005 to reach the peak in 2013 before decreasing. The other category main- \\ntained an increasing trend until 2014. Yet, a few other popular tools appeared later on. For example, EGL started appearing \\nin 2008 and had its peak in 2013. Acceleo appeared a year later and was the most popular TBCG tool in 2013‚Äì2014. Finally,',\n",
       " 'E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n57 \\nMOFScript had no more than a paper per year since 2005. StringTemplate and T4 were used scarcely since 2006 and 2009, \\nrespectively. \\n7.4. Characteristics of tools \\nWe have also analyzed each popular tool with respect to the characteristics presented in Section 5 . As mentioned earlier, \\nmost of the popular tools implement output-based template technique except the CASE tools which are designed following \\nthe predeÔ¨Åned style. \\nTools such as Acceleo, Xpand, EGL, MOFScript and 97% of the CASE tools papers are only used based on an MDE ap- \\nproach, given that they were created by this community. Nevertheless, there are tools that were never used with MDE \\nprinciples, like JET and Cheetah. Such tools can handle a program code or a schema as metamodel but have no internal \\nsupport for modeling languages. Moreover, the programmer has to write his own stream reader to parse the input, but they',\n",
       " 'support for modeling languages. Moreover, the programmer has to write his own stream reader to parse the input, but they \\nallow for a broader range of artifacts as inputs that do not have to be modeled explicitly. A few code-based tools provide \\ninternal support for model-based approaches. For instance, Velocity, XSLT, and StringTemplate can handle both UML and pro- \\ngrammed metamodel as design-time input. T4 can be integrated with MDE artifacts (e.g., generate code based on a domain- \\nspeciÔ¨Åc language in Visual studio). However, all four papers in the corpus that implemented TBCG in T4 did not use an MDE \\napproach. \\nA surprising result we found is that EGL is the only MDE tool that has its papers mostly published in MDE venues like \\nSosym , Models , and Ecmfa . All the other tools are mostly published in other venues like Icssa , whereas software engineering \\nvenues, like Ase or Icse , and MDE venues account for 26‚Äì33% of the papers for each of the rest of the MDE tools.',\n",
       " 'venues, like Ase or Icse , and MDE venues account for 26‚Äì33% of the papers for each of the rest of the MDE tools. \\nCASE tools, MOFScript, Velocity, and Simulink TLC mostly generate program code. The latter is always used in the domain \\nof embedded systems. Papers that use StringTemplate do not include any validation process, so is Velocity in 93% of the \\npapers using it. XSLT has been only used to generate structured data as anticipated. \\nOther tools are the most used TBCG in the industry. This is because the tool is often internal to the company [72] . Among \\nthe most popular tools, Xpand is the most in the industry. \\n7.5. Application scale \\nBetween application scale and tools, we found that 74% of the papers that make use of a popular tool used large scale \\napplication to illustrate their approach. Also, 62% of the papers using unpopular tools 11 use large scale applications. Small \\nscale is likely to be used in unpopular tools rather than popular tools. \\n7.6. Tool support',\n",
       " 'scale is likely to be used in unpopular tools rather than popular tools. \\n7.6. Tool support \\nIn total, we found 82 different tools named in the corpus. Among them 54% are no longer supported (i.e., no release, \\nupdate or commit since the past two years). Interestingly, 55% of the tools have been developed by the industry. 70% of \\nthese industry tools are still supported, in contrast with 51% for the academic tools. As one would expect, the tendency \\nshows that tools that are frequently mentioned in research papers are still supported and are developed by industry. \\n8. MDE and template-based code generation \\nOverall, 64% of the publications followed MDE techniques and principles. For example in [73] , the authors propose a sim- \\nulation environment with an architecture that aims at integrating tools for modeling, simulation, analysis, and collaboration. \\nAs expected, most of the publications using output-based and predeÔ¨Åned techniques are classiÔ¨Åed as model-based papers.',\n",
       " 'As expected, most of the publications using output-based and predeÔ¨Åned techniques are classiÔ¨Åed as model-based papers. \\nThe remaining 36% of the publications did not use MDE. This includes all papers that use a rule-based template style as \\nreported in Section 6 . For example, the authors in [40] developed a system that handles the implementation of dependable \\napplications and offers a better certiÔ¨Åcation process for the fault-tolerance mechanisms. \\nAs Fig. 11 shows, the evolution of the MDE category reveals that MDE-based approach started over passing non MDE- \\nbased techniques in 2005, except for 2006. It increased to reach a peak in 2013 and then started decreasing as the general \\ntrend of the corpus. Overall, MDE-based techniques for TBCG have been dominating other techniques in the past 12 years. \\nWe also analyzed the classiÔ¨Åcation of only MDE papers with respect to the characteristics presented in Section 3 . We',\n",
       " 'We also analyzed the classiÔ¨Åcation of only MDE papers with respect to the characteristics presented in Section 3 . We \\nonly focus here on facets with different results compared to the general trend of papers. We found that only half of the \\ntotal number of papers using unspeciÔ¨Åed and other tools are MDE-based papers. We only found one paper that uses a \\nprogramming language as design-time input with MDE [74] . This analysis also shows that the year 2005 clearly marked the \\nshift from schema to domain-speciÔ¨Åc design-time inputs, as witnessed in Section 5.2 . Thus after general purpose, which \\nobtains 69% of the publications, domain speciÔ¨Åc accounts for a better score of 26%, while schema obtains only 4%. With \\nrespect to the run-time category, the use of domain-speciÔ¨Åc models increased to reach a peak in 2013. As expected, no \\nprogram code is used for MDE papers, because MDE typically does not consider them as models, unless a metamodel of the \\nprogramming language is used.',\n",
       " 'program code is used for MDE papers, because MDE typically does not consider them as models, unless a metamodel of the \\nprogramming language is used. \\n11 Refers to the union of other and unspeciÔ¨Åed categories of the tool facet.',\n",
       " '58 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n2000\\n2002\\n2004\\n2006\\n2008\\n2010\\n2012\\n2014\\n2016\\nUsing MDE\\nNot using MDE\\n# of papers\\nFig. 11. Evolution of the MDE facet. \\nInterestingly, MDE venues are only the second most popular after other venues for MDE approaches. Finally, MDE journal \\npapers maintained a linear increase over the years, while MDE conference papers had a heterogeneous evolution similar to \\nthe general trend of papers. \\n9. Discussion \\n9.1. RQ1: What are the trends in TBCG? \\nThe statistical results from this signiÔ¨Åcantly large sample of papers clearly suggest that TBCG has received suÔ¨Écient \\nattention from the research community. The community has maintained a production rate in-line with the last 11 years \\naverage, especially with a constant rate of appearance in journal articles. The only exceptions were a signiÔ¨Åcant boost in',\n",
       " 'average, especially with a constant rate of appearance in journal articles. The only exceptions were a signiÔ¨Åcant boost in \\n2013 and a dip in 2015. The lack of retention of papers appearing in non MDE may indicate that TBCG is now applied \\nin development projects rather than being a critical research problem to solve. Also, conference papers as well as venues \\noutside MDE and software engineering had a signiÔ¨Åcant impact on the evolution of TBCG. Given that TBCG seems to have \\nreached a steady publication rate since 2005, we can expect contributions from the research community to continue in that \\ntrend. \\n9.2. RQ2: What are the characteristics of TBCG approaches? \\nOur classiÔ¨Åcation scheme constitutes the main source to answer this question. The results clearly indicate the preferences \\nthe research community has regarding TBCG. Output-based templates have always been the most popular style from the',\n",
       " 'the research community has regarding TBCG. Output-based templates have always been the most popular style from the \\nbeginning. Nevertheless, there have been some attempts to propose other template styles, like the rule-based style, but they \\ndid not catch on. Because of its simplicity to use, the predeÔ¨Åned style is probably still popular in practice, but it is less \\nmentioned in research papers. TBCG has been used to synthesize a variety of application code or documents. As expected, \\nthe study shows that modeling language inputs have prevailed over any other type. SpeciÔ¨Åcally for MDE approaches to TBCG, \\nthe input to transform is moving from general purpose to domain-speciÔ¨Åc models. Academic researchers have contributed \\nmost, as expected with a literature review, but we found that industry is actively and continuously using TBCG as well. The \\nstudy also shows that the community is moving from large-scale applications to smaller-sized examples in research papers.',\n",
       " 'study also shows that the community is moving from large-scale applications to smaller-sized examples in research papers. \\nThis concurs with the level of maturity of this synthesis approach. The study conÔ¨Årms that the community uses TBCG to \\ngenerate mainly source code. This trend is set to continue since the automation of computerized tasks is continuing to gain \\nground in all Ô¨Åelds. Finally, TBCG has been implemented in many domains, software engineering and embedded systems \\nbeing the most popular, but also unexpectedly in unrelated domains, such as bio-medicine and Ô¨Ånance. \\n9.3. RQ3: To what extent are TBCG tools being used in research? \\nIn this study, we discovered a total of 82 different tools for TBCG that are mentioned in research papers. Many studies \\nimplemented code generation with a custom-made tool that was never or seldom reused. This indicates that the develop-',\n",
       " 'implemented code generation with a custom-made tool that was never or seldom reused. This indicates that the develop- \\nment of new tools is still very active. MDE tools are the most popular. Since the research community has favored output- \\nbased template style, this has particularly inÔ¨Çuenced the tools implementation. This template style allows for more Ô¨Åne- \\ngrained customization of the synthesis logic which seems to be what users have favored. This particular aspect is also \\ninÔ¨Çuencing the expansion of TBCG into industry. Most popular tool are actively supported by industry. Well-known tools \\nlike Acceleo, Xpand and Velocity are moving from being simple research material to effective development resources in in- \\ndustry. Finally, the study There are many TBCG tools that are popular in industry that fall under the ‚ÄúOther tools‚Äù category \\nbecause they are rarely reported in the scientiÔ¨Åc literature (under 1% of the papers in our corpus). Since this study is a lit-',\n",
       " 'because they are rarely reported in the scientiÔ¨Åc literature (under 1% of the papers in our corpus). Since this study is a lit- \\nerature review, the presence of a tool in this study is biased towards the what is published, and may not reÔ¨Çect the reality \\nin industry. This is a common threat of SMS.',\n",
       " 'E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n59 \\n9.4. RQ4: What is the place of MDE in TBCG? \\nAll this analysis clearly concludes that the advent of MDE has been driving TBCG research. In fact, MDE has led to \\nincrease the average number of publications by a factor of four. There are many advantages to code generation, such as \\nreduced development effort, easier to write and understand domain/application concepts and less error-prone [8] . These are, \\nin fact, the pillar principles of MDE and domain-speciÔ¨Åc modeling [2] . Thus, it is not surprising to see that many, though \\nnot exclusively, code generation tools came out from the MDE community. As TBCG became a commonplace in general, \\nthe research in this area is now mostly conducted by the MDE community. Furthermore, MDE has brought very popular \\ntools that have encountered a great success, and they are also contributing to the expansion of TBCG across industry. It',\n",
       " 'tools that have encountered a great success, and they are also contributing to the expansion of TBCG across industry. It \\nis important to mention that the MDE community publishes in speciÔ¨Åc venues like Models , Sosym , or Ecmfa unlike other \\nresearch communities where the venues are very diversiÔ¨Åed. This resulted in three MDE venues at the top of the ranking. \\n9.5. IdentiÔ¨Åed challenges \\nAfter thoroughly analyzing each paper in the corpus, we noted several problems that the research community has ne- \\nglected in TBCG. First, we found 66% of the papers did not provide any assessment of the code generation process or the \\ngenerated output. We only found one paper with a formal veriÔ¨Åcation of the generated code using non-functional require- \\nment analysis [75] . Furthermore, the TBCG can be veriÔ¨Åed through benchmarks as in [55] . Second, we found no paper that \\ninvestigates eÔ¨Éciency of code generation. Researchers may be inspired from other related communities, such as compiler',\n",
       " 'investigates eÔ¨Éciency of code generation. Researchers may be inspired from other related communities, such as compiler \\noptimization [60] . Third, designing templates requires skillful engineering. Good practices, design patterns and other forms \\nof good reusable idioms would be of great value to developers [76] . \\n9.6. Threats to validity \\nThe results presented in this survey have depended on many factors that could potentially threaten its validity. \\n9.6.1. Construction validity \\nIn a strict sense, our Ô¨Åndings are valid only for our sample that we collected from 20 0 0‚Äì2016. This leads to determine \\nwhether the primary studies used in our survey are a good representation of the whole population. From Fig. 3 , we can \\nobserve that our sample can be attributed as a representative sample of the whole population. In particular, the average \\nnumber of identiÔ¨Åed primary studies per year is 28 with standard deviation 15.76. A more systematic selection process',\n",
       " 'number of identiÔ¨Åed primary studies per year is 28 with standard deviation 15.76. A more systematic selection process \\nwould have been diÔ¨Écult to be exhaustive about TBCG. We chose to obtain the best possible coverage at the cost of du- \\nplications. Nevertheless, the size of the corpus we classiÔ¨Åed is about ten times larger than other systematic reviews related \\nto code generation (see Section 2.4 ). We are, therefore, conÔ¨Ådent that this sample is a representative subset of all relevant \\npublications on TBCG. \\nAnother potential limitation is the query formulation for the keyword search. It is diÔ¨Écult to encode a query that is \\nrestrictive enough to discard unrelated publications, but at the same time retrieves all the relevant ones. In order to obtain \\na satisfactory balance, we included synonyms and captured possible declinations. In this study, we are only interested in',\n",
       " 'a satisfactory balance, we included synonyms and captured possible declinations. In this study, we are only interested in \\ncode generation. Therefore we discarded articles where TBCG was used for reporting or mass mailing, for example. We \\nbelieve that our sample is large enough that additional papers will not signiÔ¨Åcantly affect the general trends and results. \\nWe are fully aware that, since TBCG is widely used in practice, industries have their own tools and many have not been \\npublished in academic venues. Our goal was not to be exhaustive, but to get a representative sample. \\n9.6.2. Internal validity \\nA potential limitation is related to data extraction. It is diÔ¨Écult to extract data from relevant publications, especially \\nwhen the quality of the paper is low, when code generation is not the primary contribution of the paper, or when critical \\ninformation for the classiÔ¨Åcation is not directly available in the paper. For example in [77] , the authors only mention the',\n",
       " 'information for the classiÔ¨Åcation is not directly available in the paper. For example in [77] , the authors only mention the \\nname of the tool used to generate the code. In order to mitigate this threat, we had to resort to searching for additional \\ninformation about the tool: reading other publications that use the tool, traversing the website of the tool, installing the \\ntool, or discussing with the tools experts. \\nAnother possible threat is the screening of papers based on inclusion and exclusion criteria that we deÔ¨Åned before the \\nstudy was conducted. During this process, we examined only the title, the abstract. Therefore, there is a probability that we \\nexcluded relevant publications such as [55] , that do not include any TBCG terms. In order to mitigate this threat, whenever \\nwe were unsure whether a publication should be excluded or not we conservatively opted to include it. However, during',\n",
       " 'we were unsure whether a publication should be excluded or not we conservatively opted to include it. However, during \\nclassiÔ¨Åcation when reading the whole content of the paper, we may still have excluded it. \\n9.6.3. External validity \\nThe results we obtained are based on TBCG only. Even though our classiÔ¨Åcation scheme includes facets like orientation, \\napplication domain, that are not related to the area, we followed a topic based classiÔ¨Åcation. The core characteristics of our \\nstudy are strictly related to this particular code synthesis technique. We have deÔ¨Åned characteristics like template style and \\nthe two levels of inputs that we believe are exclusive to TBCG. Therefore, the results cannot be generalized to other code \\ngeneration techniques mentioned in Section 2.2 .',\n",
       " '60 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n9.6.4. Conclusion validity \\nOur study is based on a large number of primary studies. This helps us mitigate the potential threats related to the \\nconclusions of our study. A missing paper or a wrongly classiÔ¨Åed paper would have a very low impact on the statistics \\ncompared to a smaller number of primary studies. In addition, as a senior reviewer did a sanity check on the rejected \\npapers, we are conÔ¨Ådent that we did not miss a signiÔ¨Åcant number of papers. Hence, the chances for wrong conclusions are \\nsmall. Replication of this study can be achieved as we provided all the details of our research method in Section 3 . Also, our \\nstudy follows the methodology described in [20] . \\n10. Conclusion \\nThis paper reports the results of a large survey we conducted on the topic of TBCG, which has been missing in the',\n",
       " 'study follows the methodology described in [20] . \\n10. Conclusion \\nThis paper reports the results of a large survey we conducted on the topic of TBCG, which has been missing in the \\nliterature. The objectives of this study are to better understand the characteristics of TBCG techniques and associated tools, \\nidentify research trends, and assess the importance of the role that MDE plays. The analysis of this corpus is organized into \\nfacets of a novel classiÔ¨Åcation scheme, which is of great value to modeling and software engineering researchers who are \\ninterested in painting an overview of the literature on TBCG. \\nOur study shows that the community has been diversely using TBCG over the past 16 years, and that research and \\ndevelopment is still very active. TBCG has greatly beneÔ¨Åted from MDE in 2005 and 2013 which mark the two peaks of \\nthe evolution of this area, tripling the average number of publications. In addition, TBCG has favored a template style that',\n",
       " 'the evolution of this area, tripling the average number of publications. In addition, TBCG has favored a template style that \\nis output-based and modeling languages as input. It has been applied in a variety of domains. The community has been \\nfavoring the use of custom tools for code generation over popular ones. Most research using TBCG follows an MDE approach. \\nFurthermore, both MDE and non-MDE tools are becoming effective development resources in industry. \\nThe study also revealed that, although the research in TBCG is mature enough, there are many open issues that can \\nbe addressed in the future, upstream and downstream the generation itself. Upstream, the deÔ¨Ånition of templates is not \\na trivial task. Supporting the developers in such a deÔ¨Ånition is a must. Downstream, methods and techniques need to be \\ndeÔ¨Åned to assess the correctness and quality of the generated code.',\n",
       " 'a trivial task. Supporting the developers in such a deÔ¨Ånition is a must. Downstream, methods and techniques need to be \\ndeÔ¨Åned to assess the correctness and quality of the generated code. \\nWe believe this survey will be of beneÔ¨Åt to someone familiar with code generation by knowing how their favorite tool \\nranks in popularity within the research community, the relevance and importance of the use of templates, and in which \\ncontext TBCG has been applied (application domain). The relations across categories in Section VI show non-intuitive results \\nas well. The paper also promotes MDE in Ô¨Åelds that have not been traditionally exposed to it. \\nSupplementary material \\nSupplementary material associated with this article can be found, in the online version, at 10.1016/j.cl.2017.11.003 . \\nReferences \\n[1] Rich C , Waters RC . Automatic programming: myths and prospects. Computer 1988;21(8):40‚Äì51 .',\n",
       " 'References \\n[1] Rich C , Waters RC . Automatic programming: myths and prospects. Computer 1988;21(8):40‚Äì51 . \\n[2] Kelly S , Tolvanen J-P . Domain-speciÔ¨Åc modeling: enabling full code generation. John Wiley & Sons; 2008 . \\n[3] Bonta E , Bernardo M . Padl2java: a Java code generator for process algebraic architectural descriptions. In: Proceedings of the european conference on \\nsoftware architecture. IEEE; 2009. p. 161‚Äì70 . \\n[4] Tatsubori M , Chiba S , Killijian M-O , Itano K . OpenJava: a class-based macro system for Java. In: ReÔ¨Çection and software engineering. In: LNCS, 1826. \\nSpringer; 20 0 0. p. 117‚Äì33 . \\n[5] Lohmann D , Blaschke G , Spinczyk O . Generic advice: on the combination of AOP with generative programming in AspectC++. In: Proceedings of \\ninternational conference on generative programming and component engineering. In: LNCS, 3286. Berlin Heidelberg: Springer; 2004. p. 55‚Äì74 .',\n",
       " 'international conference on generative programming and component engineering. In: LNCS, 3286. Berlin Heidelberg: Springer; 2004. p. 55‚Äì74 . \\n[6] Kleppe AG , Warmer J , Bast W . MDA explained. The model driven architecture: practice and promise. Addison-Wesley; 2003 . \\n[7] J√∂rges S . Construction and evolution of code generators 7747. Ch 2 The state of the art in code generation. Berlin Heidelberg: Springer; 2013. p. 11‚Äì38 . \\n[8] Balzer R . A 15 year perspective on automatic programming. Trans Softw Eng 1985;11(11):1257‚Äì68 . \\n[9] Floch A , Yuki T , Guy C , Derrien S , Combemale B , Rajopadhye S , et al. Model-driven engineering and optimizing compilers: a bridge too far?. In: Model \\nDriven Engineering Languages and Systems. In: LNCS, 6981. Springer Berlin Heidelberg; 2011. p. 608‚Äì22 . \\n[10] Stahl T , Voelter M , Czarnecki K . Model-driven software development ‚Äì technology, engineering, management. John Wiley & Sons; 2006 .',\n",
       " '[10] Stahl T , Voelter M , Czarnecki K . Model-driven software development ‚Äì technology, engineering, management. John Wiley & Sons; 2006 . \\n[11] L√∫cio L , Amrani M , Dingel J , Lambers L , Salay R , Selim GM , et al. Model transformation intents and their properties. Softw Syst Model \\n2014;15(3):685‚Äì705 . \\n[12] Czarnecki K , Helsen S . Feature-based survey of model transformation approaches. IBM Syst J 2006;45(3):621‚Äì45 . \\n[13] Gamma E , Helm R , Johnson R , Vlissides J . Design patterns: elements of reusable object-oriented software. Addison Wesley Professional; 1994 . \\n[14] Beckmann O , Houghton A , Mellor M , Kelly PH . Runtime code generation in C++ as a foundation for domain-speciÔ¨Åc optimisation. In: Domain-SpeciÔ¨Åc \\nProgram Generation. In: LNCS, 3016. Berlin Heidelberg: Springer; 2004. p. 291‚Äì306 . \\n[15] C√≥rdoba I , de Lara J . ANN: a domain-speciÔ¨Åc language for the effective design and validation of Java annotations. Comput Lang Syst Struct \\n2016;45:164‚Äì90 .',\n",
       " '[15] C√≥rdoba I , de Lara J . ANN: a domain-speciÔ¨Åc language for the effective design and validation of Java annotations. Comput Lang Syst Struct \\n2016;45:164‚Äì90 . \\n[16] Jugel U , Preu√üner A . A case study on API generation. In: System analysis and modeling: about models. In: LNCS, 6598. Springer; 2011. p. 156‚Äì72 . \\n[17] Kitchenham BA , Dyba T , Jorgensen M . Evidence-based software engineering. In: Proceedings of international conference on software engineering. \\nWashington, DC, USA: IEEE Computer Society; 2004. p. 273‚Äì81 . \\n[18] Kitchenham BA , Budgen D , Brereton OP . Using mapping studies as the basis for further research - a participant-observer case study. Inf Softw Technol \\n2011;53(6):638‚Äì51 . \\n[19] Brereton P , Kitchenham BA , Budgen D , Turner M , Khalil M . Lessons from applying the systematic literature review process within the software engi- \\nneering domain. J Syst Softw 2007;80(4):571‚Äì83 .',\n",
       " 'neering domain. J Syst Softw 2007;80(4):571‚Äì83 . \\n[20] Petersen K , Feldt R , Mujtaba S , Mattsson M . Systematic mapping studies in software engineering. In: Proceedings of the 12th international conference \\non evaluation and assessment in software engineering, EASE‚Äô08, 17. British Computer Society; 2008. p. 68‚Äì77 . \\n[21] Mehmood A , Jawawi DN . Aspect-oriented model-driven code generation: a systematic mapping study. Inf Softw Technol 2013;55(2):395‚Äì411 .',\n",
       " 'E. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n61 \\n[22] Gurunule D , Nashipudimath M . A review: analysis of aspect orientation and model driven engineering for code generation. Procedia Comput Sci \\n2015;45:852‚Äì61 . \\n[23] Dom√≠guez E , P√©rez B , Rubio AL , Zapata MA . A systematic review of code generation proposals from state machine speciÔ¨Åcations. Inf Softw Technol \\n2012;54(10):1045‚Äì66 . \\n[24] Batot E , Sahraoui H , Syriani E , Molins P , Sboui W . Systematic mapping study of model transformations for concrete problems. In: Model-driven \\nengineering and software development. IEEE; 2016. p. 176‚Äì83 . \\n[25] Rose LM , Matragkas N , Kolovos DS , Paige RF . A feature model for model-to-text transformation languages. In: Modeling in software engineering. IEEE; \\n2012. p. 57‚Äì63 . \\n[26] Kosar T , Bohra S , Mernik M . Domain-speciÔ¨Åc languages: a systematic mapping study. Inf Softw Technol 2016;71:77‚Äì91 .',\n",
       " '2012. p. 57‚Äì63 . \\n[26] Kosar T , Bohra S , Mernik M . Domain-speciÔ¨Åc languages: a systematic mapping study. Inf Softw Technol 2016;71:77‚Äì91 . \\n[27] M√©ndez-Acu√±a D , Galindo JA , Degueule T , Combemale B , Baudry B . Leveraging software product lines engineering in the development of external \\nDSLs: a systematic literature review. Comput Lang Syst Struct 2016;46:206‚Äì35 . \\n[28] Mat√∫s Sul√≠r JP . Labeling source code with metadata: a survey and taxonomy. In: Proceedings of federated conference on computer science and infor- \\nmation systems. In: Workshop on Advances in Programming Languages (WAPL‚Äô17), CFP1785N-ART. IEEE; 2017. p. 721‚Äì9 . \\n[29] Buchmann T , Schw√§gerl F . Using meta-code generation to realize higher-order model transformations. In: Proceedings of international conference on \\nsoftware technologies; 2013. p. 536‚Äì41 . \\n[30] Seriai A , Benomar O , Cerat B , Sahraoui H . Validation of software visualization tools: a systematic mapping study. In: Proceedings of IEEE working',\n",
       " 'software technologies; 2013. p. 536‚Äì41 . \\n[30] Seriai A , Benomar O , Cerat B , Sahraoui H . Validation of software visualization tools: a systematic mapping study. In: Proceedings of IEEE working \\nconference on software visualization. VISSOFT; 2014. p. 60‚Äì9 . \\n[31] Liu Q . C++ techniques for high performance Ô¨Ånancial modelling. WIT Trans Model Simul 2006;43:1‚Äì8 . \\n[32] Fang M , Ying J , Wu M . A template engineering based framework for automated software development. In: Proceeding of the 10th international con- \\nference on computer supported cooperative work in design. IEEE; 2006. p. 1‚Äì6 . \\n[33] Singh A , Schaeffer J , Green M . A template-based approach to the generation of distributed applications using a network of workstations. IEEE Trans \\nParallel Distrib Syst 1991;2(1):52‚Äì67 . \\n[34] O‚ÄôHalloran C . Automated veriÔ¨Åcation of code automatically generated from simulink ¬Æ. Autom Softw Eng 2013;20(2):237‚Äì64 .',\n",
       " 'Parallel Distrib Syst 1991;2(1):52‚Äì67 . \\n[34] O‚ÄôHalloran C . Automated veriÔ¨Åcation of code automatically generated from simulink ¬Æ. Autom Softw Eng 2013;20(2):237‚Äì64 . \\n[35] Dahman W , Grabowski J . UML-based speciÔ¨Åcation and generation of executable web services. In: System analysis and modeling. In: LNCS, 6598. \\nSpringer; 2011. p. 91‚Äì107 . \\n[36] Gessenharter D . Mapping the UML2 semantics of associations to a Java code generation model. In: Proceedings of international conference on model \\ndriven engineering languages and systems. In: LNCS, 5301. Springer; 2008. p. 813‚Äì27 . \\n[37] Valderas P , Pelechano V , Pastor O . Towards an end-user development approach for web engineering methods. In: Proceedings of international confer- \\nence on advanced information systems engineering, 4001. Springer; 2006. p. 528‚Äì43 . \\n[38] Hemel Z , Kats LC , Groenewegen DM , Visser E . Code generation by model transformation: a case study in transformation modularity. Softw Syst Model \\n2010;9(3):375‚Äì402 .',\n",
       " '[38] Hemel Z , Kats LC , Groenewegen DM , Visser E . Code generation by model transformation: a case study in transformation modularity. Softw Syst Model \\n2010;9(3):375‚Äì402 . \\n[39] Brun M , Delatour J , Trinquet Y . Code generation from AADL to a real-time operating system: an experimentation feedback on the use of model \\ntransformation. In: Engineering of complex computer systems. IEEE; 2008. p. 257‚Äì62 . \\n[40] Buckl C , Knoll A , Schrott G . Development of dependable real-time systems with Zerberus. In: Proceedings of the 11th IEEE paciÔ¨Åc rim international \\nsymposium on dependable computing; 2005. p. 404‚Äì8 . \\n[41] Li J , Xiao H , Yi D . Designing universal template for database application system based on abstract factory. In: Computer science and information \\nprocessing. IEEE; 2012. p. 1167‚Äì70 . \\n[42] Gopinath VS , Sprinkle J , Lysecky R . Modeling of data adaptable reconÔ¨Ågurable embedded systems. In: International conference and workshops on',\n",
       " 'processing. IEEE; 2012. p. 1167‚Äì70 . \\n[42] Gopinath VS , Sprinkle J , Lysecky R . Modeling of data adaptable reconÔ¨Ågurable embedded systems. In: International conference and workshops on \\nengineering of computer based systems. IEEE; 2011. p. 276‚Äì83 . \\n[43] Buckl C , Regensburger M , Knoll A , Schrott G . Models for automatic generation of safety-critical real-time systems. In: Availability, reliability and \\nsecurity. IEEE; 2007. p. 580‚Äì7 . \\n[44] Fischer T , Kollner C , Hardle M , Muller Glaser KD . Product line development for modular FPGA-based embedded systems. In: Proceedings of symposium \\non rapid system prototyping. IEEE; 2014. p. 9‚Äì15 . \\n[45] Chen K , Chang Y-C , Wang D-W . Aspect-oriented design and implementation of adaptable access control for electronic medical records. Int J Med \\nInform 2010;79(3):181‚Äì203 . \\n[46] Brox M , S√°nchez-Solano S , del Toro E , Brox P , Moreno-Velo FJ . CAD tools for hardware implementation of embedded fuzzy systems on FPGAs. IEEE',\n",
       " 'Inform 2010;79(3):181‚Äì203 . \\n[46] Brox M , S√°nchez-Solano S , del Toro E , Brox P , Moreno-Velo FJ . CAD tools for hardware implementation of embedded fuzzy systems on FPGAs. IEEE \\nTrans Ind Inform 2013;9(3):1635‚Äì44 . \\n[47] Fraternali P , Tisi M . A higher order generative framework for weaving traceability links into a code generator for web application testing. In: Proceed- \\nings of international conference on web engineering. In: LNCS, 5648. Springer; 2009. p. 340‚Äì54 . \\n[48] Vok√°Àác M , Glattetre JM . Using a domain-speciÔ¨Åc language and custom tools to model a multi-tier service-oriented application experiences and chal- \\nlenges. In: Model Driven Engineering Languages and Systems, 3713. Springer; 2005. p. 492‚Äì506 . \\n[49] Kokar M , Baclawski K , Gao H . Category theory-based synthesis of a higher-level fusion algorithm: an example. In: Proceedings of international confer- \\nence on information fusion; 2006. p. 1‚Äì8 .',\n",
       " 'ence on information fusion; 2006. p. 1‚Äì8 . \\n[50] Hoisl B , Sobernig S , Strembeck M . Higher-order rewriting of model-to-text templates for integrating domain-speciÔ¨Åc modeling languages. In: Model‚Äì\\nDriven Engineering and Software Development. SCITEPRESS; 2013. p. 49‚Äì61 . \\n[51] Ecker W , Velten M , Zafari L , Goyal A . The metamodeling approach to system level synthesis. In: Proceedings of design, automation & test in Europe \\nconference & exhibition. IEEE; 2014. p. 1‚Äì2 . \\n[52] Behrens T , Richards S . Statelator-behavioral code generation as an instance of a model transformation. In: Proceedings of international conference on \\nadvanced information systems engineering. In: LNCS, 1789. Springer; 20 0 0. p. 401‚Äì16 . \\n[53] Durand SH , Bonato V . A tool to support Bluespec SystemVerilog coding based on UML diagrams. In: Proceedings of annual conference on IEEE indus- \\ntrial electronics society. IEEE; 2012. p. 4670‚Äì5 .',\n",
       " 'trial electronics society. IEEE; 2012. p. 4670‚Äì5 . \\n[54] Schattkowsky T , Lohmann M . Rapid development of modular dynamic web sites using UML. In: Proceedings of international conference on the uniÔ¨Åed \\nmodeling language. In: LNCS, 2460. Springer; 2002. p. 336‚Äì50 . \\n[55] Buezas N , Guerra E , de Lara J , Mart√≠n J , Monforte M , Mori F , et al. Umbra designer: graphical modelling for telephony services. In: Proceedings of \\neuropean conference on modelling foundations and applications. In: LNCS, 7949. Berlin Heidelberg: Springer; 2013. p. 179‚Äì91 . \\n[56] Manley R , Gregg D . A program generator for intel AES-NI instructions. In: Proceedings of international conference on cryptology. In: LNCS, 6498. \\nSpringer; 2010. p. 311‚Äì27 . \\n[57] Phillips J , Chilukuri R , Fragoso G , Warzel D , Covitz PA . The caCORE software development kit: streamlining construction of interoperable biomedical \\ninformation services. BMC Med Inform Decis Mak 2006;6(2):1‚Äì16 .',\n",
       " 'information services. BMC Med Inform Decis Mak 2006;6(2):1‚Äì16 . \\n[58] Fu J , Bastani FB , Yen I-L . Automated AI planning and code pattern based code synthesis. In: Proceedings of international conference on tools with \\nartiÔ¨Åcial intelligence. IEEE; 2006. p. 540‚Äì6 . \\n[59] Possatto MA , Lucr√©dio D . Automatically propagating changes from reference implementations to code generation templates. Inf Softw Technol \\n2015;67:65‚Äì78 . \\n[60] Ghodrat MA , Givargis T , Nicolau A . Control Ô¨Çow optimization in loops using interval analysis. In: Proceedings of international conference on compilers, \\narchitectures and synthesis for embedded systems. ACM; 2008. p. 157‚Äì66 . \\n[61] Guduvan A-R , Waeselynck H , Wiels V , Durrieu G , Fusero Y , Schieber M . A meta-model for tests of avionics embedded systems. In: Proceedings of \\ninternational conference on model-driven engineering and software development; 2013. p. 5‚Äì13 .',\n",
       " 'international conference on model-driven engineering and software development; 2013. p. 5‚Äì13 . \\n[62] Adamko A . Modeling data-oriented web applications using UML. In: Proceedings of the international conference on computer as a tool, EUROCON \\n2005, 1. IEEE; 2005. p. 752‚Äì5 .',\n",
       " '62 \\nE. Syriani et al. / Computer Languages, Systems & Structures 52 (2018) 43‚Äì62 \\n[63] K√∂vi A , Varr√≥ D . An eclipse-based framework for AIS service conÔ¨Ågurations. In: Proceedings of the 4th international symposium on service availability, \\nISAS. In: LNCS, 4526. Springer; 2007. p. 110‚Äì26 . \\n[64] Anjorin A , Saller K , Rose S , Sch√ºrr A . A framework for bidirectional model-to-platform transformations. In: Proceedings of the 5th international con- \\nference on software language engineering, SLE 2012. In: LNCS, 7745. Berlin Heidelberg: Springer; 2013. p. 124‚Äì43 . \\n[65] Burmester S , Giese H , Sch√§fer W . Model-driven architecture for hard real-time systems: from platform independent models to code. In: Proceedings \\nof European conference on model driven architecture-foundations and applications. In: LNCS, 3748. Berlin Heidelberg: Springer; 2005. p. 25‚Äì40 .',\n",
       " 'of European conference on model driven architecture-foundations and applications. In: LNCS, 3748. Berlin Heidelberg: Springer; 2005. p. 25‚Äì40 . \\n[66] Brown AW , Conallen J , Tropeano D . Introduction: models, modeling, and model-driven architecture (MDA). In: Proceedings of international conference \\non model-driven software development. Berlin Heidelberg: Springer; 2005. p. 1‚Äì16 . \\n[67] Basu AS , Lajolo M , Prevostini M . A methodology for bridging the gap between UML and codesign. In: UML for SOC design. US: Springer; 2005. \\np. 119‚Äì46 . \\n[68] Furusawa T . Attempting to increase longevity of applications based on new SaaS/cloud technology. Fujitsu Sci Tech J 2010;46:223‚Äì8 . \\n[69] Muller P-A , Studer P , Fondement F , B√©zivin J . Platform independent web application modeling and development with Netsilon. Softw Syst Model \\n2005;4(4):424‚Äì42 .',\n",
       " '[69] Muller P-A , Studer P , Fondement F , B√©zivin J . Platform independent web application modeling and development with Netsilon. Softw Syst Model \\n2005;4(4):424‚Äì42 . \\n[70] Antkiewicz M , Czarnecki K . Framework-speciÔ¨Åc modeling languages with round-trip engineering. In: Model driven engineering languages and systems. \\nIn: LNCS, 4199. Berlin Heidelberg: Springer; 2006. p. 692‚Äì706 . \\n[71] Hinkel G , Denninger O , Krach S , Groenda H . Experiences with model-driven engineering in neurorobotics. In: Proceedings of the 12th european con- \\nference on modelling foundations and applications, ECMFA 2016. Cham: Springer International Publishing; 2016. p. 217‚Äì28 . \\n[72] Kulkarni V , Barat S , Ramteerthkar U . Early experience with agile methodology in a model-driven approach. In: Model driven engineering languages \\nand systems. In: LNCS, 6981. Springer; 2011. p. 578‚Äì90 .',\n",
       " 'and systems. In: LNCS, 6981. Springer; 2011. p. 578‚Äì90 . \\n[73] Touraille L , Traor√© MK , Hill DR . A model-driven software environment for modeling, simulation and analysis of complex systems. In: Proceedings of \\nsymposium on theory of modeling & simulation. SCSC; 2011. p. 229‚Äì37 . \\n[74] Fertalj K , Kalpic D , Mornar V . Source code generator based on a proprietary speciÔ¨Åcation language. In: Proceedings of Hawaii international conference \\non system sciences, 9. IEEE; 2002. p. 3696‚Äì704 . \\n[75] Yen I-L, Goluguri J, Bastani F, Khan L, Linn J. A component-based approach for embedded software development. In: International symposium on \\nobject-oriented real-time distributed computing. ISORC 2002. IEEE Computer Society; 2002. p. 402‚Äì10. doi: 10.1109/ISORC.2002.1003805 . \\n[76] Luhunu L , Syriani E . Comparison of the expressiveness and performance of template-based code generation tools. In: Software Language Engineering. \\nACM; 2017 .',\n",
       " '[76] Luhunu L , Syriani E . Comparison of the expressiveness and performance of template-based code generation tools. In: Software Language Engineering. \\nACM; 2017 . \\n[77] Ma M , Meissner M , Hedrich L . A case study: automatic topology synthesis for analog circuit from an ASDEX speciÔ¨Åcation. In: Synthesis, modeling, \\nanalysis and simulation methods and applications to circuit design. IEEE; 2012. p. 9‚Äì12 .']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts=[doc.page_content for doc in chunks]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfa4f8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating embeddings for 644 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:08<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape : (644, 384)\n",
      "adding 644 documents to vector store\n",
      "successfully added 644 documents to vector store\n",
      "total documents in collection : 644\n"
     ]
    }
   ],
   "source": [
    "#generate embeddings\n",
    "embeddings =embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "#store the embeddings in vector database\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e1c14",
   "metadata": {},
   "source": [
    "## Retreiever pipeline from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd0a2330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query:str, top_k: int =5, score_threshold: float=0.0) -> List[Dict[str, Any]]:\n",
    "        print(f\"retrieving documents for query:'{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score Threshold: {score_threshold}\")\n",
    "        query_embedding =self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        try:\n",
    "            results=self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            retrieved_docs=[]\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents=results['documents'][0]\n",
    "                metadatas=results['metadatas'][0]\n",
    "                distances=results['distances'][0]\n",
    "                ids=results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    similarity_score=1-distance\n",
    "\n",
    "                    if similarity_score>=score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                        'id':doc_id,\n",
    "                        'content':document,\n",
    "                        'metadata':metadata,\n",
    "                        'similarity_score':similarity_score,\n",
    "                        'distance':distance,\n",
    "                        'rank':i+1\n",
    "                    })\n",
    "\n",
    "                print(f\"retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"no documents found\")\n",
    "\n",
    "            return retrieved_docs\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"error during retrieval: {e}\")\n",
    "            return []\n",
    "    \n",
    "rag_retriever=RAGRetriever(vectorstore, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d28eb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for query:'What are the characteristics of template-based code generation?'\n",
      "Top K: 5, Score Threshold: 0.0\n",
      "generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 120.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape : (1, 384)\n",
      "retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_48aeebcf_552',\n",
       "  'content': '2. What are the characteristics of template-based code generation approaches? We want to identify major characteristics of \\nthese techniques and their tendencies. \\n3. To what extent are template-based code generation tools being used in research? We are interested in identifying popular \\ntools and their uses. \\n4. What is the place of MDE in template-based code generation? We seek to determine whether and how MDE has inÔ¨Çuenced \\nTBCG. \\n3.2. Selection of source \\nWe delimited the scope of the search to be regular publications that mention TBCG as at least one of the approaches \\nused for code generation and published between 20 0 0‚Äì2016. Therefore, this includes publications where code generation is \\nnot necessarily the main contribution. For example, Buchmann et al. [29] used TBCG to obtain ATL code while their main \\nfocus was implementing a higher-order transformation. Given that not all publications have the term ‚Äúcode generation‚Äù in',\n",
       "  'metadata': {'author': 'Eugene Syriani',\n",
       "   'content_length': 945,\n",
       "   'moddate': '2017-12-13T18:12:45+05:30',\n",
       "   'creationdate': '2017-12-13T18:12:10+05:30',\n",
       "   'modDate': \"D:20171213181245+05'30'\",\n",
       "   'producer': 'Acrobat Distiller 10.1.10 (Windows)',\n",
       "   'keywords': 'Code generation; Systematic mapping study; Model-driven engineering',\n",
       "   'title': 'Systematic mapping study of template-based code generation',\n",
       "   'file_type': 'pdf',\n",
       "   'doc_index': 552,\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf',\n",
       "   'total_pages': 20,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf',\n",
       "   'page': 4,\n",
       "   'source_file': 'Systematic mapping study of template based code generation.pdf',\n",
       "   'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003',\n",
       "   'creator': 'Elsevier',\n",
       "   'trapped': '',\n",
       "   'format': 'PDF 1.7',\n",
       "   'creationDate': \"D:20171213181210+05'30'\"},\n",
       "  'similarity_score': 0.4995032548904419,\n",
       "  'distance': 0.5004967451095581,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_ece38334_528',\n",
       "  'content': 'Automatically generating code is an approach where the same generator can be reused to produce many different artifacts \\naccording to the varying inputs it receives. It also helps detecting errors in the input artifact early on before the generated \\ncode is compiled, when the output is source code. \\nThere are many techniques to generate code, such as programmatically [3] , using a meta-object protocol [4] , or aspect- \\noriented programming [5] . Since the mid-1990‚Äôs, template-based code generation (TBCG) emerged as an approach requiring \\nless effort for the programmers to develop code generators. Templates favor reuse following the principle of write once, \\nproduce many . The concept was heavily used in web designer software (such as Dreamweaver) to generate web pages \\nand Computer Aided Software Engineering (CASE) tools to generate source code from UML diagrams. Many development \\n‚àóCorresponding author.',\n",
       "  'metadata': {'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf',\n",
       "   'doc_index': 528,\n",
       "   'creationdate': '2017-12-13T18:12:10+05:30',\n",
       "   'producer': 'Acrobat Distiller 10.1.10 (Windows)',\n",
       "   'creationDate': \"D:20171213181210+05'30'\",\n",
       "   'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003',\n",
       "   'file_type': 'pdf',\n",
       "   'page': 0,\n",
       "   'creator': 'Elsevier',\n",
       "   'format': 'PDF 1.7',\n",
       "   'moddate': '2017-12-13T18:12:45+05:30',\n",
       "   'content_length': 916,\n",
       "   'source_file': 'Systematic mapping study of template based code generation.pdf',\n",
       "   'modDate': \"D:20171213181245+05'30'\",\n",
       "   'trapped': '',\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf',\n",
       "   'keywords': 'Code generation; Systematic mapping study; Model-driven engineering',\n",
       "   'total_pages': 20,\n",
       "   'title': 'Systematic mapping study of template-based code generation',\n",
       "   'author': 'Eugene Syriani'},\n",
       "  'similarity_score': 0.4632147550582886,\n",
       "  'distance': 0.5367852449417114,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_c1692382_621',\n",
       "  'content': 'the evolution of this area, tripling the average number of publications. In addition, TBCG has favored a template style that \\nis output-based and modeling languages as input. It has been applied in a variety of domains. The community has been \\nfavoring the use of custom tools for code generation over popular ones. Most research using TBCG follows an MDE approach. \\nFurthermore, both MDE and non-MDE tools are becoming effective development resources in industry. \\nThe study also revealed that, although the research in TBCG is mature enough, there are many open issues that can \\nbe addressed in the future, upstream and downstream the generation itself. Upstream, the deÔ¨Ånition of templates is not \\na trivial task. Supporting the developers in such a deÔ¨Ånition is a must. Downstream, methods and techniques need to be \\ndeÔ¨Åned to assess the correctness and quality of the generated code.',\n",
       "  'metadata': {'total_pages': 20,\n",
       "   'page': 17,\n",
       "   'author': 'Eugene Syriani',\n",
       "   'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf',\n",
       "   'moddate': '2017-12-13T18:12:45+05:30',\n",
       "   'producer': 'Acrobat Distiller 10.1.10 (Windows)',\n",
       "   'doc_index': 621,\n",
       "   'trapped': '',\n",
       "   'keywords': 'Code generation; Systematic mapping study; Model-driven engineering',\n",
       "   'modDate': \"D:20171213181245+05'30'\",\n",
       "   'source_file': 'Systematic mapping study of template based code generation.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 888,\n",
       "   'format': 'PDF 1.7',\n",
       "   'title': 'Systematic mapping study of template-based code generation',\n",
       "   'creationdate': '2017-12-13T18:12:10+05:30',\n",
       "   'creationDate': \"D:20171213181210+05'30'\",\n",
       "   'creator': 'Elsevier',\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf'},\n",
       "  'similarity_score': 0.44581663608551025,\n",
       "  'distance': 0.5541833639144897,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_f96748da_540',\n",
       "  'content': 'a dedicated library as in [14] . \\nCode annotations are in-line descriptions that added to statement declarations (e.g., class deÔ¨Ånition) that can either be \\ninternally transformed into more expanded code (e.g., attributes in C#) or that are processed by a tool other than the \\ncompiler of the language (e.g., the speciÔ¨Åcation of documentation comments processed by Javadoc). This approach is \\nused in [15] . \\nTemplate based is described below. \\n2.3. Template-based code generation \\nThe literature agrees on a general deÔ¨Ånition of M2T code generation [12] and on templates. J√∂rges [7] identiÔ¨Åes three \\ncomponents in TBCG: the data, the template, and the output. However, there is another component that is not mentioned, \\nwhich is the meta-information the generation logic of the template relies on. Therefore, we conducted this study according \\nto the following notion of TBCG. \\nFigure 1 summarizes the main concepts of TBCG. We consider TBCG as a synthesis technique that uses templates in order',\n",
       "  'metadata': {'total_pages': 20,\n",
       "   'format': 'PDF 1.7',\n",
       "   'creationdate': '2017-12-13T18:12:10+05:30',\n",
       "   'source_file': 'Systematic mapping study of template based code generation.pdf',\n",
       "   'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf',\n",
       "   'doc_index': 540,\n",
       "   'moddate': '2017-12-13T18:12:45+05:30',\n",
       "   'page': 2,\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf',\n",
       "   'modDate': \"D:20171213181245+05'30'\",\n",
       "   'trapped': '',\n",
       "   'title': 'Systematic mapping study of template-based code generation',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 996,\n",
       "   'author': 'Eugene Syriani',\n",
       "   'creationDate': \"D:20171213181210+05'30'\",\n",
       "   'keywords': 'Code generation; Systematic mapping study; Model-driven engineering',\n",
       "   'producer': 'Acrobat Distiller 10.1.10 (Windows)',\n",
       "   'creator': 'Elsevier'},\n",
       "  'similarity_score': 0.41209256649017334,\n",
       "  'distance': 0.5879074335098267,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_33d19389_534',\n",
       "  'content': 'compilers can be regarded as tasks or services that are incorporated in or post-positioned to code generators [7] . \\nCode generation is an important model-to-text transformation that ensures the automatic transformation of a model into \\ncode. Organization are adopting the use of code generation since it reduces the development process time and increases \\nthe productivity. Generating the code using the most appropriate technique is even more crucial since it is the key to \\nbeneÔ¨Åt from all the advantages code synthesis offers to an organization. Nowadays, TBCG has raised to be the most popular \\nsynthesis technique available. Using templates can quickly become a complex task especially when the model should satisfy \\na certain condition before a template fragment is executed. \\nAs Balzer [8] states, there are many advantages to code generation. The effort of the user is reduced as he has fewer',\n",
       "  'metadata': {'creator': 'Elsevier',\n",
       "   'doc_index': 534,\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2017-12-13T18:12:10+05:30',\n",
       "   'source_file': 'Systematic mapping study of template based code generation.pdf',\n",
       "   'title': 'Systematic mapping study of template-based code generation',\n",
       "   'modDate': \"D:20171213181245+05'30'\",\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf',\n",
       "   'subject': 'Computer Languages, Systems & Structures, 52 (2017) 43-62. doi:10.1016/j.cl.2017.11.003',\n",
       "   'keywords': 'Code generation; Systematic mapping study; Model-driven engineering',\n",
       "   'author': 'Eugene Syriani',\n",
       "   'producer': 'Acrobat Distiller 10.1.10 (Windows)',\n",
       "   'page': 1,\n",
       "   'moddate': '2017-12-13T18:12:45+05:30',\n",
       "   'creationDate': \"D:20171213181210+05'30'\",\n",
       "   'source': '..\\\\data\\\\pdf\\\\Systematic mapping study of template based code generation.pdf',\n",
       "   'content_length': 901,\n",
       "   'total_pages': 20,\n",
       "   'trapped': '',\n",
       "   'format': 'PDF 1.7'},\n",
       "  'similarity_score': 0.4096826910972595,\n",
       "  'distance': 0.5903173089027405,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What are the characteristics of template-based code generation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae406fa8",
   "metadata": {},
   "source": [
    "## Integrating VectorDB context pipeline with LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "854f8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# llm=ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-2.5-flash\",\n",
    "#     google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "#     temperature=0.1,\n",
    "#     max_tokens=None,\n",
    "#     timeout=None,\n",
    "#     max_retries=2\n",
    "# )\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "#rag function - retrieve context + generate response\n",
    "def rag(query, retriever, llm, top_k=6):\n",
    "    #retrieve the context\n",
    "    results=retriever.retrieve(query, top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question\"\n",
    "    \n",
    "    #generate a response using gemini/groq\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context, query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "979a68a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for query:'what is flashfill'\n",
      "Top K: 6, Score Threshold: 0.0\n",
      "generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 107.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape : (1, 384)\n",
      "retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'FlashFill is a Programming by Example (PBE) system that can perform text manipulation operations, but has limitations in handling datetime and number operations, treating them as standard strings.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer=rag(\"what is flashfill\", rag_retriever, llm)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37198989",
   "metadata": {},
   "source": [
    "## Enchanced RAG Pipeline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69477dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for query:'What are the characteristics of template-based code generation? Explain in detail'\n",
      "Top K: 3, Score Threshold: 0.1\n",
      "generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 85.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape : (1, 384)\n",
      "retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The characteristics of template-based code generation (TBCG) can be identified as follows:\n",
      "\n",
      "1. **Components**: TBCG consists of four main components:\n",
      "   - **Data**: The input data used to generate the code.\n",
      "   - **Template**: A predefined structure that contains placeholders for the data.\n",
      "   - **Output**: The generated code, which is the result of combining the data and template.\n",
      "   - **Meta-information**: The generation logic of the template, which is not explicitly mentioned by J√∂rges but is considered a crucial component.\n",
      "\n",
      "2. **Template**: The template is a key component of TBCG, which favors reuse following the principle of \"write once, produce many\". This means that the same template can be used to generate multiple artifacts by varying the input data.\n",
      "\n",
      "3. **Synthesis technique**: TBCG is considered a synthesis technique that uses templates to generate code. This approach requires less effort from programmers to develop code generators, making it an efficient method for code generation.\n",
      "\n",
      "4. **Code generation**: TBCG is used to generate source code from various inputs, such as UML diagrams, and can be used in various domains, including web development and Computer-Aided Software Engineering (CASE) tools.\n",
      "\n",
      "5. **Error detection**: TBCG helps detect errors in the input artifact early on, before the generated code is compiled, which improves the overall quality of the generated code.\n",
      "\n",
      "6. **Reusability**: TBCG promotes reusability, as the same template can be used to generate multiple artifacts, reducing the effort required to develop code generators.\n",
      "\n",
      "7. **Flexibility**: TBCG can be used with various programming languages and can be integrated with other code generation techniques, such as programmatically generating code or using meta-object protocols.\n",
      "\n",
      "8. **Simple to use**: TBCG is considered a simple approach to code generation, as it requires less effort from programmers to develop code generators, making it an accessible method for code generation.\n",
      "\n",
      "Overall, the characteristics of TBCG make it a popular approach for code generation, as it promotes reusability, flexibility, and simplicity, while also improving the quality of the generated code.\n",
      "Sources: [{'source': 'Systematic mapping study of template based code generation.pdf', 'page': 4, 'score': 0.48134350776672363, 'preview': '2. What are the characteristics of template-based code generation approaches? We want to identify major characteristics of \\nthese techniques and their tendencies. \\n3. To what extent are template-based code generation tools being used in research? We are interested in identifying popular \\ntools and t...'}, {'source': 'Systematic mapping study of template based code generation.pdf', 'page': 0, 'score': 0.4670332074165344, 'preview': 'Automatically generating code is an approach where the same generator can be reused to produce many different artifacts \\naccording to the varying inputs it receives. It also helps detecting errors in the input artifact early on before the generated \\ncode is compiled, when the output is source code. ...'}, {'source': 'Systematic mapping study of template based code generation.pdf', 'page': 2, 'score': 0.4381480813026428, 'preview': 'a dedicated library as in [14] . \\nCode annotations are in-line descriptions that added to statement declarations (e.g., class deÔ¨Ånition) that can either be \\ninternally transformed into more expanded code (e.g., attributes in C#) or that are processed by a tool other than the \\ncompiler of the languag...'}]\n",
      "Confidence: 0.48134350776672363\n",
      "Context Preview: 2. What are the characteristics of template-based code generation approaches? We want to identify major characteristics of \n",
      "these techniques and their tendencies. \n",
      "3. To what extent are template-based code generation tools being used in research? We are interested in identifying popular \n",
      "tools and t\n"
     ]
    }
   ],
   "source": [
    "#return answer, sources, confidence score and full context\n",
    "def rag_advanced(query, retriever, llm, top_k=8, min_score=0.2, return_context=False):\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "result = rag_advanced(\"What are the characteristics of template-based code generation? Explain in detail\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b68db",
   "metadata": {},
   "source": [
    "## Advanced RAG Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bc86c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for query:'What are the characteristics of template-based code generation? Explain in detail'\n",
      "Top K: 3, Score Threshold: 0.1\n",
      "generating embeddings for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 132.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape : (1, 384)\n",
      "retrieved 3 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "2. What are the characteristics of template-based code generation approaches? We want to identify major characteristics of \n",
      "these techniques and their tendencies. \n",
      "3. To what extent are template-based code generation tools being used in research? We a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re interested in identifying popular \n",
      "tools and their uses. \n",
      "4. What is the place of MDE in template-based code generation? We seek to determine whether and how MDE has inÔ¨Çuenced \n",
      "TBCG. \n",
      "3.2. Selection of source \n",
      "We delimited the scope of the search to be regular publications that mention TBCG as at least one of the approaches \n",
      "used for code generation and published between 20 0 0‚Äì2016. Therefore, this includes publications where code generation is \n",
      "not necessarily the main contribution. For example, Buchmann et al. [29] used TBCG to obtain ATL code while their main \n",
      "focus was implementing a higher-order transformation. Given that not all publications have the term ‚Äúcode generation‚Äù in\n",
      "\n",
      "Automatically generating code is an approach where the same generator can be reused to produce many different artifacts \n",
      "according to the varying inputs it receives. It also helps detecting errors in the input artifact early on before the generated \n",
      "code is compiled, when the output is source code. \n",
      "There are many techniques to generate code, such as programmatically [3] , using a meta-object protocol [4] , or aspect- \n",
      "oriented programming [5] . Since the mid-1990‚Äôs, template-based code generation (TBCG) emerged as an approach requiring \n",
      "less effort for the programmers to develop code generators. Templates favor reuse following the principle of write once, \n",
      "produce many . The concept was heavily used in web designer software (such as Dreamweaver) to generate web pages \n",
      "and Computer Aided Software Engineering (CASE) tools to generate source code from UML diagrams. Many development \n",
      "‚àóCorresponding author.\n",
      "\n",
      "a dedicated library as in [14] . \n",
      "Code annotations are in-line descriptions that added to statement declarations (e.g., class deÔ¨Ånition) that can either be \n",
      "internally transformed into more expanded code (e.g., attributes in C#) or that are processed by a tool other than the \n",
      "compiler of the language (e.g., the speciÔ¨Åcation of documentation comments processed by Javadoc). This approach is \n",
      "used in [15] . \n",
      "Template based is described below. \n",
      "2.3. Template-based code generation \n",
      "The literature agrees on a general deÔ¨Ånition of M2T code generation [12] and on templates. J√∂rges [7] identiÔ¨Åes three \n",
      "components in TBCG: the data, the template, and the output. However, there is another component that is not mentioned, \n",
      "which is the meta-information the generation logic of the template relies on. Therefore, we conducted this study according \n",
      "to the following notion of TBCG. \n",
      "Figure 1 summarizes the main concepts of TBCG. We consider TBCG as a synthesis technique that uses templates in order\n",
      "\n",
      "Question: What are the characteristics of template-based code generation? Explain in detail\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: The characteristics of template-based code generation (TBCG) can be identified as follows:\n",
      "\n",
      "1. **Components**: TBCG consists of four main components:\n",
      "   - **Data**: This refers to the input or the information that will be used to generate the code.\n",
      "   - **Template**: A template is a pre-defined structure or pattern that contains placeholders for the data. It serves as a blueprint for generating the output code.\n",
      "   - **Output**: The generated code, which is the result of combining the template with the data.\n",
      "   - **Meta-information/Generation Logic**: This is the underlying logic or rules that govern how the template is populated with the data to produce the output code.\n",
      "\n",
      "2. **Template Reuse**: One of the key characteristics of TBCG is the principle of \"write once, produce many.\" This means that a single template can be reused multiple times with different data sets to generate a variety of output codes, promoting code reuse and reducing development effort.\n",
      "\n",
      "3. **Separation of Concerns**: TBCG allows for a clear separation between the template (which defines the structure of the output code) and the data (which provides the specific details for each instance of code generation). This separation makes it easier to maintain and update either the template or the data without affecting the other.\n",
      "\n",
      "4. **Flexibility and Customizability**: Templates can be designed to be highly flexible, allowing for the generation of code in various programming languages, formats, and styles. This flexibility is crucial for adapting to different project requirements or technological environments.\n",
      "\n",
      "5. **Error Detection**: TBCG facilitates early detection of errors in the input data or the template itself. Since the generation process occurs before the code is compiled, errors can be identified and corrected at an early stage, improving the overall quality of the generated code.\n",
      "\n",
      "6. **Efficiency**: By automating the code generation process, TBCG can significantly reduce the time and effort required for coding, especially for repetitive or boilerplate code. This efficiency gain can lead to faster development cycles and lower costs.\n",
      "\n",
      "7. **Scalability**: Template-based code generation can handle large and complex projects by generating code for multiple components or modules from a set of templates and data sources. This scalability is essential for large-scale software development projects.\n",
      "\n",
      "8. **Integration with Development Tools and Methodologies**: TBCG can be integrated with various development tools, such as CASE (Computer-Aided Software Engineering) tools, and methodologies like Model-Driven Engineering (MDE). This integration enhances the capabilities of these tools and methodologies by automating code generation based on models or specifications.\n",
      "\n",
      "In summary, template-based code generation is characterized by its components, the principle of template reuse, separation of concerns, flexibility, error detection capabilities, efficiency, scalability, and its potential for integration with other development tools and methodologies. These characteristics make TBCG a powerful approach for automating code generation in software development.\n",
      "\n",
      "Citations:\n",
      "[1] Systematic mapping study of template based code generation.pdf (page 4)\n",
      "[2] Systematic mapping study of template based code generation.pdf (page 0)\n",
      "[3] Systematic mapping study of template based code generation.pdf (page 2)\n",
      "Summary: Template-based code generation (TBCG) is characterized by several key components and principles, including template reuse, separation of concerns, flexibility, and error detection, which enable efficient and scalable code generation. The approach offers numerous benefits, such as reduced development effort, improved code quality, and faster development cycles, making it a powerful tool for automating code generation in software development.\n",
      "History: {'question': 'What are the characteristics of template-based code generation? Explain in detail', 'answer': 'The characteristics of template-based code generation (TBCG) can be identified as follows:\\n\\n1. **Components**: TBCG consists of four main components:\\n   - **Data**: This refers to the input or the information that will be used to generate the code.\\n   - **Template**: A template is a pre-defined structure or pattern that contains placeholders for the data. It serves as a blueprint for generating the output code.\\n   - **Output**: The generated code, which is the result of combining the template with the data.\\n   - **Meta-information/Generation Logic**: This is the underlying logic or rules that govern how the template is populated with the data to produce the output code.\\n\\n2. **Template Reuse**: One of the key characteristics of TBCG is the principle of \"write once, produce many.\" This means that a single template can be reused multiple times with different data sets to generate a variety of output codes, promoting code reuse and reducing development effort.\\n\\n3. **Separation of Concerns**: TBCG allows for a clear separation between the template (which defines the structure of the output code) and the data (which provides the specific details for each instance of code generation). This separation makes it easier to maintain and update either the template or the data without affecting the other.\\n\\n4. **Flexibility and Customizability**: Templates can be designed to be highly flexible, allowing for the generation of code in various programming languages, formats, and styles. This flexibility is crucial for adapting to different project requirements or technological environments.\\n\\n5. **Error Detection**: TBCG facilitates early detection of errors in the input data or the template itself. Since the generation process occurs before the code is compiled, errors can be identified and corrected at an early stage, improving the overall quality of the generated code.\\n\\n6. **Efficiency**: By automating the code generation process, TBCG can significantly reduce the time and effort required for coding, especially for repetitive or boilerplate code. This efficiency gain can lead to faster development cycles and lower costs.\\n\\n7. **Scalability**: Template-based code generation can handle large and complex projects by generating code for multiple components or modules from a set of templates and data sources. This scalability is essential for large-scale software development projects.\\n\\n8. **Integration with Development Tools and Methodologies**: TBCG can be integrated with various development tools, such as CASE (Computer-Aided Software Engineering) tools, and methodologies like Model-Driven Engineering (MDE). This integration enhances the capabilities of these tools and methodologies by automating code generation based on models or specifications.\\n\\nIn summary, template-based code generation is characterized by its components, the principle of template reuse, separation of concerns, flexibility, error detection capabilities, efficiency, scalability, and its potential for integration with other development tools and methodologies. These characteristics make TBCG a powerful approach for automating code generation in software development.', 'sources': [{'source': 'Systematic mapping study of template based code generation.pdf', 'page': 4, 'score': 0.48134350776672363, 'preview': '2. What are the characteristics of template-based code generation approaches? We want to identify major characteristics ...'}, {'source': 'Systematic mapping study of template based code generation.pdf', 'page': 0, 'score': 0.4670332074165344, 'preview': 'Automatically generating code is an approach where the same generator can be reused to produce many different artifacts ...'}, {'source': 'Systematic mapping study of template based code generation.pdf', 'page': 2, 'score': 0.4381480813026428, 'preview': 'a dedicated library as in [14] . \\nCode annotations are in-line descriptions that added to statement declarations (e.g., ...'}], 'summary': 'Template-based code generation (TBCG) is characterized by several key components and principles, including template reuse, separation of concerns, flexibility, and error detection, which enable efficient and scalable code generation. The approach offers numerous benefits, such as reduced development effort, improved code quality, and faster development cycles, making it a powerful tool for automating code generation in software development.'}\n"
     ]
    }
   ],
   "source": [
    "# Streaming, Citations, History, Summarization\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = [] \n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"What are the characteristics of template-based code generation? Explain in detail\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_with_LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
